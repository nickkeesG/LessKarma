{"results": [{"createdAt": null, "postedAt": "2013-08-28T11:48:25.571Z", "modifiedAt": null, "url": null, "title": "Open Thread: How much strategic thinking have you done recently?", "slug": "open-thread-how-much-strategic-thinking-have-you-done", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:19.041Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Emile", "createdAt": "2009-02-27T09:35:34.359Z", "isAdmin": false, "displayName": "Emile"}, "userId": "4PkX6dj649JqKSh4s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/56QS5H4puJHDQbusx/open-thread-how-much-strategic-thinking-have-you-done", "pageUrlRelative": "/posts/56QS5H4puJHDQbusx/open-thread-how-much-strategic-thinking-have-you-done", "linkUrl": "https://www.lesswrong.com/posts/56QS5H4puJHDQbusx/open-thread-how-much-strategic-thinking-have-you-done", "postedAtFormatted": "Wednesday, August 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20How%20much%20strategic%20thinking%20have%20you%20done%20recently%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20How%20much%20strategic%20thinking%20have%20you%20done%20recently%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F56QS5H4puJHDQbusx%2Fopen-thread-how-much-strategic-thinking-have-you-done%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20How%20much%20strategic%20thinking%20have%20you%20done%20recently%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F56QS5H4puJHDQbusx%2Fopen-thread-how-much-strategic-thinking-have-you-done", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F56QS5H4puJHDQbusx%2Fopen-thread-how-much-strategic-thinking-have-you-done", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 133, "htmlBody": "<p><a href=\"/r/discussion/lw/if2/open_thread_august_26_september_1_2013/9ndg\">diegocaleiro wrote</a>:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; background-color: #f7f7f8;\">I'm tired of people never, ever, ever, EVER stopping 2 hours to 1) Think of what their goals are 2)Checking if their current path leads to desired goals 3)Correcting course and 4)Creating a system to verify, in the future, whether goals are being achieved. I'm really tired of that. Really.</span></p>\n</blockquote>\n<p>... so we may want to remind and encourage each other to do so, and exchange tips!</p>\n<ul>\n<li>Have you thought about your life goals recently?</li>\n<li>Do you know what your long-term and medium-term goals <em>are</em>?</li>\n<li>If you're facing big problems or annoyances, have you thought of ways of solving them?</li>\n<li>Do you have a system you use regularly that pushes you in the right direction?</li>\n</ul>\n<p>See also: <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">Humans are not automatically strategic</a>, <a href=\"/lw/58g/levels_of_action/\">levels of action</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "56QS5H4puJHDQbusx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 1.319307630736595e-06, "legacy": true, "legacyId": "23914", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PBRWb2Em5SNeWYwwB", "guDcrPqLsnhEjrPZj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-28T15:58:00.476Z", "modifiedAt": null, "url": null, "title": "Yet more \"stupid\" questions", "slug": "yet-more-stupid-questions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:37.800Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qxHjxjGcr2PHy4iJp/yet-more-stupid-questions", "pageUrlRelative": "/posts/qxHjxjGcr2PHy4iJp/yet-more-stupid-questions", "linkUrl": "https://www.lesswrong.com/posts/qxHjxjGcr2PHy4iJp/yet-more-stupid-questions", "postedAtFormatted": "Wednesday, August 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Yet%20more%20%22stupid%22%20questions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYet%20more%20%22stupid%22%20questions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqxHjxjGcr2PHy4iJp%2Fyet-more-stupid-questions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Yet%20more%20%22stupid%22%20questions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqxHjxjGcr2PHy4iJp%2Fyet-more-stupid-questions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqxHjxjGcr2PHy4iJp%2Fyet-more-stupid-questions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 30, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is a thread where people can ask questions that they would ordinarily feel embarrassed for not knowing the answer to. The previous thread is at close to 500 comments.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qxHjxjGcr2PHy4iJp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 8, "extendedScore": null, "score": 1.3195217238017764e-06, "legacy": true, "legacyId": "23915", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 342, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-28T19:29:17.855Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 27, chapter 98 ", "slug": "harry-potter-and-the-methods-of-rationality-discussion-42", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:31.250Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tbQZxoPfFdB3ZJz3c/harry-potter-and-the-methods-of-rationality-discussion-42", "pageUrlRelative": "/posts/tbQZxoPfFdB3ZJz3c/harry-potter-and-the-methods-of-rationality-discussion-42", "linkUrl": "https://www.lesswrong.com/posts/tbQZxoPfFdB3ZJz3c/harry-potter-and-the-methods-of-rationality-discussion-42", "postedAtFormatted": "Wednesday, August 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2027%2C%20chapter%2098%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2027%2C%20chapter%2098%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtbQZxoPfFdB3ZJz3c%2Fharry-potter-and-the-methods-of-rationality-discussion-42%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2027%2C%20chapter%2098%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtbQZxoPfFdB3ZJz3c%2Fharry-potter-and-the-methods-of-rationality-discussion-42", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtbQZxoPfFdB3ZJz3c%2Fharry-potter-and-the-methods-of-rationality-discussion-42", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 232, "htmlBody": "<div id=\"entry_t3_ibr\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is a new thread to discuss Eliezer Yudkowsky&rsquo;s&nbsp;<em><a style=\"color: #8a8a8b;\" href=\"http://www.fanfiction.net/s/5782108/1/\">Harry Potter and the Methods of Rationality</a></em>&nbsp;and anything related to it. This thread is intended for discussing&nbsp;<a href=\"http://hpmor.com/chapter/98\"><span style=\"color: #8a8a8b;\">chapter 98</span></a>.&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/ibr/harry_potter_and_the_methods_of_rationality/\">The previous thread&nbsp;</a>is at nearly 500 comments.&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">There is now a site dedicated to the story at&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/\">hpmor.com</a>, which is now the place to go to find the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/notes/\">authors notes</a>&nbsp;and all sorts of other goodies. AdeleneDawner has kept an&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">archive of Author&rsquo;s Notes</a>.  (This goes up to the notes for chapter 76, and is now not updating. The  authors notes from chapter 77 onwards are on hpmor.com.)&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">The first 5 discussion threads are on the main page under the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/tag/harry_potter/\">harry_potter tag</a>.&nbsp; Threads 6 and on (including this one) are in the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/tag/harry_potter/\">discussion section</a>&nbsp;using its separate tag system.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Also:&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/2ab/harry_potter_and_the_methods_of_rationality\">1</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/2ie/harry_potter_and_the_methods_of_rationality\">2</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/2nm/harry_potter_and_the_methods_of_rationality\">3</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/2tr/harry_potter_and_the_methods_of_rationality\">4</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/30g/harry_potter_and_the_methods_of_rationality\">5</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality/\">6</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/3rb/harry_potter_and_the_methods_of_rationality/\">7</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/797/harry_potter_and_the_methods_of_rationality/\">8</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/7jd/harry_potter_and_the_methods_of_rationality\">9</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/ams/harry_potter_and_the_methods_of_rationality\">10</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/axe/harry_potter_and_the_methods_of_rationality/\">11</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/b5s/harry_potter_and_the_methods_of_rationality/\">12</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/b7s/harry_potter_and_the_methods_of_rationality/\">13</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/bfo/harry_potter_and_the_methods_of_rationality/\">14</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/bmx/harry_potter_and_the_methods_of_rationality/\">15</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/bto/harry_potter_and_the_methods_of_rationality/\">16</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/fyv/harry_potter_and_the_methods_of_rationality/\">17</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/g1q/harry_potter_and_the_methods_of_rationality/\">18</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/huq/harry_potter_and_the_methods_of_rationality/\">19</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/hvg/harry_potter_and_the_methods_of_rationality/\">20</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/hwf/harry_potter_and_the_methods_of_rationality/\">21</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/hws/harry_potter_and_the_methods_of_rationality/\">22</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/hxg/harry_potter_and_the_methods_of_rationality/\">23</a>,&nbsp;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/r/discussion/lw/i19/harry_potter_and_the_methods_of_rationality/\">24</a>, &nbsp;<a href=\"http://lesswrong.com/r/discussion/lw/i4r/harry_potter_and_the_methods_of_rationality/\">25</a>, <a href=\"/r/discussion/lw/ibr/harry_potter_and_the_methods_of_rationality/\">26</a>.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong>Spoiler Warning</strong>:  this thread is full of spoilers. With few exceptions, spoilers for MOR  and canon are fair game to post, without warning or rot13.&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://lesswrong.com/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">More specifically</a>:</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<p style=\"margin: 0px 0px 1em;\">You do not need to rot13 anything about  HP:MoR or the original Harry Potter series unless you are posting  insider information from Eliezer Yudkowsky which is not supposed to be  publicly available (which includes public statements by Eliezer that  have been retracted).</p>\n<p style=\"margin: 0px 0px 1em;\">If there is evidence for X in MOR and/or  canon then it&rsquo;s fine to post about X without rot13, even if you also  have heard privately from Eliezer that X is true. But you should not  post that &ldquo;Eliezer said X is true&rdquo; unless you use rot13.</p>\n</blockquote>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tbQZxoPfFdB3ZJz3c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.3197030176545107e-06, "legacy": true, "legacyId": "23916", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 310, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4hKoG4e248yuXtG47", "59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD", "y2Hszb4Dsm5FggnDC", "6ae2kq3JmKvL4YPgk", "zvXfBqp6TSriNkmbg", "WQ7XMjqvuRRj8nkpu", "LKFR5pBA3bBkERDxL", "8yEdpDpGgvDWHeodM", "K4JBpAxhvstdnNbeg", "xK6Pswbozev6pv4A6", "pBTcCB5uJTzADdMm4", "XN4WDRSPFo9iGEuk3", "QkhX5YeuYHzPW7Waz", "4sY9rqAqty8rHWGSW", "35GjH7tDvNJWSHQ3H", "Pxiu5SG8gjhCh2jYd", "CEd85FLRbQWsbkrmf", "CcnpbKuRaYMjpFmQq", "smKK6yrKBehxvQq5i", "bMxxf7Wtic298LcNx", "uBpSaxteqitApiJJs", "Ey8yGkFnT7Gcgnt5r"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-28T20:25:29.857Z", "modifiedAt": null, "url": null, "title": "[LINK] - Aaron Sell (Psychology Today) on the Politicisation of Science", "slug": "link-aaron-sell-psychology-today-on-the-politicisation-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:33.738Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Salemicus", "createdAt": "2012-05-10T20:50:25.455Z", "isAdmin": false, "displayName": "Salemicus"}, "userId": "D8cdrPXwhhiPdqkSz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PRZyZuCcA2QStndRm/link-aaron-sell-psychology-today-on-the-politicisation-of", "pageUrlRelative": "/posts/PRZyZuCcA2QStndRm/link-aaron-sell-psychology-today-on-the-politicisation-of", "linkUrl": "https://www.lesswrong.com/posts/PRZyZuCcA2QStndRm/link-aaron-sell-psychology-today-on-the-politicisation-of", "postedAtFormatted": "Wednesday, August 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20-%20Aaron%20Sell%20(Psychology%20Today)%20on%20the%20Politicisation%20of%20Science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20-%20Aaron%20Sell%20(Psychology%20Today)%20on%20the%20Politicisation%20of%20Science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPRZyZuCcA2QStndRm%2Flink-aaron-sell-psychology-today-on-the-politicisation-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20-%20Aaron%20Sell%20(Psychology%20Today)%20on%20the%20Politicisation%20of%20Science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPRZyZuCcA2QStndRm%2Flink-aaron-sell-psychology-today-on-the-politicisation-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPRZyZuCcA2QStndRm%2Flink-aaron-sell-psychology-today-on-the-politicisation-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 394, "htmlBody": "<p>Dr. Aaron Sell recently wrote an interesting piece about political incentives and shoddy statistical work in science. In particular, he was highly critical of the much-publicized Conley&nbsp;<a href=\"http://www.ncbi.nlm.nih.gov/pubmed/21171789\">article</a>&nbsp;in the Journal of Personality and Social Psychology which attempted to demonstrate that \"large gender differences... [in willingness to engage in] casual sex [have]&nbsp;more to do with perceived personality characteristics of the female versus male proposers than with gender differences.\"</p>\n<p>[[The full article can be found&nbsp;<a href=\"http://www.psychologytoday.com/blog/evolutionary-entertainment/201308/is-why-conservatives-don-t-believe-in-global-warming\">here</a>.&nbsp;Please note - I have deliberately chosen to omit quoting any material which Less Wrongers may find mindkilling. It would be nice if we could keep it that way.]]</p>\n<p>He points out that Conley's results were obtained by hypothetical pseudo-experiments, and spurious controls. For example, when Conley found that men were more willing to sleep with their friends than women, she made this difference \"evaporate\" by:</p>\n<blockquote>\n<p>control[ling] for &ldquo;sexual capabilities.&rdquo; The sexual capabilities covariate contained two items: &ldquo;the proposer would be a great lover&rdquo; and &ldquo;would provide you with a positive sexual experience.&rdquo; Keep in mind, these items were rated by the subjects just&nbsp;<em>after</em>&nbsp;deciding how likely they would be to have a sexual encounter with the person! If you control for two tightly correlated variables any effect disappears. The gap in male and female pay completely disappears when you control for testicle number... So how tightly correlated is &ldquo;likelihood of agreeing to sex&rdquo; with &ldquo;would provide a positive sexual experience&rdquo;? We don&rsquo;t know; it&rsquo;s not reported.&nbsp;<em>JPSP</em>. Seriously.</p>\n</blockquote>\n<p>Yet for&nbsp;<a href=\"/lw/js/the_bottom_line/\">bottom line</a>&nbsp;reasons, we now find&nbsp;<a href=\"http://dornsife.usc.edu/assets/sites/545/docs/Wendy_Wood_Research_Articles/Gender_Differences_in_Social_Behavior/eagly.wood.2013.nature-nurture_debates.pdf\">Eagly and Woods</a>&nbsp;reading Conley uncritically, and even claiming that she has overturned&nbsp;<a href=\"http://www.elainehatfield.com/79.pdf\">Clark and Hatfield's</a>&nbsp;classic (and truly experimentally based) article demonstrating that women are radically less willing than men to have casual sex.</p>\n<p>Sell concludes:</p>\n<blockquote>\n<p>This sort of nonsense is an indictment of our science...&nbsp;As long as the most prestigious journals of our discipline publish this kind of political masturbation&nbsp;, we have no right to demand that the public take us seriously. When politics and good science collide there is no reason the public should bet on science. They are better off trusting their uninformed intuitions. An imbecile... know[s] damn well that 80% of women don&rsquo;t want casual sex with random men who are &ldquo;reputed to be sexually skilled.&rdquo;</p>\n</blockquote>\n<p>While empiricism is great, I have long believed that the social and organisational structures in which science is practised makes it especially vulnerable to political capture, so this plays right into my biases. Am I missing something important?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PRZyZuCcA2QStndRm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "23917", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["34XxbRFe54FycoCDw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-28T23:27:23.372Z", "modifiedAt": null, "url": null, "title": "Outside View(s) and MIRI's FAI Endgame", "slug": "outside-view-s-and-miri-s-fai-endgame", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:06.936Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WdibyPFqYkCkLGxCq/outside-view-s-and-miri-s-fai-endgame", "pageUrlRelative": "/posts/WdibyPFqYkCkLGxCq/outside-view-s-and-miri-s-fai-endgame", "linkUrl": "https://www.lesswrong.com/posts/WdibyPFqYkCkLGxCq/outside-view-s-and-miri-s-fai-endgame", "postedAtFormatted": "Wednesday, August 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Outside%20View(s)%20and%20MIRI's%20FAI%20Endgame&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOutside%20View(s)%20and%20MIRI's%20FAI%20Endgame%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWdibyPFqYkCkLGxCq%2Foutside-view-s-and-miri-s-fai-endgame%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Outside%20View(s)%20and%20MIRI's%20FAI%20Endgame%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWdibyPFqYkCkLGxCq%2Foutside-view-s-and-miri-s-fai-endgame", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWdibyPFqYkCkLGxCq%2Foutside-view-s-and-miri-s-fai-endgame", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 518, "htmlBody": "<p>On the subject of how an FAI team can avoid accidentally creating a UFAI, Carl Shulman <a href=\"/lw/8c3/qa_with_new_executive_director_of_singularity/596l\">wrote</a>:</p>\n<blockquote>\n<p>If we condition on having all other variables optimized, I'd expect a team to adopt very high standards of proof, and recognize limits to its own capabilities, biases, etc. One of the primary purposes of organizing a small FAI team is to create a team that can actually stop and abandon a line of research/design (Eliezer calls this \"halt, melt, and catch fire\") that cannot be shown to be safe (given limited human ability, incentives and bias).</p>\n</blockquote>\n<p>In the history of philosophy, there have been many steps in the right direction, but virtually no significant problems have been fully solved, such that philosophers can agree that some proposed idea can be the last words on a given subject. An FAI design involves making many explicit or implicit philosophical assumptions, many of which may then become fixed forever as governing principles for a new reality. They'll end up being last words on their subjects, whether we like it or not. Given the history of philosophy and applying the outside view, how can an FAI team possibly reach \"very high standards of proof\" regarding the safety of a design? But if we can foresee that they <em>can't</em>, then what is the point of aiming for that predictable outcome now?</p>\n<p>Until recently I haven't paid a lot of attention to the discussions here about inside view vs outside view, because the discussions have tended to focus on the applicability of these views to the problem of predicting intelligence explosion. It seemed obvious to me that outside views can't possibly rule out intelligence explosion scenarios, and even a small probability of a future intelligence explosion would justify a much higher than current level of investment in preparing for that possibility. But given that the inside vs outside view debate may also be relevant to the \"FAI Endgame\", I read up on Eliezer and Luke's most recent writings on the subject... and found them to be unobjectionable. Here's <a href=\"/lw/vz/the_weak_inside_view/\">Eliezer</a>:</p>\n<blockquote>\n<p>On problems that are drawn from a barrel of causally similar problems, where human optimism runs rampant and unforeseen troubles are common, the <a href=\"/lw/jg/planning_fallacy/\">Outside View beats the Inside View</a>.&nbsp;</p>\n</blockquote>\n<p>Does anyone want to argue that Eliezer's criteria for using the outside view are wrong, or don't apply here?</p>\n<p>And <a href=\"/lw/hzu/model_combination_and_adjustment/\">Luke</a>:</p>\n<blockquote>\n<p>One obvious solution is to use <em>multiple</em> reference classes, and weight them by how relevant you think they are to the phenomenon you're trying to predict.</p>\n<p>[...]</p>\n<p>Once you've combined a handful of models to arrive at a qualitative or quantitative judgment, you should still be able to \"adjust\" the judgment in some cases using an inside view.</p>\n</blockquote>\n<p>These ideas seem harder to apply, so I'll ask for readers' help. What reference classes should we use here, in addition to past attempts to solve philosophical problems? What inside view adjustments could a future FAI team make, such that they might justifiably overcome (the most obvious-to-me) outside view's conclusion that they're very unlikely to be in the possession of complete and fully correct solutions to a diverse range of philosophical problems?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 4, "rWzGNdjuep56W5u2d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WdibyPFqYkCkLGxCq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 21, "extendedScore": null, "score": 1.3199073616154958e-06, "legacy": true, "legacyId": "23913", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 60, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["w9KWNWFTXivjJ7rjF", "CPm5LTwHrvBJCa9h5", "iyRpsScBa6y4rduEt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-29T01:59:51.042Z", "modifiedAt": null, "url": null, "title": "Meetup : Fall Equinox: Festival of Heroes", "slug": "meetup-fall-equinox-festival-of-heroes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:08.957Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hamnox", "createdAt": "2011-01-17T01:16:28.722Z", "isAdmin": false, "displayName": "hamnox"}, "userId": "EY9o6qtXvYhS5CTHD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wpz7wGGGTp3PfTwzG/meetup-fall-equinox-festival-of-heroes", "pageUrlRelative": "/posts/wpz7wGGGTp3PfTwzG/meetup-fall-equinox-festival-of-heroes", "linkUrl": "https://www.lesswrong.com/posts/wpz7wGGGTp3PfTwzG/meetup-fall-equinox-festival-of-heroes", "postedAtFormatted": "Thursday, August 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fall%20Equinox%3A%20Festival%20of%20Heroes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fall%20Equinox%3A%20Festival%20of%20Heroes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwpz7wGGGTp3PfTwzG%2Fmeetup-fall-equinox-festival-of-heroes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fall%20Equinox%3A%20Festival%20of%20Heroes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwpz7wGGGTp3PfTwzG%2Fmeetup-fall-equinox-festival-of-heroes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwpz7wGGGTp3PfTwzG%2Fmeetup-fall-equinox-festival-of-heroes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 144, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qe'>Fall Equinox: Festival of Heroes</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 September 2013 03:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Sugar Lily Pavillion, Sugar House Park 1400 East 2100 South, Salt Lake City, Utah</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This event is about becoming agenty.\nAny LW concepts and exercises will be heavily simplified as it is meant to include friends and family, as well as to be a celebration of values to reinforce rather than be a study in hard-core instrumental rationality.</p>\n\n<p>3:00 pm Tournaments and Games</p>\n\n<p>4:00 pm A Toast to Heroes, and The Three Prescripts: (initiative, priorities, effectiveness)</p>\n\n<p>6:00 pm Dinner and Talk</p>\n\n<p>7:00 pm Hero\u2019s Challenge</p>\n\n<p>10:00 pm After Party, if there is critical mass.</p>\n\n<p>Please RSVP to mheisey.nox@gmail.com if you can as I'm aiming to have a personalized element. Not a big deal if you don't, but it would up the coolness factor.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qe'>Fall Equinox: Festival of Heroes</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wpz7wGGGTp3PfTwzG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3200382418166554e-06, "legacy": true, "legacyId": "23918", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fall_Equinox__Festival_of_Heroes\">Discussion article for the meetup : <a href=\"/meetups/qe\">Fall Equinox: Festival of Heroes</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 September 2013 03:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Sugar Lily Pavillion, Sugar House Park 1400 East 2100 South, Salt Lake City, Utah</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This event is about becoming agenty.\nAny LW concepts and exercises will be heavily simplified as it is meant to include friends and family, as well as to be a celebration of values to reinforce rather than be a study in hard-core instrumental rationality.</p>\n\n<p>3:00 pm Tournaments and Games</p>\n\n<p>4:00 pm A Toast to Heroes, and The Three Prescripts: (initiative, priorities, effectiveness)</p>\n\n<p>6:00 pm Dinner and Talk</p>\n\n<p>7:00 pm Hero\u2019s Challenge</p>\n\n<p>10:00 pm After Party, if there is critical mass.</p>\n\n<p>Please RSVP to mheisey.nox@gmail.com if you can as I'm aiming to have a personalized element. Not a big deal if you don't, but it would up the coolness factor.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fall_Equinox__Festival_of_Heroes1\">Discussion article for the meetup : <a href=\"/meetups/qe\">Fall Equinox: Festival of Heroes</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fall Equinox: Festival of Heroes", "anchor": "Discussion_article_for_the_meetup___Fall_Equinox__Festival_of_Heroes", "level": 1}, {"title": "Discussion article for the meetup : Fall Equinox: Festival of Heroes", "anchor": "Discussion_article_for_the_meetup___Fall_Equinox__Festival_of_Heroes1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-29T07:32:55.202Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow: The Sunday Meetup", "slug": "meetup-moscow-the-sunday-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Xvo4fbS96xaom7ZrK/meetup-moscow-the-sunday-meetup-0", "pageUrlRelative": "/posts/Xvo4fbS96xaom7ZrK/meetup-moscow-the-sunday-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/Xvo4fbS96xaom7ZrK/meetup-moscow-the-sunday-meetup-0", "postedAtFormatted": "Thursday, August 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%3A%20The%20Sunday%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%3A%20The%20Sunday%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXvo4fbS96xaom7ZrK%2Fmeetup-moscow-the-sunday-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%3A%20The%20Sunday%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXvo4fbS96xaom7ZrK%2Fmeetup-moscow-the-sunday-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXvo4fbS96xaom7ZrK%2Fmeetup-moscow-the-sunday-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qf'>Moscow: The Sunday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 September 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door. We will meet you at 16:00 with \u201cLW\u201d sign inside the hall or just look for group of geek-looking people.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li>Discussion about motivation.</li>\n<li>Discussion about cognitive biases.</li>\n<li>Game session.</li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qf'>Moscow: The Sunday Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Xvo4fbS96xaom7ZrK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3203242447256766e-06, "legacy": true, "legacyId": "23927", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__The_Sunday_Meetup\">Discussion article for the meetup : <a href=\"/meetups/qf\">Moscow: The Sunday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 September 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door. We will meet you at 16:00 with \u201cLW\u201d sign inside the hall or just look for group of geek-looking people.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li>Discussion about motivation.</li>\n<li>Discussion about cognitive biases.</li>\n<li>Game session.</li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__The_Sunday_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/qf\">Moscow: The Sunday Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow: The Sunday Meetup", "anchor": "Discussion_article_for_the_meetup___Moscow__The_Sunday_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Moscow: The Sunday Meetup", "anchor": "Discussion_article_for_the_meetup___Moscow__The_Sunday_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-29T08:53:55.914Z", "modifiedAt": null, "url": null, "title": "A basis for pattern-matching in logical uncertainty", "slug": "a-basis-for-pattern-matching-in-logical-uncertainty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:00.020Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/trjysoG4g7X79mkmM/a-basis-for-pattern-matching-in-logical-uncertainty", "pageUrlRelative": "/posts/trjysoG4g7X79mkmM/a-basis-for-pattern-matching-in-logical-uncertainty", "linkUrl": "https://www.lesswrong.com/posts/trjysoG4g7X79mkmM/a-basis-for-pattern-matching-in-logical-uncertainty", "postedAtFormatted": "Thursday, August 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20basis%20for%20pattern-matching%20in%20logical%20uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20basis%20for%20pattern-matching%20in%20logical%20uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtrjysoG4g7X79mkmM%2Fa-basis-for-pattern-matching-in-logical-uncertainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20basis%20for%20pattern-matching%20in%20logical%20uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtrjysoG4g7X79mkmM%2Fa-basis-for-pattern-matching-in-logical-uncertainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtrjysoG4g7X79mkmM%2Fa-basis-for-pattern-matching-in-logical-uncertainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 788, "htmlBody": "<p>Previous logical uncertainty robot designs (e.g. <a href=\"/lw/iax/some_miri_workshop_stuff/\">here</a>, <a href=\"/lw/eaa/a_model_of_udt_with_a_concrete_prior_over_logical/\">here</a>, <a href=\"/lw/eaa/a_model_of_udt_with_a_concrete_prior_over_logical/\">here</a>) have relied on proving theorems that relate to the statement (e.g. \"the trillionth digit of pi is 5\") under inspection (where a useful theorem would be e.g. \"there are 10 possible mutually exclusive digits [1,2,3,4,5,6,7,8,9,0] the trillionth digit of pi could be.\"). This is nice. &nbsp;But it doesn't do pattern-matching - if you tell a theorem-proving robot that a statement is true for the first thousand integers, it doesn't even suspect that the statement might be true for the 1001st too. The statements are just independent black boxes.</p>\n<p>In order to go further, our robot needs to peek inside the black box. But how, and why should it see patterns?</p>\n<p>&nbsp;</p>\n<p>We tell our robot these facts: \"3 is 'odd'. 5 is 'odd'. 7 is 'odd'. 11 is 'odd'.\"</p>\n<p>The robot asks \"Could you just tell me what this concept 'odd' means explicitly? &nbsp;I don't know English very well.\"</p>\n<p>\"No,\" we say. \"You'll have to guess.\"</p>\n<p>&nbsp;</p>\n<p>For robots that don't make up information out of thin air, guesses about 'odd'-ness will obey the maximum-entropy principle, which roughly means \"give everything equal probability until you learn more.\"</p>\n<p>&nbsp;</p>\n<p>\"Well, robot, what do you think is the probability that 9 is 'odd', given what we've told you?\"</p>\n<p>\"50 percent. &nbsp;Beep boop.\"</p>\n<p>\"But all those other things were odd, weren't they?\"</p>\n<p>\"Those other things were not the number 9. Imagine going number by number and labeling each number as 'odd' or 'not odd'. There are equal numbers of possible labelings with 9 odd and 9 not odd, regardless of what those other numbers are. Since by the principle of maximum entropy I start out with all labelings equally likely, 9 has a 50% chance of being 'odd'.\"</p>\n<p>\"But isn't it just sensible to think that the next number we give you will be 'odd' if all the others were? What of the appeal of simplicity?\"</p>\n<p>\"What's so great about simplicity? I'm trying not to make up information out of thin air here.\"</p>\n<p>&nbsp;</p>\n<p>What do we tell our robot to make it guess that 9 is probably odd? Well, we want it to think that patterns of odd-ness that are simpler are more likely. So how about we let the robot peek at our definition of 'odd', just long enough to see that for every integer we can test if it's odd, and how complicated the test is.</p>\n<p>Even if our robot has no clue what the constituent symbols <em>were</em>&nbsp;(or, more to the point, not enough processing power to fully explore the ramifications in a more difficult and realistic scenario), knowing the mere number of characters defining 'odd' puts an upper bound on the Kolmogorov complexity of how the numbers are labeled. Heck, even just learning that we can write down the oddness-test is huge, since there are an infinite number of infinite-complexity rules.</p>\n<p>Once the robot knows that the complexity of 'odd' is below a certain smallish value, low-complexity hypotheses like \"all numbers are 'odd'\" and \"odd numbers are 'odd'\" start to outweigh<sup>1</sup> bigger hypotheses like \"all numbers except {long list including 9} are 'odd'.\" &nbsp;These simple hypotheses contain just the sorts of patterns we sometimes want the robot to see, like 'odd'-ness.</p>\n<p>&nbsp;</p>\n<p>We tell our robot these facts: \"3 is odd. 5 is odd. 7 is odd. 11 is odd. A number is 'odd' if a 14-character predicate is true.\"</p>\n<p>The robot asks \"Could you just tell me what this concept 'odd' means explicitly?\"</p>\n<p>\"No,\" we say. \"You'll have to guess. What do you think is the probability that 9 is 'odd', given what we've told you?\"</p>\n<p>\"65.1 percent.\"</p>\n<p>\"Hah, got you! When we said 'odd' we secretly meant prime!\"</p>\n<p>&nbsp;</p>\n<p>I suspect that this kind of peeking handles most of the cases where we want something like pattern-matching (specifically, minimum-message-length prediction of patterns) in logical uncertainty. The obvious un-handled case - the choice of axioms, or <a href=\"/lw/g7n/secondorder_logic_the_controversy/\">logical systems</a>, or that sort of thing, seems more like model uncertainty than logical uncertainty - that is, the question is not really \"is second-order logic true,\" it's \"does second-order logic correspond to the world I want to model,\" which is beyond the scope of this Discussion article.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><span style=\"font-size: xx-small;\"><sup>1 </sup>How do you turn the number of symbols we wrote it down with into a distribution over possible rules? &nbsp;It's easiest to just maximum-entropy all valid rules with the correct number of symbols. Since many rules can be compressed, the set of rules with some number of symbols is smeared over lower possible Kolmogorov complexities, I think with an exponential preference towards lower complexity. But on the other hand, humans don't hand out maximum-entropy samples of definitions - we'll need probabilistic logic to do probabilistic logic. (Yo dawg, I heard you liked recursion, so we put this joke in itself so you can [this joke] while you [this joke].)</span>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "trjysoG4g7X79mkmM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 1.3203938252731838e-06, "legacy": true, "legacyId": "23930", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fW2nt6yNZAHvLiYsq", "PgKADaJE4ERjtMtP9", "SWn4rqdycu83ikfBa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-30T00:01:14.770Z", "modifiedAt": null, "url": null, "title": "Techniques to consciously activate a rationalist self-image", "slug": "techniques-to-consciously-activate-a-rationalist-self-image", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:02.763Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chaosmage", "createdAt": "2012-04-27T12:21:32.969Z", "isAdmin": false, "displayName": "chaosmage"}, "userId": "onF6sJLEXsAkjx9Ki", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pDNREc8bYSt65uuGq/techniques-to-consciously-activate-a-rationalist-self-image", "pageUrlRelative": "/posts/pDNREc8bYSt65uuGq/techniques-to-consciously-activate-a-rationalist-self-image", "linkUrl": "https://www.lesswrong.com/posts/pDNREc8bYSt65uuGq/techniques-to-consciously-activate-a-rationalist-self-image", "postedAtFormatted": "Friday, August 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Techniques%20to%20consciously%20activate%20a%20rationalist%20self-image&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATechniques%20to%20consciously%20activate%20a%20rationalist%20self-image%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpDNREc8bYSt65uuGq%2Ftechniques-to-consciously-activate-a-rationalist-self-image%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Techniques%20to%20consciously%20activate%20a%20rationalist%20self-image%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpDNREc8bYSt65uuGq%2Ftechniques-to-consciously-activate-a-rationalist-self-image", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpDNREc8bYSt65uuGq%2Ftechniques-to-consciously-activate-a-rationalist-self-image", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 200, "htmlBody": "<p>When I do mindless or routine tasks for a while, activities I don't need much conscious thought for, I go back to older and simpler cognitions. It takes me a little effort to, for example, notice I'm confused, and then remember the methods of rationality. That takes me a second or so, during which some attention goes inward, to flash images associated with rationality. So to me, rational, purposeful thought distinctly feels like a faculty that needs to be (re)activated to be used.</p>\n<p>That's my own experience. Does anyone experience this similarly?</p>\n<p>So I played with this activation. First I started noticing it and feeling it in detail. I don't know how much my (non-trivial) mindfulness meditation training helped with this. I related to the activation impulse as another part of my body, as if I had an extra finger and was learning how to twitch it. I found a short trigger phrase for me to associate with this activation and taught my brain the connection by simply doing the activation at the same time as I was saying the trigger phrase.</p>\n<p>Do any of you guys do something like that? Some image, phrase or visualization that helps you remember to be rational?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pDNREc8bYSt65uuGq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 1.3211735554535936e-06, "legacy": true, "legacyId": "23932", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-30T02:28:15.171Z", "modifiedAt": null, "url": null, "title": "Why Productivity? Why Gratitude?", "slug": "why-productivity-why-gratitude", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:03.023Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "feanor1600", "createdAt": "2009-07-22T18:12:08.907Z", "isAdmin": false, "displayName": "feanor1600"}, "userId": "zwyrE6j2q36cXEuAH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TtXhPXoYxwj2dHC4k/why-productivity-why-gratitude", "pageUrlRelative": "/posts/TtXhPXoYxwj2dHC4k/why-productivity-why-gratitude", "linkUrl": "https://www.lesswrong.com/posts/TtXhPXoYxwj2dHC4k/why-productivity-why-gratitude", "postedAtFormatted": "Friday, August 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20Productivity%3F%20Why%20Gratitude%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20Productivity%3F%20Why%20Gratitude%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTtXhPXoYxwj2dHC4k%2Fwhy-productivity-why-gratitude%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20Productivity%3F%20Why%20Gratitude%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTtXhPXoYxwj2dHC4k%2Fwhy-productivity-why-gratitude", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTtXhPXoYxwj2dHC4k%2Fwhy-productivity-why-gratitude", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 362, "htmlBody": "<p>I recently did a series of online seminars where productivity guru <a href=\"http://womackcompany.com/\">Jason Womack</a> tried to apply his advice for academics.</p>\n<p>The productivity advice was good but not especially new after having read a lot of anti-akrasia posts on LW; EverydayUtilitarian recently wrote a great summary of these kind of ideas <a href=\"http://www.everydayutilitarian.com/essays/how-i-am-productive/\">here</a>. I suppose the fact that the advice wasn't new to me means LW has been doing a good job of bringing in good instrumental rationality advice from elsewhere.</p>\n<p>But the most interesting parts of the seminars weren't actually ways to be more productive.</p>\n<p>One is a question: why do you want to be more productive?</p>\n<p>After asking themselves this, some people might realize that they don't actually need to be more productive. Why get more things done? If you work as a certain kind of corporate drone, becoming more productive might not make you or anyone else much better off. Perhaps you are rearranging the deck chairs on the titanic, becoming better at a job or project when you should be doing something else entirely. If your goal in work is to make you and your family better off, then it might be counterproductive to employ strategies that make you less happy or take you away from your family.</p>\n<p>Alternatively, if you realize you have all kinds of really great reasons to be more productive, this should encourage you.</p>\n<p>The other big non-exactly-productivity idea I learned about was gratitude.</p>\n<p>The most obvious reason to send people thank-you notes is that it will make them happy, and is just the right thing to do. Another good reason you have probably heard of is that it will make you happier, like keeping a <a href=\"/lw/i0c/for_happiness_keep_a_gratitude_journal/\">gratitude journal</a>.</p>\n<p>What I didn't realize before are the tangible benefits of sending thank-you notes. Jason Womack says he tries you send one a day, and has had many people respond by offering to do some project with him. I recently started sending out more thank-you emails to people who have helped improve my work, and have already had someone respond by offering a large and totally unexpected benefit (a letter of recommendation). It seems like sending out more thank you notes is just an all around win.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"a65Lgr7Q5jqRWHtM6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TtXhPXoYxwj2dHC4k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 1.3212999671189829e-06, "legacy": true, "legacyId": "23933", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xYnnRmMmGRAwZoY25"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-30T12:43:30.192Z", "modifiedAt": null, "url": null, "title": "Genies and Wishes in the context of computer science", "slug": "genies-and-wishes-in-the-context-of-computer-science", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:07.687Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "private_messaging", "createdAt": "2012-05-01T06:09:25.541Z", "isAdmin": false, "displayName": "private_messaging"}, "userId": "7hcKjaZ5cGbckjLth", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XiCG5DkrZTQT8o659/genies-and-wishes-in-the-context-of-computer-science", "pageUrlRelative": "/posts/XiCG5DkrZTQT8o659/genies-and-wishes-in-the-context-of-computer-science", "linkUrl": "https://www.lesswrong.com/posts/XiCG5DkrZTQT8o659/genies-and-wishes-in-the-context-of-computer-science", "postedAtFormatted": "Friday, August 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Genies%20and%20Wishes%20in%20the%20context%20of%20computer%20science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGenies%20and%20Wishes%20in%20the%20context%20of%20computer%20science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXiCG5DkrZTQT8o659%2Fgenies-and-wishes-in-the-context-of-computer-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Genies%20and%20Wishes%20in%20the%20context%20of%20computer%20science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXiCG5DkrZTQT8o659%2Fgenies-and-wishes-in-the-context-of-computer-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXiCG5DkrZTQT8o659%2Fgenies-and-wishes-in-the-context-of-computer-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 818, "htmlBody": "<h2>Using computers to find a cure</h2>\n<p><img style=\"float: right; margin-left: 20px; margin-right: 20px;\" src=\"http://i.imgur.com/D6c7l1p.jpg\" alt=\"Folding@home\" width=\"273\" height=\"330\" /></p>\n<p>What it could be like to make a program which would fulfill our wish to \"cure cancer\"? I'll try to briefly present the contemporary mainstream CS perspective on this.</p>\n<p>Here's how \"curing cancer using AI technologies\" could realistically work in practice. You start with a widely applicable, powerful optimization <em>algorithm</em>. This algorithm takes in a fully formal specification of a process, and then finds and returns the parameters for that process for which the output value of the process is high. (I am deliberately avoiding use of the word \"function\").</p>\n<p>If you wish to cure a cancer, even having this optimization algorithm at your disposal, you can not simply write \"cure cancer\" on the terminal. If you do so, you will get something to the general sense of:</p>\n<blockquote>\n<p>No command 'cure' found, did you mean:<br />&nbsp;Command 'cube' from package 'sgt-puzzles' (universe)<br />&nbsp;Command 'curl' from package 'curl' (main)</p>\n</blockquote>\n<p>The optimization algorithm by itself not only does not have a goal set for it, but does not even have a domain for the goal to be defined on. It can't by itself be used to cure cancer or make paperclips. It may or may not map to what you would describe as AI.</p>\n<p>First, you would have to start with the domain. You would have to make a fairly crude biochemical model of the processes in the human cells and cancer cells, crude because you have limited computational power and there is very much that is going on in a cell. <sup>1</sup><br /><br />On the model, you define what you want to optimize - you specify formally how to compute a value from the model so that the value would be maximal for what you consider a good solution. It could be something like [fraction of model cancer cells whose functionality is strongly disrupted]*[fraction of model noncancer cells whose functionality is not strongly disrupted]. And you define model's parameters - the chemicals introduced into the model.</p>\n<p>Then you use the above mentioned optimization algorithm to find which extra parameters to the model (i.e. which extra chemicals) will result in the best outcome as defined above.</p>\n<p>Similar approach can, of course, be used to find manufacturing methods for that chemical, or to solve sub-problems related to senescence, mind uploading, or even for the development of better algorithms including optimization algorithms.</p>\n<p>Note that the approach described above does not map to genies and wishes in any way. Yes, the software can produce unexpected results, but concepts from One Thousand and One Nights will not help you predict the unexpected results. More contemporary science fiction, such as the Terminator franchise where the AI had the world's nuclear arsenal and probable responses explicitly included in it's problem domain, seem more relevant.</p>\n<h2>Hypothetical wish-granting software<br /></h2>\n<p><img style=\"float: right;\" src=\"http://i.imgur.com/ip3vi7D.jpg\" alt=\"Some nanotechnological AI wonder\" width=\"220\" height=\"286\" />It is generally believed that understanding of natural language is a very difficult task which relies on intelligence. For the AI, the sentence in question is merely a sensory input, which has to be coherently accounted for in it's understanding of the world.</p>\n<p>The bits from the ADC are accounted for with an analog signal in the wire, which is accounted for with pressure waves at the microphone, which are accounted for with a human speaking from any one of a particular set of locations that are consistent with how the sound interferes with it's reflections from the walls. The motions of the tongue and larynx are accounted for with electrical signals sent to the relevant muscles, then the language level signals in the Broca's area, then some logical concepts in the frontal lobes, an entire causal diagram traced backwards. In practice, a dumber AI would have a much cruder model, while a smarter AI would have a much finer model than I can outline.</p>\n<p>If you want the AI to work like a Jinn and \"do what it is told\", you need to somehow convert this model into a goal. Potential relations between \"cure cancer\" and \"kill everyone\" which the careless wish maker has not considered, naturally, played no substantial role in the process of the formation of the sentence. Extraction of such potential relations is a separate very different, and very difficult problem.</p>\n<p>It does intuitively seem like a genie which does what it is told, but not what is meant, would be easier to make, because it is a worse, less useful genie, and if it was for sale, it would have a lower market price. But in practice, the \"told\"/\"meant\" distinction does not carve reality at the joints and primarily applies to the plausible deniability.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>footnotes:</p>\n<p>1: You may use your optimization algorithm to build the biochemical model, by searching for \"best\" parameters for a computational chemistry package. You will have to factor in the computational cost of the model, and ensure some transparency (e.g. the package may only allow models that have a spatial representation that can be drawn and inspected).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XiCG5DkrZTQT8o659", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 20, "extendedScore": null, "score": 1.3218292556573686e-06, "legacy": true, "legacyId": "23946", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Using_computers_to_find_a_cure\">Using computers to find a cure</h2>\n<p><img style=\"float: right; margin-left: 20px; margin-right: 20px;\" src=\"http://i.imgur.com/D6c7l1p.jpg\" alt=\"Folding@home\" width=\"273\" height=\"330\"></p>\n<p>What it could be like to make a program which would fulfill our wish to \"cure cancer\"? I'll try to briefly present the contemporary mainstream CS perspective on this.</p>\n<p>Here's how \"curing cancer using AI technologies\" could realistically work in practice. You start with a widely applicable, powerful optimization <em>algorithm</em>. This algorithm takes in a fully formal specification of a process, and then finds and returns the parameters for that process for which the output value of the process is high. (I am deliberately avoiding use of the word \"function\").</p>\n<p>If you wish to cure a cancer, even having this optimization algorithm at your disposal, you can not simply write \"cure cancer\" on the terminal. If you do so, you will get something to the general sense of:</p>\n<blockquote>\n<p>No command 'cure' found, did you mean:<br>&nbsp;Command 'cube' from package 'sgt-puzzles' (universe)<br>&nbsp;Command 'curl' from package 'curl' (main)</p>\n</blockquote>\n<p>The optimization algorithm by itself not only does not have a goal set for it, but does not even have a domain for the goal to be defined on. It can't by itself be used to cure cancer or make paperclips. It may or may not map to what you would describe as AI.</p>\n<p>First, you would have to start with the domain. You would have to make a fairly crude biochemical model of the processes in the human cells and cancer cells, crude because you have limited computational power and there is very much that is going on in a cell. <sup>1</sup><br><br>On the model, you define what you want to optimize - you specify formally how to compute a value from the model so that the value would be maximal for what you consider a good solution. It could be something like [fraction of model cancer cells whose functionality is strongly disrupted]*[fraction of model noncancer cells whose functionality is not strongly disrupted]. And you define model's parameters - the chemicals introduced into the model.</p>\n<p>Then you use the above mentioned optimization algorithm to find which extra parameters to the model (i.e. which extra chemicals) will result in the best outcome as defined above.</p>\n<p>Similar approach can, of course, be used to find manufacturing methods for that chemical, or to solve sub-problems related to senescence, mind uploading, or even for the development of better algorithms including optimization algorithms.</p>\n<p>Note that the approach described above does not map to genies and wishes in any way. Yes, the software can produce unexpected results, but concepts from One Thousand and One Nights will not help you predict the unexpected results. More contemporary science fiction, such as the Terminator franchise where the AI had the world's nuclear arsenal and probable responses explicitly included in it's problem domain, seem more relevant.</p>\n<h2 id=\"Hypothetical_wish_granting_software\">Hypothetical wish-granting software<br></h2>\n<p><img style=\"float: right;\" src=\"http://i.imgur.com/ip3vi7D.jpg\" alt=\"Some nanotechnological AI wonder\" width=\"220\" height=\"286\">It is generally believed that understanding of natural language is a very difficult task which relies on intelligence. For the AI, the sentence in question is merely a sensory input, which has to be coherently accounted for in it's understanding of the world.</p>\n<p>The bits from the ADC are accounted for with an analog signal in the wire, which is accounted for with pressure waves at the microphone, which are accounted for with a human speaking from any one of a particular set of locations that are consistent with how the sound interferes with it's reflections from the walls. The motions of the tongue and larynx are accounted for with electrical signals sent to the relevant muscles, then the language level signals in the Broca's area, then some logical concepts in the frontal lobes, an entire causal diagram traced backwards. In practice, a dumber AI would have a much cruder model, while a smarter AI would have a much finer model than I can outline.</p>\n<p>If you want the AI to work like a Jinn and \"do what it is told\", you need to somehow convert this model into a goal. Potential relations between \"cure cancer\" and \"kill everyone\" which the careless wish maker has not considered, naturally, played no substantial role in the process of the formation of the sentence. Extraction of such potential relations is a separate very different, and very difficult problem.</p>\n<p>It does intuitively seem like a genie which does what it is told, but not what is meant, would be easier to make, because it is a worse, less useful genie, and if it was for sale, it would have a lower market price. But in practice, the \"told\"/\"meant\" distinction does not carve reality at the joints and primarily applies to the plausible deniability.</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p>footnotes:</p>\n<p>1: You may use your optimization algorithm to build the biochemical model, by searching for \"best\" parameters for a computational chemistry package. You will have to factor in the computational cost of the model, and ensure some transparency (e.g. the package may only allow models that have a spatial representation that can be drawn and inspected).</p>", "sections": [{"title": "Using computers to find a cure", "anchor": "Using_computers_to_find_a_cure", "level": 1}, {"title": "Hypothetical wish-granting software", "anchor": "Hypothetical_wish_granting_software", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "43 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-30T17:50:11.494Z", "modifiedAt": null, "url": null, "title": "New LW Meetup: Urbana-Champaign", "slug": "new-lw-meetup-urbana-champaign", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8R7qMeggx3Ms9aZAA/new-lw-meetup-urbana-champaign", "pageUrlRelative": "/posts/8R7qMeggx3Ms9aZAA/new-lw-meetup-urbana-champaign", "linkUrl": "https://www.lesswrong.com/posts/8R7qMeggx3Ms9aZAA/new-lw-meetup-urbana-champaign", "postedAtFormatted": "Friday, August 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetup%3A%20Urbana-Champaign&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetup%3A%20Urbana-Champaign%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8R7qMeggx3Ms9aZAA%2Fnew-lw-meetup-urbana-champaign%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetup%3A%20Urbana-Champaign%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8R7qMeggx3Ms9aZAA%2Fnew-lw-meetup-urbana-champaign", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8R7qMeggx3Ms9aZAA%2Fnew-lw-meetup-urbana-champaign", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 559, "htmlBody": "<p><strong>This summary was posted to Main on August 23rd. The following week's summary is <a href=\"/lw/ih7/new_lw_meetup_boulder_co/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/q0\">New Meetup: Urbana-Champaign, Illinois.:&nbsp;<span class=\"date\">25 August 2013 02:00PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/pr\">Atlanta LessWrong: Games Night:&nbsp;<span class=\"date\">24 August 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/q4\">Helsinki Meetup:&nbsp;<span class=\"date\">08 September 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/pn\">LessWrong Israel September meetup:&nbsp;<span class=\"date\">12 September 2013 08:00PM</span></a></li>\n<li><a href=\"/meetups/q6\">Philadelphia - Humans are not automatically strategic:&nbsp;<span class=\"date\">25 August 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/q5\">VA Callibration/Biased games meetup:&nbsp;<span class=\"date\">25 August 2013 03:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/q2\">[Boston] Estimation Experiment &amp; Discussion:&nbsp;<span class=\"date\">25 August 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/q1\">Durham/RTLW HPMoR discussion, ch. 82-85:&nbsp;<span class=\"date\">24 August 2013 12:00PM</span></a></li>\n<li><a href=\"/meetups/q3\">Durham NC/Triangle Area: Cognitive Biases and Where to Find Them + games!:&nbsp;<span class=\"date\">29 August 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/q7\">[London] Comfort Zone Expansion outing - London:&nbsp;<span class=\"date\">01 September 2013 11:00AM</span></a></li>\n<li><a href=\"/meetups/px\">[Washington DC] Robin Hanson visits to talk about prediction markets:&nbsp;<span class=\"date\">08 September 2013 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8R7qMeggx3Ms9aZAA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.3220932342184644e-06, "legacy": true, "legacyId": "23834", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yS9HZ4xAspsie8gGc", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-30T20:13:42.473Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington, DC: Goals", "slug": "meetup-washington-dc-goals", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:02.973Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "maia", "createdAt": "2012-02-08T20:28:42.643Z", "isAdmin": false, "displayName": "maia"}, "userId": "QW72Lk68mKnTfmdAB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YxsZYNca4EupzYSti/meetup-washington-dc-goals", "pageUrlRelative": "/posts/YxsZYNca4EupzYSti/meetup-washington-dc-goals", "linkUrl": "https://www.lesswrong.com/posts/YxsZYNca4EupzYSti/meetup-washington-dc-goals", "postedAtFormatted": "Friday, August 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%2C%20DC%3A%20Goals&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%2C%20DC%3A%20Goals%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYxsZYNca4EupzYSti%2Fmeetup-washington-dc-goals%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%2C%20DC%3A%20Goals%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYxsZYNca4EupzYSti%2Fmeetup-washington-dc-goals", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYxsZYNca4EupzYSti%2Fmeetup-washington-dc-goals", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 126, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qg'>Washington, DC: Goals</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 September 2013 10:45:39AM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to talk about goals, both long-term and short-term. Do you have a Plan? Do you think you need one? What projects are you working on / do you want to start?\nI'd also like to start a regular system (used by the NY self-improvement group) where we talk about our goals for the next few weeks, and check in on them after that time has passed.\nI will also be bringing an array of toys (a PowerBall, a cylindrical sliding puzzle, and similar items).\nWe'll be meeting in the courtyard adjoining the National Portrait Gallery, as usual.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qg'>Washington, DC: Goals</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YxsZYNca4EupzYSti", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.3222167963033769e-06, "legacy": true, "legacyId": "23948", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington__DC__Goals\">Discussion article for the meetup : <a href=\"/meetups/qg\">Washington, DC: Goals</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 September 2013 10:45:39AM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to talk about goals, both long-term and short-term. Do you have a Plan? Do you think you need one? What projects are you working on / do you want to start?\nI'd also like to start a regular system (used by the NY self-improvement group) where we talk about our goals for the next few weeks, and check in on them after that time has passed.\nI will also be bringing an array of toys (a PowerBall, a cylindrical sliding puzzle, and similar items).\nWe'll be meeting in the courtyard adjoining the National Portrait Gallery, as usual.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington__DC__Goals1\">Discussion article for the meetup : <a href=\"/meetups/qg\">Washington, DC: Goals</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington, DC: Goals", "anchor": "Discussion_article_for_the_meetup___Washington__DC__Goals", "level": 1}, {"title": "Discussion article for the meetup : Washington, DC: Goals", "anchor": "Discussion_article_for_the_meetup___Washington__DC__Goals1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-30T22:29:32.173Z", "modifiedAt": null, "url": null, "title": "Looking for Opinions on \"Antifragile\" by N. Taleb", "slug": "looking-for-opinions-on-antifragile-by-n-taleb", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:07.556Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Torello", "createdAt": "2013-07-01T17:38:37.441Z", "isAdmin": false, "displayName": "Torello"}, "userId": "xoRpeFN7K5MgDRcvM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E2JNHRCQed6K3BT2E/looking-for-opinions-on-antifragile-by-n-taleb", "pageUrlRelative": "/posts/E2JNHRCQed6K3BT2E/looking-for-opinions-on-antifragile-by-n-taleb", "linkUrl": "https://www.lesswrong.com/posts/E2JNHRCQed6K3BT2E/looking-for-opinions-on-antifragile-by-n-taleb", "postedAtFormatted": "Friday, August 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Looking%20for%20Opinions%20on%20%22Antifragile%22%20by%20N.%20Taleb&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALooking%20for%20Opinions%20on%20%22Antifragile%22%20by%20N.%20Taleb%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE2JNHRCQed6K3BT2E%2Flooking-for-opinions-on-antifragile-by-n-taleb%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Looking%20for%20Opinions%20on%20%22Antifragile%22%20by%20N.%20Taleb%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE2JNHRCQed6K3BT2E%2Flooking-for-opinions-on-antifragile-by-n-taleb", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE2JNHRCQed6K3BT2E%2Flooking-for-opinions-on-antifragile-by-n-taleb", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 119, "htmlBody": "<p>I'm reading Antifragile, and I don't have much relevant background, so it's hard for me to evaluate what he's saying.&nbsp; If anyone has relevant background/expertise, I'd like to hear it.&nbsp;</p>\n<p>&nbsp;</p>\n<p>I can certainly see how the author's tone could annoy a lot of readers, but so far I've found his style entertaining and quite obviously (to me at least) a part of his \"shtick\", so it comes across as clever and funny instead of arrogant.&nbsp;</p>\n<p>&nbsp;</p>\n<p>I guess this could also evolve into a meta-discussion of how to evaluate books when you have little frame of reference, but I would imagine that has been discussed in other posts on this site.&nbsp; (Please link to a post of that topic if you can).&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E2JNHRCQed6K3BT2E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 1, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "23949", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-30T23:44:31.550Z", "modifiedAt": null, "url": null, "title": "Raising numerate children", "slug": "raising-numerate-children", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:50.590Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZhoyWrqXRShbee4bS/raising-numerate-children", "pageUrlRelative": "/posts/ZhoyWrqXRShbee4bS/raising-numerate-children", "linkUrl": "https://www.lesswrong.com/posts/ZhoyWrqXRShbee4bS/raising-numerate-children", "postedAtFormatted": "Friday, August 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Raising%20numerate%20children&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARaising%20numerate%20children%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZhoyWrqXRShbee4bS%2Fraising-numerate-children%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Raising%20numerate%20children%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZhoyWrqXRShbee4bS%2Fraising-numerate-children", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZhoyWrqXRShbee4bS%2Fraising-numerate-children", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 995, "htmlBody": "<p>How do you raise a child as a rationalist? I can't say that that was exactly what I had in mind but it seems to make for a fitting title here. A more precise title could have been: \"How to deeply educate a child such that it fun and natural\".</p>\n<p>Today I'd like to tell you about the lullabies I sung and to what that led.</p>\n<p>When my firstborn was very young I adapted a classic German lullaby \"Schlaf kindlein schlaf\" to numbers. It started with with only a few verses but grew over time (in part by the need to cover longer times until he slept).</p>\n<p>I did sing it in German but I tried to translate it here to give you a better idea. It goes to the melody of \"Schlaf Kindlein Schlaf which you may not know but can google easily (note that in German there are nicer rhymes for 100, million, googol):</p>\n<p style=\"padding-left: 30px;\">Sleep, baby, sleep!</p>\n<p style=\"padding-left: 30px;\">Thy father counts the sheep,</p>\n<p style=\"padding-left: 30px;\">One, two, three and four</p>\n<p style=\"padding-left: 30px;\">Little baby sleep some more</p>\n<p style=\"padding-left: 30px;\">Sleep, baby, sleep!</p>\n<p>The refrain repeats and the verses are replaced as follows:&nbsp;</p>\n<p style=\"padding-left: 30px;\">Five, six, seven, eight - Tired at the dreamland gate.</p>\n<p style=\"padding-left: 30px;\">Nine, ten and eleven - Sleeping in the number heaven.</p>\n<p style=\"padding-left: 30px;\">Twelve, thirteen and fourteen - Sleeping babies have you seen.</p>\n<p style=\"padding-left: 30px;\">Fifteen up to twentyone - Dreaming baby sleep is done.</p>\n<p style=\"padding-left: 30px;\">Twentytwo to hundredtwo - Baby I will care for you.</p>\n<p style=\"padding-left: 30px;\">Hundredthree to thousendfive - Caring for you all your life.</p>\n<p style=\"padding-left: 30px;\">Thousandsix to millionthree - Of your dream you can break free.</p>\n<p style=\"padding-left: 30px;\">Millionfour to one googol - Dreaming of a giant ball.</p>\n<p style=\"padding-left: 30px;\">Googol-one to googolplex - Steaming in the dreamland tracks.</p>\n<p style=\"padding-left: 30px;\">Googolplex to infinity - I will always care for thee.</p>\n<p>I get slower during the song and very slow with infinity - mostly they slept then.</p>\n<p>I have dreams of this song where sheep accumulate to larger and larger blocks until the block number thats raising in blocks and everything ends in white noise.</p>\n<p>I do no longer sing it to my older sons but they accompany me sometimes when singing it to my youngest (two years old). And they do know what googol means already.</p>\n<p>I also have a bed ritual where I let them give the number of times I put the blanket on their face (they like it). When they give too large numbers I use blocks. These tended to get high too.</p>\n<p>One time I asked for lower numbers (that was when my second oldest already knew halfs and quarters) which led to gaming for unusual fractions and ultimately to his insight that \"There is no larger fraction than one half that can divide one\" (by a seven year himself).</p>\n<p>It seems to have put numbers so deeply in their mind and interest that my seven year old can do simple fractions, exponentials and roots in his head. I tried hard to avoid too much arithmetic before school lest they bore of math in school and that worked for his older brother (who nonetheless tops his class in math) but he just asks and asks and I have just given up and keep just answering his questions and posing comparable return questions at his Zone Of Proximal Development. &nbsp; &nbsp;</p>\n<p>There are dialogs that run like this (contracted):</p>\n<p>He: \"In school we had to give tasks to get 50. I was allowed to give 5*10\" &nbsp;&nbsp;</p>\n<p>Me: \"Can you give some other examples?\"</p>\n<p>He: \"2*20+10\" thinking a bit \"20 time 2 and a half equals 50\"</p>\n<p>Me: \"What about division?\"</p>\n<p>He: \"100 divided by 2 obviously. Or 50/1.\"</p>\n<p>Or:</p>\n<p>Me: \"How long is the side of a cube containing one litre?\"</p>\n<p>He: \"10?\" (omitting centimeters)</p>\n<p>Me: \"How do you know that?\"</p>\n<p>He: \"You have told me.\" (*I* can't remember when; must be month's)</p>\n<p>Me: \"And how long is the side of a cube with 27 liters?\"</p>\n<p>He (dividing 27 then adding or something like that): \"18,5?\"</p>\n<p>Me: \"No. How did you get there?\"</p>\n<p>He: \"There must be some number multiplied to get 27\" (or something like that)</p>\n<p>Me. \"Yes, the side time the side times the side.\" (expecting him to try some numbers)</p>\n<p>He: \"What is the root of 27?\" (he has picked up that root is the reverse of times the same number)</p>\n<p>Me: \"Good idea. Here whe have three times or a number to the 3rd power - so we need the 3rd root.\"&nbsp;</p>\n<p>He: \"And what is the third root or 27?\"</p>\n<p>Me: \"Try it.\"</p>\n<p>He: \"2*2*2 equals 16 no 8\" (he seems to remember a few powers of 2)</p>\n<p>Me: \"Yes. That is too small\"</p>\n<p>He: \"5^3? 5*5*5?\"</p>\n<p>Me: \"That is 125 - too large\"</p>\n<p>He: \"3?\"</p>\n<p>Me: \"Yes.\"</p>\n<p>Or:</p>\n<p>He: \"What is 10*10*10*10*10?\"&nbsp;</p>\n<p>Me: \"You mean 10 to the 5th power? That's hundred thousand\"</p>\n<p>He: \"What 10*10*... [lots of 10s)?\"</p>\n<p>Me: \"You mean 10 to the 30th power? Thats nonillion.\"</p>\n<p>He: \"What is 10 to the 100th power?\"</p>\n<p>Me: \"That is called googol. A 1 with 100 zeros.\"</p>\n<p>He: \"What is 10^100^100^100?\"</p>\n<p>Me: \"Do you mean 10^100 and that to the 100th power or 10 to the 100^100th power?\"</p>\n<p>He: Somehwat confused asks differnt questions, dialog levels off.</p>\n<p>(note that in German \"to the xth power\" is simply \"hoch\" thus much easier to concatenate)</p>\n<p>I have to say that I am quite proud of my children and wouldn't be surprised when you called me overly so. I have to add that we, my wife and I, invest significant time into our children, so just singing this song may not be enough. And it also may be that I was lucky that they are (partly) gifted with math (like me). But I have to emphasize that we did no rote memoization or repeated training whatsoever (and left that to school).</p>\n<p>There are other things we do for 'rationalist training' which I will try to post some time soon. &nbsp;</p>\n<p>Teaser:&nbsp;</p>\n<p>- Bed time stories with complex patterns (endless stories, simply nested stories, parallel stories, forking stories).</p>\n<p>- Everyday Experiments for young children.</p>\n<div><br /></div>\n<div>Note 1: I place the lullaby under creative commons as checked below. You may adapt and I recommend finding better rhymes/meter.</div>\n<div><br /></div>\n<div>Note 2: This is my first real post here. Please feel free to tell if you think it inappropriate, too long or too much showing my probable pride.&nbsp;</div>\n<div><br /></div>\n<div>EDIT: fixed typos.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q55STnFh6gbSezRuR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZhoyWrqXRShbee4bS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 47, "extendedScore": null, "score": 1.3223983395781858e-06, "legacy": true, "legacyId": "23950", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-31T17:09:09.882Z", "modifiedAt": null, "url": null, "title": "Any interest in a one-time meetup in Charlotte, NC this Tuesday?", "slug": "any-interest-in-a-one-time-meetup-in-charlotte-nc-this", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pinyaka", "createdAt": "2012-09-21T12:11:45.980Z", "isAdmin": false, "displayName": "pinyaka"}, "userId": "FscpDmNcKZdbeDNZ2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PNbGQdwvSmGLj3TJz/any-interest-in-a-one-time-meetup-in-charlotte-nc-this", "pageUrlRelative": "/posts/PNbGQdwvSmGLj3TJz/any-interest-in-a-one-time-meetup-in-charlotte-nc-this", "linkUrl": "https://www.lesswrong.com/posts/PNbGQdwvSmGLj3TJz/any-interest-in-a-one-time-meetup-in-charlotte-nc-this", "postedAtFormatted": "Saturday, August 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Any%20interest%20in%20a%20one-time%20meetup%20in%20Charlotte%2C%20NC%20this%20Tuesday%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAny%20interest%20in%20a%20one-time%20meetup%20in%20Charlotte%2C%20NC%20this%20Tuesday%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPNbGQdwvSmGLj3TJz%2Fany-interest-in-a-one-time-meetup-in-charlotte-nc-this%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Any%20interest%20in%20a%20one-time%20meetup%20in%20Charlotte%2C%20NC%20this%20Tuesday%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPNbGQdwvSmGLj3TJz%2Fany-interest-in-a-one-time-meetup-in-charlotte-nc-this", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPNbGQdwvSmGLj3TJz%2Fany-interest-in-a-one-time-meetup-in-charlotte-nc-this", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<p>I am in Charlotte, NC for the week due to a family emergency and wondered if anyone from Less Wrong wanted to get together for dinner this Tuesday night? There is a small chance that my emergency will come to a head on Tuesday and I won't be able to attend, but hopefully enough people will want to get together that my absence won't matter too much if it comes to that.</p>\n<p>I've created a google group <a href=\"https://groups.google.com/forum/#!forum/one-time-lw-meetup-in-charlotte-nc\" target=\"_blank\">here</a>&nbsp;to pin down a specific time and place to get together or feel free to PM me here if you don't want to join the google group.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PNbGQdwvSmGLj3TJz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "23962", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-31T17:27:55.473Z", "modifiedAt": null, "url": null, "title": "Relevance of the STEPS project to (F)AI teams", "slug": "relevance-of-the-steps-project-to-f-ai-teams", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:04.195Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cyfjqwGM6kBMZ9zq2/relevance-of-the-steps-project-to-f-ai-teams", "pageUrlRelative": "/posts/cyfjqwGM6kBMZ9zq2/relevance-of-the-steps-project-to-f-ai-teams", "linkUrl": "https://www.lesswrong.com/posts/cyfjqwGM6kBMZ9zq2/relevance-of-the-steps-project-to-f-ai-teams", "postedAtFormatted": "Saturday, August 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Relevance%20of%20the%20STEPS%20project%20to%20(F)AI%20teams&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARelevance%20of%20the%20STEPS%20project%20to%20(F)AI%20teams%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcyfjqwGM6kBMZ9zq2%2Frelevance-of-the-steps-project-to-f-ai-teams%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Relevance%20of%20the%20STEPS%20project%20to%20(F)AI%20teams%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcyfjqwGM6kBMZ9zq2%2Frelevance-of-the-steps-project-to-f-ai-teams", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcyfjqwGM6kBMZ9zq2%2Frelevance-of-the-steps-project-to-f-ai-teams", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 295, "htmlBody": "<p>The View Point Reasearch Institute&nbsp;<a href=\"http://www.vpri.org/\">http://www.vpri.org/</a>&nbsp;(which by the way brought us the squeek etoys) has a project called STEPS&nbsp;Toward The Reinvention of Programming.</p>\n<p>The goal is basically to create a <strong>complete</strong> operating system, tool chain and program suite in <strong>20000 </strong>lines of code. The reason for this and other goals are not relevant for this post but I recommend that you take a look into it anyway.</p>\n<p>I have followed the progress of the STEPS project more or less since its beginning in 2007: http://www.vpri.org/pdf/tr2007008_steps.pdf</p>\n<p>This project is interesting to the (F)AI community in so far as it contains a complete operating system in a form that contains more or less no noise or redundancy in its implementation, is extremely powerful (in terms of expressivity and generality) and thus more amenable to artificial reasoning and optimization than any other system.</p>\n<p>The current report&nbsp;<a href=\"http://www.vpri.org/pdf/tr2011004_steps11.pdf\">http://www.vpri.org/pdf/tr2011004_steps11.pdf</a>&nbsp;contains a very interesting paragraph on page 22:</p>\n<p>\"An awkward and complicated \u02b9optimization tower of babel\u02b9 is avoided by giving our DSLs the ability to act on their own implementation and optimizations, flattening the \u02b9tower\u02b9 into a collection of reflexive and metacircular mechanisms. The line between strategy and implementation, between coordination and computation, is eliminated \u2011\u2011 or at least guaranteed to be eliminable whenever necessary.\"</p>\n<p>That means that an AI system running on top of that could optimize itself as is needed for an intelligence explosion I guess. And you may guess who might be running AI on top of it.</p>\n<p>EDIT: Some seem to think that this post is somewhat inappropriate. But I can't make out why. Because it is too short? Because it is about AI? Because it requires reading up on that project? Because it is about software engineering instead of rationality? I'm a newbie and don't know which or how to improve. Please comment your negative rating.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cyfjqwGM6kBMZ9zq2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 4, "extendedScore": null, "score": 1.323314744757209e-06, "legacy": true, "legacyId": "23963", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-08-31T22:04:22.523Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, September 1-15", "slug": "group-rationality-diary-september-1-15-0", "viewCount": null, "lastCommentedAt": "2013-09-10T19:57:56.544Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XPsft9gKw2tPQBuC2/group-rationality-diary-september-1-15-0", "pageUrlRelative": "/posts/XPsft9gKw2tPQBuC2/group-rationality-diary-september-1-15-0", "linkUrl": "https://www.lesswrong.com/posts/XPsft9gKw2tPQBuC2/group-rationality-diary-september-1-15-0", "postedAtFormatted": "Saturday, August 31st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20September%201-15&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20September%201-15%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXPsft9gKw2tPQBuC2%2Fgroup-rationality-diary-september-1-15-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20September%201-15%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXPsft9gKw2tPQBuC2%2Fgroup-rationality-diary-september-1-15-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXPsft9gKw2tPQBuC2%2Fgroup-rationality-diary-september-1-15-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 216, "htmlBody": "<p>This is the public group instrumental rationality diary for September 1-15.</p>\n<blockquote style=\"font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">\n<p style=\"margin: 0px 0px 1em;\"><span style=\"color: #333333;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/therufs/submitted/r/discussion/lw/hg0/group_rationality_diary_may_1631/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating!</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><a href=\"/r/discussion/lw/iho/group_rationality_diary_september_1630/\">Next diary</a>:&nbsp; September 16-30</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><a href=\"/lw/ic6/group_rationality_diary_august_1631/\">Immediate past diary</a>:&nbsp; August 16-31<a href=\"/lw/hvy/group_rationality_diary_july_115/\"><br /></a></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><a href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality Diaries archive</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XPsft9gKw2tPQBuC2", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "23964", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XPsft9gKw2tPQBuC2", "TDZbs6Ah34PTd9Txw", "QXYsonygGQRc6fjJy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-08-31T22:04:22.523Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-01T11:05:11.336Z", "modifiedAt": null, "url": null, "title": "September 2013 Media Thread", "slug": "september-2013-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:30.817Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ArisKatsaris", "createdAt": "2010-10-07T10:24:25.721Z", "isAdmin": false, "displayName": "ArisKatsaris"}, "userId": "fLbksBTnFsbwYmzsT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HXNkaNaS4N9sC4GSg/september-2013-media-thread", "pageUrlRelative": "/posts/HXNkaNaS4N9sC4GSg/september-2013-media-thread", "linkUrl": "https://www.lesswrong.com/posts/HXNkaNaS4N9sC4GSg/september-2013-media-thread", "postedAtFormatted": "Sunday, September 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20September%202013%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeptember%202013%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHXNkaNaS4N9sC4GSg%2Fseptember-2013-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=September%202013%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHXNkaNaS4N9sC4GSg%2Fseptember-2013-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHXNkaNaS4N9sC4GSg%2Fseptember-2013-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>This is the monthly thread for posting media of various types that you've found that you enjoy. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the <a href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p>Rules:</p>\n<ul>\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a <a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you think there should be a thread for a particular genre of media, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HXNkaNaS4N9sC4GSg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.3242269782150453e-06, "legacy": true, "legacyId": "23967", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-01T12:26:25.320Z", "modifiedAt": null, "url": null, "title": "Bed time stories with clear concepts", "slug": "bed-time-stories-with-clear-concepts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:05.055Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NAfZr8tHe5fPZyab4/bed-time-stories-with-clear-concepts", "pageUrlRelative": "/posts/NAfZr8tHe5fPZyab4/bed-time-stories-with-clear-concepts", "linkUrl": "https://www.lesswrong.com/posts/NAfZr8tHe5fPZyab4/bed-time-stories-with-clear-concepts", "postedAtFormatted": "Sunday, September 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bed%20time%20stories%20with%20clear%20concepts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABed%20time%20stories%20with%20clear%20concepts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNAfZr8tHe5fPZyab4%2Fbed-time-stories-with-clear-concepts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bed%20time%20stories%20with%20clear%20concepts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNAfZr8tHe5fPZyab4%2Fbed-time-stories-with-clear-concepts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNAfZr8tHe5fPZyab4%2Fbed-time-stories-with-clear-concepts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1401, "htmlBody": "<p><strong>Followup of:</strong>&nbsp;<a href=\"/r/discussion/lw/iha/raising_numerate_children/\">Raising numerate children</a></p>\n<p>As promised I will now give one more parenting technique for raising a child as a rationalist.</p>\n<p>This is about a special kind of bed time stories that I tell. The idea is to use clear concepts and pack them into a form accessible to small children (ages four to ten). I decided on the structure beforehand but the plot was mostly made up on the spot (based on available experiences of the day).</p>\n<h3>Endless story</h3>\n<p>This is the most well known and elementary form of a nested story. You could call it tail recursion without termination.</p>\n<p>In German there is a well known song that goes as follow:</p>\n<p style=\"padding-left: 30px;\">\"Ein Hund lief in die K&uuml;che und stahl dem Koch ein Ei.</p>\n<p style=\"padding-left: 30px;\">Da nahm der Koch den L&ouml;ffel und schlug den Hund entzwei.</p>\n<p style=\"padding-left: 30px;\">Da kamen alle Hunde und gruben ihm ein Grab,</p>\n<p style=\"padding-left: 30px;\">und setzten einen Grabstein, worauf geschrieben ward:\" (repeat)</p>\n<p>I couldn't find an English translations of this (does anybody volunteer?) But I found a comparable song in English:</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://en.wikipedia.org/wiki/The_Song_That_Never_Ends\">\"This is the song that never ends.\"</a></p>\n<p>(which has the nice additional benefit of being self-referential)</p>\n<p>Now there are multiple ways to use and vary this basic pattern:</p>\n<ul>\n<li>Stop when you get bored. If done well this emphasizes certain aspects of infinity - that nothing new is added; everything stays exactly the same and would continue on 'ad inifinitum'. &nbsp;</li>\n<li>Introduce an explicit change breaking the loop with another explanation (e.g. \"... und stahl dem Koch <strong>kein </strong>Ei\" or \"...because they like it\"). &nbsp;</li>\n<li>Vary (reduce) the number of participants each time until it gets to zero (\"Viele Hunde\", \"Einige Hunde\", \"Ein Hund\", \"keine Hunde\" or \"a few people\", \"no people\").&nbsp;</li>\n<li>Increase speed of singing it each time until it becomes noise (or if you do it well roughly doubling each time you can even 'solve' the infinite regress via Zeno) that's what my wife does.</li>\n<li>Talk about what is written on the stone and that the text has to be smaller each time (only suitable for cases where a picture can be imagined).</li>\n</ul>\n<h3>Strictly nested story</h3>\n<p>This developed from a story in which a boy reads a book. Neverbug mentioned the Neverending Story which is the 'classical' example. But you can find pictures of books with pictures of themselves a lot even in childrens picture books.</p>\n<p>I adapted this pattern one evening when my son had refused to clean up his toys. It went like this:</p>\n<p style=\"padding-left: 30px;\">\"There was a boy called Anton who had cleaned up his room quickly and was in bed early. His mother came to say goodnight and complimented him and told him a long story of</p>\n<p style=\"padding-left: 60px;\">\"A boy called Bill who has played long and then cleaned up his room just in time for his mother to tell him a story of</p>\n<p style=\"padding-left: 90px;\">\"A boy called Charles who didn't like cleaning up and argued with his mother but in the end tidied up early enough for a short story of</p>\n<p style=\"padding-left: 120px;\">\"A boy called Dylan who had quite a heap in his room, raged when begged to clean up and thus had to put his pieces away together with his mother and just got a short hug and good night kiss.\"</p>\n<p style=\"padding-left: 120px;\">Then the story of Dylen ended and the mother of Charles gave him a good night kiss.\" &nbsp; &nbsp; &nbsp;&nbsp;</p>\n<p style=\"padding-left: 90px;\">That was the end of the story of Charles and the mother of Bill gave him a hug and kiss and left.\"</p>\n<p style=\"padding-left: 60px;\">And that was the end of the story of Bill and the mother of Anton gave him a hug and a kiss and left.\"</p>\n<p>And that is the end of the story of Anton and I give you a kiss and will go. Good night.&nbsp;</p>\n<p>This can be varied in lots of ways:</p>\n<ul>\n<li>nesting depth</li>\n<li>topic - here the topic (cleaning up) is related on the meta level to the length of the story &nbsp;</li>\n<li>naming of the boy in the story (here I chose alphabetical)</li>\n<li>alternating between boys/girls mother/father (thus showning multiple recursion)</li>\n</ul>\n<p>The general pattern is simple nested recursion with exit condition.</p>\n<h3>Nested story with interrupt</h3>\n<p>This varies the simple nested story with a fun element that at the same time allows reflection of the meta level and external effects. For comparison I will use the same story as before (of course I don't want to avoid typing):</p>\n<p style=\"padding-left: 30px;\">\"There was a boy called Anton who had cleaned up his room quickly and was in bed early. His mother came to say goodnight and complimented him and told him a long story of</p>\n<p style=\"padding-left: 60px;\">\"A boy called Bill who has played long and then cleaned up his room just in time for his mother to tell him a story of</p>\n<p style=\"padding-left: 90px;\">\"A boy called Charles who didn't like cleaning up and argued with his mother but in the end tidied up\"\"\"</p>\n<p>(here I make a sudden motion or sound and say \"interrupt\")&nbsp;</p>\n<p style=\"padding-left: 60px;\">\"\"when suddenly the mother of Bill left\" and the mother of Anton left\"</p>\n<p>and now I have to go too. &nbsp;</p>\n<p>The pattern is recursion with interrupt or exception handling.</p>\n<h3>Parallel story</h3>\n<p>This is a method to train keeping track of multiple threads and their relation in simple settings.</p>\n<p>There are two main variants:</p>\n<ol>\n<li>Both plots are in the same world and are coordinated or interact</li>\n<li>Both plots are in parallel universes and elucidate differences of action and results&nbsp;</li>\n</ol>\n<p>The common aspect is that these stories use coordinated time. Time in both plots is advanced at the same rate and the plots are continuously alternated.</p>\n<p>The stories were longer and I will try to give only a feel for them here:</p>\n<p style=\"padding-left: 30px;\">Sandra is a strong girl but often angry she thinks that nobody likes her.</p>\n<p style=\"padding-left: 30px;\">The other Sandra is also strong but calm and friendly.</p>\n<p style=\"padding-left: 30px;\">Both have had their first day at school recently.</p>\n<p style=\"padding-left: 30px;\">The first Sandra gets bumped into in turbulence in the morning. She gets angry and pushes back. A boy stumbles and cries and calls her names. Two other children try to moderate but scold her. She trudges to her place sadly. &nbsp;</p>\n<p style=\"padding-left: 30px;\">The second Sandra also gets bumped into but smiles and makes a joke about the bumping and all laugh. She quickly reaches her place and can prepare for class.</p>\n<p>This repeats with more typical school situations. Where the first Sandra gets more and more cast out. Then it ends with a situation where the anger of the first Sandra actually gives her the power to help some other girl which is attacked by some boys. This situation gives Sandra positive feedback and appreciation by the class.</p>\n<p>This structure can use to compare the following aspects on the two threads:</p>\n<ul>\n<li>opposites (one actor acts positively while the other acts negatively and the consequences are elaborated as above)&nbsp;</li>\n<li>effect of small random perturbations (butterfly effect)</li>\n<li>coordination (the actors interact which each other (or a common third actors) to some end) &nbsp; &nbsp;</li>\n</ul>\n<h3>Forking stories</h3>\n<p>These are basically the same as the stories told by the game master in <a href=\"http://en.wikipedia.org/wiki/Narrativist \">role playing</a> or the story in a <a href=\"http://en.wikipedia.org/wiki/Gamebook\">game book</a>. The main differences are that choices are limited, highlighted and possibly discussed. The idea is to make the concept of a fork in the story or process clear.</p>\n<p>I have told stories about&nbsp;</p>\n<ul>\n<li>being alone in the woods (exploring ways to get home; this developed in the end into two parallel tracks when at a choice point both ways were followed)</li>\n<li>conflict in school</li>\n<li>a quest with a dragon (where the dragon is found to be a large herbivore like in The Name of the Wind http://en.wikipedia.org/wiki/The_Name_of_the_Wind )</li>\n<li>a serial story which is modeled on simple computer games (navigate thru a labyrinth, fight transforming monsters, climb a hill, best a giant - actually the giant from Enders Game) &nbsp; &nbsp;</li>\n</ul>\n<h3>General remark about these stories</h3>\n<p>Stories are a way to avoid appearing indoctrinating by using an analog situation and leaving the choice to the child or even to explore both aspects of a choice.&nbsp;</p>\n<p>Sure, children will learn most of the principles taught earlier or later anyway. But research (see e.g. the refs by Trevor_Blake) shows that the advantage of children acquiring these earlier (or more reliable) will help them at least until college.</p>\n<p>Some concepts will not stick. Some are not necessary for normal adult life. But nothing is lost compared to 'standard' bed time stories (except possibly some culturally relevant stories like little red ridinghood; but you can still read these too).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q55STnFh6gbSezRuR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NAfZr8tHe5fPZyab4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 40, "extendedScore": null, "score": 0.000116, "legacy": true, "legacyId": "23968", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Followup of:</strong>&nbsp;<a href=\"/r/discussion/lw/iha/raising_numerate_children/\">Raising numerate children</a></p>\n<p>As promised I will now give one more parenting technique for raising a child as a rationalist.</p>\n<p>This is about a special kind of bed time stories that I tell. The idea is to use clear concepts and pack them into a form accessible to small children (ages four to ten). I decided on the structure beforehand but the plot was mostly made up on the spot (based on available experiences of the day).</p>\n<h3 id=\"Endless_story\">Endless story</h3>\n<p>This is the most well known and elementary form of a nested story. You could call it tail recursion without termination.</p>\n<p>In German there is a well known song that goes as follow:</p>\n<p style=\"padding-left: 30px;\">\"Ein Hund lief in die K\u00fcche und stahl dem Koch ein Ei.</p>\n<p style=\"padding-left: 30px;\">Da nahm der Koch den L\u00f6ffel und schlug den Hund entzwei.</p>\n<p style=\"padding-left: 30px;\">Da kamen alle Hunde und gruben ihm ein Grab,</p>\n<p style=\"padding-left: 30px;\">und setzten einen Grabstein, worauf geschrieben ward:\" (repeat)</p>\n<p>I couldn't find an English translations of this (does anybody volunteer?) But I found a comparable song in English:</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://en.wikipedia.org/wiki/The_Song_That_Never_Ends\">\"This is the song that never ends.\"</a></p>\n<p>(which has the nice additional benefit of being self-referential)</p>\n<p>Now there are multiple ways to use and vary this basic pattern:</p>\n<ul>\n<li>Stop when you get bored. If done well this emphasizes certain aspects of infinity - that nothing new is added; everything stays exactly the same and would continue on 'ad inifinitum'. &nbsp;</li>\n<li>Introduce an explicit change breaking the loop with another explanation (e.g. \"... und stahl dem Koch <strong>kein </strong>Ei\" or \"...because they like it\"). &nbsp;</li>\n<li>Vary (reduce) the number of participants each time until it gets to zero (\"Viele Hunde\", \"Einige Hunde\", \"Ein Hund\", \"keine Hunde\" or \"a few people\", \"no people\").&nbsp;</li>\n<li>Increase speed of singing it each time until it becomes noise (or if you do it well roughly doubling each time you can even 'solve' the infinite regress via Zeno) that's what my wife does.</li>\n<li>Talk about what is written on the stone and that the text has to be smaller each time (only suitable for cases where a picture can be imagined).</li>\n</ul>\n<h3 id=\"Strictly_nested_story\">Strictly nested story</h3>\n<p>This developed from a story in which a boy reads a book. Neverbug mentioned the Neverending Story which is the 'classical' example. But you can find pictures of books with pictures of themselves a lot even in childrens picture books.</p>\n<p>I adapted this pattern one evening when my son had refused to clean up his toys. It went like this:</p>\n<p style=\"padding-left: 30px;\">\"There was a boy called Anton who had cleaned up his room quickly and was in bed early. His mother came to say goodnight and complimented him and told him a long story of</p>\n<p style=\"padding-left: 60px;\">\"A boy called Bill who has played long and then cleaned up his room just in time for his mother to tell him a story of</p>\n<p style=\"padding-left: 90px;\">\"A boy called Charles who didn't like cleaning up and argued with his mother but in the end tidied up early enough for a short story of</p>\n<p style=\"padding-left: 120px;\">\"A boy called Dylan who had quite a heap in his room, raged when begged to clean up and thus had to put his pieces away together with his mother and just got a short hug and good night kiss.\"</p>\n<p style=\"padding-left: 120px;\">Then the story of Dylen ended and the mother of Charles gave him a good night kiss.\" &nbsp; &nbsp; &nbsp;&nbsp;</p>\n<p style=\"padding-left: 90px;\">That was the end of the story of Charles and the mother of Bill gave him a hug and kiss and left.\"</p>\n<p style=\"padding-left: 60px;\">And that was the end of the story of Bill and the mother of Anton gave him a hug and a kiss and left.\"</p>\n<p>And that is the end of the story of Anton and I give you a kiss and will go. Good night.&nbsp;</p>\n<p>This can be varied in lots of ways:</p>\n<ul>\n<li>nesting depth</li>\n<li>topic - here the topic (cleaning up) is related on the meta level to the length of the story &nbsp;</li>\n<li>naming of the boy in the story (here I chose alphabetical)</li>\n<li>alternating between boys/girls mother/father (thus showning multiple recursion)</li>\n</ul>\n<p>The general pattern is simple nested recursion with exit condition.</p>\n<h3 id=\"Nested_story_with_interrupt\">Nested story with interrupt</h3>\n<p>This varies the simple nested story with a fun element that at the same time allows reflection of the meta level and external effects. For comparison I will use the same story as before (of course I don't want to avoid typing):</p>\n<p style=\"padding-left: 30px;\">\"There was a boy called Anton who had cleaned up his room quickly and was in bed early. His mother came to say goodnight and complimented him and told him a long story of</p>\n<p style=\"padding-left: 60px;\">\"A boy called Bill who has played long and then cleaned up his room just in time for his mother to tell him a story of</p>\n<p style=\"padding-left: 90px;\">\"A boy called Charles who didn't like cleaning up and argued with his mother but in the end tidied up\"\"\"</p>\n<p>(here I make a sudden motion or sound and say \"interrupt\")&nbsp;</p>\n<p style=\"padding-left: 60px;\">\"\"when suddenly the mother of Bill left\" and the mother of Anton left\"</p>\n<p>and now I have to go too. &nbsp;</p>\n<p>The pattern is recursion with interrupt or exception handling.</p>\n<h3 id=\"Parallel_story\">Parallel story</h3>\n<p>This is a method to train keeping track of multiple threads and their relation in simple settings.</p>\n<p>There are two main variants:</p>\n<ol>\n<li>Both plots are in the same world and are coordinated or interact</li>\n<li>Both plots are in parallel universes and elucidate differences of action and results&nbsp;</li>\n</ol>\n<p>The common aspect is that these stories use coordinated time. Time in both plots is advanced at the same rate and the plots are continuously alternated.</p>\n<p>The stories were longer and I will try to give only a feel for them here:</p>\n<p style=\"padding-left: 30px;\">Sandra is a strong girl but often angry she thinks that nobody likes her.</p>\n<p style=\"padding-left: 30px;\">The other Sandra is also strong but calm and friendly.</p>\n<p style=\"padding-left: 30px;\">Both have had their first day at school recently.</p>\n<p style=\"padding-left: 30px;\">The first Sandra gets bumped into in turbulence in the morning. She gets angry and pushes back. A boy stumbles and cries and calls her names. Two other children try to moderate but scold her. She trudges to her place sadly. &nbsp;</p>\n<p style=\"padding-left: 30px;\">The second Sandra also gets bumped into but smiles and makes a joke about the bumping and all laugh. She quickly reaches her place and can prepare for class.</p>\n<p>This repeats with more typical school situations. Where the first Sandra gets more and more cast out. Then it ends with a situation where the anger of the first Sandra actually gives her the power to help some other girl which is attacked by some boys. This situation gives Sandra positive feedback and appreciation by the class.</p>\n<p>This structure can use to compare the following aspects on the two threads:</p>\n<ul>\n<li>opposites (one actor acts positively while the other acts negatively and the consequences are elaborated as above)&nbsp;</li>\n<li>effect of small random perturbations (butterfly effect)</li>\n<li>coordination (the actors interact which each other (or a common third actors) to some end) &nbsp; &nbsp;</li>\n</ul>\n<h3 id=\"Forking_stories\">Forking stories</h3>\n<p>These are basically the same as the stories told by the game master in <a href=\"http://en.wikipedia.org/wiki/Narrativist \">role playing</a> or the story in a <a href=\"http://en.wikipedia.org/wiki/Gamebook\">game book</a>. The main differences are that choices are limited, highlighted and possibly discussed. The idea is to make the concept of a fork in the story or process clear.</p>\n<p>I have told stories about&nbsp;</p>\n<ul>\n<li>being alone in the woods (exploring ways to get home; this developed in the end into two parallel tracks when at a choice point both ways were followed)</li>\n<li>conflict in school</li>\n<li>a quest with a dragon (where the dragon is found to be a large herbivore like in The Name of the Wind http://en.wikipedia.org/wiki/The_Name_of_the_Wind )</li>\n<li>a serial story which is modeled on simple computer games (navigate thru a labyrinth, fight transforming monsters, climb a hill, best a giant - actually the giant from Enders Game) &nbsp; &nbsp;</li>\n</ul>\n<h3 id=\"General_remark_about_these_stories\">General remark about these stories</h3>\n<p>Stories are a way to avoid appearing indoctrinating by using an analog situation and leaving the choice to the child or even to explore both aspects of a choice.&nbsp;</p>\n<p>Sure, children will learn most of the principles taught earlier or later anyway. But research (see e.g. the refs by Trevor_Blake) shows that the advantage of children acquiring these earlier (or more reliable) will help them at least until college.</p>\n<p>Some concepts will not stick. Some are not necessary for normal adult life. But nothing is lost compared to 'standard' bed time stories (except possibly some culturally relevant stories like little red ridinghood; but you can still read these too).</p>", "sections": [{"title": "Endless story", "anchor": "Endless_story", "level": 1}, {"title": "Strictly nested story", "anchor": "Strictly_nested_story", "level": 1}, {"title": "Nested story with interrupt", "anchor": "Nested_story_with_interrupt", "level": 1}, {"title": "Parallel story", "anchor": "Parallel_story", "level": 1}, {"title": "Forking stories", "anchor": "Forking_stories", "level": 1}, {"title": "General remark about these stories", "anchor": "General_remark_about_these_stories", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZhoyWrqXRShbee4bS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-01T17:40:52.171Z", "modifiedAt": null, "url": null, "title": "MIRI course list book reviews, part 1: G\u00f6del, Escher, Bach", "slug": "miri-course-list-book-reviews-part-1-goedel-escher-bach", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:27.500Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oHAQD53FhpHte4dEM/miri-course-list-book-reviews-part-1-goedel-escher-bach", "pageUrlRelative": "/posts/oHAQD53FhpHte4dEM/miri-course-list-book-reviews-part-1-goedel-escher-bach", "linkUrl": "https://www.lesswrong.com/posts/oHAQD53FhpHte4dEM/miri-course-list-book-reviews-part-1-goedel-escher-bach", "postedAtFormatted": "Sunday, September 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20MIRI%20course%20list%20book%20reviews%2C%20part%201%3A%20G%C3%B6del%2C%20Escher%2C%20Bach&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMIRI%20course%20list%20book%20reviews%2C%20part%201%3A%20G%C3%B6del%2C%20Escher%2C%20Bach%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoHAQD53FhpHte4dEM%2Fmiri-course-list-book-reviews-part-1-goedel-escher-bach%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=MIRI%20course%20list%20book%20reviews%2C%20part%201%3A%20G%C3%B6del%2C%20Escher%2C%20Bach%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoHAQD53FhpHte4dEM%2Fmiri-course-list-book-reviews-part-1-goedel-escher-bach", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoHAQD53FhpHte4dEM%2Fmiri-course-list-book-reviews-part-1-goedel-escher-bach", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 867, "htmlBody": "<p>I'm <a href=\"/lw/i4z/welcome_to_less_wrong_6th_thread_july_2013/9gh5\">Nate</a>. I was recently introduced to the ideas of existential risk and unfriendly AI. I've decided to read the books suggested in the&nbsp;<a href=\"http://intelligence.org/courses/\">MIRI course list</a>.&nbsp;I'll review the books as I read them. Repeating the knowledge is expected to help solidify it. Public accountability is expected to help keep me on track. Hopefully my notes will also be useful to others. This is the first such review.</p>\n<h1>G&ouml;del, Escher, Bach</h1>\n<p>I'll be reviewing this book from memory. I started it in 2010 at the recommendation of a friend. I got frustrated early on when Hofstadter introduced topics that I already knew well (such as recursion). This turned out to be a mistake; the book picks up shortly thereafter. I finished the rest of it maybe four months ago, around the time that I finished the sequences.</p>\n<h2>Overview</h2>\n<p><em>G&ouml;del, Escher, Bach</em> is an incredibly well-written foray into the intersection of art, mathematics, philosophy, and biology. It explores the border between syntax and semantics. It's not a textbook, but you'll learn more about logic than most introductory courses will teach you.</p>\n<p>The book is composed of alternating dialogs and chapters. The dialogs are witty narratives which take on the structure of concepts discussed in the following chapter. It's hard to describe how this works, but it's very effective. The dialogs are brilliantly designed and are by far the most entertaining part of the book.</p>\n<p><a id=\"more\"></a></p>\n<h2>What I didn't like</h2>\n<p>I'd already been exposed to many of the topics in GEB. I have an aversion to being taught things I already understand, so some chapters frustrated me. The new-signal to old-signal ratio was lowest in chapters 5-8. Computer scientists beware: the book will feel slow early on. Chapter 5 is skippable. 6-8 are not, because the specific variants of propositional calculus and number theory that Hofstadter sets up are used (to brilliant effect) in later chapters.</p>\n<p>One of the late sections of GEB discusses reductionism. The discussion is quite good, and I would recommend it to people who misunderstand the reductionist argument. But personally I found it boring, as I need no further convincing (and it didn't introduce any Nate!novel arguments).</p>\n<p>The final section of GEB is a discussion of the artificial intelligence field, including predictions made decades ago. While most of the predictions were solid, the chapter is no longer relevant. For some reason, temporally-irrelevant text in narratives makes me feel uncomfortable.</p>\n<p>These concerns were dwarfed by the wit, fun, and knowledge embedded in the rest of the book.</p>\n<h2>What I learned</h2>\n<p>GEB didn't introduce that many new concepts to me. What it did was bring old concepts to life. Before reading GEB, I knew about incompleteness and could state the theorem. After reading GEB, I can construct actual G&ouml;del sentences. GEB will take you from superficial knowledge to full grok.</p>\n<p>A number of tidbits stuck with me:</p>\n<hr />\n<p>Every message has many levels:</p>\n<ol>\n<li>The framing (Whatever makes you recognize a message)</li>\n<li>The outer message (The symbols/words)</li>\n<li>The intended meaning</li>\n</ol>\n<p>I hadn't thought about the frame message as a layer of the message before GEB.</p>\n<hr />\n<p>There's a hard distinction between the symbol-game of formal logic and the interpretation of those symbols. (The difference between the symbol &and; and its meaning as a conjunction).</p>\n<p>Before GEB, I would nod my head at this fact, but it sounded silly. Of course the symbols were just symbols: but they were created with human meanings in mind, alongside rules that preserve said meanings. I didn't see a need to enforce the distinction.</p>\n<p>In reading GEB I saw firsthand the different ways that the symbols could be interpreted, and the power available when multiple interpretations are considered simultaneously. Now the distinction is of paramount importance to me.</p>\n<p>(As a side note, this realization helped me retroactively gain a higher understanding of Type Theory, which is all about making use of many different levels of interpretation at once.)</p>\n<hr />\n<p>Some questions must be unasked.</p>\n<blockquote>\n<p>Have you stopped beating your wife?</p>\n</blockquote>\n<p>The word 'mu' unaskes a question. I really wish the word 'mu' was better known, so I could use it in daily conversation without drawing strange looks.</p>\n<hr />\n<p>Though you may find this to be obvious, I cannot assert its truth:</p>\n<blockquote>\n<p>Nate Soares cannot consistently assert this sentence.</p>\n</blockquote>\n<p>This is my new favorite way to illustrate incompleteness.</p>\n<hr />\n<p>There were many more little things that clicked, and a lot of pleasant surprises which aren't coming to mind. Also, the book isn't only about G&ouml;del: I learned a lot about Escher and Bach along the way. If you read this book, I definitely recommend having a means to play Bach's music on-hand: many of the chapters are more powerful while listening to the Bach compositions that they allude to.</p>\n<h2>Conclusions</h2>\n<p>If nothing else, I recommend reading GEB for the sake of art. It's masterfully constructed. It will hopefully help you understand math that you know superficially. Even if it can't teach you the math it can probably teach you about art and music.</p>\n<p>If you're new to the fields of math / logic / computer science, then <em>G&ouml;del, Escher, Bach</em> is a must. However, I can't guarantee that the new-signal to old-signal ratio is favorable for people already knowledgeable in relevant fields.</p>\n<p><em><span style=\"font-family: mceinline;\"><span style=\"font-family: mceinline;\">Side question: Is this the sort of thing people want to see in main?</span></span></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2, "4Kcm4etxAJjmeDkHP": 2, "AJDHQ4mFnsNbBzPhT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oHAQD53FhpHte4dEM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 26, "extendedScore": null, "score": 6.5e-05, "legacy": true, "legacyId": "23969", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I'm <a href=\"/lw/i4z/welcome_to_less_wrong_6th_thread_july_2013/9gh5\">Nate</a>. I was recently introduced to the ideas of existential risk and unfriendly AI. I've decided to read the books suggested in the&nbsp;<a href=\"http://intelligence.org/courses/\">MIRI course list</a>.&nbsp;I'll review the books as I read them. Repeating the knowledge is expected to help solidify it. Public accountability is expected to help keep me on track. Hopefully my notes will also be useful to others. This is the first such review.</p>\n<h1 id=\"G_del__Escher__Bach\">G\u00f6del, Escher, Bach</h1>\n<p>I'll be reviewing this book from memory. I started it in 2010 at the recommendation of a friend. I got frustrated early on when Hofstadter introduced topics that I already knew well (such as recursion). This turned out to be a mistake; the book picks up shortly thereafter. I finished the rest of it maybe four months ago, around the time that I finished the sequences.</p>\n<h2 id=\"Overview\">Overview</h2>\n<p><em>G\u00f6del, Escher, Bach</em> is an incredibly well-written foray into the intersection of art, mathematics, philosophy, and biology. It explores the border between syntax and semantics. It's not a textbook, but you'll learn more about logic than most introductory courses will teach you.</p>\n<p>The book is composed of alternating dialogs and chapters. The dialogs are witty narratives which take on the structure of concepts discussed in the following chapter. It's hard to describe how this works, but it's very effective. The dialogs are brilliantly designed and are by far the most entertaining part of the book.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"What_I_didn_t_like\">What I didn't like</h2>\n<p>I'd already been exposed to many of the topics in GEB. I have an aversion to being taught things I already understand, so some chapters frustrated me. The new-signal to old-signal ratio was lowest in chapters 5-8. Computer scientists beware: the book will feel slow early on. Chapter 5 is skippable. 6-8 are not, because the specific variants of propositional calculus and number theory that Hofstadter sets up are used (to brilliant effect) in later chapters.</p>\n<p>One of the late sections of GEB discusses reductionism. The discussion is quite good, and I would recommend it to people who misunderstand the reductionist argument. But personally I found it boring, as I need no further convincing (and it didn't introduce any Nate!novel arguments).</p>\n<p>The final section of GEB is a discussion of the artificial intelligence field, including predictions made decades ago. While most of the predictions were solid, the chapter is no longer relevant. For some reason, temporally-irrelevant text in narratives makes me feel uncomfortable.</p>\n<p>These concerns were dwarfed by the wit, fun, and knowledge embedded in the rest of the book.</p>\n<h2 id=\"What_I_learned\">What I learned</h2>\n<p>GEB didn't introduce that many new concepts to me. What it did was bring old concepts to life. Before reading GEB, I knew about incompleteness and could state the theorem. After reading GEB, I can construct actual G\u00f6del sentences. GEB will take you from superficial knowledge to full grok.</p>\n<p>A number of tidbits stuck with me:</p>\n<hr>\n<p>Every message has many levels:</p>\n<ol>\n<li>The framing (Whatever makes you recognize a message)</li>\n<li>The outer message (The symbols/words)</li>\n<li>The intended meaning</li>\n</ol>\n<p>I hadn't thought about the frame message as a layer of the message before GEB.</p>\n<hr>\n<p>There's a hard distinction between the symbol-game of formal logic and the interpretation of those symbols. (The difference between the symbol \u2227 and its meaning as a conjunction).</p>\n<p>Before GEB, I would nod my head at this fact, but it sounded silly. Of course the symbols were just symbols: but they were created with human meanings in mind, alongside rules that preserve said meanings. I didn't see a need to enforce the distinction.</p>\n<p>In reading GEB I saw firsthand the different ways that the symbols could be interpreted, and the power available when multiple interpretations are considered simultaneously. Now the distinction is of paramount importance to me.</p>\n<p>(As a side note, this realization helped me retroactively gain a higher understanding of Type Theory, which is all about making use of many different levels of interpretation at once.)</p>\n<hr>\n<p>Some questions must be unasked.</p>\n<blockquote>\n<p>Have you stopped beating your wife?</p>\n</blockquote>\n<p>The word 'mu' unaskes a question. I really wish the word 'mu' was better known, so I could use it in daily conversation without drawing strange looks.</p>\n<hr>\n<p>Though you may find this to be obvious, I cannot assert its truth:</p>\n<blockquote>\n<p>Nate Soares cannot consistently assert this sentence.</p>\n</blockquote>\n<p>This is my new favorite way to illustrate incompleteness.</p>\n<hr>\n<p>There were many more little things that clicked, and a lot of pleasant surprises which aren't coming to mind. Also, the book isn't only about G\u00f6del: I learned a lot about Escher and Bach along the way. If you read this book, I definitely recommend having a means to play Bach's music on-hand: many of the chapters are more powerful while listening to the Bach compositions that they allude to.</p>\n<h2 id=\"Conclusions\">Conclusions</h2>\n<p>If nothing else, I recommend reading GEB for the sake of art. It's masterfully constructed. It will hopefully help you understand math that you know superficially. Even if it can't teach you the math it can probably teach you about art and music.</p>\n<p>If you're new to the fields of math / logic / computer science, then <em>G\u00f6del, Escher, Bach</em> is a must. However, I can't guarantee that the new-signal to old-signal ratio is favorable for people already knowledgeable in relevant fields.</p>\n<p><em><span style=\"font-family: mceinline;\"><span style=\"font-family: mceinline;\">Side question: Is this the sort of thing people want to see in main?</span></span></em></p>", "sections": [{"title": "G\u00f6del, Escher, Bach", "anchor": "G_del__Escher__Bach", "level": 1}, {"title": "Overview", "anchor": "Overview", "level": 2}, {"title": "What I didn't like", "anchor": "What_I_didn_t_like", "level": 2}, {"title": "What I learned", "anchor": "What_I_learned", "level": 2}, {"title": "Conclusions", "anchor": "Conclusions", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-01T18:36:40.217Z", "modifiedAt": null, "url": null, "title": "P/S/A - Sam Harris offering money for a little good philosophy", "slug": "p-s-a-sam-harris-offering-money-for-a-little-good-philosophy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:27.651Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benito", "createdAt": "2012-03-14T21:58:54.405Z", "isAdmin": true, "displayName": "Ben Pace"}, "userId": "EQNTWXLKMeWMp2FQS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q9bmnbH3pTyTMSv5s/p-s-a-sam-harris-offering-money-for-a-little-good-philosophy", "pageUrlRelative": "/posts/Q9bmnbH3pTyTMSv5s/p-s-a-sam-harris-offering-money-for-a-little-good-philosophy", "linkUrl": "https://www.lesswrong.com/posts/Q9bmnbH3pTyTMSv5s/p-s-a-sam-harris-offering-money-for-a-little-good-philosophy", "postedAtFormatted": "Sunday, September 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20P%2FS%2FA%20-%20Sam%20Harris%20offering%20money%20for%20a%20little%20good%20philosophy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AP%2FS%2FA%20-%20Sam%20Harris%20offering%20money%20for%20a%20little%20good%20philosophy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ9bmnbH3pTyTMSv5s%2Fp-s-a-sam-harris-offering-money-for-a-little-good-philosophy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=P%2FS%2FA%20-%20Sam%20Harris%20offering%20money%20for%20a%20little%20good%20philosophy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ9bmnbH3pTyTMSv5s%2Fp-s-a-sam-harris-offering-money-for-a-little-good-philosophy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ9bmnbH3pTyTMSv5s%2Fp-s-a-sam-harris-offering-money-for-a-little-good-philosophy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 137, "htmlBody": "<p>Sam Harris is here offering a substantial amount of money to anyone who can show a flaw in the philosophy of 'The Moral Landscape' in 1000 word or less, or at least the best attempt.</p>\n<p style=\"text-align: -webkit-auto;\"><span style=\"font-family: '.HelveticaNeueUI'; font-size: 15px; line-height: 19px; white-space: nowrap; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">http://www.samharris.org/blog/item/the-moral-landscape-challenge1</span></p>\n<p style=\"text-align: -webkit-auto;\">Up to $20,000 is on offer, although that's only if you change his mind. Whilst we know that this is very difficult, note how few people offer large sums of money for the privelage of being disproven.</p>\n<p style=\"text-align: -webkit-auto;\">In case anyone does win, I will remind you that this site is created and maintained by people who work at MIRI and CFAR, which rely on outside donations, and with whom I am not affiliated.</p>\n<p style=\"text-align: -webkit-auto;\">&nbsp;</p>\n<p style=\"text-align: -webkit-auto;\">Note: Is this misplaced in Discussion? I imagine that it could be easily overlooked in an open thread by the sorts of people who would be able to use this information well?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q9bmnbH3pTyTMSv5s", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 19, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "23970", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 78, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-01T21:01:11.873Z", "modifiedAt": null, "url": null, "title": "Advanced Placement exam cutoffs and superficial knowledge over deep knowledge", "slug": "advanced-placement-exam-cutoffs-and-superficial-knowledge", "viewCount": null, "lastCommentedAt": "2021-05-16T01:55:31.014Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4W9i78uxX8Dibk77L/advanced-placement-exam-cutoffs-and-superficial-knowledge", "pageUrlRelative": "/posts/4W9i78uxX8Dibk77L/advanced-placement-exam-cutoffs-and-superficial-knowledge", "linkUrl": "https://www.lesswrong.com/posts/4W9i78uxX8Dibk77L/advanced-placement-exam-cutoffs-and-superficial-knowledge", "postedAtFormatted": "Sunday, September 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Advanced%20Placement%20exam%20cutoffs%20and%20superficial%20knowledge%20over%20deep%20knowledge&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdvanced%20Placement%20exam%20cutoffs%20and%20superficial%20knowledge%20over%20deep%20knowledge%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4W9i78uxX8Dibk77L%2Fadvanced-placement-exam-cutoffs-and-superficial-knowledge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Advanced%20Placement%20exam%20cutoffs%20and%20superficial%20knowledge%20over%20deep%20knowledge%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4W9i78uxX8Dibk77L%2Fadvanced-placement-exam-cutoffs-and-superficial-knowledge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4W9i78uxX8Dibk77L%2Fadvanced-placement-exam-cutoffs-and-superficial-knowledge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 580, "htmlBody": "<p>When I was in high school, I noticed is that it was possible to score the top mark on an <a href=\"http://en.wikipedia.org/wiki/Advanced_Placement_exams\">Advanced Placement (AP) Exam</a>&nbsp;by answering a relatively small portion of the questions correctly.</p>\n<p><a id=\"more\"></a>During my junior year, I self-studied calculus, and took the <a href=\"http://en.wikipedia.org/wiki/AP_Calculus#AP_Calculus_AB\">AP Calculus AB</a>&nbsp;exam. I was very surprised that I scored a 5 (the top mark), because at the time when I took the exam, I didn't know some very basic things that were on the syllabus.</p>\n<p>The College Board gives the <a href=\"http://apcentral.collegeboard.com/apc/public/courses/212187.html\">raw score to AP score conversions</a>&nbsp;for the exams that have been most recently released. The percentages needed to get a 5 are as follows:</p>\n<ul>\n<li>Art History: 71%</li>\n<li>Biology: 63%</li>\n<li>Calculus AB: 63%</li>\n<li>Calculus BC: 63%</li>\n<li>Chemistry: 67%</li>\n<li>Computer Science A: 77%</li>\n<li>English Language and Composition: 75%</li>\n<li>English Literature and Composition: 76%</li>\n<li>Environmental Science: 71%</li>\n<li>European History: 66%</li>\n<li>French Language: 80%</li>\n<li>German Language: 86%</li>\n<li>Comparative Government &amp; Politics: 70%</li>\n<li>US Government and Politics: 77%</li>\n<li>Human Geography: 61%</li>\n<li>Latin: Vergil: 69%</li>\n<li>Music Theory: 70%</li>\n<li>Macroeconomics: 81%</li>\n<li>Microeconomics: 83%</li>\n<li>Physics B: 62%</li>\n<li>Physics C: Mechanics: 55%</li>\n<li>Physics C: Electricity and Magnetism: 59%</li>\n<li>Psychology: 75%</li>\n<li>Spanish Language: 78%</li>\n<li>Spanish Literature: 76%</li>\n<li>Statistics: 63%</li>\n<li>US History: 61%</li>\n<li>World History 64%</li>\n</ul>\n<div>\n<div>Conspicuously, these percentages are all well below the standard 90% needed for the top mark (A) in a high school or college course. Of course, the threshold of 90% that's typically used is arbitrary, and some courses have harder exams than others, but the fact that the above percentages are lower than 90% (and in some cases *much* lower) is a&nbsp;<a href=\"/lw/hmb/many_weak_arguments_vs_one_relatively_strong/\">weak argument</a>&nbsp;that the standards for getting a 5 on an AP exam are lenient (and in some cases *very* lenient).</div>\n</div>\n<div><br /></div>\n<div>On an object level, based on my experience taking the AP calculus exams as a high schooler, my experience teaching calculus for three years at University of Illinois, and my revisiting the exams, I think that students who score 90% on an AP calculus exam know the material very well, and that students who score 63% (the lowest percentage needed to get a 5) have only marginal knowledge of the material.</div>\n<div><br /></div>\n<div>It's natural to ask what effect this has on incentives.&nbsp;Note that AP courses are taught to prepare students to get high marks on the AP exams, so that the grading scheme for AP exams propagates backward to the grading schemes for courses, which feed into college admissions. Taking many AP courses and performing just above the \"5\" level looks better than taking few AP courses and performing at the ~90% level.</div>\n<div><br /></div>\n<div>So one might reasonably suppose that high schoolers who want to get into selective colleges focus on developing superficial knowledge of lots of academic subjects rather than deep knowledge of a smaller number of academic subjects. And indeed, this is precisely what I remember most students doing in high school.</div>\n<div><br /></div>\n<div>\n<ul>\n<li>To what degree does your own experience reflect this as well?<br /></li>\n<li>What are some other contexts in which this sort of thing occurs?<br /></li>\n<li>How much of a problem is this (if at all)?</li>\n</ul>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 2, "3uE2pXvbcnS9nnZRE": 2, "fkABsGCJZ6y9qConW": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4W9i78uxX8Dibk77L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "23972", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9W9P2snxu5Px746LD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-01T22:21:32.456Z", "modifiedAt": null, "url": null, "title": "Teapots and Soda Cans", "slug": "teapots-and-soda-cans", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:07.801Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Odinn", "createdAt": "2013-03-06T05:03:47.992Z", "isAdmin": false, "displayName": "Odinn"}, "userId": "nPG9pJSH2R2p4CTYZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GjK5of6Esc6SjjinW/teapots-and-soda-cans", "pageUrlRelative": "/posts/GjK5of6Esc6SjjinW/teapots-and-soda-cans", "linkUrl": "https://www.lesswrong.com/posts/GjK5of6Esc6SjjinW/teapots-and-soda-cans", "postedAtFormatted": "Sunday, September 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Teapots%20and%20Soda%20Cans&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATeapots%20and%20Soda%20Cans%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGjK5of6Esc6SjjinW%2Fteapots-and-soda-cans%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Teapots%20and%20Soda%20Cans%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGjK5of6Esc6SjjinW%2Fteapots-and-soda-cans", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGjK5of6Esc6SjjinW%2Fteapots-and-soda-cans", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 525, "htmlBody": "<p><span style=\"white-space: pre;\"> </span>Reading an earnest and thought provoking editorial<sub>1</sub> from one James Wood, reviewing 'Letter To a Christian Nation' by Sam Harris. Though atheist himself, he admits a flagging patience with certain attitudes of atheists. I can concede that an atheist's superior and glib demeanor may be due to frustration and no small amount of pessimistic inference about the human condition, though I had to comment about a rebuttal he gives regarding Bertrand Russell's celestial teapot<sub>2</sub>.</p>\n<p><span style=\"white-space: pre;\"> </span>He claims that God, so much grander and more complex than a teapot, cannot be banished with such a simplistic comparison, when I would insist that God is actually much less believable than the teapot for that exact reason. I think Russell's teapot is due for an update which is more approachable and grounded. Here goes:</p>\n<p><span style=\"white-space: pre;\"> </span>I claim that there is a discarded Coke can somewhere in the vastness of the Sahara, but I will brook absolutely no discussion about doubting my claim or investigating it for veracity. \"Okay,\" you think, \"I suppose I can assume that much to be true. Whatever this man's sources, the odds of a Coke can being somewhere in the desert must be considerable.\" But I then elaborate with claims that it's actually many, many cans, folded into glorious and artistically pleasing forms, and my obdurate refusal to discuss how it can be proved continues. At this point even the most generous theists would likely start getting annoyed with my odd behavior, yet at the very least what I'm asking you to believe isn't outside the realm of possibility. For all you know (though I refuse to allow you to check) there could be a folk art bazaar currently set up in the Sahara, so really it costs you very little to entertain my view.</p>\n<p><span style=\"white-space: pre;\"> </span>And then I say that the cans have taken on beautiful, shimmering consciousness and are forming a society which hides from humanity, burying their chrome castles beneath the sand and moving their aluminum cities whenever we get too close to discovering them. \"But...\" you try to cut in. Before you can even begin to tell me what you find odd about my fantasy, I'm on the next detail. I claim that all of our major technological achievements of the last several hundred years are all thanks to the secret influence of the Shiny Can People.</p>\n<p><span style=\"white-space: pre;\"> </span>Now you have countless legitimate doubts, but every time you try to tell me that, for starters, soda didn't even come in aluminum cans several hundred years ago, I insist that you weren't there so you can't be sure, and how could a mere burden of proof destroy the mighty empire of the Shiny Cans?</p>\n<p><span style=\"white-space: pre;\"> </span>I like the utility of the can people because it doesn't start with an outlandish proposition, but if you stick around it gets absolutely ridiculous. Not only does that remind me more of how religion is actually sold, but it also serves to strengthen the original analogy of the teapot by reminding the curious mind that Russell's teapot is infinitely smaller and less complex than God, making it much less embarrassing to genuinely believe in since it would have so much more room to hide.</p>\n<p><span style=\"white-space: pre;\"> </span>Odinn Celusta</p>\n<p>1)&nbsp;<a href=\"http://www.newrepublic.com/article/the-celestial-teapot\">http://www.newrepublic.com/article/the-celestial-teapot</a></p>\n<p>2)&nbsp;<a href=\"http://en.wikipedia.org/wiki/Russell's_teapot\">http://en.wikipedia.org/wiki/Russell's_teapot</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GjK5of6Esc6SjjinW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 8, "extendedScore": null, "score": 1.3248111348524573e-06, "legacy": true, "legacyId": "23966", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-02T06:12:39.019Z", "modifiedAt": null, "url": null, "title": "Artificial explosion of the Sun: a new x-risk?", "slug": "artificial-explosion-of-the-sun-a-new-x-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:38.764Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dkvoauE8tXKZKAXDq/artificial-explosion-of-the-sun-a-new-x-risk", "pageUrlRelative": "/posts/dkvoauE8tXKZKAXDq/artificial-explosion-of-the-sun-a-new-x-risk", "linkUrl": "https://www.lesswrong.com/posts/dkvoauE8tXKZKAXDq/artificial-explosion-of-the-sun-a-new-x-risk", "postedAtFormatted": "Monday, September 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Artificial%20explosion%20of%20the%20Sun%3A%20a%20new%20x-risk%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArtificial%20explosion%20of%20the%20Sun%3A%20a%20new%20x-risk%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdkvoauE8tXKZKAXDq%2Fartificial-explosion-of-the-sun-a-new-x-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Artificial%20explosion%20of%20the%20Sun%3A%20a%20new%20x-risk%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdkvoauE8tXKZKAXDq%2Fartificial-explosion-of-the-sun-a-new-x-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdkvoauE8tXKZKAXDq%2Fartificial-explosion-of-the-sun-a-new-x-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 520, "htmlBody": "<p><a href=\"http://www.scirp.org/journal/PaperDownload.aspx?paperID=34277\">Bolonkin &amp; Friedlander (2013)</a> argues that it might be possible for \"a dying dictator\" to blow up the Sun, and thus destroy all life on Earth:</p>\n<blockquote>\n<p>The Sun contains ~74% hydrogen by weight. The isotope hydrogen-1 (99.985% of hydrogen in nature) is a usable fuel for fusion thermonuclear reactions. This reaction runs slowly within the Sun because its temperature is low (relative to the needs of nuclear reactions). If we create higher temperature and density in a limited region of the solar interior, we may be able to produce self-supporting detonation thermonuclear reactions that spread to the full solar volume. This is analogous to the triggering mechanisms in a thermonuclear bomb. Conditions within the bomb can be optimized in a small area to initiate ignition, then spread to a larger area, allowing producing a hydrogen bomb of any power. In the case of the Sun certain targeting practices may greatly increase the chances of an artificial explosion of the Sun. This explosion would annihilate the Earth and the Solar System, as we know them today. The reader naturally asks: Why even contemplate such a horrible scenario? It is necessary because as thermonuclear and space technology spreads to even the least powerful nations in the centuries ahead, a dying dictator having thermonuclear missile weapons can [produce] (with some considerable mobilization of his military/industrial complex)&mdash;an artificial explosion of the Sun and take into his grave the whole of humanity. It might take tens of thousands of people to make and launch the hardware, but only a very few need know the final targeting data of what might be otherwise a weapon purely thought of (within the dictator&rsquo;s defense industry) as being built for peaceful, deterrent use. Those concerned about Man&rsquo;s future must know about this possibility and create some protective system&mdash;or ascertain on theoretical grounds that it is entirely [impossible]. Humanity has fears, justified to greater or lesser degrees, about asteroids, warming of Earthly climate, extinctions, etc. which have very small probability. But all these would leave survivors&mdash;nobody thinks that the terrible annihilation of the Solar System would leave a single person alive. That explosion appears possible at the present time. In this paper is derived the &ldquo;AB-Criterion&rdquo; which shows conditions wherein the artificial explosion of Sun is possible. The author urges detailed investigation and proving or disproving of this rather horrifying possibility, so that it may be dismissed from mind&mdash;or defended against.</p>\n</blockquote>\n<p><strong>Warning</strong>: the paper is published in an obscure journal by publisher #206 on&nbsp;<a href=\"http://scholarlyoa.com/2012/12/06/bealls-list-of-predatory-publishers-2013/\">Beall&rsquo;s List of Predatory Publishers 2013</a>, and I was unable to find confirmation of the authors' <a href=\"http://lifeboat.com/ex/bios.alexander.bolonkin\">claimed</a> <a href=\"http://lifeboat.com/ex/bios.joseph.friedlander\">credentials</a> from any reputable sources with 5 minutes of Googling. It also has two spelling errors <em>in the abstract</em>. (It has no citations on Google scholar, but I wouldn't expect it to have any since it was only released in July 2013.)</p>\n<p>I haven't read the paper, and I'd love to see someone fluent in astrophysics comment on its contents.&nbsp;</p>\n<p>My guess is that this is <em>not a risk at all</em>&nbsp;or,&nbsp;as with <a href=\"http://arxiv.org/pdf/hep-ph/9910333.pdf\">proposed high-energy physics disasters</a>,&nbsp;the risk is extremely low-probability but physically conceivable (though perhaps not by methods imagined by Bolonkin &amp; Friedlander).&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dkvoauE8tXKZKAXDq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 6, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "23977", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-02T10:49:53.145Z", "modifiedAt": null, "url": null, "title": "Rudeness", "slug": "rudeness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:05.096Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Odinn", "createdAt": "2013-03-06T05:03:47.992Z", "isAdmin": false, "displayName": "Odinn"}, "userId": "nPG9pJSH2R2p4CTYZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BqJLnudE3MTMo5hZ6/rudeness", "pageUrlRelative": "/posts/BqJLnudE3MTMo5hZ6/rudeness", "linkUrl": "https://www.lesswrong.com/posts/BqJLnudE3MTMo5hZ6/rudeness", "postedAtFormatted": "Monday, September 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rudeness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARudeness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBqJLnudE3MTMo5hZ6%2Frudeness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rudeness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBqJLnudE3MTMo5hZ6%2Frudeness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBqJLnudE3MTMo5hZ6%2Frudeness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 210, "htmlBody": "<p><span style=\"white-space: pre;\"> </span>Really new to this site, I'm hitting a problem I've experienced in other aspects of my life as a student, employee and comedic performer: I'm extremely rude. I don't realize it at the time, thinking that I'm just being blunt, forceful and direct. In the sense that those can all be definitions of similar concepts, then yeah! Well, confidence in myself is a great asset, and I've turned it to positive effect, especially when I need to intimidate someone with a roguish smile and a calm, iron-hard assertion backed up by a blistering intensity (I sound like a Marty-Stu right now. Draco will want his leather pants back.)</p>\n<p><span style=\"white-space: pre;\"> </span>But making rationally sound argument should not be about winning. It should be about accuracy, clarity and sanity. If you disagree with me on something I need to remind myself not to automatically fight. What good is being alpha when I'm ignoring my confusion and avoiding my embarrassment at possibly being mistaken. Not to mention that this is an internet forum, so limitations of the medium means attempting to look like a tough guy winds up hollow and sad, like a chocolate Easter bunny that's gone off.</p>\n<p><span style=\"white-space: pre;\"> </span>Okay, I'm not one for similes, but I am one for trying to make myself more sane.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BqJLnudE3MTMo5hZ6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 4, "extendedScore": null, "score": 1.3254580021991372e-06, "legacy": true, "legacyId": "23980", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-02T12:13:36.749Z", "modifiedAt": "2022-03-19T02:22:07.768Z", "url": null, "title": "Baseline of my opinion on LW topics", "slug": "baseline-of-my-opinion-on-lw-topics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:03.545Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tHv3gb3cwPg5EJxSx/baseline-of-my-opinion-on-lw-topics", "pageUrlRelative": "/posts/tHv3gb3cwPg5EJxSx/baseline-of-my-opinion-on-lw-topics", "linkUrl": "https://www.lesswrong.com/posts/tHv3gb3cwPg5EJxSx/baseline-of-my-opinion-on-lw-topics", "postedAtFormatted": "Monday, September 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Baseline%20of%20my%20opinion%20on%20LW%20topics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABaseline%20of%20my%20opinion%20on%20LW%20topics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtHv3gb3cwPg5EJxSx%2Fbaseline-of-my-opinion-on-lw-topics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Baseline%20of%20my%20opinion%20on%20LW%20topics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtHv3gb3cwPg5EJxSx%2Fbaseline-of-my-opinion-on-lw-topics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtHv3gb3cwPg5EJxSx%2Fbaseline-of-my-opinion-on-lw-topics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1876, "htmlBody": "<p>To avoid repeatedly saying the same, I'd like to state my opinion on a few topics I expect to be relevant to my future posts here.</p><p>You can take it as a baseline or reference for these topics. I do not plan to go into any detail here, and I will not state all my reasons or sources. You may ask for separate posts if you are interested. This is only to provide a context for my comments and posts elsewhere.</p><p>If you google me, you may find some of my old (but not that off the mark) posts about these positions, e.g., here:</p><p><a href=\"http://grault.net/adjunct/index.cgi?GunnarZarncke/MyWorldView\">http://grault.net/adjunct/index.cgi?GunnarZarncke/MyWorldView</a></p><p>Now my position on LW topics.&nbsp;</p><h2>The Simulation Argument and The Great Filter</h2><p>On The Simulation Argument, I go for&nbsp;</p><p>\"(1) the human species is very likely to go extinct before reaching a \"posthuman\" stage.\"</p><p>Correspondingly on The Great Filter, I go for failure to reach&nbsp;</p><p>\"9. Colonization explosion\".</p><p>This is not because I think humanity will self-annihilate soon (though this is a possibility). Instead, I hope that humanity will earlier or later come to terms with its planet. My utopia could be like The Pacifists (<a href=\"http://www.phantastik-couch.de/hans-joachim-alpers-analog-5.html\">a short story in Analog 5</a>).</p><p>Why? Because of essential complexity limits.</p><p>This falls into the same range as&nbsp;\"<a href=\"http://en.wikipedia.org/wiki/Fermi_paradox#It_is_too_expensive_to_spread_physically_throughout_the_galaxy\">It is too expensive to spread physically throughout the galaxy.\"</a> I know that negative proofs about engineering are notoriously wrong - but that is my best guess. Simplified, one could say that the low-hanging fruits have been taken, and I have empirical evidence on multiple levels to support this view.</p><p>Correspondingly there is no singularity because progress is not limited by raw thinking speed but by effective aggregate thinking speed and physical feedback. &nbsp;</p><p>What could prove me wrong?&nbsp;</p><p>If a serious discussion would ruin my well-prepared arguments and evidence to shreds (quite possible).</p><p>At the very high end, a singularity might be possible if one could find a way to simulate physics faster than physics itself (trading physical space for time).&nbsp;</p><h2>AI</h2><p>I don't have the least problem with artificial intelligence or artificial emotion being possible. Philosophical note: I don't care on what substrate my consciousness runs, and maybe I am simulated. &nbsp;</p><p>I think strong AI is possible and maybe not that far away.</p><p>But I also don't think this will bring the singularity because of the complexity limits mentioned above. Strong AI will speed up some cognitive tasks with compound interest - but only until the physical feedback level is reached. Or a social feedback level is reached if AI should be designed to be so.</p><p>One temporary dystopia that I see is that cognitive tasks are outsourced to AI, and a new round of unemployment drives humans into depression.&nbsp;</p><p>I have studied artificial intelligence and played around with two models a long time ago:</p><ol><li>A simplified layered model of the brain; deep learning applied to free inputs (I canceled this when it became clear that it was too simple and low level and thus computationally inefficient)</li><li>A nested semantic graph approach with the propagation of symbol patterns representing thought (only concept; not realized)</li></ol><p>I'd like to try a 'synthesis' of these where microstructure-of-cognition like activation patterns of multiple deep learning networks are combined with a specialized language and pragmatics structure acquisition model a la <a href=\"https://www.google.de/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;ved=0CEIQFjAB&amp;url=http%3A%2F%2Fkybele.psych.cornell.edu%2F~edelman%2FSolanHornRuppinEdelman-PNAS05.pdf&amp;ei=NJUjUrCzENOGswbvl4GYBA&amp;usg=AFQjCNFP5zzLo89mSW-uP51CO1qGG-gTHQ&amp;sig2=PRMywVQNOyPUhkoTbUVumQ&amp;bvm=bv.51495398,d.Yms\">Unsupervised learning of natural languages</a>. See my opinion on cognition below for more in this line.</p><p>What could prove me wrong?</p><p>On the low success end, if it takes longer than I think it would take me given unlimited funding.&nbsp;</p><p>On the high end, if I'm wrong with the complexity limits mentioned above.&nbsp;</p><h2>Conquering space</h2><p>Humanity might succeed at leaving the planet but at high costs.</p><p>By leaving the planet, I mean permanently independent of Earth but not necessarily leaving the solar system any time soon (speculating on that is beyond my confidence interval).</p><p>I think it more likely that <i>life</i> leaves the planet - that can be&nbsp;</p><ol><li>artificial intelligence with a robotic body - think of curiosity rover 2.0 (most likely).</li><li>intelligent life-forms bred for life in space - think of Magpies, those are already smart, small, reproducing fast, and have 3D navigation.</li><li>actual humans in a suitable protective environment with small autonomous biospheres harvesting asteroids or mars.</li><li>'cyborgs' - humans altered or bred to better deal with problems in space like radiation and missing gravity.</li><li>other - including misc ideas from science fiction (least likely or latest).</li></ol><p>For most of these (esp. those depending on breeding), I'd estimate a time range of a few thousand years.</p><p>What could prove me wrong?</p><p>If I'm wrong on the singularity aspect too.</p><p>If I'm wrong on the timeline, I will be long dead, likely in any case except (1), which I expect to see in my lifetime.</p><h2>Cognitive Base of Rationality, Vagueness, Foundations of Math</h2><p>How can we as humans create meaning out of noise?</p><p>How can we know truth? How does it come that we know that '<a href=\"/lw/jl/what_is_evidence/\">snow is white' when snow is white</a>?</p><p>Cognitive neuroscience and artificial learning seem to point toward two aspects:</p><p><strong>Fuzzy learning aspect</strong></p><p>Correlated patterns of internal and external perception are recognized (detected) via multiple specialized layered neural nets (basically). This yields qualia like 'spoon', 'fear', 'running', 'hot', 'near', 'I'. These are symbols, but they are vague with respect to meaning because they result from a recognition process that optimizes for matching, not correctness or uniqueness.</p><p><strong>Semantic learning aspect</strong></p><p>Upon the qualia builds the semantic part, which takes the qualia and, instead of acting directly on them (as is the normal effect for animals), finds patterns in their activation which is not related to immediate perception or action but at most to memory. These may form new qualia/symbols.</p><p>The use of these patterns is that the patterns allow capturing concepts that are detached from reality (detached in so far as they do not need a stimulus connected in any way to perception).</p><p>Concepts like ('cry-sound' 'fear') or ('digitalis' 'time-forward' 'heartache') or ('snow' 'white') or - and that is probably the demain of humans: (('one' 'successor') 'two') or (('I' 'happy') ('I' 'think')). &nbsp;</p><p><strong>Concepts</strong></p><p>Learning works on these concepts like on the normal neuronal nets too. Thus concepts reinforced by positive feedback will stabilize, and mutually with them the qualia they derive from (if any) will also stabilize.</p><p>For certain pure concepts, the usability of the concept hinges not on any external factor (like \"how does this help me survive\") but on social feedback about structure and the process of the formation of the concepts themselves.&nbsp;</p><p>And this is where we arrive at such concepts as 'truth' or 'proposition.'</p><p>These are no longer vague - but not because they are represented differently in the brain than other concepts but because they stabilize toward maximized validity (that is, stability due to the absence of external factors, possibly with a speed-up due to social pressure to stabilize). I have written elsewhere that everything that derives its utility not from external use but internal consistency could be called math.</p><p>And that is why math is so hard for some: If you never gained a sufficient core of self-consistent stabilized concepts and/or the usefulness doesn't derive from internal consistency, but from external (\"<a href=\"/lw/iq/guessing_the_teachers_password/\">teachers password\"</a>) usefulness, then it will just not scale to more concepts (and the reason why science works at all is that science values internal consistency so high and there is little more dangerous to science that allowing other incentives).</p><p>I hope that this all makes sense. I haven't summarized this for quite some time.</p><p>A few random links that may provide some context:</p><p><a href=\"http://www.blutner.de/NeuralNets/\">http://www.blutner.de/NeuralNets/</a> (this is about the AI context we are talking about)</p><p><a href=\"http://www.blutner.de/NeuralNets/Texts/mod_comp_by_dyn_bin_synf.pdf\">http://www.blutner.de/NeuralNets/Texts/mod_comp_by_dyn_bin_synf.pdf</a> (research applicable to the above in particular)&nbsp;</p><p><a href=\"http://c2.com/cgi/wiki?LeibnizianDefinitionOfConsciousness\">http://c2.com/cgi/wiki?LeibnizianDefinitionOfConsciousness</a> (funny description of levels of consciousness)</p><p><a href=\"http://c2.com/cgi/wiki?FuzzyAndSymbolicLearning\">http://c2.com/cgi/wiki?FuzzyAndSymbolicLearning</a> (old post by me)</p><p><a href=\"http://grault.net/adjunct/index.cgi?VaguesDependingOnVagues\">http://grault.net/adjunct/index.cgi?VaguesDependingOnVagues</a> (dito)</p><p>Note: Details about the modeling of the semantic part are mostly in my head.&nbsp;</p><p>What could prove me wrong?</p><p>Well. Wrong is too hard here. This is just my model and it is not really that concrete. A longer discussion with someone more experienced with AI than I am (and there should be many here) might suffice to rip this apart (provided that I'd find time to prepare my model suitably).&nbsp;</p><h2>God and Religion</h2><p>I wasn't indoctrinated as a child. My truly loving mother is a baptized Christian living it and not being sanctimony. She always hoped that I would receive my epiphany. My father has a scientifically influenced personal Christian belief.&nbsp;</p><p>I can <i>imagine</i> a God consistent with science on the one hand and on the other hand with free will, soul, afterlife, trinity, and the bible (understood as a mix of non-literal word of God and history tale).</p><p>It is not that hard if you can imagine a timeless (simulation of) the universe. If you are god and have whatever plan on Earth but empathize with your creations, then it is not hard to add a few more constraints to certain aggregates called existences or 'person lifes.' Constraints that realize free will in the sense of 'not subject to the whole universe plan satisfaction algorithm.' &nbsp;</p><p>Surely not more difficult than consistent time-travel.</p><p>And souls and the afterlife should be easy to envision for any science fiction reader familiar with superintelligences.</p><p>But why? Occams razor applies.&nbsp;</p><p>There could be a God. And his promise could be real. And it could be a story seeded by an empathizing God - but also a 'human' God with his own inconsistencies and moods.</p><p>But it also could be that this is all a fairy tale run amok in human brains searching for explanations where there are none. A mass delusion. A fixated meme.</p><p>Which is right? It is difficult to put probabilities to stories. I see that I have slowly moved from 50/50 agnosticism to tolerant atheism.</p><p>I can't say that I wait for my epiphany. I know too well that my brain will happily find patterns when I let it. But I have encouraged to pray for me.</p><p>My epiphanies - the aha feelings of clarity that I did experience - have all been about deeply connected patterns building on other such patterns building on reliable facts mostly scientific in nature.</p><p>But I haven't lost my morality. It has deepened and widened, and I have become even more tolerant (I hope).&nbsp;</p><p>So if God does, against all odds, exist, I hope he will understand my doubts, weigh my good deeds and forgive me. You could tag me godless Christian.&nbsp;</p><p>What could prove me wrong?&nbsp;</p><p>On the atheist side, I could be moved further by more evidence of religion being a human artifact. &nbsp;&nbsp;</p><p>On the theist side, there are two possible avenues:</p><ol><li>If I'd have an unsearched for an epiphany - a real one where I can't say I was hallucinating but, e.g., a major consistent insight or a proof of God.</li><li>If I'd be convinced that the singularity is possible. This is because I'd need to update toward being in a simulation as per Simulation argument option 3. That's because then the next likely explanation for all this god business is actually some imperfect being running the simulation.</li></ol><p>Thus I'd like to close with this corollary to the simulation argument:</p><p><strong>Arguments for the singularity are also (weak) arguments for theism.</strong></p><p>Note: I am aware that this long post of controversial opinions unsupported by evidence (in this post) is bound to draw flak. That is the reason I post it in Comments, lest my small karma is lost completely. I have to repeat that this is meant as context and that I want to elaborate on these points on LW in due time with more and better-organized evidence.</p><p>Edited: Fixed more typos.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tHv3gb3cwPg5EJxSx", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 8, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "23981", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>To avoid repeatedly saying the same, I'd like to state my opinion on a few topics I expect to be relevant to my future posts here.</p><p>You can take it as a baseline or reference for these topics. I do not plan to go into any detail here, and I will not state all my reasons or sources. You may ask for separate posts if you are interested. This is only to provide a context for my comments and posts elsewhere.</p><p>If you google me, you may find some of my old (but not that off the mark) posts about these positions, e.g., here:</p><p><a href=\"http://grault.net/adjunct/index.cgi?GunnarZarncke/MyWorldView\">http://grault.net/adjunct/index.cgi?GunnarZarncke/MyWorldView</a></p><p>Now my position on LW topics.&nbsp;</p><h2 id=\"The_Simulation_Argument_and_The_Great_Filter\">The Simulation Argument and The Great Filter</h2><p>On The Simulation Argument, I go for&nbsp;</p><p>\"(1) the human species is very likely to go extinct before reaching a \"posthuman\" stage.\"</p><p>Correspondingly on The Great Filter, I go for failure to reach&nbsp;</p><p>\"9. Colonization explosion\".</p><p>This is not because I think humanity will self-annihilate soon (though this is a possibility). Instead, I hope that humanity will earlier or later come to terms with its planet. My utopia could be like The Pacifists (<a href=\"http://www.phantastik-couch.de/hans-joachim-alpers-analog-5.html\">a short story in Analog 5</a>).</p><p>Why? Because of essential complexity limits.</p><p>This falls into the same range as&nbsp;\"<a href=\"http://en.wikipedia.org/wiki/Fermi_paradox#It_is_too_expensive_to_spread_physically_throughout_the_galaxy\">It is too expensive to spread physically throughout the galaxy.\"</a> I know that negative proofs about engineering are notoriously wrong - but that is my best guess. Simplified, one could say that the low-hanging fruits have been taken, and I have empirical evidence on multiple levels to support this view.</p><p>Correspondingly there is no singularity because progress is not limited by raw thinking speed but by effective aggregate thinking speed and physical feedback. &nbsp;</p><p>What could prove me wrong?&nbsp;</p><p>If a serious discussion would ruin my well-prepared arguments and evidence to shreds (quite possible).</p><p>At the very high end, a singularity might be possible if one could find a way to simulate physics faster than physics itself (trading physical space for time).&nbsp;</p><h2 id=\"AI\">AI</h2><p>I don't have the least problem with artificial intelligence or artificial emotion being possible. Philosophical note: I don't care on what substrate my consciousness runs, and maybe I am simulated. &nbsp;</p><p>I think strong AI is possible and maybe not that far away.</p><p>But I also don't think this will bring the singularity because of the complexity limits mentioned above. Strong AI will speed up some cognitive tasks with compound interest - but only until the physical feedback level is reached. Or a social feedback level is reached if AI should be designed to be so.</p><p>One temporary dystopia that I see is that cognitive tasks are outsourced to AI, and a new round of unemployment drives humans into depression.&nbsp;</p><p>I have studied artificial intelligence and played around with two models a long time ago:</p><ol><li>A simplified layered model of the brain; deep learning applied to free inputs (I canceled this when it became clear that it was too simple and low level and thus computationally inefficient)</li><li>A nested semantic graph approach with the propagation of symbol patterns representing thought (only concept; not realized)</li></ol><p>I'd like to try a 'synthesis' of these where microstructure-of-cognition like activation patterns of multiple deep learning networks are combined with a specialized language and pragmatics structure acquisition model a la <a href=\"https://www.google.de/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;ved=0CEIQFjAB&amp;url=http%3A%2F%2Fkybele.psych.cornell.edu%2F~edelman%2FSolanHornRuppinEdelman-PNAS05.pdf&amp;ei=NJUjUrCzENOGswbvl4GYBA&amp;usg=AFQjCNFP5zzLo89mSW-uP51CO1qGG-gTHQ&amp;sig2=PRMywVQNOyPUhkoTbUVumQ&amp;bvm=bv.51495398,d.Yms\">Unsupervised learning of natural languages</a>. See my opinion on cognition below for more in this line.</p><p>What could prove me wrong?</p><p>On the low success end, if it takes longer than I think it would take me given unlimited funding.&nbsp;</p><p>On the high end, if I'm wrong with the complexity limits mentioned above.&nbsp;</p><h2 id=\"Conquering_space\">Conquering space</h2><p>Humanity might succeed at leaving the planet but at high costs.</p><p>By leaving the planet, I mean permanently independent of Earth but not necessarily leaving the solar system any time soon (speculating on that is beyond my confidence interval).</p><p>I think it more likely that <i>life</i> leaves the planet - that can be&nbsp;</p><ol><li>artificial intelligence with a robotic body - think of curiosity rover 2.0 (most likely).</li><li>intelligent life-forms bred for life in space - think of Magpies, those are already smart, small, reproducing fast, and have 3D navigation.</li><li>actual humans in a suitable protective environment with small autonomous biospheres harvesting asteroids or mars.</li><li>'cyborgs' - humans altered or bred to better deal with problems in space like radiation and missing gravity.</li><li>other - including misc ideas from science fiction (least likely or latest).</li></ol><p>For most of these (esp. those depending on breeding), I'd estimate a time range of a few thousand years.</p><p>What could prove me wrong?</p><p>If I'm wrong on the singularity aspect too.</p><p>If I'm wrong on the timeline, I will be long dead, likely in any case except (1), which I expect to see in my lifetime.</p><h2 id=\"Cognitive_Base_of_Rationality__Vagueness__Foundations_of_Math\">Cognitive Base of Rationality, Vagueness, Foundations of Math</h2><p>How can we as humans create meaning out of noise?</p><p>How can we know truth? How does it come that we know that '<a href=\"/lw/jl/what_is_evidence/\">snow is white' when snow is white</a>?</p><p>Cognitive neuroscience and artificial learning seem to point toward two aspects:</p><p><strong id=\"Fuzzy_learning_aspect\">Fuzzy learning aspect</strong></p><p>Correlated patterns of internal and external perception are recognized (detected) via multiple specialized layered neural nets (basically). This yields qualia like 'spoon', 'fear', 'running', 'hot', 'near', 'I'. These are symbols, but they are vague with respect to meaning because they result from a recognition process that optimizes for matching, not correctness or uniqueness.</p><p><strong id=\"Semantic_learning_aspect\">Semantic learning aspect</strong></p><p>Upon the qualia builds the semantic part, which takes the qualia and, instead of acting directly on them (as is the normal effect for animals), finds patterns in their activation which is not related to immediate perception or action but at most to memory. These may form new qualia/symbols.</p><p>The use of these patterns is that the patterns allow capturing concepts that are detached from reality (detached in so far as they do not need a stimulus connected in any way to perception).</p><p>Concepts like ('cry-sound' 'fear') or ('digitalis' 'time-forward' 'heartache') or ('snow' 'white') or - and that is probably the demain of humans: (('one' 'successor') 'two') or (('I' 'happy') ('I' 'think')). &nbsp;</p><p><strong id=\"Concepts\">Concepts</strong></p><p>Learning works on these concepts like on the normal neuronal nets too. Thus concepts reinforced by positive feedback will stabilize, and mutually with them the qualia they derive from (if any) will also stabilize.</p><p>For certain pure concepts, the usability of the concept hinges not on any external factor (like \"how does this help me survive\") but on social feedback about structure and the process of the formation of the concepts themselves.&nbsp;</p><p>And this is where we arrive at such concepts as 'truth' or 'proposition.'</p><p>These are no longer vague - but not because they are represented differently in the brain than other concepts but because they stabilize toward maximized validity (that is, stability due to the absence of external factors, possibly with a speed-up due to social pressure to stabilize). I have written elsewhere that everything that derives its utility not from external use but internal consistency could be called math.</p><p>And that is why math is so hard for some: If you never gained a sufficient core of self-consistent stabilized concepts and/or the usefulness doesn't derive from internal consistency, but from external (\"<a href=\"/lw/iq/guessing_the_teachers_password/\">teachers password\"</a>) usefulness, then it will just not scale to more concepts (and the reason why science works at all is that science values internal consistency so high and there is little more dangerous to science that allowing other incentives).</p><p>I hope that this all makes sense. I haven't summarized this for quite some time.</p><p>A few random links that may provide some context:</p><p><a href=\"http://www.blutner.de/NeuralNets/\">http://www.blutner.de/NeuralNets/</a> (this is about the AI context we are talking about)</p><p><a href=\"http://www.blutner.de/NeuralNets/Texts/mod_comp_by_dyn_bin_synf.pdf\">http://www.blutner.de/NeuralNets/Texts/mod_comp_by_dyn_bin_synf.pdf</a> (research applicable to the above in particular)&nbsp;</p><p><a href=\"http://c2.com/cgi/wiki?LeibnizianDefinitionOfConsciousness\">http://c2.com/cgi/wiki?LeibnizianDefinitionOfConsciousness</a> (funny description of levels of consciousness)</p><p><a href=\"http://c2.com/cgi/wiki?FuzzyAndSymbolicLearning\">http://c2.com/cgi/wiki?FuzzyAndSymbolicLearning</a> (old post by me)</p><p><a href=\"http://grault.net/adjunct/index.cgi?VaguesDependingOnVagues\">http://grault.net/adjunct/index.cgi?VaguesDependingOnVagues</a> (dito)</p><p>Note: Details about the modeling of the semantic part are mostly in my head.&nbsp;</p><p>What could prove me wrong?</p><p>Well. Wrong is too hard here. This is just my model and it is not really that concrete. A longer discussion with someone more experienced with AI than I am (and there should be many here) might suffice to rip this apart (provided that I'd find time to prepare my model suitably).&nbsp;</p><h2 id=\"God_and_Religion\">God and Religion</h2><p>I wasn't indoctrinated as a child. My truly loving mother is a baptized Christian living it and not being sanctimony. She always hoped that I would receive my epiphany. My father has a scientifically influenced personal Christian belief.&nbsp;</p><p>I can <i>imagine</i> a God consistent with science on the one hand and on the other hand with free will, soul, afterlife, trinity, and the bible (understood as a mix of non-literal word of God and history tale).</p><p>It is not that hard if you can imagine a timeless (simulation of) the universe. If you are god and have whatever plan on Earth but empathize with your creations, then it is not hard to add a few more constraints to certain aggregates called existences or 'person lifes.' Constraints that realize free will in the sense of 'not subject to the whole universe plan satisfaction algorithm.' &nbsp;</p><p>Surely not more difficult than consistent time-travel.</p><p>And souls and the afterlife should be easy to envision for any science fiction reader familiar with superintelligences.</p><p>But why? Occams razor applies.&nbsp;</p><p>There could be a God. And his promise could be real. And it could be a story seeded by an empathizing God - but also a 'human' God with his own inconsistencies and moods.</p><p>But it also could be that this is all a fairy tale run amok in human brains searching for explanations where there are none. A mass delusion. A fixated meme.</p><p>Which is right? It is difficult to put probabilities to stories. I see that I have slowly moved from 50/50 agnosticism to tolerant atheism.</p><p>I can't say that I wait for my epiphany. I know too well that my brain will happily find patterns when I let it. But I have encouraged to pray for me.</p><p>My epiphanies - the aha feelings of clarity that I did experience - have all been about deeply connected patterns building on other such patterns building on reliable facts mostly scientific in nature.</p><p>But I haven't lost my morality. It has deepened and widened, and I have become even more tolerant (I hope).&nbsp;</p><p>So if God does, against all odds, exist, I hope he will understand my doubts, weigh my good deeds and forgive me. You could tag me godless Christian.&nbsp;</p><p>What could prove me wrong?&nbsp;</p><p>On the atheist side, I could be moved further by more evidence of religion being a human artifact. &nbsp;&nbsp;</p><p>On the theist side, there are two possible avenues:</p><ol><li>If I'd have an unsearched for an epiphany - a real one where I can't say I was hallucinating but, e.g., a major consistent insight or a proof of God.</li><li>If I'd be convinced that the singularity is possible. This is because I'd need to update toward being in a simulation as per Simulation argument option 3. That's because then the next likely explanation for all this god business is actually some imperfect being running the simulation.</li></ol><p>Thus I'd like to close with this corollary to the simulation argument:</p><p><strong id=\"Arguments_for_the_singularity_are_also__weak__arguments_for_theism_\">Arguments for the singularity are also (weak) arguments for theism.</strong></p><p>Note: I am aware that this long post of controversial opinions unsupported by evidence (in this post) is bound to draw flak. That is the reason I post it in Comments, lest my small karma is lost completely. I have to repeat that this is meant as context and that I want to elaborate on these points on LW in due time with more and better-organized evidence.</p><p>Edited: Fixed more typos.</p>", "sections": [{"title": "The Simulation Argument and The Great Filter", "anchor": "The_Simulation_Argument_and_The_Great_Filter", "level": 1}, {"title": "AI", "anchor": "AI", "level": 1}, {"title": "Conquering space", "anchor": "Conquering_space", "level": 1}, {"title": "Cognitive Base of Rationality, Vagueness, Foundations of Math", "anchor": "Cognitive_Base_of_Rationality__Vagueness__Foundations_of_Math", "level": 1}, {"title": "Fuzzy learning aspect", "anchor": "Fuzzy_learning_aspect", "level": 2}, {"title": "Semantic learning aspect", "anchor": "Semantic_learning_aspect", "level": 2}, {"title": "Concepts", "anchor": "Concepts", "level": 2}, {"title": "God and Religion", "anchor": "God_and_Religion", "level": 1}, {"title": "Arguments for the singularity are also (weak) arguments for theism.", "anchor": "Arguments_for_the_singularity_are_also__weak__arguments_for_theism_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.2.0", "pingbacks": {"Posts": ["6s3xABaXKPdFwA3FS", "NMoLJuDJEms7Ku9XS"]}, "moderationGuidelinesVersion": "1.1.0", "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-02T14:07:55.794Z", "modifiedAt": null, "url": null, "title": "Open thread, September 2-8, 2013", "slug": "open-thread-september-2-8-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:34.648Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bqLC3AQuKCdccsccg/open-thread-september-2-8-2013", "pageUrlRelative": "/posts/bqLC3AQuKCdccsccg/open-thread-september-2-8-2013", "linkUrl": "https://www.lesswrong.com/posts/bqLC3AQuKCdccsccg/open-thread-september-2-8-2013", "postedAtFormatted": "Monday, September 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20September%202-8%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20September%202-8%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbqLC3AQuKCdccsccg%2Fopen-thread-september-2-8-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20September%202-8%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbqLC3AQuKCdccsccg%2Fopen-thread-september-2-8-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbqLC3AQuKCdccsccg%2Fopen-thread-september-2-8-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<div id=\"entry_t3_icj\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_ib0\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_i93\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p>If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bqLC3AQuKCdccsccg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 3, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "23982", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 379, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-02T15:37:15.211Z", "modifiedAt": null, "url": null, "title": "Book Review: Heuristics and Biases (MIRI course list)", "slug": "book-review-heuristics-and-biases-miri-course-list", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:05.237Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gc6foBcbozvEJ3HbG/book-review-heuristics-and-biases-miri-course-list", "pageUrlRelative": "/posts/gc6foBcbozvEJ3HbG/book-review-heuristics-and-biases-miri-course-list", "linkUrl": "https://www.lesswrong.com/posts/gc6foBcbozvEJ3HbG/book-review-heuristics-and-biases-miri-course-list", "postedAtFormatted": "Monday, September 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Book%20Review%3A%20Heuristics%20and%20Biases%20(MIRI%20course%20list)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABook%20Review%3A%20Heuristics%20and%20Biases%20(MIRI%20course%20list)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgc6foBcbozvEJ3HbG%2Fbook-review-heuristics-and-biases-miri-course-list%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Book%20Review%3A%20Heuristics%20and%20Biases%20(MIRI%20course%20list)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgc6foBcbozvEJ3HbG%2Fbook-review-heuristics-and-biases-miri-course-list", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgc6foBcbozvEJ3HbG%2Fbook-review-heuristics-and-biases-miri-course-list", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6007, "htmlBody": "<p>I'm <a href=\"/lw/iht/miri_course_list_book_reviews_part_1_g%C3%B6del_escher/\">reviewing</a> the books on the <a href=\"http://intelligence.org/courses/\">MIRI course list</a>.</p>\n<p>Upon deciding to read the course list, the first book I picked up was <em>Heuristics and Biases</em>. It's a tome of 42 papers in psychology detailing a vast array of human biases. This post constitutes a review of the book in general, as well as a brief summary of each paper.</p>\n<p>This review includes a number of biases not covered by the Sequences.</p>\n<h1 id=\"heuristicsandbiasesthepsychologyofintuitivejudgement\">Heuristics and Biases: The Psychology of Intuitive Judgement</h1>\n<p style=\"text-align:center\"><img style=\"vertical-align: middle;margin: 0 auto;\" src=\"http://assets.cambridge.org/97805217/96798/cover/9780521796798.jpg\" alt=\"Heuristics and Biases cover\" width=\"180\" height=\"272\" /></p>\n<p>This book is <em>dry</em>. Dry like old sand. On Venus. When the sun's out.</p>\n<p>I'd never had my mind blown by text so dry before.</p>\n<p>Most of the 42 papers introduced studies that revealed cognitive biases. A few other papers summarized and discussed results, proposing models that could explain many biases at once. Each paper came with lots of data.</p>\n<p>The book had two main themes. The first was an exploration of heuristics and biases. The second was an argument that biases are not secretly great.</p>\n<p>There's a segment of the psychologist population who argue that biases are not actually failures but instead the result of contrived experiments. Researchers are just looking at people wrong.</p>\n<p>This book dedicates a handful of papers to tearing such arguments apart (via compelling biases and studies on real world data). There was one particular quote that stuck with me (which I'll have to paraphrase, since I lent the book out a few days ago):</p>\n<blockquote>\n<p>People in the human-optimality camp seem to think there is only one type of error in human judgement, that being the error of psychologists when they design tests.</p>\n</blockquote>\n<p>I was sort of hoping for a paper studying the bias of psychologists as they design tests for biases, just so I could read the fallout.</p>\n<p>But I digress. Such arguments didn't interest me much, as I came to the table ready and willing to believe that the brain has many pathological failure modes.</p>\n<p>Fortunately, the main focus of the book was the varied ways in which brains fail at simple tasks. Many of the biases within are famous or were discussed in the sequences. In those cases, it was good to see the actual experimental setups and the real data. Many other biases were completely new to me.</p>\n<p>The high points of each chapter are summarized below. I've marked the most salient chapters with green numbers (in the table of contents) and green headers (in the text).</p>\n<p><a id=\"more\"></a></p>\n<h2>Table of Contents<br /></h2>\n<ol>\n<li><a href=\"#1extensionalvsintuitivereasoningtheconjunctionfallacyinprobabilityjudgement\">Extensional vs Intuitive reasoning: The Conjunction Fallacy in Probability Judgement</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor0077002representativenessrevisitedattributesubstitutioninintuitivejudgementspan\">Representativeness Revisited: Attribute Substitution in Intuitive Judgement</a></li>\n<li><a href=\"#3howalikeisitversushowlikelyisitadisjunctionfallacyinprobabilityjudgements\">How Alike Is It? versus How Likely Is It?: A Disjunction Fallacy in Probability Judgements</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor0077004imaginingcanheightenorlowertheperceivedlikelihoodofcontractingadiseasethemediatingeffectofeaseofimageryspan\">Imagining Can Heighten or Lower the Perceived Likelihood of Contracting a Disease: The Mediating Effect of Ease of Imagery</a></li>\n<li><a href=\"#5theavailabilityheuristicrevisitedeaseofrecallandcontentofrecallasdistinctsourcesofinformation\">The Availability Heuristic Revisited: Ease of Recall and Content of Recall as Distinct Sources of Information</a></li>\n<li><a href=\"#6incorporatingtheirrelevantanchorsinjudgementsofbeliefandvalue\">Incorporating the Irrelevant: Anchors in Judgements of Belief and Value.</a></li>\n<li><a href=\"#7puttingadjustmentbackintheanchoringandadjustmentheuristic\">Putting Adjustment Back in the Anchoring and Adjustment heuristic</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor0077008selfanchoringinconversationwhylanguageusersdonotdowhattheyshouldspan\">Self-Anchoring in Conversation: Why Language Users Do Not Do What They \"Should\"</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor0077009inferentialcorrectionspan\">Inferential Correction</a></li>\n<li><a href=\"#10mentalcontaminationandthedebiasingproblem\">Mental Contamination and the Debiasing Problem</a></li>\n<li><a href=\"#11sympatheticmagicalthinkingthecontagionandsimilarityheuristics\">Sympathetic Magical Thinking: The Contagion and Similarity \"Heuristics\"</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770012compatibilityeffectsinjudgementandchoicespan\">Compatibility Effects in Judgement and Choice</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770013theweightingofevidenceandthedeterminantsofconfidencespan\">The Weighting of Evidence and the Determinants of Confidence</a></li>\n<li><a href=\"#14insidetheplanningfallacythecausesandconsequencesofoptimistictimepredictions\">Inside the Planning Fallacy: The Causes and Consequences of Optimistic Time Predictions</a></li>\n<li><a href=\"#15probabilityjudgementacrosscultures\">Probability Judgement Across Cultures</a></li>\n<li><a href=\"#16durabilitybiasinaffectiveforecasting\">Durability Bias in Affective Forecasting</a></li>\n<li><a href=\"#17resistanceofpersonalriskperceptionstodebiasinginterventions\">Resistance of Personal Risk Perceptions to Debiasing Interventions</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770018ambiguityandselfevaluationtheroleofidiosyncratictraitdefinitionsinselfservingassesmentsofabilityspan\">Ambiguity and Self-Evaluation: The Role of Idiosyncratic Trait Definitions in Self-Serving Assesments of Ability</a></li>\n<li><a href=\"#19whenpredictionsfailthedilemmaofunrealisticoptimism\">When Predictions Fail: The Dilemma of Unrealistic Optimism</a></li>\n<li><a href=\"#20normtheorycomparingrealitytoitsalternatives\">Norm Theory: Comparing Reality to its Alternatives</a></li>\n<li><a href=\"#21counterfactualthoughtregretandsuperstitionhowtoavoidkickingyourself\">Counterfactual Thought, Regret, and Superstition: How To Avoid Kicking Yourself</a></li>\n<li><a href=\"#22twosystemsofreasoning\">Two Systems of Reasoning</a></li>\n<li><a href=\"#23theaffectheuristic\">The Affect Heuristic</a></li>\n<li><a href=\"#24individualdifferencesinreasoningimplicationsfortherationalitydebate\">Individual Differences in Reasoning: Implications for the Rationality Debate?</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770025supporttheoryanonextensionalrepresentationofsubjectiveprobabilityspan\">Support Theory: A Nonextensional Representation of Subjective Probability</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770026unpackingrepackingandanchoringadvancesinsupporttheoryspan\">Unpacking, Repacking, and Anchoring: Advances in Support Theory</a></li>\n<li><a href=\"#27remarksonsupporttheoryrecentadvancesandfuturedirections\">Remarks on Support Theory: Recent Advances and Future Directions</a></li>\n<li><a href=\"#28theuseofstatisticalheuristicsineverydayinductivereasoning\">The Use of Statistical Heuristics in Everyday Inductive Reasoning</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770029feelingsasinformationmoodsinfluencejudgementsandprocessingstrategiesspan\">Feelings as Information: Moods Influence Judgements and Processing Strategies</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770030automatedchoiceheuristicsspan\">Automated Choice Heuristics</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770031howgoodarefastandfrugalheuristicsspan\">How Good are Fast and Frugal Heuristics?</a></li>\n<li><a href=\"#32intuitivepoliticianstheologiansandprosecutorsexploringtheempiricalimplicationsofdeviantfunctionalistmetaphors\">Intuitive Politicians, Theologians, and Prosecutors: Exploring the Empirical Implications of Deviant Functionalist Metaphors</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770033thehothandinbasketballonthemispredictionofrandomsequencesspan\">The Hot Hand in Basketball: On the Misprediction of Random Sequences</a></li>\n<li><a href=\"#34likegoeswithliketheroleofrepresentativenessinerroneousandpesudoscientificbeliefs\">Like Goes With Like: The Role of Representativeness in Erroneous and Pesudo-Scientific Beliefs</a></li>\n<li><a href=\"#35whenlessismorecounterfactualthinkingandsatisfactionamongolympicmedalists\">When Less is More: Counterfactual thinking and Satisfaction among Olympic Medalists</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770036understandingmisunderstandingssocialpsychologicalperspectivesspan\">Understanding Misunderstandings: Social Psychological Perspectives</a></li>\n<li><a href=\"#37assessinguncertaintyinphysicalconstants\">Assessing Uncertainty in Physical Constants</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770038doanalystsoverreactspan\">Do Analysts Overreact?</a></li>\n<li><a href=\"#39thecalibrationofexpertjudgementheuristicsandbiasesbeyondthelaboratory\">The Calibration of Expert Judgement: Heuristics and Biases Beyond the Laboratory</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770040clinicalversusactuarialjudgementspan\">Clinical versus Actuarial Judgement</a></li>\n<li><a href=\"#41heuristicsandbiasesinapplication\">Heuristics and Biases in Application</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770042theorydrivenreasoningaboutplausiblepastsandprobablefuturesinworldpoliticsspan\">Theory-Driven Reasoning about Plausible Pasts and Probable Futures in World Politics</a></li>\n</ol>\n<h3 id=\"1extensionalvsintuitivereasoningtheconjunctionfallacyinprobabilityjudgement\">1. Extensional vs Intuitive reasoning: The Conjunction Fallacy in Probability Judgement</h3>\n<p>This chapter mainly covered the <a href=\"/lw/ji/conjunction_fallacy/\">conjunction fallacy</a>: the phenomenon where people think a bookish introvert is more likely to be an accountant who plays jazz on the side than someone (in general) who plays jazz on the side.</p>\n<p>It also generalized the conjunction fallacy by introducing a \"narrative effect\": events seem more likely if you are given an example of how they could occur.</p>\n<p>For instance, given a description of a scummy employer, people rate \"He killed an employee\" as less likely than \"He killed an employee to stop them from going to the police\".</p>\n<p><strong>Suggestion</strong>: Whenever you're presented with a hypothetical, generate at least one (preferably more) explanations for how it could occur (unpack hypotheticals manually).</p>\n<h3 id=\"spanstylecolor0077002representativenessrevisitedattributesubstitutioninintuitivejudgementspan\"><span style=\"color:#007700\"> \n<hr />\n<a id=\"more\"></a>2. Representativeness Revisited: Attribute Substitution in Intuitive Judgement</span></h3>\n<p>Your intuition's likelihood rating is more a measure of representativeness than of probability.</p>\n<p>For example, if you give people five suspects in a murder mystery, with little evidence for each, then people will give low probabilities that each individual committed the crime. If you then give each of them a motive, then people will give higher probabilities <em>for all suspects</em>.</p>\n<p>The number that your gut throws out isn't probability (which should be roughly 1/5 for each suspect in both cases). Rather, it's a function of argument strength (how much dirt you have on that one person).</p>\n<p>Note that people still rate any two people as equally likely, and that they rate each as 20% likely when asked to analyze all five at once. However, when people rate suspects individually, their likelihood ratings are a good predictor of how much dirt they have on the person (without taking other suspects into account).</p>\n<p>Further studies concluded that when you ask people for probability they give you a measure of how well the considered object (one suspect) represents the parent group (suspicious people).</p>\n<p>This theme emerges throughout the book in different forms.</p>\n<h3 id=\"3howalikeisitversushowlikelyisitadisjunctionfallacyinprobabilityjudgements\">\n<hr />\n<a id=\"more\"></a>3. How Alike Is It? versus How Likely Is It?: A Disjunction Fallacy in Probability Judgements</h3>\n<p>This was basically an exploration of the conjunction bias and representation theory.</p>\n<p>For starters, it flipped the conjunction bias around and examined the disjunction bias (the propensity to rate the likelihood of X higher than the likelihood of X &or; Y). It found a disjunction bias that was weaker than the conjunction bias, and then explored the bias in the light of representativeness. It exploited the fact that subcategories can appear more representative than parent categories. For example, a sparrow is a representative bird but not a very representative animal. They found that perceived likelihood was better predicted by representativeness than by actual likelihood.</p>\n<h3 id=\"spanstylecolor0077004imaginingcanheightenorlowertheperceivedlikelihoodofcontractingadiseasethemediatingeffectofeaseofimageryspan\"><span style=\"color:#007700\"> \n<hr />\n<a id=\"more\"></a>4. Imagining Can Heighten or Lower the Perceived Likelihood of Contracting a Disease: The Mediating Effect of Ease of Imagery</span></h3>\n<p>Things that are easy to imagine are judged to be more likely. If you ask people to imagine breaking an arm, they can do it easily, and their reported likelihood of breaking an arm goes up. But if you ask people to imagine an abstract genetic thyroid disease, they'll have trouble picturing it, and their reported likelihood of contracting the disease will go down.</p>\n<p><strong>Takeaway 1</strong>: Be careful about telling people to picture rare things. If they are hard to imagine, people might end up <em>less</em> worried after trying to imagine themselves as victims.</p>\n<p><strong>Takeaway 2</strong>: When you're assessing probabilities, be careful not to conflate \"easy to picture\" with \"likely to happen\".</p>\n<p>Your brain is wont to do this for you, so you should explicitly discount probabilities for easy-to-picture futures and explicitly increase probabilities for hard-to-picture ones.</p>\n<h3 id=\"5theavailabilityheuristicrevisitedeaseofrecallandcontentofrecallasdistinctsourcesofinformation\"><span style=\"font-size: 15px;\"> \n<hr />\n<a id=\"more\"></a>5. The Availability Heuristic Revisited: Ease of Recall and Content of Recall as Distinct Sources of Information</span></h3>\n<p>This study was pretty interesting. They gave people sheets of paper with grayed-out 't's on each line. They asked the subjects to think of words that started with the letter 't'.</p>\n<p>A control group was given no further instruction. The facilitated group was told that the gray 't's would facilitate. The inhibited group was told that the gray 't's would inhibit. Afterwards, each person was asked what proportion of words began with 't'.</p>\n<p>The facilitated group gave lower proportions than the control, the inhibited group gave higher.</p>\n<p>The conclusion was that people used the perceived difficulty as a source of input. All groups found it relatively easy to think of t-words. The facilitated group attributed some of this to the paper (and concluded that there were less t-words). The inhibited group concluded that t-words must be <em>really</em> prevalent, because even with the inhibiting paper they thought of a bunch of t-words. (Note that the paper was the same in all scenarios.)</p>\n<p>More generally, it seems that people treat ease-of-recall as a heuristic to measure prevalence-of-item.</p>\n<p>This heuristic is often good, but be wary. It's relatively easy to exploit this.</p>\n<h3 id=\"6incorporatingtheirrelevantanchorsinjudgementsofbeliefandvalue\">\n<hr />\n<a id=\"more\"></a>6. Incorporating the Irrelevant: Anchors in Judgements of Belief and Value.</h3>\n<p>This was a pretty standard introduction to <a href=\"/lw/j7/anchoring_and_adjustment/\">anchoring and adjustment</a>.</p>\n<h3 id=\"7puttingadjustmentbackintheanchoringandadjustmentheuristic\">\n<hr />\n<a id=\"more\"></a>7. Putting Adjustment Back in the Anchoring and Adjustment heuristic</h3>\n<p>Anchoring isn't just something that happens in your environment. You self-anchor. For example, when did George Washington become president? Most people self-anchor at 1776.</p>\n<h3 id=\"spanstylecolor0077008selfanchoringinconversationwhylanguageusersdonotdowhattheyshouldspan\"><span style=\"color:#007700\"> \n<hr />\n<a id=\"more\"></a>8. Self-Anchoring in Conversation: Why Language Users Do Not Do What They \"Should\"</span></h3>\n<p>Your brain doesn't auto-filter irrelevant data.</p>\n<p>There was a game where people had a bunch of boxes facing them with little items in them.</p>\n<p>A partner sat facing them. Some of the boxes were clearly open, so that the partner could see the contents. Some of the boxes were clearly closed, and this was made obvious to the subject.</p>\n<pre><code>---------\n| |6| | |\n---------\n| | |5| |\n---------\n|4| | | |\n---------\n</code></pre>\n<p>In this example, imagine that the 5 and 6 boxes are open but the 4-box is closed. (In other words, partner can see 5 and 6 but not 4, and the subject knows this.)</p>\n<p>The partner would give instructions like \"Move the low number one square to the left\". The subject would reach for the number 4 before self-correcting and reaching for the number 5 (the smallest number that the partner can see).</p>\n<p>Moral: your brain won't restrict itself to mutual information. You've got to do it manually. This wasn't too surprising to me until more studies built off of these results:</p>\n<p>Subjects who listened to the same voicemail were either told that the intent was sarcastic or sincere. Subjects on both sides thought it equally likely that a third observer would pick up the sarcastic/sincere intent &mdash; they failed to control for personal information.</p>\n<p>Subjects asked to tap out songs thought that a majority of people would know what song they were tapping out. Listeners guessed about 2% of the time. Tappers blamed this on poor attention.</p>\n<p>Another study observed speakers accidentally using information not available to an audience.</p>\n<p>The conclusion was basically that the brain automatically uses all of the information available. It requires conscious effort to restrict yourself to only mutual information. See also the <a href=\"/lw/ke/illusion_of_transparency_why_no_one_understands/\">Illusion of Transparency</a> and <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">Inferential Distance</a>.</p>\n<p><strong>Takeaway</strong>: Assume that intentions are opaque. Monitor your conversations to make sure you're restricting yourself to mutual information.</p>\n<h3 id=\"spanstylecolor0077009inferentialcorrectionspan\"><span style=\"color:#007700\"> \n<hr />\n<a id=\"more\"></a>9. Inferential Correction</span></h3>\n<p>This paper had three points. First, the <a href=\"/lw/lg/the_affect_heuristic/\">Affect Heuristic</a>. If you see a person acting anxious, you're more likely to conclude that they're an \"anxious person\" instead of that they're in an anxious situation.</p>\n<p>The second point is that you're more susceptible to this problem when you're under cognitive load (time pressure, busy with other tasks, etc.)</p>\n<p>Third and most terrifying is the study referenced in <a href=\"/lw/k4/do_we_believe_everything_were_told/\">this part of the sequences</a> &mdash; You believe everything you read unless you exert cognitive power to disbelieve things.</p>\n<p>In sum, there are many things that your brain does without your consent (jump to conclusions, believe what it reads) that require cognitive effort to undo.</p>\n<p><strong>Takaway</strong>: Any conclusions drawn while you're distracted should be thrown out. Make sure you're not under cognitive load before deciding something important.</p>\n<p>(Also known as the \"always masturbate before making relationship decisions\" rule.)</p>\n<h3 id=\"10mentalcontaminationandthedebiasingproblem\">\n<hr />\n<a id=\"more\"></a>10. Mental Contamination and the Debiasing Problem</h3>\n<p>This chapter was an exploration of debasing. The conclusion is that you have to control your exposure to stimuli that influence your responses. People are notoriously bad at de-biasing themselves. Once you're exposed to biasers (anchors, etc.) you're going to be biased.</p>\n<p>A poignant example was the fact that many psychologists who know about the halo effect still fail to grade papers blind, thinking that they can debias themselves.</p>\n<p><strong>Moral</strong>: Don't be that guy. Grade papers blind. Audition musicians behind a screen. Remove names from r&eacute;sum&eacute;s. Control your exposure.</p>\n<h3 id=\"11sympatheticmagicalthinkingthecontagionandsimilarityheuristics\">\n<hr />\n<a id=\"more\"></a>11. Sympathetic Magical Thinking: The Contagion and Similarity \"Heuristics\"</h3>\n<p>Would you drink a glass of orange juice after I dip a cockroach in it?</p>\n<p>Probably not, and understandably so: cockroaches can be dirty.</p>\n<p>But would you drink a glass of orange juice after I dip a sterilized cockroach in it?</p>\n<p>Your concern in the second case is hard to justify. It seems that humans still have a lot of \"magical\" thinking in them &mdash; touching gross things makes things gross.</p>\n<p>Furthermore, it appears that some people root their aversion in things like germ theory, whereas others root their aversion in fear. For example, most people don't want to wear a sweater worn by a person with AIDS. Washing it is sufficient to remove the malaise for some but not others.</p>\n<h3 id=\"spanstylecolor00770012compatibilityeffectsinjudgementandchoicespan\"><span style=\"color:#007700\"> \n<hr />\n<a id=\"more\"></a>12. Compatibility Effects in Judgement and Choice</span></h3>\n<p>If you ask people to rank bets then they will weight the odds too heavily. If you ask them to price bets, they'll weight the payoffs too heavily. Generally, compatible units get more weight.</p>\n<p>(As a corollary, anchors work much better when they share a unit with the thing being anchored.)</p>\n<p>This was simple but surprising.</p>\n<p><strong>Recommendation</strong>: Find ways to normalize things before assessing them. (For example, normalize bets to risk-adjusted expected payoffs.) If you try to let your gut do things, it will give additional weight to dimensions of the problem that are compatible with the question.</p>\n<h3 id=\"spanstylecolor00770013theweightingofevidenceandthedeterminantsofconfidencespan\"><span style=\"color:#007700\"> \n<hr />\n<a id=\"more\"></a>13. The Weighting of Evidence and the Determinants of Confidence</span></h3>\n<p>People basically only use argument strength (how much the evidence looks like the hypothesis) when judging likelihood. They completely ignore base rates (the prior probability) and disregard the weight of evidence (how much evidence there is).</p>\n<p>Base rate neglect is prevalent. Think back to the representativeness effect: when people analyze five suspects, the suspects seem more suspicious if they have motives. The assessed likelihood that each was the murderer goes up in proportion to the amount of dirt that people have, <em>even if everyone else is just as suspicious</em>. When you ask people to assess the likelihood that the janitor was the murderer, they think about reasons why the janitor is suspicious &mdash; <em>without regard for who else is also suspicious.</em></p>\n<p>This neglect is probably the biggest hurdle between most people and Bayesian reasoning. It's the root of doctors believing tests with low false positive rates despite high base rates, and many other deviations from \"rational\" reasoning.</p>\n<p>Argument weight neglect is also prevalent. People act like five coinflips (all heads) are good evidence that a coin is weighted towards heads, despite how small the sample size is. Jokingly, the authors note that people seem to believe in the \"law of small numbers\": that evidence reflects the data even in small amounts. Experts manage to factor in argument weight, but even they do not give it enough credence.</p>\n<p>Your brain treats evidence representationally: it's very good at measuring how well the evidence matches the hypothesis, and very bad at assessing how much the evidence should count for.</p>\n<p>Turns out your intuition just doesn't know how to take argument weight or base rates into account. You've got to do it manually.</p>\n<p><strong>Suggestion</strong>: When you're assessing likelihood, start with your intuitive likelihood. Treat that number as <em>argument strength</em>. Ignore it completely. Force yourself to bring the base rates to mind. Force yourself to consider the amount of evidence, not just the degree to which it looks good. Once you have the base rates and the evidence weight in mind, factor in your original number accordingly.</p>\n<p><em>You have to do this consciously</em>: Your brain just isn't going to do it for you.</p>\n<h3 id=\"14insidetheplanningfallacythecausesandconsequencesofoptimistictimepredictions\">\n<hr />\n<a id=\"more\"></a>14. Inside the Planning Fallacy: The Causes and Consequences of Optimistic Time Predictions</h3>\n<p><a href=\"/lw/jg/planning_fallacy/\">Inside views suck</a>. People's predictions are the same as their best case predictions. And when their predictions fail, people blame it on exceptional circumstances and fail to learn in the general case.</p>\n<p>In other words, when people miss a deadline because their computer crashed, they update to think that computer crashes were more likely, but fail to draw the more general conclusion that their estimates were too optimistic.</p>\n<p>This paper sowed the seeds for a later revelation about updating both your model and the likelihood that you should switch models.</p>\n<h3 id=\"15probabilityjudgementacrosscultures\">\n<hr />\n<a id=\"more\"></a>15. Probability Judgement Across Cultures</h3>\n<p>This paper basically showed that Asian people (China and Thailand, IIRC) are way more overconfident than American people, even though both groups expect the opposite to be true. It might have had some other points, but I forget. What I took away from this paper is that common knowledge can be very wrong: you've got to go out and collect real data.</p>\n<h3 id=\"16durabilitybiasinaffectiveforecasting\">\n<hr />\n<a id=\"more\"></a>16. Durability Bias in Affective Forecasting</h3>\n<p>You expect good/bad events to make you feel happy/sad for a long time. People expect that disabilities will make them sadder for longer than they actually do. People expect winning the lottery will make them happier for longer than it actually does.</p>\n<p>There was also a study showing that durability bias is stronger for bad events (i.e. bad-event-recovery happens faster than good-event-regression), perhaps due to some sort of \"psychological immune system\" that people deploy to deal with bad things after the fact (look at the bright side, etc.).</p>\n<h3 id=\"17resistanceofpersonalriskperceptionstodebiasinginterventions\">\n<hr />\n<a id=\"more\"></a>17. Resistance of Personal Risk Perceptions to Debiasing Interventions</h3>\n<p>People are hard to debias, despite many varied interventions.</p>\n<p>Seriously, the interventions here were many, varied, and clever. None of them worked.</p>\n<p><strong>Moral</strong>: Control your exposure.</p>\n<h3 id=\"spanstylecolor00770018ambiguityandselfevaluationtheroleofidiosyncratictraitdefinitionsinselfservingassesmentsofabilityspan\"><span style=\"color:#007700\"> \n<hr />\n<a id=\"more\"></a>18. Ambiguity and Self-Evaluation: The Role of Idiosyncratic Trait Definitions in Self-Serving Assesments of Ability</span></h3>\n<p>You've probably heard of the \"better than average\" effect &mdash; everyone thinks that they're better than average at driving/leadership/etc. What this paper showed was really cool, though: this effect is due in large part to the fact that everyone defines ambiguous attributes differently.</p>\n<p>For example, an organized person might define \"good leadership\" as organizational skills and the ability to keep things moving, whereas an outgoing person might define \"good leadership\" as the ability to inspire others and garner loyalty. It should come as no surprise, then, that everybody thinks they're better than average at \"leadership\".</p>\n<p>This paper had a study where half the subjects would define how an ambiguous trait was measured, and the other half would rate themselves according to that measurement. When this was done, the \"better than average\" effect disappeared.</p>\n<h3 id=\"19whenpredictionsfailthedilemmaofunrealisticoptimism\">\n<hr />\n<a id=\"more\"></a>19. When Predictions Fail: The Dilemma of Unrealistic Optimism</h3>\n<p>Turns out it's hard to reduce optimistic biases.</p>\n<p>This paper explored how optimistic biases can persist, given that you'd expect evolution to have weeded out overconfident people. (It's just you and me, Mr. Tiger.) But as it turns out, overconfidence bias melts away as events draw closer. This paper was a really interesting read.</p>\n<p>Basically, it showed how optimism bias is strongest when it's most useful (when it can act as a self-fulfilling prophecy) and disappears when it's harmful (people are often underconfident in the moment).</p>\n<h3 id=\"20normtheorycomparingrealitytoitsalternatives\">\n<hr />\n<a id=\"more\"></a>20. Norm Theory: Comparing Reality to its Alternatives</h3>\n<p>You think it's worse to miss a plane by 5 minutes than by 30 minutes. Some counterfactual worlds seem closer than others.</p>\n<h3 id=\"21counterfactualthoughtregretandsuperstitionhowtoavoidkickingyourself\">\n<hr />\n<a id=\"more\"></a>21. Counterfactual Thought, Regret, and Superstition: How To Avoid Kicking Yourself</h3>\n<p>It hurts more for bad things to happen in exceptional cases. It feels worse when someone is killed in an accident while taking the scenic route home for the first time than when they're killed in an accident during their daily routine.</p>\n<p>Given that this is the case, superstitions make sense.</p>\n<p><strong>Takaway</strong>: You will naturally be more outraged when bad things happen in exceptional cases. You probably need to manually scale down concern for the exceptional cases.</p>\n<h3 id=\"22twosystemsofreasoning\">\n<hr />\n<a id=\"more\"></a>22. Two Systems of Reasoning</h3>\n<p>Introduces System 1 (the subconscious processes that provide your intuitions) and System 2 (the conscious thinker that corrects System 1) and explores their strengths and weaknesses. Notes that all biases can be viewed as two failures: The failure of System 1, who used a heuristic that was not actually applicable, and the failure of System 2 to notice the error and correct it.</p>\n<h3 id=\"23theaffectheuristic\">\n<hr />\n<a id=\"more\"></a>23. The Affect Heuristic</h3>\n<p>Explicitly covered the <a href=\"/lw/lg/the_affect_heuristic/\">Affect Heuristic</a>. People were asked to rate bets. Each person would only see one of the below bets. Bet B was consistently rated better than bet A.</p>\n<p><strong>Bet A</strong>: 20%: Win 9$</p>\n<p><strong>Bet B</strong>: 20%: Win 9$, 80%: Lose 5&cent;</p>\n<p>A 9$ win looks much better compared to a 5&cent; loss than it does without context. This can be used to induce preference reversals.</p>\n<p>This chapter felt somewhat redundant at this point in the book.</p>\n<h3 id=\"24individualdifferencesinreasoningimplicationsfortherationalitydebate\">\n<hr />\n<a id=\"more\"></a>24. Individual Differences in Reasoning: Implications for the Rationality Debate?</h3>\n<p>This paper was mostly an argument against people who think that human biases are actually optimal and we're just looking at them wrong. It ruled out things like random error, contrived tests, and bad questions via a clever series of studies.</p>\n<h3 id=\"spanstylecolor00770025supporttheoryanonextensionalrepresentationofsubjectiveprobabilityspan\"><span style=\"color:#007700\"> \n<hr />\n<a id=\"more\"></a>25. Support Theory: A Nonextensional Representation of Subjective Probability</span></h3>\n<p>This paper was really cool. It basically said two things:</p>\n<ol>\n<li>Your brain doesn't assign likelihoods to events. It assigns likelihoods to descriptions. Different descriptions of the same event can yield wildly different probabilities.</li>\n<li>When you unpack an event, your subjective probabilities are subadditive. In other words, the subjective probability of A is consistently lower than the combined subjective probabilities of all of A's components.</li>\n</ol>\n<p>The remainder of the paper formally introduced this mathematical system and showed how it generalizes Representativeness and the Conjunction fallacy.</p>\n<p><strong>Takeaway</strong>: Normalize events before assessing their probability. This is hard, as it's not always obvious how events should be normalized.</p>\n<h3 id=\"spanstylecolor00770026unpackingrepackingandanchoringadvancesinsupporttheoryspan\"><span style=\"color:#007700\"> \n<hr />\n<a id=\"more\"></a>26. Unpacking, Repacking, and Anchoring: Advances in Support Theory</span></h3>\n<p>This paper was a deeper exploration of Support Theory (introduced in the preceding chapter). It mostly explored unpacking and repacking: the propensity for an argument to feel more likely as you consider more of its components. (This is a generalization of the narrative effect from earlier.)</p>\n<p>Basically, the likelihood you assign for a broad category (1000 people killed by floods in North America next year) is lower than the summed probability of its component parts (the likelihood of a flood caused by a California earthquake, caused by the eruption of Mt. Rainier, etc.).</p>\n<p>This bias is difficult to defend against, because people aren't going to ask you to rate both \"chance of a flood\" and \"chance of a flood caused by an earthquake in California\" at the same time. Somehow, you've got to know to unpack/repack arguments even when you're asked only one side of the question.</p>\n<p><strong>Recommendation</strong>: manually try to unpack/repack any scenarios that come your way before giving probability assessments.</p>\n<p>If someone asks you to assess the probability that the USA strikes Syria due to chemical weapons use, you should first try to repack this argument (what is the chance that the US strikes Syria?) and unpack this argument (What is the chance they strike Syria due to chemical weapons use by the rebels? What about by the government?) before answering.</p>\n<h3 id=\"27remarksonsupporttheoryrecentadvancesandfuturedirections\">\n<hr />\n<a id=\"more\"></a>27. Remarks on Support Theory: Recent Advances and Future Directions</h3>\n<p>This chapter felt a bit redundant. It tied more biases into the support theory model. I don't remember anything novel, but something could have slipped my mind. You could skip this chapter.</p>\n<h3 id=\"28theuseofstatisticalheuristicsineverydayinductivereasoning\">\n<hr />\n<a id=\"more\"></a>28. The Use of Statistical Heuristics in Everyday Inductive Reasoning</h3>\n<p>This paper was another one arguing against the human-optimality camp. Basically, the paper rejected the argument that statistical reasoning isn't applicable to human lives by pointing out that people reason statistically (and do better) when they've been trained in statistics.</p>\n<h3 id=\"spanstylecolor00770029feelingsasinformationmoodsinfluencejudgementsandprocessingstrategiesspan\"><span style=\"color:#007700\"> \n<hr />\n<a id=\"more\"></a>29. Feelings as Information: Moods Influence Judgements and Processing Strategies</span></h3>\n<p>Happy people are more overconfident. Sad people are more underconfident. <em>However</em>, having people list reasons for their current mood removes the effect of mood upon predictions: priming reasons why your mood is not related to the question at hand helps people remove mood bias.</p>\n<p><strong>Takeaway</strong>: Before making big decisions, list the reasons why your mood is what it is.</p>\n<h3 id=\"spanstylecolor00770030automatedchoiceheuristicsspan\"><span style=\"color:#007700\"> \n<hr />\n<a id=\"more\"></a>30. Automated Choice Heuristics</span></h3>\n<p>People tend to \"choose by liking\", taking the choice that looks better. This can lead to some serious problems. For example:</p>\n<p>I have a die. It has 2 green sides, 1 blue side, 2 red sides, and 1 yellow side. You get to choose between the two bets:</p>\n<p><strong>Bet A</strong></p>\n<ul>\n<li>Green or Blue: +20$</li>\n<li>Red: -10$</li>\n<li>Yellow: -10$</li>\n</ul>\n<p><strong>Bet B</strong></p>\n<ul>\n<li>Green: +20$</li>\n<li>Blue: +15$</li>\n<li>Red or Yellow: -10$</li>\n</ul>\n<p>Many people choose bet B over bet A.</p>\n<p>Now unpack the bets:</p>\n<p><strong>Bet A</strong></p>\n<ul>\n<li>Green: +20$</li>\n<li>Blue: +20$</li>\n<li>Red: -10$</li>\n<li>Yellow: -10$</li>\n</ul>\n<p><strong>Bet B</strong></p>\n<ul>\n<li>Green: +20$</li>\n<li>Blue: +15$</li>\n<li>Red: -10$</li>\n<li>Yellow: -10$</li>\n</ul>\n<p>In this format, it's clear that A dominates. Nobody picks B.</p>\n<p>I found this study astonishing.</p>\n<p><strong>Moral</strong>: Normalize options before choosing between them! Unpack things manually, and <em>stop trusting your brain to intuit probabilities.</em></p>\n<h3 id=\"spanstylecolor00770031howgoodarefastandfrugalheuristicsspan\"><span style=\"color:#007700\"> \n<hr />\n<a id=\"more\"></a>31. How Good are Fast and Frugal Heuristics?</span></h3>\n<p>Bayesianism is great. Yay Bayesianism. But it's expensive. Turns out that simple algorithms, like choosing according to a few binary attributes, are much faster and almost as good. This helps explain many human biases.</p>\n<p>This paper was an interesting read from a \"how might by brain be making these failures\" perspective, but didn't introduce any new biases.</p>\n<h3 id=\"32intuitivepoliticianstheologiansandprosecutorsexploringtheempiricalimplicationsofdeviantfunctionalistmetaphors\">\n<hr />\n<a id=\"more\"></a>32. Intuitive Politicians, Theologians, and Prosecutors: Exploring the Empirical Implications of Deviant Functionalist Metaphors</h3>\n<p>This paper basically argued that certain biases make sense from social perspectives: nobody elects the underconfidant politician. This was interesting, but didn't teach any new biases. Skippable.</p>\n<h3 id=\"spanstylecolor00770033thehothandinbasketballonthemispredictionofrandomsequencesspan\"><span style=\"color:#007700\"> \n<hr />\n<a id=\"more\"></a>33. The Hot Hand in Basketball: On the Misprediction of Random Sequences</span></h3>\n<p>Which of these streams was randomly generated?</p>\n<ol>\n<li>HTTHTHTHHHTTHTTHTHHT</li>\n<li>HHHHHHTTTHHHTTHTTHTH</li>\n</ol>\n<p>The answer is #2. I just generated both sequences a few seconds ago, the first by hand and the second at random. I didn't expect it to be quite so perfect an example of the point of this chapter, which was:</p>\n<p>People don't know how randomness works. People see \"streaks\" in purely random data. If most people saw the second sequence as a H=hit, T=miss in a series of basketball free throws, they would conclude that the player was on a \"hot streak\" in the first part of the sequence.</p>\n<h3 id=\"34likegoeswithliketheroleofrepresentativenessinerroneousandpesudoscientificbeliefs\">\n<hr />\n<a id=\"more\"></a>34. Like Goes With Like: The Role of Representativeness in Erroneous and Pesudo-Scientific Beliefs</h3>\n<p>This chapter tied homeopathy, \"rebirth\", and other scientific hokey into representativeness bias. This was pretty redundant at this point in the book. I felt it was fairly skippable.</p>\n<h3 id=\"35whenlessismorecounterfactualthinkingandsatisfactionamongolympicmedalists\">\n<hr />\n<a id=\"more\"></a>35. When Less is More: Counterfactual thinking and Satisfaction among Olympic Medalists</h3>\n<p>Silver medalists feel worse than Bronze medalists. Upon interviewing runners up, studies found them more likely to have thoughts along the lines of \"if only&hellip;\", whereas third placers had more thoughts along the lines of \"at least&hellip;\"</p>\n<p><strong>Takeaway</strong>: Come in first or third? I'm not sure you can hack your brain to make it more satisfied with second than third.</p>\n<h3 id=\"spanstylecolor00770036understandingmisunderstandingssocialpsychologicalperspectivesspan\"><span style=\"color:#007700\"> \n<hr />\n<a id=\"more\"></a>36. Understanding Misunderstandings: Social Psychological Perspectives</span></h3>\n<p>This article covered things like the polarization effect and the false polarization effect. Turns out there's a false polarization effect. I had no idea.</p>\n<p>People in group A picture others in group A as extreme, and picture themselves as a moderate:</p>\n<pre><code>B--------|--me----A\n</code></pre>\n<p>People in group B have the same image:</p>\n<pre><code>B----me--|--------A\n</code></pre>\n<p>In reality, neither group is as extreme as each group thinks:</p>\n<pre><code>------B--|--A------\n</code></pre>\n<p>But people argue as if each side is extreme:</p>\n<pre><code>B--------|--------A\n</code></pre>\n<p>Even when common ground exists, the false polarization effect can make it very difficult to find.</p>\n<p><strong>Takeaway</strong>: Don't assume that members of The Other Side are extremist. Search for common ground.</p>\n<p>Other neat tidbits:__</p>\n<ul>\n<li>One effect of confirmation bias is that I can give conflicting evidence to two opposed groups, and both will leave the room more convinced of their own arguments.</li>\n<li>Everybody thinks that the news is biased in favor of the other side.</li>\n</ul>\n<p>I've got to watch out for that last one, personally. I get frustrated when people give \"obviously wrong\" arguments (such as the Chinese Room) any consideration whatsoever. This is something I'm working on.</p>\n<h3 id=\"37assessinguncertaintyinphysicalconstants\">\n<hr />\n<a id=\"more\"></a>37. Assessing Uncertainty in Physical Constants</h3>\n<p>The values of physical constants (in the literature, as decided by panels of physicists) are refined as time goes on. The updated values regularly differ from the old values by 3+ standard deviations (well outside the 99% confidence interval). This happens decade after decade after decade.</p>\n<p><strong>Moral of the story</strong>: Visualize the ways you could be way off. Use outside views. Increase your error bars.</p>\n<h3 id=\"spanstylecolor00770038doanalystsoverreactspan\"><span style=\"color:#007700\"> \n<hr />\n<a id=\"more\"></a>38. Do Analysts Overreact?</span></h3>\n<p>Short version: many people don't understand regression to the mean.</p>\n<p>When something does really well, you should expect it to perform more averagely in the future.</p>\n<p>When something does really poorly, you should expect it to to perform more averagely in the future.</p>\n<p><em>In general</em>, expect things to perform more averagely in the future.</p>\n<p><strong>Moral</strong>: Don't get caught up thinking something is really good or really bad just from a few data points. It's likely just at a statistical high/low, and it's likely to regress to the mean.</p>\n<h3>\n<hr />\n<a id=\"more\"></a>39. The Calibration of Expert Judgement: Heuristics and Biases Beyond the Laboratory</h3>\n<p>This paper was staged as somewhat of a coup-de-grace for the humans-are-secretly-optimal group. It studied many experts in their natural environment, using data collected from the wild (not from laboratories). It concluded that experts are consistently poorly calibrated in ways that can't be explained by raw error, but can be explained fairly well by documented biases.</p>\n<p>An interesting tidbit here was that overconfidence bias is actually usually overextremity bias: underconfidence when p is below some threshold, overconfidence when it's above.</p>\n<p>The scary thing was that even experts with constant and reliable feedback weren't able to debias themselves. It took both feedback <em>and</em> knowledge of biases to approach good calibration.</p>\n<h3 id=\"spanstylecolor00770040clinicalversusactuarialjudgementspan\"><span style=\"color:#007700\"> \n<hr />\n<a id=\"more\"></a>40. Clinical versus Actuarial Judgement</span></h3>\n<p>This chapter was my favorite.</p>\n<p>Basically, an algorithmic procedures for classifying psychoses was developed from data using Bayesian analysis and linear regression. The resulting decision rule had a 82% success rate. The average expert had a 69% success rate, with none of them scoring about 75%.</p>\n<p>Here's the fun part:</p>\n<p>If you <em>give the experts the decision rule</em>, along with the relevant probabilities, <strong>they still do worse than the decision rule</strong>.</p>\n<p>They could just go with the decision rule every time and increase their accuracy. However, they know that the decision rule is sometimes wrong, and so they occasionally choose to override it. In this particular study, not one expert was capable of beating the decision rule <em>even when they got to cheat and use the decision rule's answer</em>.</p>\n<p><strong>Moral</strong>: shut up and multiply.</p>\n<h3 id=\"41heuristicsandbiasesinapplication\">\n<hr />\n<a id=\"more\"></a>41. Heuristics and Biases in Application</h3>\n<p>This was pretty much a summary of the book. Its major points were that humans are not secretly optimal, that you've just got to know about your biases, and that you should externalize your decision procedures as much as possible.</p>\n<h3 id=\"spanstylecolor00770042theorydrivenreasoningaboutplausiblepastsandprobablefuturesinworldpoliticsspan\"><span style=\"color:#007700\"> \n<hr />\n<a id=\"more\"></a>42. Theory-Driven Reasoning about Plausible Pasts and Probable Futures in World Politics</span></h3>\n<p>If Frans Ferdinand hadn't been shot, would World War I still have started? Most experts say yes. Europe was at a boiling point. It was going to occur sooner or later.</p>\n<p>Now ask experts to predict something (like the collapse of the Euro in the 90s). Interview the people who predicted wrong. Most of them will point out that it \"almost happened\".</p>\n<p>Distant history is immutable. Near history is mutable.</p>\n<p>More generally, the data that your model was trained on is immutable, but the predictions that your model got wrong were mutable. This was just one example of data supporting the chapter's thesis, which was that it's very difficult for people to consider that their entire model is wrong.</p>\n<p>Moral of the story: When your predictions were wrong but your model still seems accurate, stop. Just stop. Take a deep breath. It will <em>look</em> like there was a black swan, or an extreme circumstance, or a violated assumption. It will look like your model would have been right, except for some fluke. And maybe that's true.</p>\n<p>But <em>your predictions were wrong</em>, and you need to update accordingly. When your predictions are wrong, you shouldn't just patch your model: you should also downgrade the probability that you're using the correct model.</p>\n<p>This is one of the scarier things I learned. One of the points made by this paper was that people who value parsimony highly are more likely to defend their models to the death. This is particularly relevant to me, as I value parsimony highly and I have a lot of models that I'm known to defend.</p>\n<p><em>When your model fails, you can't just patch your model</em> &mdash; you also have to update the probability that you should be using a different model entirely.</p>\n<h2 id=\"discussion\">\n<hr />\n<a id=\"more\"></a>Discussion</h2>\n<p>This is only the tip of the iceberg: there were many smaller revelations I had while reading this book that I just don't have time to cover.</p>\n<p>I've presented a lot of these biases in absolute terms for the sake of brevity. As you might expect, the effects listed above are not binary. Seeing the actual data is illustrative.</p>\n<p>One of the biggest reasons it's important to see the actual studies here is because of the gap between data and interpretation. The conclusions of the psychologists might be incorrect. The data may be saying something that the authors missed.</p>\n<p>It's important to read experiments with an eye for alternative interpretations. Fortunately, professional psychologists are better at this than me. 95% of the alternate interpretations I cooked up were ruled out by follow-up studies in the next paragraph.</p>\n<p>In fact, a large number of follow-up studies ruled out interpretations that hadn't even crossed my mind. I made a little game of it: before reading the follow-up studies, I'd think up alternative interpretations. For every one that was addressed, I got a point. For every one I missed, I lost a point. (Whenever one wasn't addressed, I went back and made sure I understood the study.)</p>\n<h2 id=\"whatshouldiread\">What to read</h2>\n<p>If you have doubts about the biases mentioned above, I strongly recommend picking up this book and reading chapters corresponding to the biases that you doubt. The book rules out many alternative interpretations and will put many doubts to rest.</p>\n<p>If you're really serious in reducing your biases, the whole book is quite useful (except perhaps for chapters 27, 32, and 34). Even if you've already heard about the biases, there's just no substitute for actual data.</p>\n<p>If you're already familiar with more famous biases and you've already read the sequences, you'd still do well to read the papers headlined in green: 2, 4, 8, 9, 12, 13, 18, 25, 26, 29, 30, 31, 33, 36, 38, 40, 42. I expect those chapters to contain the most new content for this community.</p>\n<p>Altogether, this book was too large and dry for most people. I expect casual rationalists to be fine with high-level summaries.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2, "4R8JYu4QF2FqzJxE5": 2, "4Kcm4etxAJjmeDkHP": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gc6foBcbozvEJ3HbG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 41, "extendedScore": null, "score": 0.000113, "legacy": true, "legacyId": "23976", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I'm <a href=\"/lw/iht/miri_course_list_book_reviews_part_1_g%C3%B6del_escher/\">reviewing</a> the books on the <a href=\"http://intelligence.org/courses/\">MIRI course list</a>.</p>\n<p>Upon deciding to read the course list, the first book I picked up was <em>Heuristics and Biases</em>. It's a tome of 42 papers in psychology detailing a vast array of human biases. This post constitutes a review of the book in general, as well as a brief summary of each paper.</p>\n<p>This review includes a number of biases not covered by the Sequences.</p>\n<h1 id=\"Heuristics_and_Biases__The_Psychology_of_Intuitive_Judgement\">Heuristics and Biases: The Psychology of Intuitive Judgement</h1>\n<p style=\"text-align:center\"><img style=\"vertical-align: middle;margin: 0 auto;\" src=\"http://assets.cambridge.org/97805217/96798/cover/9780521796798.jpg\" alt=\"Heuristics and Biases cover\" width=\"180\" height=\"272\"></p>\n<p>This book is <em>dry</em>. Dry like old sand. On Venus. When the sun's out.</p>\n<p>I'd never had my mind blown by text so dry before.</p>\n<p>Most of the 42 papers introduced studies that revealed cognitive biases. A few other papers summarized and discussed results, proposing models that could explain many biases at once. Each paper came with lots of data.</p>\n<p>The book had two main themes. The first was an exploration of heuristics and biases. The second was an argument that biases are not secretly great.</p>\n<p>There's a segment of the psychologist population who argue that biases are not actually failures but instead the result of contrived experiments. Researchers are just looking at people wrong.</p>\n<p>This book dedicates a handful of papers to tearing such arguments apart (via compelling biases and studies on real world data). There was one particular quote that stuck with me (which I'll have to paraphrase, since I lent the book out a few days ago):</p>\n<blockquote>\n<p>People in the human-optimality camp seem to think there is only one type of error in human judgement, that being the error of psychologists when they design tests.</p>\n</blockquote>\n<p>I was sort of hoping for a paper studying the bias of psychologists as they design tests for biases, just so I could read the fallout.</p>\n<p>But I digress. Such arguments didn't interest me much, as I came to the table ready and willing to believe that the brain has many pathological failure modes.</p>\n<p>Fortunately, the main focus of the book was the varied ways in which brains fail at simple tasks. Many of the biases within are famous or were discussed in the sequences. In those cases, it was good to see the actual experimental setups and the real data. Many other biases were completely new to me.</p>\n<p>The high points of each chapter are summarized below. I've marked the most salient chapters with green numbers (in the table of contents) and green headers (in the text).</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Table_of_Contents\">Table of Contents<br></h2>\n<ol>\n<li><a href=\"#1extensionalvsintuitivereasoningtheconjunctionfallacyinprobabilityjudgement\">Extensional vs Intuitive reasoning: The Conjunction Fallacy in Probability Judgement</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor0077002representativenessrevisitedattributesubstitutioninintuitivejudgementspan\">Representativeness Revisited: Attribute Substitution in Intuitive Judgement</a></li>\n<li><a href=\"#3howalikeisitversushowlikelyisitadisjunctionfallacyinprobabilityjudgements\">How Alike Is It? versus How Likely Is It?: A Disjunction Fallacy in Probability Judgements</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor0077004imaginingcanheightenorlowertheperceivedlikelihoodofcontractingadiseasethemediatingeffectofeaseofimageryspan\">Imagining Can Heighten or Lower the Perceived Likelihood of Contracting a Disease: The Mediating Effect of Ease of Imagery</a></li>\n<li><a href=\"#5theavailabilityheuristicrevisitedeaseofrecallandcontentofrecallasdistinctsourcesofinformation\">The Availability Heuristic Revisited: Ease of Recall and Content of Recall as Distinct Sources of Information</a></li>\n<li><a href=\"#6incorporatingtheirrelevantanchorsinjudgementsofbeliefandvalue\">Incorporating the Irrelevant: Anchors in Judgements of Belief and Value.</a></li>\n<li><a href=\"#7puttingadjustmentbackintheanchoringandadjustmentheuristic\">Putting Adjustment Back in the Anchoring and Adjustment heuristic</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor0077008selfanchoringinconversationwhylanguageusersdonotdowhattheyshouldspan\">Self-Anchoring in Conversation: Why Language Users Do Not Do What They \"Should\"</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor0077009inferentialcorrectionspan\">Inferential Correction</a></li>\n<li><a href=\"#10mentalcontaminationandthedebiasingproblem\">Mental Contamination and the Debiasing Problem</a></li>\n<li><a href=\"#11sympatheticmagicalthinkingthecontagionandsimilarityheuristics\">Sympathetic Magical Thinking: The Contagion and Similarity \"Heuristics\"</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770012compatibilityeffectsinjudgementandchoicespan\">Compatibility Effects in Judgement and Choice</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770013theweightingofevidenceandthedeterminantsofconfidencespan\">The Weighting of Evidence and the Determinants of Confidence</a></li>\n<li><a href=\"#14insidetheplanningfallacythecausesandconsequencesofoptimistictimepredictions\">Inside the Planning Fallacy: The Causes and Consequences of Optimistic Time Predictions</a></li>\n<li><a href=\"#15probabilityjudgementacrosscultures\">Probability Judgement Across Cultures</a></li>\n<li><a href=\"#16durabilitybiasinaffectiveforecasting\">Durability Bias in Affective Forecasting</a></li>\n<li><a href=\"#17resistanceofpersonalriskperceptionstodebiasinginterventions\">Resistance of Personal Risk Perceptions to Debiasing Interventions</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770018ambiguityandselfevaluationtheroleofidiosyncratictraitdefinitionsinselfservingassesmentsofabilityspan\">Ambiguity and Self-Evaluation: The Role of Idiosyncratic Trait Definitions in Self-Serving Assesments of Ability</a></li>\n<li><a href=\"#19whenpredictionsfailthedilemmaofunrealisticoptimism\">When Predictions Fail: The Dilemma of Unrealistic Optimism</a></li>\n<li><a href=\"#20normtheorycomparingrealitytoitsalternatives\">Norm Theory: Comparing Reality to its Alternatives</a></li>\n<li><a href=\"#21counterfactualthoughtregretandsuperstitionhowtoavoidkickingyourself\">Counterfactual Thought, Regret, and Superstition: How To Avoid Kicking Yourself</a></li>\n<li><a href=\"#22twosystemsofreasoning\">Two Systems of Reasoning</a></li>\n<li><a href=\"#23theaffectheuristic\">The Affect Heuristic</a></li>\n<li><a href=\"#24individualdifferencesinreasoningimplicationsfortherationalitydebate\">Individual Differences in Reasoning: Implications for the Rationality Debate?</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770025supporttheoryanonextensionalrepresentationofsubjectiveprobabilityspan\">Support Theory: A Nonextensional Representation of Subjective Probability</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770026unpackingrepackingandanchoringadvancesinsupporttheoryspan\">Unpacking, Repacking, and Anchoring: Advances in Support Theory</a></li>\n<li><a href=\"#27remarksonsupporttheoryrecentadvancesandfuturedirections\">Remarks on Support Theory: Recent Advances and Future Directions</a></li>\n<li><a href=\"#28theuseofstatisticalheuristicsineverydayinductivereasoning\">The Use of Statistical Heuristics in Everyday Inductive Reasoning</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770029feelingsasinformationmoodsinfluencejudgementsandprocessingstrategiesspan\">Feelings as Information: Moods Influence Judgements and Processing Strategies</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770030automatedchoiceheuristicsspan\">Automated Choice Heuristics</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770031howgoodarefastandfrugalheuristicsspan\">How Good are Fast and Frugal Heuristics?</a></li>\n<li><a href=\"#32intuitivepoliticianstheologiansandprosecutorsexploringtheempiricalimplicationsofdeviantfunctionalistmetaphors\">Intuitive Politicians, Theologians, and Prosecutors: Exploring the Empirical Implications of Deviant Functionalist Metaphors</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770033thehothandinbasketballonthemispredictionofrandomsequencesspan\">The Hot Hand in Basketball: On the Misprediction of Random Sequences</a></li>\n<li><a href=\"#34likegoeswithliketheroleofrepresentativenessinerroneousandpesudoscientificbeliefs\">Like Goes With Like: The Role of Representativeness in Erroneous and Pesudo-Scientific Beliefs</a></li>\n<li><a href=\"#35whenlessismorecounterfactualthinkingandsatisfactionamongolympicmedalists\">When Less is More: Counterfactual thinking and Satisfaction among Olympic Medalists</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770036understandingmisunderstandingssocialpsychologicalperspectivesspan\">Understanding Misunderstandings: Social Psychological Perspectives</a></li>\n<li><a href=\"#37assessinguncertaintyinphysicalconstants\">Assessing Uncertainty in Physical Constants</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770038doanalystsoverreactspan\">Do Analysts Overreact?</a></li>\n<li><a href=\"#39thecalibrationofexpertjudgementheuristicsandbiasesbeyondthelaboratory\">The Calibration of Expert Judgement: Heuristics and Biases Beyond the Laboratory</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770040clinicalversusactuarialjudgementspan\">Clinical versus Actuarial Judgement</a></li>\n<li><a href=\"#41heuristicsandbiasesinapplication\">Heuristics and Biases in Application</a></li>\n<li style=\"color:#007700\"><a href=\"#spanstylecolor00770042theorydrivenreasoningaboutplausiblepastsandprobablefuturesinworldpoliticsspan\">Theory-Driven Reasoning about Plausible Pasts and Probable Futures in World Politics</a></li>\n</ol>\n<h3 id=\"1__Extensional_vs_Intuitive_reasoning__The_Conjunction_Fallacy_in_Probability_Judgement\">1. Extensional vs Intuitive reasoning: The Conjunction Fallacy in Probability Judgement</h3>\n<p>This chapter mainly covered the <a href=\"/lw/ji/conjunction_fallacy/\">conjunction fallacy</a>: the phenomenon where people think a bookish introvert is more likely to be an accountant who plays jazz on the side than someone (in general) who plays jazz on the side.</p>\n<p>It also generalized the conjunction fallacy by introducing a \"narrative effect\": events seem more likely if you are given an example of how they could occur.</p>\n<p>For instance, given a description of a scummy employer, people rate \"He killed an employee\" as less likely than \"He killed an employee to stop them from going to the police\".</p>\n<p><strong>Suggestion</strong>: Whenever you're presented with a hypothetical, generate at least one (preferably more) explanations for how it could occur (unpack hypotheticals manually).</p>\n<h3 id=\"___2__Representativeness_Revisited__Attribute_Substitution_in_Intuitive_Judgement\"><span style=\"color:#007700\"> \n<hr>\n<a id=\"more\"></a>2. Representativeness Revisited: Attribute Substitution in Intuitive Judgement</span></h3>\n<p>Your intuition's likelihood rating is more a measure of representativeness than of probability.</p>\n<p>For example, if you give people five suspects in a murder mystery, with little evidence for each, then people will give low probabilities that each individual committed the crime. If you then give each of them a motive, then people will give higher probabilities <em>for all suspects</em>.</p>\n<p>The number that your gut throws out isn't probability (which should be roughly 1/5 for each suspect in both cases). Rather, it's a function of argument strength (how much dirt you have on that one person).</p>\n<p>Note that people still rate any two people as equally likely, and that they rate each as 20% likely when asked to analyze all five at once. However, when people rate suspects individually, their likelihood ratings are a good predictor of how much dirt they have on the person (without taking other suspects into account).</p>\n<p>Further studies concluded that when you ask people for probability they give you a measure of how well the considered object (one suspect) represents the parent group (suspicious people).</p>\n<p>This theme emerges throughout the book in different forms.</p>\n<h3 id=\"_3__How_Alike_Is_It__versus_How_Likely_Is_It___A_Disjunction_Fallacy_in_Probability_Judgements\">\n<hr>\n<a id=\"more\"></a>3. How Alike Is It? versus How Likely Is It?: A Disjunction Fallacy in Probability Judgements</h3>\n<p>This was basically an exploration of the conjunction bias and representation theory.</p>\n<p>For starters, it flipped the conjunction bias around and examined the disjunction bias (the propensity to rate the likelihood of X higher than the likelihood of X \u2228 Y). It found a disjunction bias that was weaker than the conjunction bias, and then explored the bias in the light of representativeness. It exploited the fact that subcategories can appear more representative than parent categories. For example, a sparrow is a representative bird but not a very representative animal. They found that perceived likelihood was better predicted by representativeness than by actual likelihood.</p>\n<h3 id=\"___4__Imagining_Can_Heighten_or_Lower_the_Perceived_Likelihood_of_Contracting_a_Disease__The_Mediating_Effect_of_Ease_of_Imagery\"><span style=\"color:#007700\"> \n<hr>\n<a id=\"more\"></a>4. Imagining Can Heighten or Lower the Perceived Likelihood of Contracting a Disease: The Mediating Effect of Ease of Imagery</span></h3>\n<p>Things that are easy to imagine are judged to be more likely. If you ask people to imagine breaking an arm, they can do it easily, and their reported likelihood of breaking an arm goes up. But if you ask people to imagine an abstract genetic thyroid disease, they'll have trouble picturing it, and their reported likelihood of contracting the disease will go down.</p>\n<p><strong>Takeaway 1</strong>: Be careful about telling people to picture rare things. If they are hard to imagine, people might end up <em>less</em> worried after trying to imagine themselves as victims.</p>\n<p><strong>Takeaway 2</strong>: When you're assessing probabilities, be careful not to conflate \"easy to picture\" with \"likely to happen\".</p>\n<p>Your brain is wont to do this for you, so you should explicitly discount probabilities for easy-to-picture futures and explicitly increase probabilities for hard-to-picture ones.</p>\n<h3 id=\"___5__The_Availability_Heuristic_Revisited__Ease_of_Recall_and_Content_of_Recall_as_Distinct_Sources_of_Information\"><span style=\"font-size: 15px;\"> \n<hr>\n<a id=\"more\"></a>5. The Availability Heuristic Revisited: Ease of Recall and Content of Recall as Distinct Sources of Information</span></h3>\n<p>This study was pretty interesting. They gave people sheets of paper with grayed-out 't's on each line. They asked the subjects to think of words that started with the letter 't'.</p>\n<p>A control group was given no further instruction. The facilitated group was told that the gray 't's would facilitate. The inhibited group was told that the gray 't's would inhibit. Afterwards, each person was asked what proportion of words began with 't'.</p>\n<p>The facilitated group gave lower proportions than the control, the inhibited group gave higher.</p>\n<p>The conclusion was that people used the perceived difficulty as a source of input. All groups found it relatively easy to think of t-words. The facilitated group attributed some of this to the paper (and concluded that there were less t-words). The inhibited group concluded that t-words must be <em>really</em> prevalent, because even with the inhibiting paper they thought of a bunch of t-words. (Note that the paper was the same in all scenarios.)</p>\n<p>More generally, it seems that people treat ease-of-recall as a heuristic to measure prevalence-of-item.</p>\n<p>This heuristic is often good, but be wary. It's relatively easy to exploit this.</p>\n<h3 id=\"_6__Incorporating_the_Irrelevant__Anchors_in_Judgements_of_Belief_and_Value_\">\n<hr>\n<a id=\"more\"></a>6. Incorporating the Irrelevant: Anchors in Judgements of Belief and Value.</h3>\n<p>This was a pretty standard introduction to <a href=\"/lw/j7/anchoring_and_adjustment/\">anchoring and adjustment</a>.</p>\n<h3 id=\"_7__Putting_Adjustment_Back_in_the_Anchoring_and_Adjustment_heuristic\">\n<hr>\n<a id=\"more\"></a>7. Putting Adjustment Back in the Anchoring and Adjustment heuristic</h3>\n<p>Anchoring isn't just something that happens in your environment. You self-anchor. For example, when did George Washington become president? Most people self-anchor at 1776.</p>\n<h3 id=\"___8__Self_Anchoring_in_Conversation__Why_Language_Users_Do_Not_Do_What_They__Should_\"><span style=\"color:#007700\"> \n<hr>\n<a id=\"more\"></a>8. Self-Anchoring in Conversation: Why Language Users Do Not Do What They \"Should\"</span></h3>\n<p>Your brain doesn't auto-filter irrelevant data.</p>\n<p>There was a game where people had a bunch of boxes facing them with little items in them.</p>\n<p>A partner sat facing them. Some of the boxes were clearly open, so that the partner could see the contents. Some of the boxes were clearly closed, and this was made obvious to the subject.</p>\n<pre><code>---------\n| |6| | |\n---------\n| | |5| |\n---------\n|4| | | |\n---------\n</code></pre>\n<p>In this example, imagine that the 5 and 6 boxes are open but the 4-box is closed. (In other words, partner can see 5 and 6 but not 4, and the subject knows this.)</p>\n<p>The partner would give instructions like \"Move the low number one square to the left\". The subject would reach for the number 4 before self-correcting and reaching for the number 5 (the smallest number that the partner can see).</p>\n<p>Moral: your brain won't restrict itself to mutual information. You've got to do it manually. This wasn't too surprising to me until more studies built off of these results:</p>\n<p>Subjects who listened to the same voicemail were either told that the intent was sarcastic or sincere. Subjects on both sides thought it equally likely that a third observer would pick up the sarcastic/sincere intent \u2014 they failed to control for personal information.</p>\n<p>Subjects asked to tap out songs thought that a majority of people would know what song they were tapping out. Listeners guessed about 2% of the time. Tappers blamed this on poor attention.</p>\n<p>Another study observed speakers accidentally using information not available to an audience.</p>\n<p>The conclusion was basically that the brain automatically uses all of the information available. It requires conscious effort to restrict yourself to only mutual information. See also the <a href=\"/lw/ke/illusion_of_transparency_why_no_one_understands/\">Illusion of Transparency</a> and <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">Inferential Distance</a>.</p>\n<p><strong>Takeaway</strong>: Assume that intentions are opaque. Monitor your conversations to make sure you're restricting yourself to mutual information.</p>\n<h3 id=\"___9__Inferential_Correction\"><span style=\"color:#007700\"> \n<hr>\n<a id=\"more\"></a>9. Inferential Correction</span></h3>\n<p>This paper had three points. First, the <a href=\"/lw/lg/the_affect_heuristic/\">Affect Heuristic</a>. If you see a person acting anxious, you're more likely to conclude that they're an \"anxious person\" instead of that they're in an anxious situation.</p>\n<p>The second point is that you're more susceptible to this problem when you're under cognitive load (time pressure, busy with other tasks, etc.)</p>\n<p>Third and most terrifying is the study referenced in <a href=\"/lw/k4/do_we_believe_everything_were_told/\">this part of the sequences</a> \u2014 You believe everything you read unless you exert cognitive power to disbelieve things.</p>\n<p>In sum, there are many things that your brain does without your consent (jump to conclusions, believe what it reads) that require cognitive effort to undo.</p>\n<p><strong>Takaway</strong>: Any conclusions drawn while you're distracted should be thrown out. Make sure you're not under cognitive load before deciding something important.</p>\n<p>(Also known as the \"always masturbate before making relationship decisions\" rule.)</p>\n<h3 id=\"_10__Mental_Contamination_and_the_Debiasing_Problem\">\n<hr>\n<a id=\"more\"></a>10. Mental Contamination and the Debiasing Problem</h3>\n<p>This chapter was an exploration of debasing. The conclusion is that you have to control your exposure to stimuli that influence your responses. People are notoriously bad at de-biasing themselves. Once you're exposed to biasers (anchors, etc.) you're going to be biased.</p>\n<p>A poignant example was the fact that many psychologists who know about the halo effect still fail to grade papers blind, thinking that they can debias themselves.</p>\n<p><strong>Moral</strong>: Don't be that guy. Grade papers blind. Audition musicians behind a screen. Remove names from r\u00e9sum\u00e9s. Control your exposure.</p>\n<h3 id=\"_11__Sympathetic_Magical_Thinking__The_Contagion_and_Similarity__Heuristics_\">\n<hr>\n<a id=\"more\"></a>11. Sympathetic Magical Thinking: The Contagion and Similarity \"Heuristics\"</h3>\n<p>Would you drink a glass of orange juice after I dip a cockroach in it?</p>\n<p>Probably not, and understandably so: cockroaches can be dirty.</p>\n<p>But would you drink a glass of orange juice after I dip a sterilized cockroach in it?</p>\n<p>Your concern in the second case is hard to justify. It seems that humans still have a lot of \"magical\" thinking in them \u2014 touching gross things makes things gross.</p>\n<p>Furthermore, it appears that some people root their aversion in things like germ theory, whereas others root their aversion in fear. For example, most people don't want to wear a sweater worn by a person with AIDS. Washing it is sufficient to remove the malaise for some but not others.</p>\n<h3 id=\"___12__Compatibility_Effects_in_Judgement_and_Choice\"><span style=\"color:#007700\"> \n<hr>\n<a id=\"more\"></a>12. Compatibility Effects in Judgement and Choice</span></h3>\n<p>If you ask people to rank bets then they will weight the odds too heavily. If you ask them to price bets, they'll weight the payoffs too heavily. Generally, compatible units get more weight.</p>\n<p>(As a corollary, anchors work much better when they share a unit with the thing being anchored.)</p>\n<p>This was simple but surprising.</p>\n<p><strong>Recommendation</strong>: Find ways to normalize things before assessing them. (For example, normalize bets to risk-adjusted expected payoffs.) If you try to let your gut do things, it will give additional weight to dimensions of the problem that are compatible with the question.</p>\n<h3 id=\"___13__The_Weighting_of_Evidence_and_the_Determinants_of_Confidence\"><span style=\"color:#007700\"> \n<hr>\n<a id=\"more\"></a>13. The Weighting of Evidence and the Determinants of Confidence</span></h3>\n<p>People basically only use argument strength (how much the evidence looks like the hypothesis) when judging likelihood. They completely ignore base rates (the prior probability) and disregard the weight of evidence (how much evidence there is).</p>\n<p>Base rate neglect is prevalent. Think back to the representativeness effect: when people analyze five suspects, the suspects seem more suspicious if they have motives. The assessed likelihood that each was the murderer goes up in proportion to the amount of dirt that people have, <em>even if everyone else is just as suspicious</em>. When you ask people to assess the likelihood that the janitor was the murderer, they think about reasons why the janitor is suspicious \u2014 <em>without regard for who else is also suspicious.</em></p>\n<p>This neglect is probably the biggest hurdle between most people and Bayesian reasoning. It's the root of doctors believing tests with low false positive rates despite high base rates, and many other deviations from \"rational\" reasoning.</p>\n<p>Argument weight neglect is also prevalent. People act like five coinflips (all heads) are good evidence that a coin is weighted towards heads, despite how small the sample size is. Jokingly, the authors note that people seem to believe in the \"law of small numbers\": that evidence reflects the data even in small amounts. Experts manage to factor in argument weight, but even they do not give it enough credence.</p>\n<p>Your brain treats evidence representationally: it's very good at measuring how well the evidence matches the hypothesis, and very bad at assessing how much the evidence should count for.</p>\n<p>Turns out your intuition just doesn't know how to take argument weight or base rates into account. You've got to do it manually.</p>\n<p><strong>Suggestion</strong>: When you're assessing likelihood, start with your intuitive likelihood. Treat that number as <em>argument strength</em>. Ignore it completely. Force yourself to bring the base rates to mind. Force yourself to consider the amount of evidence, not just the degree to which it looks good. Once you have the base rates and the evidence weight in mind, factor in your original number accordingly.</p>\n<p><em>You have to do this consciously</em>: Your brain just isn't going to do it for you.</p>\n<h3 id=\"_14__Inside_the_Planning_Fallacy__The_Causes_and_Consequences_of_Optimistic_Time_Predictions\">\n<hr>\n<a id=\"more\"></a>14. Inside the Planning Fallacy: The Causes and Consequences of Optimistic Time Predictions</h3>\n<p><a href=\"/lw/jg/planning_fallacy/\">Inside views suck</a>. People's predictions are the same as their best case predictions. And when their predictions fail, people blame it on exceptional circumstances and fail to learn in the general case.</p>\n<p>In other words, when people miss a deadline because their computer crashed, they update to think that computer crashes were more likely, but fail to draw the more general conclusion that their estimates were too optimistic.</p>\n<p>This paper sowed the seeds for a later revelation about updating both your model and the likelihood that you should switch models.</p>\n<h3 id=\"_15__Probability_Judgement_Across_Cultures\">\n<hr>\n<a id=\"more\"></a>15. Probability Judgement Across Cultures</h3>\n<p>This paper basically showed that Asian people (China and Thailand, IIRC) are way more overconfident than American people, even though both groups expect the opposite to be true. It might have had some other points, but I forget. What I took away from this paper is that common knowledge can be very wrong: you've got to go out and collect real data.</p>\n<h3 id=\"_16__Durability_Bias_in_Affective_Forecasting\">\n<hr>\n<a id=\"more\"></a>16. Durability Bias in Affective Forecasting</h3>\n<p>You expect good/bad events to make you feel happy/sad for a long time. People expect that disabilities will make them sadder for longer than they actually do. People expect winning the lottery will make them happier for longer than it actually does.</p>\n<p>There was also a study showing that durability bias is stronger for bad events (i.e. bad-event-recovery happens faster than good-event-regression), perhaps due to some sort of \"psychological immune system\" that people deploy to deal with bad things after the fact (look at the bright side, etc.).</p>\n<h3 id=\"_17__Resistance_of_Personal_Risk_Perceptions_to_Debiasing_Interventions\">\n<hr>\n<a id=\"more\"></a>17. Resistance of Personal Risk Perceptions to Debiasing Interventions</h3>\n<p>People are hard to debias, despite many varied interventions.</p>\n<p>Seriously, the interventions here were many, varied, and clever. None of them worked.</p>\n<p><strong>Moral</strong>: Control your exposure.</p>\n<h3 id=\"___18__Ambiguity_and_Self_Evaluation__The_Role_of_Idiosyncratic_Trait_Definitions_in_Self_Serving_Assesments_of_Ability\"><span style=\"color:#007700\"> \n<hr>\n<a id=\"more\"></a>18. Ambiguity and Self-Evaluation: The Role of Idiosyncratic Trait Definitions in Self-Serving Assesments of Ability</span></h3>\n<p>You've probably heard of the \"better than average\" effect \u2014 everyone thinks that they're better than average at driving/leadership/etc. What this paper showed was really cool, though: this effect is due in large part to the fact that everyone defines ambiguous attributes differently.</p>\n<p>For example, an organized person might define \"good leadership\" as organizational skills and the ability to keep things moving, whereas an outgoing person might define \"good leadership\" as the ability to inspire others and garner loyalty. It should come as no surprise, then, that everybody thinks they're better than average at \"leadership\".</p>\n<p>This paper had a study where half the subjects would define how an ambiguous trait was measured, and the other half would rate themselves according to that measurement. When this was done, the \"better than average\" effect disappeared.</p>\n<h3 id=\"_19__When_Predictions_Fail__The_Dilemma_of_Unrealistic_Optimism\">\n<hr>\n<a id=\"more\"></a>19. When Predictions Fail: The Dilemma of Unrealistic Optimism</h3>\n<p>Turns out it's hard to reduce optimistic biases.</p>\n<p>This paper explored how optimistic biases can persist, given that you'd expect evolution to have weeded out overconfident people. (It's just you and me, Mr. Tiger.) But as it turns out, overconfidence bias melts away as events draw closer. This paper was a really interesting read.</p>\n<p>Basically, it showed how optimism bias is strongest when it's most useful (when it can act as a self-fulfilling prophecy) and disappears when it's harmful (people are often underconfident in the moment).</p>\n<h3 id=\"_20__Norm_Theory__Comparing_Reality_to_its_Alternatives\">\n<hr>\n<a id=\"more\"></a>20. Norm Theory: Comparing Reality to its Alternatives</h3>\n<p>You think it's worse to miss a plane by 5 minutes than by 30 minutes. Some counterfactual worlds seem closer than others.</p>\n<h3 id=\"_21__Counterfactual_Thought__Regret__and_Superstition__How_To_Avoid_Kicking_Yourself\">\n<hr>\n<a id=\"more\"></a>21. Counterfactual Thought, Regret, and Superstition: How To Avoid Kicking Yourself</h3>\n<p>It hurts more for bad things to happen in exceptional cases. It feels worse when someone is killed in an accident while taking the scenic route home for the first time than when they're killed in an accident during their daily routine.</p>\n<p>Given that this is the case, superstitions make sense.</p>\n<p><strong>Takaway</strong>: You will naturally be more outraged when bad things happen in exceptional cases. You probably need to manually scale down concern for the exceptional cases.</p>\n<h3 id=\"_22__Two_Systems_of_Reasoning\">\n<hr>\n<a id=\"more\"></a>22. Two Systems of Reasoning</h3>\n<p>Introduces System 1 (the subconscious processes that provide your intuitions) and System 2 (the conscious thinker that corrects System 1) and explores their strengths and weaknesses. Notes that all biases can be viewed as two failures: The failure of System 1, who used a heuristic that was not actually applicable, and the failure of System 2 to notice the error and correct it.</p>\n<h3 id=\"_23__The_Affect_Heuristic\">\n<hr>\n<a id=\"more\"></a>23. The Affect Heuristic</h3>\n<p>Explicitly covered the <a href=\"/lw/lg/the_affect_heuristic/\">Affect Heuristic</a>. People were asked to rate bets. Each person would only see one of the below bets. Bet B was consistently rated better than bet A.</p>\n<p><strong>Bet A</strong>: 20%: Win 9$</p>\n<p><strong>Bet B</strong>: 20%: Win 9$, 80%: Lose 5\u00a2</p>\n<p>A 9$ win looks much better compared to a 5\u00a2 loss than it does without context. This can be used to induce preference reversals.</p>\n<p>This chapter felt somewhat redundant at this point in the book.</p>\n<h3 id=\"_24__Individual_Differences_in_Reasoning__Implications_for_the_Rationality_Debate_\">\n<hr>\n<a id=\"more\"></a>24. Individual Differences in Reasoning: Implications for the Rationality Debate?</h3>\n<p>This paper was mostly an argument against people who think that human biases are actually optimal and we're just looking at them wrong. It ruled out things like random error, contrived tests, and bad questions via a clever series of studies.</p>\n<h3 id=\"___25__Support_Theory__A_Nonextensional_Representation_of_Subjective_Probability\"><span style=\"color:#007700\"> \n<hr>\n<a id=\"more\"></a>25. Support Theory: A Nonextensional Representation of Subjective Probability</span></h3>\n<p>This paper was really cool. It basically said two things:</p>\n<ol>\n<li>Your brain doesn't assign likelihoods to events. It assigns likelihoods to descriptions. Different descriptions of the same event can yield wildly different probabilities.</li>\n<li>When you unpack an event, your subjective probabilities are subadditive. In other words, the subjective probability of A is consistently lower than the combined subjective probabilities of all of A's components.</li>\n</ol>\n<p>The remainder of the paper formally introduced this mathematical system and showed how it generalizes Representativeness and the Conjunction fallacy.</p>\n<p><strong>Takeaway</strong>: Normalize events before assessing their probability. This is hard, as it's not always obvious how events should be normalized.</p>\n<h3 id=\"___26__Unpacking__Repacking__and_Anchoring__Advances_in_Support_Theory\"><span style=\"color:#007700\"> \n<hr>\n<a id=\"more\"></a>26. Unpacking, Repacking, and Anchoring: Advances in Support Theory</span></h3>\n<p>This paper was a deeper exploration of Support Theory (introduced in the preceding chapter). It mostly explored unpacking and repacking: the propensity for an argument to feel more likely as you consider more of its components. (This is a generalization of the narrative effect from earlier.)</p>\n<p>Basically, the likelihood you assign for a broad category (1000 people killed by floods in North America next year) is lower than the summed probability of its component parts (the likelihood of a flood caused by a California earthquake, caused by the eruption of Mt. Rainier, etc.).</p>\n<p>This bias is difficult to defend against, because people aren't going to ask you to rate both \"chance of a flood\" and \"chance of a flood caused by an earthquake in California\" at the same time. Somehow, you've got to know to unpack/repack arguments even when you're asked only one side of the question.</p>\n<p><strong>Recommendation</strong>: manually try to unpack/repack any scenarios that come your way before giving probability assessments.</p>\n<p>If someone asks you to assess the probability that the USA strikes Syria due to chemical weapons use, you should first try to repack this argument (what is the chance that the US strikes Syria?) and unpack this argument (What is the chance they strike Syria due to chemical weapons use by the rebels? What about by the government?) before answering.</p>\n<h3 id=\"_27__Remarks_on_Support_Theory__Recent_Advances_and_Future_Directions\">\n<hr>\n<a id=\"more\"></a>27. Remarks on Support Theory: Recent Advances and Future Directions</h3>\n<p>This chapter felt a bit redundant. It tied more biases into the support theory model. I don't remember anything novel, but something could have slipped my mind. You could skip this chapter.</p>\n<h3 id=\"_28__The_Use_of_Statistical_Heuristics_in_Everyday_Inductive_Reasoning\">\n<hr>\n<a id=\"more\"></a>28. The Use of Statistical Heuristics in Everyday Inductive Reasoning</h3>\n<p>This paper was another one arguing against the human-optimality camp. Basically, the paper rejected the argument that statistical reasoning isn't applicable to human lives by pointing out that people reason statistically (and do better) when they've been trained in statistics.</p>\n<h3 id=\"___29__Feelings_as_Information__Moods_Influence_Judgements_and_Processing_Strategies\"><span style=\"color:#007700\"> \n<hr>\n<a id=\"more\"></a>29. Feelings as Information: Moods Influence Judgements and Processing Strategies</span></h3>\n<p>Happy people are more overconfident. Sad people are more underconfident. <em>However</em>, having people list reasons for their current mood removes the effect of mood upon predictions: priming reasons why your mood is not related to the question at hand helps people remove mood bias.</p>\n<p><strong>Takeaway</strong>: Before making big decisions, list the reasons why your mood is what it is.</p>\n<h3 id=\"___30__Automated_Choice_Heuristics\"><span style=\"color:#007700\"> \n<hr>\n<a id=\"more\"></a>30. Automated Choice Heuristics</span></h3>\n<p>People tend to \"choose by liking\", taking the choice that looks better. This can lead to some serious problems. For example:</p>\n<p>I have a die. It has 2 green sides, 1 blue side, 2 red sides, and 1 yellow side. You get to choose between the two bets:</p>\n<p><strong id=\"Bet_A\">Bet A</strong></p>\n<ul>\n<li>Green or Blue: +20$</li>\n<li>Red: -10$</li>\n<li>Yellow: -10$</li>\n</ul>\n<p><strong id=\"Bet_B\">Bet B</strong></p>\n<ul>\n<li>Green: +20$</li>\n<li>Blue: +15$</li>\n<li>Red or Yellow: -10$</li>\n</ul>\n<p>Many people choose bet B over bet A.</p>\n<p>Now unpack the bets:</p>\n<p><strong id=\"Bet_A1\">Bet A</strong></p>\n<ul>\n<li>Green: +20$</li>\n<li>Blue: +20$</li>\n<li>Red: -10$</li>\n<li>Yellow: -10$</li>\n</ul>\n<p><strong id=\"Bet_B1\">Bet B</strong></p>\n<ul>\n<li>Green: +20$</li>\n<li>Blue: +15$</li>\n<li>Red: -10$</li>\n<li>Yellow: -10$</li>\n</ul>\n<p>In this format, it's clear that A dominates. Nobody picks B.</p>\n<p>I found this study astonishing.</p>\n<p><strong>Moral</strong>: Normalize options before choosing between them! Unpack things manually, and <em>stop trusting your brain to intuit probabilities.</em></p>\n<h3 id=\"___31__How_Good_are_Fast_and_Frugal_Heuristics_\"><span style=\"color:#007700\"> \n<hr>\n<a id=\"more\"></a>31. How Good are Fast and Frugal Heuristics?</span></h3>\n<p>Bayesianism is great. Yay Bayesianism. But it's expensive. Turns out that simple algorithms, like choosing according to a few binary attributes, are much faster and almost as good. This helps explain many human biases.</p>\n<p>This paper was an interesting read from a \"how might by brain be making these failures\" perspective, but didn't introduce any new biases.</p>\n<h3 id=\"_32__Intuitive_Politicians__Theologians__and_Prosecutors__Exploring_the_Empirical_Implications_of_Deviant_Functionalist_Metaphors\">\n<hr>\n<a id=\"more\"></a>32. Intuitive Politicians, Theologians, and Prosecutors: Exploring the Empirical Implications of Deviant Functionalist Metaphors</h3>\n<p>This paper basically argued that certain biases make sense from social perspectives: nobody elects the underconfidant politician. This was interesting, but didn't teach any new biases. Skippable.</p>\n<h3 id=\"___33__The_Hot_Hand_in_Basketball__On_the_Misprediction_of_Random_Sequences\"><span style=\"color:#007700\"> \n<hr>\n<a id=\"more\"></a>33. The Hot Hand in Basketball: On the Misprediction of Random Sequences</span></h3>\n<p>Which of these streams was randomly generated?</p>\n<ol>\n<li>HTTHTHTHHHTTHTTHTHHT</li>\n<li>HHHHHHTTTHHHTTHTTHTH</li>\n</ol>\n<p>The answer is #2. I just generated both sequences a few seconds ago, the first by hand and the second at random. I didn't expect it to be quite so perfect an example of the point of this chapter, which was:</p>\n<p>People don't know how randomness works. People see \"streaks\" in purely random data. If most people saw the second sequence as a H=hit, T=miss in a series of basketball free throws, they would conclude that the player was on a \"hot streak\" in the first part of the sequence.</p>\n<h3 id=\"_34__Like_Goes_With_Like__The_Role_of_Representativeness_in_Erroneous_and_Pesudo_Scientific_Beliefs\">\n<hr>\n<a id=\"more\"></a>34. Like Goes With Like: The Role of Representativeness in Erroneous and Pesudo-Scientific Beliefs</h3>\n<p>This chapter tied homeopathy, \"rebirth\", and other scientific hokey into representativeness bias. This was pretty redundant at this point in the book. I felt it was fairly skippable.</p>\n<h3 id=\"_35__When_Less_is_More__Counterfactual_thinking_and_Satisfaction_among_Olympic_Medalists\">\n<hr>\n<a id=\"more\"></a>35. When Less is More: Counterfactual thinking and Satisfaction among Olympic Medalists</h3>\n<p>Silver medalists feel worse than Bronze medalists. Upon interviewing runners up, studies found them more likely to have thoughts along the lines of \"if only\u2026\", whereas third placers had more thoughts along the lines of \"at least\u2026\"</p>\n<p><strong>Takeaway</strong>: Come in first or third? I'm not sure you can hack your brain to make it more satisfied with second than third.</p>\n<h3 id=\"___36__Understanding_Misunderstandings__Social_Psychological_Perspectives\"><span style=\"color:#007700\"> \n<hr>\n<a id=\"more\"></a>36. Understanding Misunderstandings: Social Psychological Perspectives</span></h3>\n<p>This article covered things like the polarization effect and the false polarization effect. Turns out there's a false polarization effect. I had no idea.</p>\n<p>People in group A picture others in group A as extreme, and picture themselves as a moderate:</p>\n<pre><code>B--------|--me----A\n</code></pre>\n<p>People in group B have the same image:</p>\n<pre><code>B----me--|--------A\n</code></pre>\n<p>In reality, neither group is as extreme as each group thinks:</p>\n<pre><code>------B--|--A------\n</code></pre>\n<p>But people argue as if each side is extreme:</p>\n<pre><code>B--------|--------A\n</code></pre>\n<p>Even when common ground exists, the false polarization effect can make it very difficult to find.</p>\n<p><strong>Takeaway</strong>: Don't assume that members of The Other Side are extremist. Search for common ground.</p>\n<p>Other neat tidbits:__</p>\n<ul>\n<li>One effect of confirmation bias is that I can give conflicting evidence to two opposed groups, and both will leave the room more convinced of their own arguments.</li>\n<li>Everybody thinks that the news is biased in favor of the other side.</li>\n</ul>\n<p>I've got to watch out for that last one, personally. I get frustrated when people give \"obviously wrong\" arguments (such as the Chinese Room) any consideration whatsoever. This is something I'm working on.</p>\n<h3 id=\"_37__Assessing_Uncertainty_in_Physical_Constants\">\n<hr>\n<a id=\"more\"></a>37. Assessing Uncertainty in Physical Constants</h3>\n<p>The values of physical constants (in the literature, as decided by panels of physicists) are refined as time goes on. The updated values regularly differ from the old values by 3+ standard deviations (well outside the 99% confidence interval). This happens decade after decade after decade.</p>\n<p><strong>Moral of the story</strong>: Visualize the ways you could be way off. Use outside views. Increase your error bars.</p>\n<h3 id=\"___38__Do_Analysts_Overreact_\"><span style=\"color:#007700\"> \n<hr>\n<a id=\"more\"></a>38. Do Analysts Overreact?</span></h3>\n<p>Short version: many people don't understand regression to the mean.</p>\n<p>When something does really well, you should expect it to perform more averagely in the future.</p>\n<p>When something does really poorly, you should expect it to to perform more averagely in the future.</p>\n<p><em>In general</em>, expect things to perform more averagely in the future.</p>\n<p><strong>Moral</strong>: Don't get caught up thinking something is really good or really bad just from a few data points. It's likely just at a statistical high/low, and it's likely to regress to the mean.</p>\n<h3 id=\"_39__The_Calibration_of_Expert_Judgement__Heuristics_and_Biases_Beyond_the_Laboratory\">\n<hr>\n<a id=\"more\"></a>39. The Calibration of Expert Judgement: Heuristics and Biases Beyond the Laboratory</h3>\n<p>This paper was staged as somewhat of a coup-de-grace for the humans-are-secretly-optimal group. It studied many experts in their natural environment, using data collected from the wild (not from laboratories). It concluded that experts are consistently poorly calibrated in ways that can't be explained by raw error, but can be explained fairly well by documented biases.</p>\n<p>An interesting tidbit here was that overconfidence bias is actually usually overextremity bias: underconfidence when p is below some threshold, overconfidence when it's above.</p>\n<p>The scary thing was that even experts with constant and reliable feedback weren't able to debias themselves. It took both feedback <em>and</em> knowledge of biases to approach good calibration.</p>\n<h3 id=\"___40__Clinical_versus_Actuarial_Judgement\"><span style=\"color:#007700\"> \n<hr>\n<a id=\"more\"></a>40. Clinical versus Actuarial Judgement</span></h3>\n<p>This chapter was my favorite.</p>\n<p>Basically, an algorithmic procedures for classifying psychoses was developed from data using Bayesian analysis and linear regression. The resulting decision rule had a 82% success rate. The average expert had a 69% success rate, with none of them scoring about 75%.</p>\n<p>Here's the fun part:</p>\n<p>If you <em>give the experts the decision rule</em>, along with the relevant probabilities, <strong>they still do worse than the decision rule</strong>.</p>\n<p>They could just go with the decision rule every time and increase their accuracy. However, they know that the decision rule is sometimes wrong, and so they occasionally choose to override it. In this particular study, not one expert was capable of beating the decision rule <em>even when they got to cheat and use the decision rule's answer</em>.</p>\n<p><strong>Moral</strong>: shut up and multiply.</p>\n<h3 id=\"_41__Heuristics_and_Biases_in_Application\">\n<hr>\n<a id=\"more\"></a>41. Heuristics and Biases in Application</h3>\n<p>This was pretty much a summary of the book. Its major points were that humans are not secretly optimal, that you've just got to know about your biases, and that you should externalize your decision procedures as much as possible.</p>\n<h3 id=\"___42__Theory_Driven_Reasoning_about_Plausible_Pasts_and_Probable_Futures_in_World_Politics\"><span style=\"color:#007700\"> \n<hr>\n<a id=\"more\"></a>42. Theory-Driven Reasoning about Plausible Pasts and Probable Futures in World Politics</span></h3>\n<p>If Frans Ferdinand hadn't been shot, would World War I still have started? Most experts say yes. Europe was at a boiling point. It was going to occur sooner or later.</p>\n<p>Now ask experts to predict something (like the collapse of the Euro in the 90s). Interview the people who predicted wrong. Most of them will point out that it \"almost happened\".</p>\n<p>Distant history is immutable. Near history is mutable.</p>\n<p>More generally, the data that your model was trained on is immutable, but the predictions that your model got wrong were mutable. This was just one example of data supporting the chapter's thesis, which was that it's very difficult for people to consider that their entire model is wrong.</p>\n<p>Moral of the story: When your predictions were wrong but your model still seems accurate, stop. Just stop. Take a deep breath. It will <em>look</em> like there was a black swan, or an extreme circumstance, or a violated assumption. It will look like your model would have been right, except for some fluke. And maybe that's true.</p>\n<p>But <em>your predictions were wrong</em>, and you need to update accordingly. When your predictions are wrong, you shouldn't just patch your model: you should also downgrade the probability that you're using the correct model.</p>\n<p>This is one of the scarier things I learned. One of the points made by this paper was that people who value parsimony highly are more likely to defend their models to the death. This is particularly relevant to me, as I value parsimony highly and I have a lot of models that I'm known to defend.</p>\n<p><em>When your model fails, you can't just patch your model</em> \u2014 you also have to update the probability that you should be using a different model entirely.</p>\n<h2 id=\"_Discussion\">\n<hr>\n<a id=\"more\"></a>Discussion</h2>\n<p>This is only the tip of the iceberg: there were many smaller revelations I had while reading this book that I just don't have time to cover.</p>\n<p>I've presented a lot of these biases in absolute terms for the sake of brevity. As you might expect, the effects listed above are not binary. Seeing the actual data is illustrative.</p>\n<p>One of the biggest reasons it's important to see the actual studies here is because of the gap between data and interpretation. The conclusions of the psychologists might be incorrect. The data may be saying something that the authors missed.</p>\n<p>It's important to read experiments with an eye for alternative interpretations. Fortunately, professional psychologists are better at this than me. 95% of the alternate interpretations I cooked up were ruled out by follow-up studies in the next paragraph.</p>\n<p>In fact, a large number of follow-up studies ruled out interpretations that hadn't even crossed my mind. I made a little game of it: before reading the follow-up studies, I'd think up alternative interpretations. For every one that was addressed, I got a point. For every one I missed, I lost a point. (Whenever one wasn't addressed, I went back and made sure I understood the study.)</p>\n<h2 id=\"What_to_read\">What to read</h2>\n<p>If you have doubts about the biases mentioned above, I strongly recommend picking up this book and reading chapters corresponding to the biases that you doubt. The book rules out many alternative interpretations and will put many doubts to rest.</p>\n<p>If you're really serious in reducing your biases, the whole book is quite useful (except perhaps for chapters 27, 32, and 34). Even if you've already heard about the biases, there's just no substitute for actual data.</p>\n<p>If you're already familiar with more famous biases and you've already read the sequences, you'd still do well to read the papers headlined in green: 2, 4, 8, 9, 12, 13, 18, 25, 26, 29, 30, 31, 33, 36, 38, 40, 42. I expect those chapters to contain the most new content for this community.</p>\n<p>Altogether, this book was too large and dry for most people. I expect casual rationalists to be fine with high-level summaries.</p>", "sections": [{"title": "Heuristics and Biases: The Psychology of Intuitive Judgement", "anchor": "Heuristics_and_Biases__The_Psychology_of_Intuitive_Judgement", "level": 1}, {"title": "Table of Contents", "anchor": "Table_of_Contents", "level": 2}, {"title": "1. Extensional vs Intuitive reasoning: The Conjunction Fallacy in Probability Judgement", "anchor": "1__Extensional_vs_Intuitive_reasoning__The_Conjunction_Fallacy_in_Probability_Judgement", "level": 3}, {"title": " \n\n2. Representativeness Revisited: Attribute Substitution in Intuitive Judgement", "anchor": "___2__Representativeness_Revisited__Attribute_Substitution_in_Intuitive_Judgement", "level": 3}, {"title": "\n3. How Alike Is It? versus How Likely Is It?: A Disjunction Fallacy in Probability Judgements", "anchor": "_3__How_Alike_Is_It__versus_How_Likely_Is_It___A_Disjunction_Fallacy_in_Probability_Judgements", "level": 3}, {"title": " \n\n4. Imagining Can Heighten or Lower the Perceived Likelihood of Contracting a Disease: The Mediating Effect of Ease of Imagery", "anchor": "___4__Imagining_Can_Heighten_or_Lower_the_Perceived_Likelihood_of_Contracting_a_Disease__The_Mediating_Effect_of_Ease_of_Imagery", "level": 3}, {"title": " \n\n5. The Availability Heuristic Revisited: Ease of Recall and Content of Recall as Distinct Sources of Information", "anchor": "___5__The_Availability_Heuristic_Revisited__Ease_of_Recall_and_Content_of_Recall_as_Distinct_Sources_of_Information", "level": 3}, {"title": "\n6. Incorporating the Irrelevant: Anchors in Judgements of Belief and Value.", "anchor": "_6__Incorporating_the_Irrelevant__Anchors_in_Judgements_of_Belief_and_Value_", "level": 3}, {"title": "\n7. Putting Adjustment Back in the Anchoring and Adjustment heuristic", "anchor": "_7__Putting_Adjustment_Back_in_the_Anchoring_and_Adjustment_heuristic", "level": 3}, {"title": " \n\n8. Self-Anchoring in Conversation: Why Language Users Do Not Do What They \"Should\"", "anchor": "___8__Self_Anchoring_in_Conversation__Why_Language_Users_Do_Not_Do_What_They__Should_", "level": 3}, {"title": " \n\n9. Inferential Correction", "anchor": "___9__Inferential_Correction", "level": 3}, {"title": "\n10. Mental Contamination and the Debiasing Problem", "anchor": "_10__Mental_Contamination_and_the_Debiasing_Problem", "level": 3}, {"title": "\n11. Sympathetic Magical Thinking: The Contagion and Similarity \"Heuristics\"", "anchor": "_11__Sympathetic_Magical_Thinking__The_Contagion_and_Similarity__Heuristics_", "level": 3}, {"title": " \n\n12. Compatibility Effects in Judgement and Choice", "anchor": "___12__Compatibility_Effects_in_Judgement_and_Choice", "level": 3}, {"title": " \n\n13. The Weighting of Evidence and the Determinants of Confidence", "anchor": "___13__The_Weighting_of_Evidence_and_the_Determinants_of_Confidence", "level": 3}, {"title": "\n14. Inside the Planning Fallacy: The Causes and Consequences of Optimistic Time Predictions", "anchor": "_14__Inside_the_Planning_Fallacy__The_Causes_and_Consequences_of_Optimistic_Time_Predictions", "level": 3}, {"title": "\n15. Probability Judgement Across Cultures", "anchor": "_15__Probability_Judgement_Across_Cultures", "level": 3}, {"title": "\n16. Durability Bias in Affective Forecasting", "anchor": "_16__Durability_Bias_in_Affective_Forecasting", "level": 3}, {"title": "\n17. Resistance of Personal Risk Perceptions to Debiasing Interventions", "anchor": "_17__Resistance_of_Personal_Risk_Perceptions_to_Debiasing_Interventions", "level": 3}, {"title": " \n\n18. Ambiguity and Self-Evaluation: The Role of Idiosyncratic Trait Definitions in Self-Serving Assesments of Ability", "anchor": "___18__Ambiguity_and_Self_Evaluation__The_Role_of_Idiosyncratic_Trait_Definitions_in_Self_Serving_Assesments_of_Ability", "level": 3}, {"title": "\n19. When Predictions Fail: The Dilemma of Unrealistic Optimism", "anchor": "_19__When_Predictions_Fail__The_Dilemma_of_Unrealistic_Optimism", "level": 3}, {"title": "\n20. Norm Theory: Comparing Reality to its Alternatives", "anchor": "_20__Norm_Theory__Comparing_Reality_to_its_Alternatives", "level": 3}, {"title": "\n21. Counterfactual Thought, Regret, and Superstition: How To Avoid Kicking Yourself", "anchor": "_21__Counterfactual_Thought__Regret__and_Superstition__How_To_Avoid_Kicking_Yourself", "level": 3}, {"title": "\n22. Two Systems of Reasoning", "anchor": "_22__Two_Systems_of_Reasoning", "level": 3}, {"title": "\n23. The Affect Heuristic", "anchor": "_23__The_Affect_Heuristic", "level": 3}, {"title": "\n24. Individual Differences in Reasoning: Implications for the Rationality Debate?", "anchor": "_24__Individual_Differences_in_Reasoning__Implications_for_the_Rationality_Debate_", "level": 3}, {"title": " \n\n25. Support Theory: A Nonextensional Representation of Subjective Probability", "anchor": "___25__Support_Theory__A_Nonextensional_Representation_of_Subjective_Probability", "level": 3}, {"title": " \n\n26. Unpacking, Repacking, and Anchoring: Advances in Support Theory", "anchor": "___26__Unpacking__Repacking__and_Anchoring__Advances_in_Support_Theory", "level": 3}, {"title": "\n27. Remarks on Support Theory: Recent Advances and Future Directions", "anchor": "_27__Remarks_on_Support_Theory__Recent_Advances_and_Future_Directions", "level": 3}, {"title": "\n28. The Use of Statistical Heuristics in Everyday Inductive Reasoning", "anchor": "_28__The_Use_of_Statistical_Heuristics_in_Everyday_Inductive_Reasoning", "level": 3}, {"title": " \n\n29. Feelings as Information: Moods Influence Judgements and Processing Strategies", "anchor": "___29__Feelings_as_Information__Moods_Influence_Judgements_and_Processing_Strategies", "level": 3}, {"title": " \n\n30. Automated Choice Heuristics", "anchor": "___30__Automated_Choice_Heuristics", "level": 3}, {"title": "Bet A", "anchor": "Bet_A", "level": 4}, {"title": "Bet B", "anchor": "Bet_B", "level": 4}, {"title": "Bet A", "anchor": "Bet_A1", "level": 4}, {"title": "Bet B", "anchor": "Bet_B1", "level": 4}, {"title": " \n\n31. How Good are Fast and Frugal Heuristics?", "anchor": "___31__How_Good_are_Fast_and_Frugal_Heuristics_", "level": 3}, {"title": "\n32. Intuitive Politicians, Theologians, and Prosecutors: Exploring the Empirical Implications of Deviant Functionalist Metaphors", "anchor": "_32__Intuitive_Politicians__Theologians__and_Prosecutors__Exploring_the_Empirical_Implications_of_Deviant_Functionalist_Metaphors", "level": 3}, {"title": " \n\n33. The Hot Hand in Basketball: On the Misprediction of Random Sequences", "anchor": "___33__The_Hot_Hand_in_Basketball__On_the_Misprediction_of_Random_Sequences", "level": 3}, {"title": "\n34. Like Goes With Like: The Role of Representativeness in Erroneous and Pesudo-Scientific Beliefs", "anchor": "_34__Like_Goes_With_Like__The_Role_of_Representativeness_in_Erroneous_and_Pesudo_Scientific_Beliefs", "level": 3}, {"title": "\n35. When Less is More: Counterfactual thinking and Satisfaction among Olympic Medalists", "anchor": "_35__When_Less_is_More__Counterfactual_thinking_and_Satisfaction_among_Olympic_Medalists", "level": 3}, {"title": " \n\n36. Understanding Misunderstandings: Social Psychological Perspectives", "anchor": "___36__Understanding_Misunderstandings__Social_Psychological_Perspectives", "level": 3}, {"title": "\n37. Assessing Uncertainty in Physical Constants", "anchor": "_37__Assessing_Uncertainty_in_Physical_Constants", "level": 3}, {"title": " \n\n38. Do Analysts Overreact?", "anchor": "___38__Do_Analysts_Overreact_", "level": 3}, {"title": "\n39. The Calibration of Expert Judgement: Heuristics and Biases Beyond the Laboratory", "anchor": "_39__The_Calibration_of_Expert_Judgement__Heuristics_and_Biases_Beyond_the_Laboratory", "level": 3}, {"title": " \n\n40. Clinical versus Actuarial Judgement", "anchor": "___40__Clinical_versus_Actuarial_Judgement", "level": 3}, {"title": "\n41. Heuristics and Biases in Application", "anchor": "_41__Heuristics_and_Biases_in_Application", "level": 3}, {"title": " \n\n42. Theory-Driven Reasoning about Plausible Pasts and Probable Futures in World Politics", "anchor": "___42__Theory_Driven_Reasoning_about_Plausible_Pasts_and_Probable_Futures_in_World_Politics", "level": 3}, {"title": "\nDiscussion", "anchor": "_Discussion", "level": 2}, {"title": "What to read", "anchor": "What_to_read", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "22 comments"}], "headingsCount": 52}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["oHAQD53FhpHte4dEM", "QAK43nNCTQQycAcYe", "bMkCEZoBNhgRBtzoj", "sSqoEw9eRP2kPKLCz", "Kow8xRzpfkoY7pa69", "TiDGXt3WrQwtCdDj3", "CPm5LTwHrvBJCa9h5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-02T17:45:58.356Z", "modifiedAt": null, "url": null, "title": "Reflective Control", "slug": "reflective-control", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:32.570Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lionhearted", "createdAt": "2010-07-29T13:30:07.417Z", "isAdmin": false, "displayName": "lionhearted"}, "userId": "tooJeLNxoeccqGEky", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9TCH6qtKkfSnjjieP/reflective-control", "pageUrlRelative": "/posts/9TCH6qtKkfSnjjieP/reflective-control", "linkUrl": "https://www.lesswrong.com/posts/9TCH6qtKkfSnjjieP/reflective-control", "postedAtFormatted": "Monday, September 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reflective%20Control&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReflective%20Control%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9TCH6qtKkfSnjjieP%2Freflective-control%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reflective%20Control%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9TCH6qtKkfSnjjieP%2Freflective-control", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9TCH6qtKkfSnjjieP%2Freflective-control", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 462, "htmlBody": "<p>You've had those moments -- the ones where you're <em>very</em>&nbsp;aware of where you're at in the world, and you're mapping out your future and plans very smartly, and you're feeling great about taking action and pushing important things forwards.</p>\n<p>I used to find myself only reaching that place, at random, once or twice per year.</p>\n<p>But every time I did, I would spend just a few hours sketching out plans, thinking about my priorities, discarding old things I used to do that didn't bring much value, and pushing my limits to do new worthwhile things. I thought, \"This is really valuable. I should do this more often.\"</p>\n<p>Eventually, I named that state: Reflective Control.</p>\n<p>As often happens, by naming something it becomes easier to do it more often.</p>\n<p>At this time, I still had a hazy poorly working feeling about what <em>it</em>&nbsp;was. So I tried to define it. After many attempts, I came to this:</p>\n<p>&gt; Reflective Control is when you're firmly off autopilot, in a high-positive and high-willpower state, and are able to take action.</p>\n<p>You'll note there's four discreet components to it: firmly off autopilot (reflective), high positivity, high will, and cable of and oriented towards taking action.</p>\n<p>I also asked myself, \"How to know if you're in Reflective Control?\"</p>\n<p class=\"p1\">My best answer of an exercise for it is,</p>\n<p class=\"p1\">&gt; You set aside the impulses/distractions, and try to set a concrete Control-related goal. This is meta-work, meaning the process of defining your life and what needs to happen next. You do this calmly. By setting a concrete Control-related goal successfully and then executing on it, you know you're in an RC state.</p>\n<p class=\"p2\">&gt; Example: \"I will identify all the open projects I've got, and the next steps for each of them.\"</p>\n<p>&nbsp;</p>\n<p class=\"p1\">With that definition and that exercise in hand, I was able to do something which works almost magically when I wanted to take on big challenges: I could rate myself from 1-100 on the four key elements of the component, and then set a concrete goal to achieve, and analyze a little about which factor might be holding me back. Here is an example from my journal:</p>\n<p class=\"p1\">&gt; Reflective 70/100, positive 70/100, will 65/100, action 40/100&hellip;&nbsp;ok, I'm feeling good once a good, just some anxiety suppressing will a little and action quite a bit, but no problem. My goal is to finish the xxx outline before I leave here.</p>\n<p class=\"p1\">I've found this incredibly useful. Summary:</p>\n<p class=\"p1\">*There's a state I call \"Reflective Control\" where I'm off autopilot and thinking (reflective), in a positive mood, with willpower and action-oriented.</p>\n<p class=\"p1\">*I can put explicit numbers on this, somewhat subjectively, from 1-100. This lets me see where the link in the chain is, if any.</p>\n<p class=\"p1\">*By setting a concrete goal and working towards it, you can get more objective feedback and balance whichever element is lowest with some practical actions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9TCH6qtKkfSnjjieP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 21, "extendedScore": null, "score": 1.3258179082910967e-06, "legacy": true, "legacyId": "23985", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-02T18:01:13.433Z", "modifiedAt": null, "url": null, "title": "How valuable is it to learn math deeply?", "slug": "how-valuable-is-it-to-learn-math-deeply", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:17.561Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fE3PTs3M5Ky4DgERh/how-valuable-is-it-to-learn-math-deeply", "pageUrlRelative": "/posts/fE3PTs3M5Ky4DgERh/how-valuable-is-it-to-learn-math-deeply", "linkUrl": "https://www.lesswrong.com/posts/fE3PTs3M5Ky4DgERh/how-valuable-is-it-to-learn-math-deeply", "postedAtFormatted": "Monday, September 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20valuable%20is%20it%20to%20learn%20math%20deeply%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20valuable%20is%20it%20to%20learn%20math%20deeply%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfE3PTs3M5Ky4DgERh%2Fhow-valuable-is-it-to-learn-math-deeply%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20valuable%20is%20it%20to%20learn%20math%20deeply%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfE3PTs3M5Ky4DgERh%2Fhow-valuable-is-it-to-learn-math-deeply", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfE3PTs3M5Ky4DgERh%2Fhow-valuable-is-it-to-learn-math-deeply", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 942, "htmlBody": "<p>I've been wondering how useful it is for the typical academically strong high schooler to learn math deeply. Here by \"learn deeply\" I mean \"understanding the concepts and their interrelations\" as opposed to learning narrow technical procedures exclusively.</p>\n<p><a id=\"more\"></a></p>\n<h2>My experience learning math deeply</h2>\n<p>When I started high school, I wasn't interested in math and I wasn't good at my math coursework. I even got a D in high school geometry, and had to repeat a semester of math.</p>\n<p>I subsequently became interested in chemistry, and I thought that I might become a chemist, and so figured that I should learn math better. During my junior year of high school, I supplemented the classes that I was taking by studying calculus on my own, and auditing a course on analytic geometry. I also took physics concurrently.</p>\n<p>Through my studies, I started seeing the same concepts over and over again in different contexts, and I became versatile with them, capable of fluently applying them in conjunction with one another. This awakened a new sense of awareness in me, of the type that Bill Thurston described in his essay <a href=\"http://arxiv.org/pdf/math/0503081.pdf\">Mathematics Education</a>:&nbsp;</p>\n<blockquote>\n<p>Mathematics is like a flight of fancy, but one in which the fanciful turns out to be real and to have been present all along. Doing mathematics has the feel of fanciful invention, but it is really a process of sharpening our perception so that we discover patterns that are everywhere around.</p>\n</blockquote>\n<p>I understood the physical world, the human world, and myself in a way that I had never before. Reality seemed full of limitless possibilities. Those months were the happiest of my life to date.</p>\n<p>More prosaically, my academic performance improved a lot, and I found it much easier to understand technical content (physics, economics, statistics etc.) ever after.</p>\n<p>So in my own case, learning math deeply had very high returns.</p>\n<h2>How generalizable is this?</h2>\n<p>I have an intuition that many other people would benefit a great deal from learning math deeply, but I know that I'm unusual, and I'm aware of&nbsp;<a href=\"/lw/dr/generalizing_from_one_example/\">the human tendency to implicitly assume that others are similar to us</a>.&nbsp;So I would like to test my beliefs by soliciting feedback from others.</p>\n<p>Some ways in which learning math deeply can help are:</p>\n<ul>\n<li><strong>Reduced need for memorization (while learning math).</strong>&nbsp;When you understand math deeply, you see how many different mathematical problems are special cases of a single more general problem, so that in order to remember how to do all of the problems, it suffices to remember the solution to that more general problem. This reduces the cognitive load of doing math relative to what it would be if one was considering each individual problem in isolation.&nbsp;When I taught calculus to freshmen at University of Illinois, I got the impression that many of the students studied for tests by trying to memorize all of the homework problems individually. There were too many homework problems to memorize, so this didn't work very well. Had they learned the material on a deep level, they wouldn't have had this problem.</li>\n<li><strong>Ability to apply knowledge in novel contexts (that require mathematical reasoning).</strong>&nbsp;When you understand general mathematical principles, you can apply mathematical knowledge to tackle mathematical problems that you've never seen before. This contrasts with mathematical knowledge that's restricted to knowledge of how to solve specified problems.</li>\n<li><strong>Higher retention of (mathematical) material.</strong>&nbsp;Cognitive psychologists have found that students retain information better when they engage in \"deep level processing\" rather than \"shallow level processing\" (see the&nbsp;<a href=\"http://www.samford.edu/uploadedFiles/How_to_Study/Teaching_Resources.pdf\">notes</a>&nbsp;on Video 2 of&nbsp;<a href=\"http://howard.samford.edu/psychology/bio.aspx?id=2147485427\">Stephen Chew's</a>&nbsp;\"How to Get the Most Out of Studying\" video series). Developing deep understanding of math reduces need to review mathematical material when one needs to know it for future units and courses (whether within math or adjacent to math). This cuts down on the amount of study time necessary to master later material.&nbsp;</li>\n<li><strong>Developing better general reasoning skills (across domains).</strong>&nbsp;Learning math deeply is closely connected with developing mathematical reasoning skills. Distilling general principles from special cases involves abstract reasoning. In the other direction, when you understand general principles, it makes mathematical reasoning feel a lot less cumbersome, which incentivizes one to do more of it (relative to the counterfactual). Mathematical reasoning ability may be transferable to reasoning ability in other contexts, so that learning math deeply builds general reasoning skills.</li>\n</ul>\n<p>Some arguments against learning math deeply being useful are:</p>\n<ul>\n<li><strong>It may be too hard.</strong>&nbsp;Sometimes when I suggest that learning math deeply is helpful, people respond by saying that most people aren't capable of learning abstract concepts with enough ease so that it makes sense for them to try to learn math deeply rather than just memorizing how to do specific problems. This is an ill-defined claim, but it can be made precise by specifying a population and a given level of mathematical abstraction.&nbsp;</li>\n<li><strong>The span of the payoff may be too short.</strong> For people who won't go on to take many math courses, the benefits of reduced future study time and higher retention might not be worth the upfront investment of learning math deeply.</li>\n<li><strong>Mathematical reasoning may not be very transferable.</strong>&nbsp;A counterpoint to the \"developing better reasoning skills\" point above: it's known that&nbsp;<a href=\"http://en.wikipedia.org/wiki/Transfer_of_learning\">transfer of learning</a>&nbsp;from one domain to another is&nbsp;<a href=\"http://econlog.econlib.org/archives/2012/08/low_transfer_of.html\">often very low</a>. So learning mathematical reasoning skills may not be an efficient way of developing reasoning skills that can be used in the context of one's career or personal life.</li>\n</ul>\n<p>I'd be grateful to anyone who's able to expand on these three considerations, or who offers additional considerations against the utility of learning math deeply. I would also be interested in any anecdotal evidence about benefits (or lack thereof) that readers have received from learning math deeply.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1, "fF9GEdWXKJ3z73TmB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fE3PTs3M5Ky4DgERh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 33, "extendedScore": null, "score": 8.3e-05, "legacy": true, "legacyId": "23974", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I've been wondering how useful it is for the typical academically strong high schooler to learn math deeply. Here by \"learn deeply\" I mean \"understanding the concepts and their interrelations\" as opposed to learning narrow technical procedures exclusively.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"My_experience_learning_math_deeply\">My experience learning math deeply</h2>\n<p>When I started high school, I wasn't interested in math and I wasn't good at my math coursework. I even got a D in high school geometry, and had to repeat a semester of math.</p>\n<p>I subsequently became interested in chemistry, and I thought that I might become a chemist, and so figured that I should learn math better. During my junior year of high school, I supplemented the classes that I was taking by studying calculus on my own, and auditing a course on analytic geometry. I also took physics concurrently.</p>\n<p>Through my studies, I started seeing the same concepts over and over again in different contexts, and I became versatile with them, capable of fluently applying them in conjunction with one another. This awakened a new sense of awareness in me, of the type that Bill Thurston described in his essay <a href=\"http://arxiv.org/pdf/math/0503081.pdf\">Mathematics Education</a>:&nbsp;</p>\n<blockquote>\n<p>Mathematics is like a flight of fancy, but one in which the fanciful turns out to be real and to have been present all along. Doing mathematics has the feel of fanciful invention, but it is really a process of sharpening our perception so that we discover patterns that are everywhere around.</p>\n</blockquote>\n<p>I understood the physical world, the human world, and myself in a way that I had never before. Reality seemed full of limitless possibilities. Those months were the happiest of my life to date.</p>\n<p>More prosaically, my academic performance improved a lot, and I found it much easier to understand technical content (physics, economics, statistics etc.) ever after.</p>\n<p>So in my own case, learning math deeply had very high returns.</p>\n<h2 id=\"How_generalizable_is_this_\">How generalizable is this?</h2>\n<p>I have an intuition that many other people would benefit a great deal from learning math deeply, but I know that I'm unusual, and I'm aware of&nbsp;<a href=\"/lw/dr/generalizing_from_one_example/\">the human tendency to implicitly assume that others are similar to us</a>.&nbsp;So I would like to test my beliefs by soliciting feedback from others.</p>\n<p>Some ways in which learning math deeply can help are:</p>\n<ul>\n<li><strong>Reduced need for memorization (while learning math).</strong>&nbsp;When you understand math deeply, you see how many different mathematical problems are special cases of a single more general problem, so that in order to remember how to do all of the problems, it suffices to remember the solution to that more general problem. This reduces the cognitive load of doing math relative to what it would be if one was considering each individual problem in isolation.&nbsp;When I taught calculus to freshmen at University of Illinois, I got the impression that many of the students studied for tests by trying to memorize all of the homework problems individually. There were too many homework problems to memorize, so this didn't work very well. Had they learned the material on a deep level, they wouldn't have had this problem.</li>\n<li><strong>Ability to apply knowledge in novel contexts (that require mathematical reasoning).</strong>&nbsp;When you understand general mathematical principles, you can apply mathematical knowledge to tackle mathematical problems that you've never seen before. This contrasts with mathematical knowledge that's restricted to knowledge of how to solve specified problems.</li>\n<li><strong>Higher retention of (mathematical) material.</strong>&nbsp;Cognitive psychologists have found that students retain information better when they engage in \"deep level processing\" rather than \"shallow level processing\" (see the&nbsp;<a href=\"http://www.samford.edu/uploadedFiles/How_to_Study/Teaching_Resources.pdf\">notes</a>&nbsp;on Video 2 of&nbsp;<a href=\"http://howard.samford.edu/psychology/bio.aspx?id=2147485427\">Stephen Chew's</a>&nbsp;\"How to Get the Most Out of Studying\" video series). Developing deep understanding of math reduces need to review mathematical material when one needs to know it for future units and courses (whether within math or adjacent to math). This cuts down on the amount of study time necessary to master later material.&nbsp;</li>\n<li><strong>Developing better general reasoning skills (across domains).</strong>&nbsp;Learning math deeply is closely connected with developing mathematical reasoning skills. Distilling general principles from special cases involves abstract reasoning. In the other direction, when you understand general principles, it makes mathematical reasoning feel a lot less cumbersome, which incentivizes one to do more of it (relative to the counterfactual). Mathematical reasoning ability may be transferable to reasoning ability in other contexts, so that learning math deeply builds general reasoning skills.</li>\n</ul>\n<p>Some arguments against learning math deeply being useful are:</p>\n<ul>\n<li><strong>It may be too hard.</strong>&nbsp;Sometimes when I suggest that learning math deeply is helpful, people respond by saying that most people aren't capable of learning abstract concepts with enough ease so that it makes sense for them to try to learn math deeply rather than just memorizing how to do specific problems. This is an ill-defined claim, but it can be made precise by specifying a population and a given level of mathematical abstraction.&nbsp;</li>\n<li><strong>The span of the payoff may be too short.</strong> For people who won't go on to take many math courses, the benefits of reduced future study time and higher retention might not be worth the upfront investment of learning math deeply.</li>\n<li><strong>Mathematical reasoning may not be very transferable.</strong>&nbsp;A counterpoint to the \"developing better reasoning skills\" point above: it's known that&nbsp;<a href=\"http://en.wikipedia.org/wiki/Transfer_of_learning\">transfer of learning</a>&nbsp;from one domain to another is&nbsp;<a href=\"http://econlog.econlib.org/archives/2012/08/low_transfer_of.html\">often very low</a>. So learning mathematical reasoning skills may not be an efficient way of developing reasoning skills that can be used in the context of one's career or personal life.</li>\n</ul>\n<p>I'd be grateful to anyone who's able to expand on these three considerations, or who offers additional considerations against the utility of learning math deeply. I would also be interested in any anecdotal evidence about benefits (or lack thereof) that readers have received from learning math deeply.</p>", "sections": [{"title": "My experience learning math deeply", "anchor": "My_experience_learning_math_deeply", "level": 1}, {"title": "How generalizable is this?", "anchor": "How_generalizable_is_this_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "79 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 79, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["baTWMegR42PAsH9qJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-03T01:58:16.970Z", "modifiedAt": null, "url": null, "title": "Useful Habits Repository", "slug": "useful-habits-repository", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:09.130Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XFrequentist", "createdAt": "2009-03-22T17:06:22.991Z", "isAdmin": false, "displayName": "XFrequentist"}, "userId": "zfW5w3TbDWjRW3YaD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qY4APiXbXESb7yTEG/useful-habits-repository", "pageUrlRelative": "/posts/qY4APiXbXESb7yTEG/useful-habits-repository", "linkUrl": "https://www.lesswrong.com/posts/qY4APiXbXESb7yTEG/useful-habits-repository", "postedAtFormatted": "Tuesday, September 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Useful%20Habits%20Repository&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUseful%20Habits%20Repository%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqY4APiXbXESb7yTEG%2Fuseful-habits-repository%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Useful%20Habits%20Repository%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqY4APiXbXESb7yTEG%2Fuseful-habits-repository", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqY4APiXbXESb7yTEG%2Fuseful-habits-repository", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<p><a href=\"/lw/i64/repository_repository/\">Repositories</a>&nbsp;are awesome, and <a href=\"/lw/hyu/meta_open_threads_and_repository_threads_are/\">we should have more of them</a>.</p>\n<p>I recently completed <a href=\"http://tinyhabits.com/\">BJ Fogg's Tiny Habits</a>, and it's a pretty lightweight way to install new habits (recommended). However, I realized I could use a better repository of useful habits.</p>\n<p>So, please use this thread to suggest habits that you've found useful. Bonus points for evidence/anecdata of usefulness.</p>\n<p>Obviously, 1 habit per comment makes upvotes a clearer signal of collective approval.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qY4APiXbXESb7yTEG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 14, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "23987", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 95, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sEaDmtwrmTC7kTqcf", "P8Qc7te5qcEnpD3nv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-03T03:50:06.585Z", "modifiedAt": null, "url": null, "title": "True Optimisation ", "slug": "true-optimisation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:05.821Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "LearnFromObservation", "createdAt": "2012-03-23T15:42:29.886Z", "isAdmin": false, "displayName": "LearnFromObservation"}, "userId": "hfo34BKj78EEpbXwh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NnkZZoPcfxfWKzo6o/true-optimisation", "pageUrlRelative": "/posts/NnkZZoPcfxfWKzo6o/true-optimisation", "linkUrl": "https://www.lesswrong.com/posts/NnkZZoPcfxfWKzo6o/true-optimisation", "postedAtFormatted": "Tuesday, September 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20True%20Optimisation%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATrue%20Optimisation%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNnkZZoPcfxfWKzo6o%2Ftrue-optimisation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=True%20Optimisation%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNnkZZoPcfxfWKzo6o%2Ftrue-optimisation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNnkZZoPcfxfWKzo6o%2Ftrue-optimisation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 347, "htmlBody": "<p>Hello less wrong community! This is my first post here, so I know that my brain has not (obviously) been optimised to its fullest, but I've decided to give posting a try.&nbsp;</p>\n<p>Recently, someone very close to me has unfortunately passed away, leading to the invitable inner dilemma about death. I don't know how many of you are fans of HPMOR, but the way that Harry's dark side feels about death? Pretty much me around death, dying, etc. however, I've decided to push that to the side for the time being, because that is not a useful of efficient way to think.&nbsp;</p>\n<p>I was raised by a religious family, but from the age of about 11 stopped believing in deities and religious services. However, I've always clung to the idea of an afterlife for people, mainly because my brain seems incapable of handling the idea of ceasing to exist. I know that we as a scientific community know that thoughts are electrical impulses, so is there any way of storing them outside of brain matter? Can they exist freely out of brain matter, or could they be stored in a computer chip or AI?&nbsp;</p>\n<p>The conflict lies here: is immortality or mortality rational?&nbsp;</p>\n<p>Every fibre in my being tells me that death is irrational and wrong. It is irrational for humanity to not try and prevent death. It is irrational for people to not try and bring back people who have died. Because of this, we have lost some of the greatest minds, scientific and artistic, that will probably ever exist. Although the worlds number of talented and intelligent people does not appear to be finite, I find it hard to live in a world where so muh knowledge is being lost every day.</p>\n<p>but on the other hand, how would we feed all those people? What if the world's resources run out? As a transhumanist, I believe that we can use science to prevent things like death, but nature wasn't designed to support a population like that.&nbsp;</p>\n<p>How do we truly optimise the world: no death and without destruction of the planet?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NnkZZoPcfxfWKzo6o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -5, "extendedScore": null, "score": 1.3263407818358057e-06, "legacy": true, "legacyId": "23988", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-03T05:40:20.396Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign, Illinois", "slug": "meetup-urbana-champaign-illinois", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:07.998Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CHYQ7eP5fZGLdeGsn/meetup-urbana-champaign-illinois", "pageUrlRelative": "/posts/CHYQ7eP5fZGLdeGsn/meetup-urbana-champaign-illinois", "linkUrl": "https://www.lesswrong.com/posts/CHYQ7eP5fZGLdeGsn/meetup-urbana-champaign-illinois", "postedAtFormatted": "Tuesday, September 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%2C%20Illinois&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%2C%20Illinois%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCHYQ7eP5fZGLdeGsn%2Fmeetup-urbana-champaign-illinois%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%2C%20Illinois%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCHYQ7eP5fZGLdeGsn%2Fmeetup-urbana-champaign-illinois", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCHYQ7eP5fZGLdeGsn%2Fmeetup-urbana-champaign-illinois", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 75, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qh'>Urbana-Champaign, Illinois</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 September 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Illini Union South Lounge 1401 W Green St Urbana, IL 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup topic will again be determined by popular consensus at the actual meetup. I will have Zendo, Wits and Wagers (with cards that can be used independently for calibration games), and Pandemic, a cooperative strategy board game. <a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/KxDmbfWTz3M\">Cross posted on the mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qh'>Urbana-Champaign, Illinois</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CHYQ7eP5fZGLdeGsn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.3264362241030973e-06, "legacy": true, "legacyId": "23989", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Illinois\">Discussion article for the meetup : <a href=\"/meetups/qh\">Urbana-Champaign, Illinois</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 September 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Illini Union South Lounge 1401 W Green St Urbana, IL 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup topic will again be determined by popular consensus at the actual meetup. I will have Zendo, Wits and Wagers (with cards that can be used independently for calibration games), and Pandemic, a cooperative strategy board game. <a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/KxDmbfWTz3M\">Cross posted on the mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Illinois1\">Discussion article for the meetup : <a href=\"/meetups/qh\">Urbana-Champaign, Illinois</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign, Illinois", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Illinois", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign, Illinois", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Illinois1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-03T10:00:18.863Z", "modifiedAt": null, "url": null, "title": "Ignorance in parenting", "slug": "ignorance-in-parenting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:39.239Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/75jogPJeSCBtPArex/ignorance-in-parenting", "pageUrlRelative": "/posts/75jogPJeSCBtPArex/ignorance-in-parenting", "linkUrl": "https://www.lesswrong.com/posts/75jogPJeSCBtPArex/ignorance-in-parenting", "postedAtFormatted": "Tuesday, September 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ignorance%20in%20parenting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIgnorance%20in%20parenting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F75jogPJeSCBtPArex%2Fignorance-in-parenting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ignorance%20in%20parenting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F75jogPJeSCBtPArex%2Fignorance-in-parenting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F75jogPJeSCBtPArex%2Fignorance-in-parenting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 421, "htmlBody": "<p><strong>Followup to:</strong> <a href=\"/lw/72d/strategic_ignorance_and_plausible_deniability/\">Strategic ignorance and plausible deniability</a></p>\n<p>My in-law always says: \"For children it is easier be forgiven then to get permission.\"&nbsp;</p>\n<p><strong>EDIT:</strong> This post is superseeded by my&nbsp;<a href=\"/r/discussion/lw/jzg/book_review_kazdins_the_everyday_parenting_toolkit/\">Book Review: Kazdin's The Everyday Parenting Toolkit</a>&nbsp;I recommend reading only that. The remaining insight of this post is: Children expend more brain power on their parents than the parents on them.&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<div>Kaj_Sotala wrote</div>\n<blockquote>\n<p>Parents may also pretend that they don't notice their kids engaging in some minor misbehavior, if they don't want to lose their authority but don't feel like interfering either.</p>\n</blockquote>\n<p>I can say from experience: <strong>That is risky.</strong></p>\n<p>Children (esp. small ones) expend significantly more brain power on their parents than the parents on their children (your mileage may vary). I can assure you that they will notice these cases - at least some - and take that into account one way or the other.</p>\n<p>If the children notice this they may assume that you either condone, accept, bear or ignore it. None of these has positive effects.</p>\n<p><strong>Possible alternative strategies:</strong></p>\n<ul>\n<li>Invest the energy on the necessary consequences (these will be well invested because you save on lots of further occassions)&nbsp;</li>\n<li>Signal the child that you did notice, tolerate but do not accept it. This indicates to the child that the border to really unacceptable behavior is near. If the child still continues you do not get around to act. I do this with a smile and treat it as a gamed played by the child. This breaks a possible tension but may have disadvantages for discipline. &nbsp;</li>\n<li>Establish clear limits to acceptable behavior that avoid repeated testing of where the border actually is.&nbsp;</li>\n<li>Less restrictions that may be broken to begin with.</li>\n</ul>\n<p>I am influenced by <a href=\"http://en.wikipedia.org/wiki/Alfred_Adler#The_Adlerian_School\">The Adlerian School</a>.&nbsp;Of relevance here is <a href=\"http://en.wikipedia.org/wiki/Adlerian_psychology#Striving_for_significance\">Striving for significance</a>.</p>\n<p>The testing of limits and the resulting interaction with the parent give the child a feeling of significance if the parent acknoledges the act of the child even if he doesn't agree with it. On the other hand ignoring the act of the child is negative feedback about significance.</p>\n<p>EDIT: The asymmetry between parents and children with respect to the effectiveness of deniability can be generalized to any situation where one actor has significantly less overall information about the situation than another actor and thus might not be able to reliably estimate whether deniability is possible.</p>\n<p>ADDED:&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 14.399999618530273px; text-align: justify; background-color: #f7f7f8;\"><span class=\"author\"><a href=\"/lw/iif/ignorance_in_parenting/adm5\"><span style=\"color: #8a8a8b;\">tadamsmar</span>&nbsp; pointed out</a>&nbsp;</span></span><span style=\"background-color: #f7f7f8; font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 14.399999618530273px; text-align: justify;\">that ignoring is scientifically known to be effective and the advice or rather personal expierence I have related in this post may be contraproductive (at least if applied in isolation).&nbsp;</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q55STnFh6gbSezRuR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "75jogPJeSCBtPArex", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 17, "extendedScore": null, "score": 6.1e-05, "legacy": true, "legacyId": "23991", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fxgkYCbG5Hgy58TyC", "Sw22Fr6kv42kmLpHx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-04T00:44:02.428Z", "modifiedAt": null, "url": null, "title": "Help describing decibans?", "slug": "help-describing-decibans", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:07.675Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hPR4jF8jJaQrJSyyZ/help-describing-decibans", "pageUrlRelative": "/posts/hPR4jF8jJaQrJSyyZ/help-describing-decibans", "linkUrl": "https://www.lesswrong.com/posts/hPR4jF8jJaQrJSyyZ/help-describing-decibans", "postedAtFormatted": "Wednesday, September 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20describing%20decibans%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20describing%20decibans%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhPR4jF8jJaQrJSyyZ%2Fhelp-describing-decibans%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20describing%20decibans%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhPR4jF8jJaQrJSyyZ%2Fhelp-describing-decibans", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhPR4jF8jJaQrJSyyZ%2Fhelp-describing-decibans", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1504, "htmlBody": "<p>I'm in the process of writing up an <a href=\"https://datatracker.ietf.org/doc/draft-boese-vcarddav-signedvcard/\">Internet Draft</a> for a file-format, part of which involves assigning logarithmic confidence values measured in <a href=\"https://en.wikipedia.org/wiki/Ban_%28information%29\">decibans</a>. There's still a good ways to go before it'll be in good enough shape to even have a chance at being considered for an RFC; part of that is describing \"decibans\", and how to use them, to people who've never heard of the things before. I'd like to get a good introductory text for that nailed down before submitting the next revision; and since LW is where I first learned of decibans, I'd like to evoke as much constructive criticism as I can get here.</p>\n<p><a id=\"more\"></a></p>\n<p>Here's my current draft of the text to replace the relevant section of the current revision:</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>3.1.&nbsp; Parameter: CONFIDENCE<br /><br />Namespace:<br /><br />Parameter name: CONFIDENCE<br /><br />Purpose: To specify the confidence of the authority that the <br />information of the given parameter is accurate, measured in decibans.<br /><br />Value type: A single number, usually an integer.<br /><br /><br />Description:<br /><br />A CONFIDENCE value of 0 decibans indicates odds of 1:1 (ie, 50%) that <br />the information is correct. A change of 10 decibans changes the odds by <br />a factor of 10; 10 decibans means 1:10 odds (~90%), 100 decibans 1:100 <br />odds (~99%), -10 decibans 10:1 odds against (~10%). A change of 1 <br />deciban is roughly equivalent to changing the odds by a factor of 5:4.<br /><br />Here is a table covering enough integer deciban values to allow for <br />easy reference.<br /><br />Decibans / Level of belief / Rough Odds / notes<br /><br />-127 / 0.00000000002% / 1:5 trillion<br /><br />-30 / 0.1% / 1:1,000<br />-20 / 1.0% / 1:100<br /><br />-10 / 9.9% / 1:10<br />-9 / 11.2% / 1:8<br />-8 / 13.7% / 1:6<br />-7 / 16.7% / 1:5<br />-6 / 20.0% / 1:4<br />-5 / 24.0% / 1:3<br />-4 / 28.5% / 2:5 / a reasonable doubt<br />-3 / 33.3% / 1:2<br />-2 / 38.7% / 2:3 / probable cause<br />-1 / 44.3% / 4:5<br />0 / 50.0% / 1:1 / neither belief nor disbelief; agnosticism, 1 bit<br />1 / 55.7% / 5:4, 1.25:1 / preponderance of the evidence<br />2 / 61.3% / 3:2, 1.5:1<br />3 / 66.6% / 2:1 / clear and convincing evidence<br />4 / 71.5% / 5:2, 2.5:1<br />5 / 76.0% / 3:1 / beyond a reasonable doubt, 1 standard deviation, 2 bits<br />6 / 80.0% / 4:1<br />7 / 83.3% / 5:1<br />8 / 86.3% / 6:1<br />9 / 88.8% / 8:1 / 3 bits<br />10 / 90.9% / 10:1 / one nine<br />11 / 92.6% / 25:2, 12.5:1<br />12 / 92.5% / 15:1 / 4 bits<br />13 / 95.2% / 20:1 / lone studies with p=0.05, 2 standard deviations<br />14 / 96.2% / 25:1<br />15 / 96.8% / 30:1 / 5 bits<br />16 / 97.6% / 40:1<br />17 / 98.0% / 50:1<br />18 / 98.4% / 60:1 / 6 bits<br />19 / 98.8% / 80:1<br />20 / 99.0% / 100:1 / two nines, lone studies with p=0.01<br /><br />21 / 99.26% / 125:1 / 7 bits<br />24 / 99.62% / 250:1 / 8 bits<br />26 / 99.7% / 400:1 / confirmed studies with p=0.05, 3 standard deviations<br />27 / 99.80% / 500:1 / 9 bits<br />30 / 99.9% / 1,000:1 / three nines, 10 bits<br />40 / 99.99% / 10,000:1 / four nines, confirmed studies with p=0.01<br />42 / 99.993% / 16,000:1 / 4 standard deviations, 14 bits<br />45 / 99.9968% / 30,000:1 / 15 bits<br />50 / 99.999% / 100,000:1 / five nines<br />60 / 99.9999% / 1 million:1 / six nines, 20 bits<br />62 / 99.99994% / 1.5 million:1 / 5 standard deviations<br />70 / 99.99999% / 10 million:1 / seven nines<br />80 / 99.999999% / 100 million:1 / eight nines<br />87 / 99.9999998% / 500 million:1 / 6 standard deviations, 29 bits<br />90 / 99.9999999% / 1 billion:1 / nine nines, 30 bits<br />99 / 99.99999998% / 8 billion:1 / 33 bits<br />100 / 99.99999999% / 10 billion:1 / ten nines<br />110 / 99.999999999% / 100 billion:1 / eleven nines<br />116 / 99.9999999997% / 400 billion:1 / 7 standard deviations<br />120 / 99.9999999999% / 1 trillion:1 / twelve nines, 40 bits<br />127 / 99.99999999998% / 5 trillion:1<br /><br />Given human factors, it is rare for hand-typed data to be able to have <br />a CONFIDENCE that every single bit is accurate of more than 50 <br />decibans. Without getting into the details of recursion, and given that <br />at least one out of roughly ten billion people is thoroughly <br />disconnected from reality, it's very difficult for a human to have more <br />than 100 decibans of confidence in anything, even that H2O is a useful <br />description of water or that the subjective reality they are <br />experiencing is connected to the same subjective reality experienced by <br />other humans.<br /><br /><br />If a user wishes to manually generate signed vCards, but does not have <br />much experience with mathematics, then one option to get rough <br />estimates of what CONFIDENCE values are appropriate could be to use <br />Laplace's Sunrise Formula, also known as the Rule of Succession. This <br />takes two pieces of input: the number of times in which something might <br />have gone one way or the other; and the number of times it went one <br />way. For example, it might be used with the number of times an email <br />has been received from a particular address, and the number of times <br />that email has been from the owner of that address instead of viral <br />spam. The formula produces an estimate of the odds that future trials<br />will go the same way, by calculating:<br /><br />FutureProbability = (Successes + 1) / (TotalTrials + 2)<br /><br />For the example, if one has received 1,000 emails from an address, out <br />of which 1 was spam, then the formula says that the future probability<br />will be on the order of (999+1) / (1,000+2) = 1,000/1,002. This implies<br />that a CONFIDENCE value based on this data would be on the order of 30 <br />decibans - but, barring other forms of evidence, it would take around <br />10,000 such emails before a claim of 40 decibans of confidence would be <br />warranted.<br /><br />Note that this is an extremely simple formula, and there are many <br />better ones that can provide more accurate results, and take into <br />account more kinds of evidence. Any user who knows of a better method <br />to generate CONFIDENCE values should use those ways; the Sunrise <br />Formula is provided as a basis for users who have nothing else to <br />create estimates with.<br /><br />More sophisticated Bayesian analyses can be used to create ad-hoc <br />certificate authority systems. This would involve one vCard with an <br />authority describing itself, and signing it; another vCard where that <br />authority issues a card describing a second entity and its key, using <br />the CONFIDENCE parameter to give its Bayesianically-generated level of <br />belief; and a third card where that second entity describes a third, <br />offering its CONFIDENCE level. A user with access to all the vCards <br />could then determine, based on its own trust-level of the root <br />authority, how much to trust the other entities. This trust of the <br />root authority could be generated either with the Sunrise Formula, or <br />with an analysis of web-of-trust data.<br /><br /><br />Examples:<br /><br />BIRTHPLACE;CONFIDENCE=50:Winnipeg, Manitoba, Canada<br /><br />ABNF:<br /><br />confidence-param = \"CONFIDENCE=\" (INTEGER / FLOAT)<br /></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hPR4jF8jJaQrJSyyZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 1.3274272093635706e-06, "legacy": true, "legacyId": "23993", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-04T01:44:05.445Z", "modifiedAt": null, "url": null, "title": "Meetup : Phoenix Tuesday Lunch Group", "slug": "meetup-phoenix-tuesday-lunch-group", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:05.121Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Danny_Hintze", "createdAt": "2010-12-04T23:01:40.826Z", "isAdmin": false, "displayName": "Danny_Hintze"}, "userId": "2fHm6t2WFDMPShg5b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MssR2ezmSeMbEqatA/meetup-phoenix-tuesday-lunch-group", "pageUrlRelative": "/posts/MssR2ezmSeMbEqatA/meetup-phoenix-tuesday-lunch-group", "linkUrl": "https://www.lesswrong.com/posts/MssR2ezmSeMbEqatA/meetup-phoenix-tuesday-lunch-group", "postedAtFormatted": "Wednesday, September 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Phoenix%20Tuesday%20Lunch%20Group&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Phoenix%20Tuesday%20Lunch%20Group%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMssR2ezmSeMbEqatA%2Fmeetup-phoenix-tuesday-lunch-group%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Phoenix%20Tuesday%20Lunch%20Group%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMssR2ezmSeMbEqatA%2Fmeetup-phoenix-tuesday-lunch-group", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMssR2ezmSeMbEqatA%2Fmeetup-phoenix-tuesday-lunch-group", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qi'>Phoenix Tuesday Lunch Group</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 September 2013 12:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1290 S. Normal Ave., Tempe, AZ, 85287</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is now a weekly thing! We will be meeting at Engrained in the MU. Currently we are getting to know each other, and have hopes of planning some future meetings on specific topics in evening time slots.</p>\n\n<p>My phone number is 602-501-9420, text is best!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qi'>Phoenix Tuesday Lunch Group</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MssR2ezmSeMbEqatA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.327479277462143e-06, "legacy": true, "legacyId": "23994", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Phoenix_Tuesday_Lunch_Group\">Discussion article for the meetup : <a href=\"/meetups/qi\">Phoenix Tuesday Lunch Group</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 September 2013 12:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1290 S. Normal Ave., Tempe, AZ, 85287</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is now a weekly thing! We will be meeting at Engrained in the MU. Currently we are getting to know each other, and have hopes of planning some future meetings on specific topics in evening time slots.</p>\n\n<p>My phone number is 602-501-9420, text is best!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Phoenix_Tuesday_Lunch_Group1\">Discussion article for the meetup : <a href=\"/meetups/qi\">Phoenix Tuesday Lunch Group</a></h2>", "sections": [{"title": "Discussion article for the meetup : Phoenix Tuesday Lunch Group", "anchor": "Discussion_article_for_the_meetup___Phoenix_Tuesday_Lunch_Group", "level": 1}, {"title": "Discussion article for the meetup : Phoenix Tuesday Lunch Group", "anchor": "Discussion_article_for_the_meetup___Phoenix_Tuesday_Lunch_Group1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-04T05:02:05.267Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes September 2013", "slug": "rationality-quotes-september-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:08.775Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FujR4L9NbGfxa3Ydm/rationality-quotes-september-2013", "pageUrlRelative": "/posts/FujR4L9NbGfxa3Ydm/rationality-quotes-september-2013", "linkUrl": "https://www.lesswrong.com/posts/FujR4L9NbGfxa3Ydm/rationality-quotes-september-2013", "postedAtFormatted": "Wednesday, September 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20September%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20September%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFujR4L9NbGfxa3Ydm%2Frationality-quotes-september-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20September%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFujR4L9NbGfxa3Ydm%2Frationality-quotes-september-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFujR4L9NbGfxa3Ydm%2Frationality-quotes-september-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<div id=\"entry_t3_hlk\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>\n<ul>\n<li>Please post all quotes separately, so that they can be upvoted or  downvoted separately. (If they are strongly related, reply to your own  comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or  Robin Hanson. If you'd like to revive an old quote from one of those  sources, please do so <a href=\"http://lesswrong.com/r/discussion/lw/i6h/rationality_quotes_from_people_associated_with/\">here</a>.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FujR4L9NbGfxa3Ydm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "23973", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 457, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iWTZj26MfR8e8b9nm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-04T08:07:26.074Z", "modifiedAt": null, "url": null, "title": "Supposing you inherited an AI project...", "slug": "supposing-you-inherited-an-ai-project", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:02.807Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bokov", "createdAt": "2010-01-11T01:11:23.480Z", "isAdmin": false, "displayName": "bokov"}, "userId": "4sgsBYAsjDHNvB7Q6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fcLBxzJRo2E6JbNHq/supposing-you-inherited-an-ai-project", "pageUrlRelative": "/posts/fcLBxzJRo2E6JbNHq/supposing-you-inherited-an-ai-project", "linkUrl": "https://www.lesswrong.com/posts/fcLBxzJRo2E6JbNHq/supposing-you-inherited-an-ai-project", "postedAtFormatted": "Wednesday, September 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Supposing%20you%20inherited%20an%20AI%20project...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASupposing%20you%20inherited%20an%20AI%20project...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfcLBxzJRo2E6JbNHq%2Fsupposing-you-inherited-an-ai-project%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Supposing%20you%20inherited%20an%20AI%20project...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfcLBxzJRo2E6JbNHq%2Fsupposing-you-inherited-an-ai-project", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfcLBxzJRo2E6JbNHq%2Fsupposing-you-inherited-an-ai-project", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 225, "htmlBody": "<p>Supposing you have been recruited to be the main developer on an AI project. The previous developer died in a car crash and left behind an unfinished AI. It consists of:</p>\n<p>A. A thoroughly documented scripting language specification that appears to be capable of representing any real-life program as a network diagram so long as you can provide the following:</p>\n<p>&nbsp;A.1. A node within the network whose value you want to maximize or minimize.</p>\n<p>&nbsp;A.2. Conversion modules that transform data about the real-world phenomena your network represents into a form that the program can read.</p>\n<p>B. Source code from which a program can be compiled that will read scripts in the above language. The program outputs a set of values for each node that will optimize the output (you can optionally specify which nodes can and cannot be directly altered, and the granularity with which they can be altered).</p>\n<p>It gives remarkably accurate answers for well-formulated questions. Where there is a theoretical limit to the accuracy of an answer to a particular type of question, its answer usually comes close to that limit, plus or minus some tiny rounding error.</p>\n<p>&nbsp;</p>\n<p>Given that, what is the minimum set of additional features you believe would absolutely have to be implemented before this program can be enlisted to save the world and make everyone live happily forever? Try to be as specific as possible.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fcLBxzJRo2E6JbNHq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -10, "extendedScore": null, "score": 1.327811750572159e-06, "legacy": true, "legacyId": "23997", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-04T16:56:17.883Z", "modifiedAt": null, "url": null, "title": "Confidence In Opinions, Intensity In Opinion", "slug": "confidence-in-opinions-intensity-in-opinion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:07.640Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lionhearted", "createdAt": "2010-07-29T13:30:07.417Z", "isAdmin": false, "displayName": "lionhearted"}, "userId": "tooJeLNxoeccqGEky", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zW4RkvzzRyK8xRDH6/confidence-in-opinions-intensity-in-opinion", "pageUrlRelative": "/posts/zW4RkvzzRyK8xRDH6/confidence-in-opinions-intensity-in-opinion", "linkUrl": "https://www.lesswrong.com/posts/zW4RkvzzRyK8xRDH6/confidence-in-opinions-intensity-in-opinion", "postedAtFormatted": "Wednesday, September 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Confidence%20In%20Opinions%2C%20Intensity%20In%20Opinion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConfidence%20In%20Opinions%2C%20Intensity%20In%20Opinion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzW4RkvzzRyK8xRDH6%2Fconfidence-in-opinions-intensity-in-opinion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Confidence%20In%20Opinions%2C%20Intensity%20In%20Opinion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzW4RkvzzRyK8xRDH6%2Fconfidence-in-opinions-intensity-in-opinion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzW4RkvzzRyK8xRDH6%2Fconfidence-in-opinions-intensity-in-opinion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 209, "htmlBody": "<p>On a scale of 1 to 100, how sure are you?</p>\n<p>It's a good thing to ask yourself from time to time about intense beliefs, especially if you're having a disagreement with someone else smart.</p>\n<p>Just putting a number on something is good. If you're in business, putting any number in the high 90's is dangerous and shouldn't happen too often.</p>\n<p><em>Yet, you still have to aggressively and intensely pursue your plans.</em></p>\n<p>You can be only 80% sure you're correct, and still intensely pursue a course of action.</p>\n<p>Most people make a mistake: they only go intensely after things they have a very high certainty will work.</p>\n<p>But this is backwards. It's absolutely right to say \"I'm only 80% sure that going and making a great talk to this group will help develop my business,\" <em>and to still aggressively pursue giving a great talk.</em></p>\n<p>The same is true with having ridiculously exceptionally good service. You can say, \"I'm only 60% sure that doing this is going to lead to more customer loyalty... this might just be a time sink and cost more than it returns. But let's kill it on it, and find it.\"</p>\n<p>You don't need to be highly confident to intensely pursue something.</p>\n<p>In fact, intensely pursuing not-certain things seems to be how the world develops.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zW4RkvzzRyK8xRDH6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 0, "extendedScore": null, "score": 1.3282706744974408e-06, "legacy": true, "legacyId": "23998", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-04T17:38:43.779Z", "modifiedAt": null, "url": null, "title": "Meetup : Zagreb Meetup", "slug": "meetup-zagreb-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "IsTheLittleLion", "createdAt": "2011-10-05T22:34:47.894Z", "isAdmin": false, "displayName": "IsTheLittleLion"}, "userId": "Pn3ReoS9v58Jb8prT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GLB7jvofkQusPbPZ6/meetup-zagreb-meetup", "pageUrlRelative": "/posts/GLB7jvofkQusPbPZ6/meetup-zagreb-meetup", "linkUrl": "https://www.lesswrong.com/posts/GLB7jvofkQusPbPZ6/meetup-zagreb-meetup", "postedAtFormatted": "Wednesday, September 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Zagreb%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Zagreb%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGLB7jvofkQusPbPZ6%2Fmeetup-zagreb-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Zagreb%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGLB7jvofkQusPbPZ6%2Fmeetup-zagreb-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGLB7jvofkQusPbPZ6%2Fmeetup-zagreb-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qj'>Zagreb Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 September 2013 05:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Stupni\u010dka 14, Zagreb, Croatia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup will start with open discussion, but we have prepared some exercises and games to play :)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qj'>Zagreb Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GLB7jvofkQusPbPZ6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.3283075068785762e-06, "legacy": true, "legacyId": "23999", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Zagreb_Meetup\">Discussion article for the meetup : <a href=\"/meetups/qj\">Zagreb Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 September 2013 05:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Stupni\u010dka 14, Zagreb, Croatia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meetup will start with open discussion, but we have prepared some exercises and games to play :)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Zagreb_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/qj\">Zagreb Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Zagreb Meetup", "anchor": "Discussion_article_for_the_meetup___Zagreb_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Zagreb Meetup", "anchor": "Discussion_article_for_the_meetup___Zagreb_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-04T17:48:55.709Z", "modifiedAt": null, "url": null, "title": "Types of recursion", "slug": "types-of-recursion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:02.623Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnthonyC", "createdAt": "2011-03-27T21:10:52.616Z", "isAdmin": false, "displayName": "AnthonyC"}, "userId": "E7Y53DiubddWFRLwE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5Mu3pniP9SB4FiJRn/types-of-recursion", "pageUrlRelative": "/posts/5Mu3pniP9SB4FiJRn/types-of-recursion", "linkUrl": "https://www.lesswrong.com/posts/5Mu3pniP9SB4FiJRn/types-of-recursion", "postedAtFormatted": "Wednesday, September 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Types%20of%20recursion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATypes%20of%20recursion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5Mu3pniP9SB4FiJRn%2Ftypes-of-recursion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Types%20of%20recursion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5Mu3pniP9SB4FiJRn%2Ftypes-of-recursion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5Mu3pniP9SB4FiJRn%2Ftypes-of-recursion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 256, "htmlBody": "<p>As a freshman in college I took an intro linguistics class where we spent a lot of time discussing universal grammar and recursive phrase structures. One of the examples we looked at I still don't fully understand - it illustrated two distinct forms of nested phrases that he mind handles very differently.</p>\n<p>1. Nested prepositional phrases</p>\n<p><span style=\"white-space: pre;\"> </span>The car in the driveway of the house on the street in NY...</p>\n<p>I can make that sentence go on indefinitely, and while a reader (or listener) might get bored or forget parts, it will never feel confusing. It's just (The car (in the driveway (of the house (on the street (...))))).</p>\n<p>2. Nested tense phrases</p>\n<p><span style=\"white-space: pre;\"> </span>The mouse the cat the dog the man walked barked at chased ate the cheese.</p>\n<p><span style=\"white-space: pre;\"> </span>Yes, it's grammatical. The mouse ate the cheese. (The mouse the cat chased) ate the cheese. &nbsp;The mouse (the cat the dog barked at) chased ate the cheese. The mouse the cat (the dog the man walked) barked at chased at the cheese.</p>\n<p><span style=\"white-space: pre;\"> </span>Personally, I lose track with the introduction of the dog. At first I thought it was just a matter of working memory, but the information content is not that high. I can even turn it back into the first kind of recursion and then suddenly have no difficulty keeping it all in my head: The man walked the dog that barked at the cat that chased the mouse that ate the cheese. It seems to be more a bug in my natural language processing module.&nbsp;</p>\n<p>Any suggestions on what might be going on here?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5Mu3pniP9SB4FiJRn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 22, "extendedScore": null, "score": 5.5e-05, "legacy": true, "legacyId": "24000", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-04T17:56:33.729Z", "modifiedAt": null, "url": null, "title": "Which subreddits should we create on Less Wrong?", "slug": "which-subreddits-should-we-create-on-less-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:01.009Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kFwtkx9wtSzssYv9W/which-subreddits-should-we-create-on-less-wrong", "pageUrlRelative": "/posts/kFwtkx9wtSzssYv9W/which-subreddits-should-we-create-on-less-wrong", "linkUrl": "https://www.lesswrong.com/posts/kFwtkx9wtSzssYv9W/which-subreddits-should-we-create-on-less-wrong", "postedAtFormatted": "Wednesday, September 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Which%20subreddits%20should%20we%20create%20on%20Less%20Wrong%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhich%20subreddits%20should%20we%20create%20on%20Less%20Wrong%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFwtkx9wtSzssYv9W%2Fwhich-subreddits-should-we-create-on-less-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Which%20subreddits%20should%20we%20create%20on%20Less%20Wrong%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFwtkx9wtSzssYv9W%2Fwhich-subreddits-should-we-create-on-less-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFwtkx9wtSzssYv9W%2Fwhich-subreddits-should-we-create-on-less-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 339, "htmlBody": "<p>Less Wrong is based on <a href=\"http://www.reddit.com/\">reddit</a> code, which means we can create <a href=\"http://www.reddit.com/reddits/\">subreddits</a> with relative ease.</p>\n<p>Right now we have two subreddits, Main and Discussion. These are distinguished not by subject matter, but by whether a post is the <em>type</em> of thing that might be promoted to the front page or not (e.g. a meetup announcement, or a particularly well-composed and useful post).</p>\n<p>As a result, almost everything is published to Discussion, and thus <strong>it is difficult for busy people to follow only the subjects they care about</strong>. More people will be able to engage if we split things into topic-specific subreddits, and make it easy to follow only what they care about.</p>\n<p>To make it easier for people to follow only what they care about, we're building the code for a Dashboard thingie.</p>\n<p>But we also need to figure out <em>which</em> subreddits to create, and we'd like community feedback about that.</p>\n<p>We'll probably start small, with just 1-5 new subreddits.</p>\n<p>Below are some initial ideas, to get the conversation started.</p>\n<p>&nbsp;</p>\n<p><strong>Idea 1</strong></p>\n<p>\n<ul>\n<li><em>Main</em>: still the place for things that might be promoted.</li>\n<li><em>Applied Rationality</em>: for articles about&nbsp;<a href=\"/lw/gs5/improving_human_rationality_through_cognitive/\">what Jonathan Baron would call</a>&nbsp;descriptive and prescriptive rationality, for both epistemic and instrumental rationality (stuff about biases, self-improvement stuff, etc.).</li>\n<li><em>Normative Rationality</em>: for articles about what Baron would call normative rationality, for both epistemic and instrumental rationality (examining the foundations of probability theory, decision theory, anthropics, and lots of stuff that is called \"philosophy\").&nbsp;</li>\n<li><em>The Future</em>: for articles about forecasting, x-risk, and future technologies.</li>\n<li><em>Misc</em>:&nbsp;Discussion, renamed, for everything that doesn't belong in the other subreddits.</li>\n</ul>\n<strong></strong></p>\n<p>&nbsp;</p>\n<p><strong>Idea 2</strong></p>\n<p>\n<ul>\n<li><em>Main</em></li>\n<li><em>Epistemic Rationality</em>: for articles about how to figure out the world, spanning the descriptive, prescriptive, and normative.</li>\n<li><em>Instrumental Rationality</em>: for articles about how to take action to achieve your goals, spanning the descriptive, prescriptive, and normative. (One difficulty with the epistemic/instrumental split is that many (most?) applied rationality techniques seem to be relevant to both epistemic and instrumental rationality.)</li>\n<li><em>The Future</em></li>\n<li><em>Misc.</em></li>\n</ul>\n<div><br /></div>\n<div><br /></div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "ZWmB62xB6uLyRuAtX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kFwtkx9wtSzssYv9W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 34, "extendedScore": null, "score": 1.328322986726988e-06, "legacy": true, "legacyId": "24001", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Less Wrong is based on <a href=\"http://www.reddit.com/\">reddit</a> code, which means we can create <a href=\"http://www.reddit.com/reddits/\">subreddits</a> with relative ease.</p>\n<p>Right now we have two subreddits, Main and Discussion. These are distinguished not by subject matter, but by whether a post is the <em>type</em> of thing that might be promoted to the front page or not (e.g. a meetup announcement, or a particularly well-composed and useful post).</p>\n<p>As a result, almost everything is published to Discussion, and thus <strong>it is difficult for busy people to follow only the subjects they care about</strong>. More people will be able to engage if we split things into topic-specific subreddits, and make it easy to follow only what they care about.</p>\n<p>To make it easier for people to follow only what they care about, we're building the code for a Dashboard thingie.</p>\n<p>But we also need to figure out <em>which</em> subreddits to create, and we'd like community feedback about that.</p>\n<p>We'll probably start small, with just 1-5 new subreddits.</p>\n<p>Below are some initial ideas, to get the conversation started.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Idea_1\">Idea 1</strong></p>\n<p>\n</p><ul>\n<li><em>Main</em>: still the place for things that might be promoted.</li>\n<li><em>Applied Rationality</em>: for articles about&nbsp;<a href=\"/lw/gs5/improving_human_rationality_through_cognitive/\">what Jonathan Baron would call</a>&nbsp;descriptive and prescriptive rationality, for both epistemic and instrumental rationality (stuff about biases, self-improvement stuff, etc.).</li>\n<li><em>Normative Rationality</em>: for articles about what Baron would call normative rationality, for both epistemic and instrumental rationality (examining the foundations of probability theory, decision theory, anthropics, and lots of stuff that is called \"philosophy\").&nbsp;</li>\n<li><em>The Future</em>: for articles about forecasting, x-risk, and future technologies.</li>\n<li><em>Misc</em>:&nbsp;Discussion, renamed, for everything that doesn't belong in the other subreddits.</li>\n</ul>\n<strong></strong><p></p>\n<p>&nbsp;</p>\n<p><strong id=\"Idea_2\">Idea 2</strong></p>\n<p>\n</p><ul>\n<li><em>Main</em></li>\n<li><em>Epistemic Rationality</em>: for articles about how to figure out the world, spanning the descriptive, prescriptive, and normative.</li>\n<li><em>Instrumental Rationality</em>: for articles about how to take action to achieve your goals, spanning the descriptive, prescriptive, and normative. (One difficulty with the epistemic/instrumental split is that many (most?) applied rationality techniques seem to be relevant to both epistemic and instrumental rationality.)</li>\n<li><em>The Future</em></li>\n<li><em>Misc.</em></li>\n</ul>\n<div><br></div>\n<div><br></div>\n<p></p>", "sections": [{"title": "Idea 1", "anchor": "Idea_1", "level": 1}, {"title": "Idea 2", "anchor": "Idea_2", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "78 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 78, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hR92kW2ZSvmuca5Nf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-04T18:14:21.891Z", "modifiedAt": null, "url": null, "title": "LINK: Study demonstrates politically motivated innumeracy", "slug": "link-study-demonstrates-politically-motivated-innumeracy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:07.243Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alejandro1", "createdAt": "2011-09-14T21:04:19.242Z", "isAdmin": false, "displayName": "Alejandro1"}, "userId": "K4b3vEKg7EGRr2o9A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cAfcBMRqWX8jwFwaM/link-study-demonstrates-politically-motivated-innumeracy", "pageUrlRelative": "/posts/cAfcBMRqWX8jwFwaM/link-study-demonstrates-politically-motivated-innumeracy", "linkUrl": "https://www.lesswrong.com/posts/cAfcBMRqWX8jwFwaM/link-study-demonstrates-politically-motivated-innumeracy", "postedAtFormatted": "Wednesday, September 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LINK%3A%20Study%20demonstrates%20politically%20motivated%20innumeracy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALINK%3A%20Study%20demonstrates%20politically%20motivated%20innumeracy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcAfcBMRqWX8jwFwaM%2Flink-study-demonstrates-politically-motivated-innumeracy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LINK%3A%20Study%20demonstrates%20politically%20motivated%20innumeracy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcAfcBMRqWX8jwFwaM%2Flink-study-demonstrates-politically-motivated-innumeracy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcAfcBMRqWX8jwFwaM%2Flink-study-demonstrates-politically-motivated-innumeracy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 184, "htmlBody": "<p>Original article:&nbsp;<a title=\"Motivated Numeracy and Enlightened Self-Government\" href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2319992\" target=\"_self\">Motivated Numeracy and Enlightened Self-Government</a>.</p>\n<p>Easy-to-read summary of the results: <a title=\"Science Confirms: Politics Wrecks Your Ability to Do Math \" href=\"http://www.motherjones.com/politics/2013/09/new-study-politics-makes-you-innumerate\" target=\"_self\">Science Confirms: Politics Wrecks Your Ability to Do Math</a>.</p>\n<p>Easier-to-read summary of the summary: A group including both liberals and conservatives were first tested for numeracy, then given either fictional stats on the efficacy of a rash treatment (how many people get better/worse, with/without the treatment) or quantitatively identical fictional stats on gun regulation and crime (in how many cities crime went up/down, with/without gun control laws). They had to say whether, according to the stats, the treatment was effective or whether the laws reduced crime. (In half of the cases the answer was yes, in half no). For the rash problem, numerate people did better in giving the right answer, irrespectively of their politics. For the gun control problem, numerate people (of both political sides) were much better at giving the right answer when it agreed with their political beliefs; when the conclusion implied by the stats disagreed with their political beliefs, they did no better than innumerate people.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "LDTSbmXtokYAsEq8e": 2, "iP2X4jQNHMWHRNPne": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cAfcBMRqWX8jwFwaM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 24, "extendedScore": null, "score": 1.3283384410541653e-06, "legacy": true, "legacyId": "24002", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-04T19:43:37.202Z", "modifiedAt": null, "url": null, "title": "Eudaimonic Utilitarianism", "slug": "eudaimonic-utilitarianism", "viewCount": null, "lastCommentedAt": "2019-04-19T20:18:14.694Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Darklight", "createdAt": "2013-09-01T21:44:31.447Z", "isAdmin": false, "displayName": "Darklight"}, "userId": "3ovWJeXAjCSj9Lwg6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4Rwh8fKgX2ewDtyPH/eudaimonic-utilitarianism", "pageUrlRelative": "/posts/4Rwh8fKgX2ewDtyPH/eudaimonic-utilitarianism", "linkUrl": "https://www.lesswrong.com/posts/4Rwh8fKgX2ewDtyPH/eudaimonic-utilitarianism", "postedAtFormatted": "Wednesday, September 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Eudaimonic%20Utilitarianism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEudaimonic%20Utilitarianism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Rwh8fKgX2ewDtyPH%2Feudaimonic-utilitarianism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Eudaimonic%20Utilitarianism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Rwh8fKgX2ewDtyPH%2Feudaimonic-utilitarianism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Rwh8fKgX2ewDtyPH%2Feudaimonic-utilitarianism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3043, "htmlBody": "<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">Eliezer Yudkowsky on several occasions has used the term &ldquo;Eudaimonia&rdquo; to describe an objectively desirable state of existence. While the meta-ethics sequence on Less Wrong has been rather emphatic that simple universal moral theories are inadequate due to the complex nature of human values, one wonders, just what would happen if we tried anyway to build a moral theory around the notion of Eudaimonia. The following is a cursory attempt to do so. Even if you don&rsquo;t agree with everything I say here, I ask that you please bear with me to the end before making judgments about this theory. Also, if you choose to downvote this post, please offer some criticism in the comments to explain why you choose to do so. I am admittedly new to posting in the Less Wrong community, and would greatly appreciate your comments and criticisms. Even though I use imperative language to argue my ideas, I consider this theory to be a work in progress at best. So without further ado, let us begin&hellip;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">Classical Utilitarianism allows for situations where you could theoretically justify universal drug addiction as a way to maximize happiness if you could find some magical drug that made people super happy all the time with no side effects. There's a book called Brave New World by Aldous Huxley, where this drug called Soma is used to sedate the entire population, making them docile and dependent and very, very happy. Now, John Stuart Mill does argue that some pleasures are of a higher quality than others, but how exactly do you define and compare that quality? What exactly makes Shakespeare better than Reality TV? Arguably a lot of people are bored by Shakespeare and made happier by Reality TV.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">Enter Aristotle. Aristotle had his own definition of happiness, which he called Eudaimonia. Roughly translated, it means \"Human Flourishing\". It is a complex concept, but I like to think of it as \"reaching your full potential as a human being\", \"being the best that you can be\", \"fulfilling your purpose in life\", and &ldquo;authentic happiness&rdquo; (based on the existential notion of authenticity). I think a better way to explain it is like this. The Classical Utilitarian concept of happiness is subjective. It is just the happiness that you feel in your limited understanding of everything. The Eudaimonic Utilitarian concept of happiness is objective. It is the happiness you would have if you did know everything that was really happening. If you, from the perspective of an impartial observer, knew the total truth (perfect information), would you be happy with the situation? You would probably only be truly happy if you were in the process of being the best possible you, and if it was the best possible reality. Theists have another name for this, and it is God's Will (See: Divine Benevolence, or an Attempt to Prove That the Principal End of the Divine Providence and Government is the Happiness of His Creatures (1731) by Thomas Bayes) (yes, that Bayes).</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">Looking at the metaphor of God, an omnibenevolent God wants everyone to be happy. But more than just happy as docile creatures, he wants them to fulfill their purpose and destiny and achieve their fullest potential for greatness because doing so allows them to contribute so much more to everything, and make the whole universe and His creation better. Now, it's quite possible that God does not exist. But His perspective, that of the impartial observer with perfect information and rationality, is still a tremendously useful perspective to have to make the best moral decisions, and is essentially the one that Eudaimonic Utilitarianism would like to be able to reason from.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">Such happiness would be based on perfect rationality, and the assumption that happiness is the emotional goal state. It is the state that we achieve when we accomplish our goals, that is to say, we are being rational, and committing rational activity, also known as Ar&ecirc;te. For this reason, Eudaimonia as a state is not necessarily human-specific. Any rational agent with goals, including, say a Paperclip Maximizer, might reach a Eudaimonic state even if it isn't \"sentient\" or \"intelligent\" in the way that we would understand it. It need not \"feel happy\" in a biochemical manner, only be goal-directed and have some sort of desired success state. Though I could argue that this desired success state would be the mechanical equivalent of happiness to a Really Powerful Optimization Process, that in its own way the Paperclip Maximizer feels pleasure when it succeeds at maximizing paperclips, and pain when it fails to do so.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">Regardless, Eudaimonia would not be maximized by taking Soma. Eudaimonia would not be achieved by hooking up to the matrix if the matrix was a perfect utopia of happiness, because that utopia and happiness aren't real. They're a fantasy, a drug that prevents them from actually living and being who they're supposed to be, who they can be. They would be living a lie. Eudaimonia is based on the truth. It is based on reality and what can and should be done. It requires performing rational activity or actually achieving goals. It is an optimization given all the data.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">I have begun by explaining how Eudaimonic Utilitarianism is superior to Classical Utilitarianism. I will now try to explain how Eudaimonic Utilitarianism is both superior and compatible to Preference Utilitarianism. Regular Preference Utilitarianism is arguably even more subjective than Classical Utilitarianism. With Preference Utilitarianism, you&rsquo;re essentially saying that whatever people think is in their interests, is what should be maximized. But this assumes that their preferences are rational. In reality, most people&rsquo;s preferences are strongly influenced by emotions and bounded rationality.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">For instance, take the example of a suicidal and depressed man. Due to emotional factors, this man has the irrational desire to kill himself. Preference Utilitarianism would either have to accept this preference even though most would agree it is objectively &ldquo;bad&rdquo; for him, or do something like call this &ldquo;manifest&rdquo; preference to be inferior to the man&rsquo;s &ldquo;true&rdquo; preferences. &ldquo;Manifest&rdquo; preferences are what a person&rsquo;s actual behaviour would suggest, while &ldquo;true&rdquo; preferences are what they would have if they could view the situation with all relevant information and rational care. But how do we go about determining a person&rsquo;s &ldquo;true&rdquo; preferences? Do we not have to resort to some kind of objective criterion of what is rational behaviour?</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">But where is this objective criterion coming from? Well a Classical Utilitarian would argue that suicide would lead to a negation of all the potential happiness that the person could feel in the future, and that rationality is what maximizes happiness. A Eudaimonic Utilitarian would go further and state that if the person knew everything, both their happiness and their preferences would be aligned towards rational activity and therefore not only would their objective happiness be maximized by not committing suicide, but their &ldquo;true&rdquo; preferences would also be maximized. Eudaimonia therefore is the objective criterion of rational behaviour. It is not merely subjective preference, but a kind of objective preference based on perfect information and perfect rationality.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">Preference Utilitarianism only really works as a moral theory if the person&rsquo;s preferences are based on rationality and complete knowledge of everything. Coincidentally, Eudaimonic Utilitarianism, assumes this position. It assumes that what should be maximized is the person&rsquo;s preferences if they were completely rational and knew everything, because those preferences would naturally align with achieving Eudaimonia.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">Therefore, Eudaimonic Utilitarianism can be seen as a merging, a unification of both Classical and Preference Utilitarianism because, from the perspective of an objective impartial observer, the state of Eudaimonia is simultaneously happiness and rational preference achieved through Ar&ecirc;te, or rational activity, which is equivalent to &ldquo;doing your best&rdquo; or &ldquo;maximizing your potential&rdquo;.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">Preference Utilitarianism is neutral as to whether or not to take Soma or plug into the Utopia Matrix. For Preference Utilitarianism, it&rsquo;s up to the individual&rsquo;s &ldquo;rational&rdquo; preference. Eudaimonic Utilitarianism on the other hand would argue that it is only rational to take Soma or plug into the Utopia Matrix if doing so still allows you to achieve Eudaimonia, which is unlikely, as doing so prevents one from performing Ar&ecirc;te in the real world. At the very least, rather than basing it on a subjective preference, we are now using an objective evaluation function.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">The main challenge of Eudaimonic Utilitarianism of course is that we as human beings with bounded rationality, do not have access to the position of God with regards to perfect information. Nevertheless, we can still apply Eudaimonic Utilitarianism in everyday scenarios.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\" lang=\"en-CA\">For instance,<span lang=\"en-US\"> consider the problem of Adultery. A common criticism of Classical Utilitarianism is that it doesn&rsquo;t condemn acts like Adultery because at first glance, an act like Adultery seems like it would increase net happiness and therefore be condoned. This does not take into account the probabilities of being caught however. Given uncertainty, it is usually safe to assume a uniform distribution of probabilities, which means that getting caught has a 0.5 probability. We must then compare the utilities of not getting caught, and getting caught. It doesn&rsquo;t really matter what the exact numbers are, so much as the relative relationship of the values. So for instance, we can say that Adultery in the not getting caught scenario has a +5 to each member of the Adultery, for a total of +10. However, in the getting caught scenario, there is a +5 to the uncoupled member, but a net loss of -20 to the coupled member, and -20 to the wronged partner, due to the potential falling out and loss of trust resulting from the discovered Adultery.</span></p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">&nbsp;</p>\n<table border=\"0\" cellspacing=\"0\" cellpadding=\"7\" width=\"598\">\n<colgroup><col width=\"184\"></col> <col width=\"185\"></col> <col width=\"186\"></col> </colgroup> \n<tbody>\n<tr valign=\"TOP\">\n<td style=\"border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0in; padding-bottom: 0in; padding-left: 0.08in; padding-right: 0in;\" width=\"184\">\n<p class=\"western\">&nbsp;</p>\n</td>\n<td style=\"border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0in; padding-bottom: 0in; padding-left: 0.08in; padding-right: 0in;\" width=\"185\">\n<p class=\"western\">Commit Adultery</p>\n</td>\n<td style=\"border: 1px solid #000000; padding: 0in 0.08in;\" width=\"186\">\n<p class=\"western\">Don&rsquo;t Commit Adultery</p>\n</td>\n</tr>\n<tr valign=\"TOP\">\n<td style=\"border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0in; padding-bottom: 0in; padding-left: 0.08in; padding-right: 0in;\" width=\"184\">\n<p class=\"western\">Truth Discovered</p>\n</td>\n<td style=\"border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0in; padding-bottom: 0in; padding-left: 0.08in; padding-right: 0in;\" width=\"185\">\n<p class=\"western\">-35 effect x 0.5 probability</p>\n</td>\n<td style=\"border: 1px solid #000000; padding: 0in 0.08in;\" width=\"186\">\n<p class=\"western\">0 effect x 0.5 probability</p>\n</td>\n</tr>\n<tr valign=\"TOP\">\n<td style=\"border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0in; padding-bottom: 0in; padding-left: 0.08in; padding-right: 0in;\" width=\"184\">\n<p class=\"western\">Truth Not Discovered</p>\n</td>\n<td style=\"border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0in; padding-bottom: 0in; padding-left: 0.08in; padding-right: 0in;\" width=\"185\">\n<p class=\"western\">+10 effect x 0.5 probability</p>\n</td>\n<td style=\"border: 1px solid #000000; padding: 0in 0.08in;\" width=\"186\">\n<p class=\"western\">0 effect x 0.5 probability</p>\n</td>\n</tr>\n<tr valign=\"TOP\">\n<td style=\"border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0in; padding-bottom: 0in; padding-left: 0.08in; padding-right: 0in;\" width=\"184\">\n<p class=\"western\">Potential Consequences</p>\n</td>\n<td style=\"border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0in; padding-bottom: 0in; padding-left: 0.08in; padding-right: 0in;\" width=\"185\">\n<p class=\"western\">-12.5</p>\n</td>\n<td style=\"border: 1px solid #000000; padding: 0in 0.08in;\" width=\"186\">\n<p class=\"western\">0</p>\n</td>\n</tr>\n</tbody>\n</table>\n<p class=\"western\" style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">Thus the net total effect of Adultery in the caught scenario is -35. If we assign the probabilities to each scenario, +10 x 0.5 = +5, while -35 x 0.5 = -17.5. +5 &ndash; 17.5 = -12.5, therefore the probable net effect of Adultery is actually negative and therefore morally wrong.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">But what if getting caught is very unlikely? Well, we can show that to a true agnostic at least, the probability of getting caught would be at least 0.5, because if we assume total ignorance, the probability that God and/or an afterlife exist would be a uniform distribution, as suggested by the Principle of Indifference and the Principle of Maximum Entropy. Thus there is at least a 0.5 chance that eventually the other partner will find out. But assuming instead a strong atheistic view, there is the danger that hypothetically, if the probability of truth not discovered was 1, then this calculation would actually suggest that committing Adultery would be moral.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">The previous example is based on the subjective happiness of Classical Utilitarianism, but what if we used a criterion of Eudaimonia, or the objective happiness we would feel if we knew everything? In that case the Adultery scenario looks even more negative.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">In this instance, we can say that Adultery in the not getting caught scenario has a +5 to each member of the Adultery, but also a -20 to the partner who is being wronged because that is how much they would suffer if they knew, which is a net -10. In the getting caught scenario, there is a +5 to the uncoupled member, but a net loss of -20 to the coupled member and an additional -20 to the partner being wronged, due to the potential falling out and loss of trust resulting from the discovered Adultery.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">&nbsp;</p>\n<table border=\"0\" cellspacing=\"0\" cellpadding=\"7\" width=\"598\">\n<colgroup><col width=\"184\"></col> <col width=\"185\"></col> <col width=\"186\"></col> </colgroup> \n<tbody>\n<tr valign=\"TOP\">\n<td style=\"border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0in; padding-bottom: 0in; padding-left: 0.08in; padding-right: 0in;\" width=\"184\">\n<p class=\"western\">&nbsp;</p>\n</td>\n<td style=\"border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0in; padding-bottom: 0in; padding-left: 0.08in; padding-right: 0in;\" width=\"185\">\n<p class=\"western\">Commit Adultery</p>\n</td>\n<td style=\"border: 1px solid #000000; padding: 0in 0.08in;\" width=\"186\">\n<p class=\"western\">Don&rsquo;t Commit Adultery</p>\n</td>\n</tr>\n<tr valign=\"TOP\">\n<td style=\"border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0in; padding-bottom: 0in; padding-left: 0.08in; padding-right: 0in;\" width=\"184\">\n<p class=\"western\">Truth Discovered</p>\n</td>\n<td style=\"border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0in; padding-bottom: 0in; padding-left: 0.08in; padding-right: 0in;\" width=\"185\">\n<p class=\"western\">-35 effect x 0.5 probability</p>\n</td>\n<td style=\"border: 1px solid #000000; padding: 0in 0.08in;\" width=\"186\">\n<p class=\"western\">0 effect x 0.5 probability</p>\n</td>\n</tr>\n<tr valign=\"TOP\">\n<td style=\"border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0in; padding-bottom: 0in; padding-left: 0.08in; padding-right: 0in;\" width=\"184\">\n<p class=\"western\">Truth Not Discovered</p>\n</td>\n<td style=\"border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0in; padding-bottom: 0in; padding-left: 0.08in; padding-right: 0in;\" width=\"185\">\n<p class=\"western\">-10 effect x 0.5 probability</p>\n</td>\n<td style=\"border: 1px solid #000000; padding: 0in 0.08in;\" width=\"186\">\n<p class=\"western\">0 effect x 0.5 probability</p>\n</td>\n</tr>\n<tr valign=\"TOP\">\n<td style=\"border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0in; padding-bottom: 0in; padding-left: 0.08in; padding-right: 0in;\" width=\"184\">\n<p class=\"western\">Potential Consequences</p>\n</td>\n<td style=\"border-top: 1px solid #000000; border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: none; padding-top: 0in; padding-bottom: 0in; padding-left: 0.08in; padding-right: 0in;\" width=\"185\">\n<p class=\"western\">-22.5</p>\n</td>\n<td style=\"border: 1px solid #000000; padding: 0in 0.08in;\" width=\"186\">\n<p class=\"western\">0</p>\n</td>\n</tr>\n</tbody>\n</table>\n<p class=\"western\" style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">As you can see, with a Eudaimonic Utilitarian criterion, even if the probability of truth not discovered was 1, it would still be negative and therefore morally wrong. Thus, whereas Classical Utilitarianism based on subjective happiness bases its case against Adultery on the probability of being caught and the potential negative consequences, Eudaimonic Utilitarianism takes a more solid case that Adultery would always be wrong because regardless of the probability of being caught, the consequences are inherently negative. It is therefore unnecessary to resort to traditional Preference Utilitarianism to achieve our moral intuitions about Adultery.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">Consider another scenario. You are planning a surprise birthday party for your friend, and she asks you what you are doing. You can either tell the truth or lie. Classical Utilitarianism would say to lie because the happiness of the surprise birthday party outweighs the happiness of being told the truth. Preference Utilitarianism however would argue that it is rational for the friend to want to know the truth and not have her friends lie to her generally, that this would be her &ldquo;true&rdquo; preference. Thus, Preference Utilitarianism would argue in favour of telling the truth and spoiling the surprise. The happiness that the surprise would cause does not factor into Preference Utilitarianism at all, and the friend has no prior preference for a surprise party she doesn&rsquo;t even know about.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">What does Eudaimonic Utilitarianism say? Well, if the friend really knew everything that was going on, would she be happier and prefer to know the truth in this situation, or be happier and prefer not to know? I would suggest she would be happier and prefer not to know, in which case Eudaimonic Utilitarianism agrees with Classical Utilitarianism and says we should lie to protect the secret of the surprise birthday party.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">Again, what's the difference between eudaimonia and preference-fulfillment? Basically, preference-fulfillment is based on people's subjective preferences, while Eudaimonia is based on objective well-being, or as I like to explain, the happiness they would feel if they had perfect information.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">The difference is somewhat subtle to the extent that a person's \"true\" preferences are supposed to be &ldquo;the preferences he would have if he had all the relevant factual information, always reasoned with the greatest possible care, and were in a state of mind most conducive to rational choice.&rdquo; (Harsanyi 1982) Note that relevant factual information is not the same thing as perfect information.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">For instance, take the classic criticism of Utilitarianism in the form of the scenario where you hang an innocent man to satisfy the desires for justice of the unruly mob. Under both hedonistic and preference utilitarianism, the hanging of the innocent man can be justified because hanging the innocent man satisfies both the happiness of the mob, and the preferences of the mob. However, hanging an innocent man does not satisfy the Eudaimonia of the mob, because if the people in the mob knew that the man was innocent and were truly rational, they would not want to hang him after all. Note that in this case they only have this information under perfect information, as it is assumed that the man appears to all rational parties to be guilty even though he is actually innocent.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">So, Eudaimonia assumes that in a hypothetical state of perfect information and rationality (that is to say objectivity), a person's happiness would best be satisfied by actions that might differ from what they might prefer in their normal subjective state, and that we should commit to the actions that satisfy this objective happiness (or well-being), rather than satisfy subjective happiness or subjective preferences.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">For instance, we can take the example from John Rawls of the grass-counter. \"Imagine a brilliant Harvard mathematician, fully informed about the options available to her, who develops an overriding desire to count the blades of grass on the lawns of Harvard.\" Under both hedonistic and preference utilitarianism, this would be acceptable. However, a Eudaimonic interpretation would argue that counting blades of grass would not maximize her objective happiness, that there is an objective state of being that would actually make her happier, even if it went against her personal preferences, and that this state of being is what should be maximized. Similarly, consider the rational philosopher who has come to the conclusion that life is meaningless and not worth living and therefore develops a preference to commit suicide. This would be his \"true\" preference, but it would not maximize his Eudaimonia. For this reason, we should try to persuade the suicidal philosopher not to commit suicide, rather than helping him do so.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">How does Eudaimonia compare with Eliezer Yudkowsky&rsquo;s concept of Coherent Extrapolated Volition (CEV)? Similarly to Eudaimonia, CEV is based on what an idealized version of us would want \"if we knew more, thought faster, were more the people we wished we were, had grown up farther together\". This is similar to but not the same thing as an idealized version of us with perfect information and with perfect rationality. Arguably Eudaimonia is sort of an extreme form of CEV that endorses the limits in this regard.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">Furthermore, CEV assumes that the desires of humanity converge. The concept of Eudaimonia does not require this. The Eudaimonia of different sentient beings may well conflict, in which case Eudaimonic Utilitarianism takes the Utilitarian route and suggests the compromise of maximizing Eudaimonia for the greatest number of sentient beings, with a hierarchical preference for more conscious beings such as humans, over say ants. This is not to say that humans are necessarily absolute utility monsters to the ants. One could instead set it up so that the humans are much more heavily weighted in the moral calculus by their level of consciousness. Though that could conceivably lead to the situation where a billion ants might be more heavily weighted than a single human. If such a notion is anathema to you, then perhaps making humans absolute utility monsters may be reasonable to you after all. However, keep in mind that the same argument can be made that a superintelligent A.I. is a utility monster to humans. The idea that seven billion humans might outweigh one superintelligent A.I. in the moral calculus may not be such a bad idea.</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in;\">In any case, Eudaimonic Utilitarianism does away with many of the unintuitive weaknesses of both Classical Hedonistic Utilitarianism, and Preference Utilitarianism. It validates our intuitions about the importance of authenticity and rationality in moral behaviour. It also attempts to unify morality and rationality. Though it is not without its issues, not the least of which being that it incorporates a very simplified view of human values, I nevertheless offer it as an alternative to other existing forms of Utilitarianism for your consideration.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4Rwh8fKgX2ewDtyPH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 10, "extendedScore": null, "score": 1.3284159273464641e-06, "legacy": true, "legacyId": "24003", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-04T21:22:06.770Z", "modifiedAt": null, "url": null, "title": "Should We Tell People That Giving Makes Them Happier?", "slug": "should-we-tell-people-that-giving-makes-them-happier", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:27.727Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h5B4aYb9whZPKgb7b/should-we-tell-people-that-giving-makes-them-happier", "pageUrlRelative": "/posts/h5B4aYb9whZPKgb7b/should-we-tell-people-that-giving-makes-them-happier", "linkUrl": "https://www.lesswrong.com/posts/h5B4aYb9whZPKgb7b/should-we-tell-people-that-giving-makes-them-happier", "postedAtFormatted": "Wednesday, September 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20We%20Tell%20People%20That%20Giving%20Makes%20Them%20Happier%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20We%20Tell%20People%20That%20Giving%20Makes%20Them%20Happier%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh5B4aYb9whZPKgb7b%2Fshould-we-tell-people-that-giving-makes-them-happier%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20We%20Tell%20People%20That%20Giving%20Makes%20Them%20Happier%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh5B4aYb9whZPKgb7b%2Fshould-we-tell-people-that-giving-makes-them-happier", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh5B4aYb9whZPKgb7b%2Fshould-we-tell-people-that-giving-makes-them-happier", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1484, "htmlBody": "<p>Why do people give to charity?</p>\n<p>It seems strange to even ask. Most people would point to the fact that they&rsquo;re altruistic and want to make a difference. Others are concerned with inequality and justice. Another group points to the concept of &ldquo;paying it forward&rdquo; or repaying a debt to society. Other explanations cite various religious or social reasons.</p>\n<p>Not too many people cite the fact that giving makes them happier. Even if people agree this is true, I don&rsquo;t often hear it as people&rsquo;s main reason. Instead, it&rsquo;s more like a beneficial side effect. In fact, it seems pretty odd to me to hear someone boldly proclaim that they give only because it makes them happier, even if it might be true.</p>\n<p>&nbsp;</p>\n<p><strong>But if it&rsquo;s true that giving does make people happier, should we be promoting that publicly and loudly?</strong> &nbsp;Luke's article <a href=\"/lw/6py/optimal_philanthropy_for_human_beings/\">\"Optimal Philanthropy for Human Beings\"</a> suggests that we should tell people to enjoy the happiness that giving brings. &nbsp;Perhaps it might make a great opportunity to tap into groups who wouldn&rsquo;t consider giving otherwise or have misconceptions that giving would make them miserable?</p>\n<p>However, <strong>I&rsquo;m a bit worried about how it might affect people&rsquo;s incentives.</strong> &nbsp;In this essay, I follow the evidence provided in the Harvard Business School working paper <a href=\"http://hbswk.hbs.edu/item/6272.html\">\"Feeling Good About Giving: The Benefits (and Costs) of Self-Interested Charitable Behavior\"</a> by Lalin Anik, Lara B. Aknin, Michael I. Norton, and Elizabeth W. Dunn. <strong>Overall, in light of potential incentive effects, I think caution and further investigation is warranted when promoting the happiness side of giving.</strong></p>\n<p>&nbsp;</p>\n<h2>Giving and Happiness</h2>\n<p>Giving What We Can has published its own review of research on happiness and giving and find a pretty strong connection. And it&rsquo;s true -- lots of evidence confirms the connection and even indicates that it&rsquo;s a causal relationship rather than a misleading correlation. In fact, it goes in both directions -- giving makes people happier and happier people are more likely to give[1].</p>\n<p>Neurological studies of people found that people experienced pleasure when they saw money go to charity, even when it wasn&rsquo;t their own, but experienced even more pleasure when they gave to charity directly[2], a conclusion that has been backed up with revealed preference tests in the lab[3, 4].</p>\n<p>This connection has also been backed up in numerous experimental studies. Asking people to commit random acts of kindness can significantly increase self-reported levels of happiness compared to a control group[5]. Further research found that the amount people spent on gifts for others and donations to charity correlates with their self-reported happiness, while the amount they spent on bills, expenses, and gifts for themselves did not[6]. Additionally, people given money and randomly assigned to spend money on others were happier than those randomly assigned to spend the same amount of money on themselves[7].</p>\n<p>&nbsp;</p>\n<h2>Altering Incentives</h2>\n<p>People generally believe that spending on themselves will make them much happier than spending on others[6], which, given that this isn&rsquo;t the case, means there is plenty of room for changing people&rsquo;s minds. However, any social scientist or avid reader of Freakonomics knows that altering incentives can create unintended effects. So is there a potential harm in getting people to do more giving via advertising self-interested motive?</p>\n<p>The classic example is that of the childcare center that had problems with parents who were late to pick up their children. They reasoned that if they charged fines, parents would stop being late, because they would have an economic incentive not to. They found instead, however, that introducing a fine actually created even more tardiness[8], presumably because what once was seen as rude and bad faith now could be made up for with a small economic cost. More surprisingly, the amount of lateness did not return to pre-fine levels even after the owners stopped the policy[9].</p>\n<p>Other studies have found similar effects. A study of 3-5 year old nursery students who all initially seemed intrinsically interested in various activities were randomly put into three groups. One group made a pre-arranged deal to do a one of the activities in which they seemed interested in exchange for a reward, another group was surprised with a reward after doing the activity in question, and the third group was not rewarded at all. Those who were given an award upfront ended up significantly less intrinsically interested in the task than the other groups after the study was finished[10]. A similar study found that students who were interested in solving puzzles stopped solving those puzzles after a period ended where they were paid to solve puzzles[11].</p>\n<p>&nbsp;</p>\n<p>In general, money and reminders of money tend to make people less pro-social[12]. This has also been found to some degree specifically in the world of charity. In a randomized field experiment, donors were encouraged to donate to disaster relief in the US and were randomly either enticed with an offer of donation matching or not. The study found that while people donated more often with the promise of donation matching, their contributions after the donation matching dropped below the control group, ending with a negative net effect overall[13].</p>\n<p>Another study found that when gifts were sent out to donors, larger gifts resulted in a larger response rate of returned donations, but yielded a smaller average donation[14], though I suppose this could just be because more people who usually would give nothing were giving a small amount, bringing the average down. More importantly, this study found no net decrease in future donations after gifts were no longer sent out; instead, donations returned to their normal levels[14].</p>\n<p>And certainly it&rsquo;s worth noting some times when appeals to self-interest are successful. I couldn&rsquo;t find any studies where this was the case. However, there is one anecdotal example: as Nick Cooney points out in <a href=\"http://qz.com/91716/self-interest-can-make-the-world-a-better-place-for-animals-at-least/\">\"Self-Interest Can Make the World a Better Place -- For Animals, At Least\"</a>, reduction in people eating factory farmed meat is coming almost entirely from people motivated not by concern for animal cruelty, but concern for their own health. Could advocating self-interested donations be the same as advocating health-motivated vegetarianism?</p>\n<p>&nbsp;</p>\n<h2>Opportunities for Further Investigation</h2>\n<p>It&rsquo;s not very good to just let things be unclear if they don&rsquo;t have to be, and I think we can resolve this issue with more scientific study. For example, one could randomly select one group to receive information about giving and happiness, another group to receive other standard arguments for giving, and a control group to receive no arguments or information about giving at all, and track their donation habits in a longitudinal study. This study would have it&rsquo;s complications for sure, but could help see if information about giving and happiness backfires or not.</p>\n<p>Or perhaps one could perform a field experiment. You could set up a booth asking people to donate to your cause and randomly include information about giving and happiness or not in your pitch and see how this affects immediate and long-term contributions. Doing this would have added advantages of being much quicker to run and not leading to people donating only because they think they&rsquo;re being observed.</p>\n<p>&nbsp;</p>\n<h2>References</h2>\n<p>[1]: Anik, Lalin, Lara B. Aknin, Michael I. Norton, Elizabeth W. Dunn. 2009. <a href=\"http://www.hbs.edu/faculty/Publication%20Files/10-012.pdf\">&ldquo;Feeling Good about Giving: The Benefits (and Costs) of Self-Interested Charitable Behavior&rdquo;</a>. Harvard Business School Working Paper 10-012.</p>\n<p>[2]: Harbaugh, William T. 2007. <a href=\"http://www.wisebrain.org/papers/NeuralAltruism.pdf\">\"Neural Responses to Taxation and Voluntary Giving Reveal Motives for Charitable Donations.\"</a><em> Science</em> 316: 1622-1625.</p>\n<p>[3]: Andreoni, James, William T. Harbaugh, and Lise Vesterlund. 2007. <a href=\"http://www.pitt.edu/~vester/Palgrave.pdf\">\"Altruism in Experiments\"</a>. <strong>New Palgrave Dictionary of Economics</strong>.</p>\n<p>[4]: Mayr, Ulrich, William T. Harbaugh , and Dharol Tankersley. 2008. <a href=\"http://darkwing.uoregon.edu/~thinking/documents/MayrHarbaughTankersley.pdf\">\"Neuroeconomics of Charitable Giving and Philanthropy\"</a>. In Glimcher, Paul W., Ernest Fehr, Colin Camerer, and Russel Alan Poldrack (eds.) 2009. <strong><a href=\"http://www.amazon.com/books/dp/0123741769\">Neuroeconomics: Decision Making and the Brain</a></strong>. Academic Press: London.</p>\n<p>[5]: Lyubomirsky, Sonja, Kennon M. Sheldon, and David Schkade. 2005. <a href=\"http://sonjalyubomirsky.com/wp-content/themes/sonjalyubomirsky/papers/LSS2005.pdf\">\"Pursuing Happiness: The Architecture of Sustainable Change.\"</a>&nbsp;<em>Review of General Psychology</em> 9 (2): 111&ndash;131.</p>\n<p>[6]: Akin, Lara B., et. al. 2010. <a href=\"http://barringtonleigh.net/publications/w16415.pdf\">\"Pro-social Spending And Well-Being: Cross-Cultural Evidence for a Psychological Universal.\"</a>&nbsp;National Bureau of Economic Research Working Paper #16415.</p>\n<p>[7]: Dunn, Elizabeth W., Lara B. Aknin, and Michael I. Norton. 2008. <a href=\"http://www.people.hbs.edu/mnorton/dunn%20aknin%20norton.pdf\">&ldquo;Spending Money on Others Promotes Happiness.&rdquo;</a>&nbsp;<em>Science</em> 319: 1687-1688.</p>\n<p>[8]: Gneezy, Uri and Aldo Rustichini. 2000a. <a href=\"http://rady.ucsd.edu/faculty/directory/gneezy/pub/docs/fine.pdf\">&ldquo;A fine is a price.&rdquo;</a>&nbsp;<em>Journal of Legal Studies</em> 29: 1-18.</p>\n<p>[9]: Gneezy, Uri and Aldo Rustichini. 2000b. <a href=\"http://management.ucsd.edu/faculty/directory/gneezy/pub/docs/pay-enough.pdf\">&ldquo;Pay enough or don't pay at all.&rdquo;</a>&nbsp;<em>Quarterly Journal of Economics</em> 115: 791-810.</p>\n<p>[10]: Lepper, Mark R., David Greene, and Richard E. Nisbett. 1973. <a href=\"http://psycnet.apa.org/index.cfm?fa=buy.optionToBuy&amp;uid=1974-10497-001\">&ldquo;Undermining Children's Intrinsic Interest with Extrinsic Reward: A Test of the &lsquo;Overjustification&rsquo; Hypothesis.&rdquo;</a>&nbsp;<em>Journal of Personality and Social Psychology</em> 28(1): 129-137.</p>\n<p>[11]: Deci, Edward L. 1971. <a href=\"http://psycnet.apa.org/index.cfm?fa=buy.optionToBuy&amp;uid=1971-22190-001\">&ldquo;Effects of Externally Mediated Rewards on Intrinsic Motivation.&rdquo;</a>&nbsp;<em>Journal of Personality and Social Psychology </em>18(1): 105-115.</p>\n<p>[12]: Vohs, Kathleen D., Nicole L. Mead, and Miranda R. Goode. 2006. <a href=\"http://www.sciencemag.org/content/314/5802/1154.short\">&ldquo;The Psychological Consequences of Money&rdquo;</a>. <em>Science</em> 17 (314): 1154-1156.</p>\n<p>[13]: Meier, Stephan. 2007. <a href=\"http://www.bos.frb.org/economic/wp/wp2006/wp0618.pdf\">&ldquo;Do Subsidies Increase Charitable Giving in the Long Run? Matching Donations in a Field Experiment&rdquo;</a>. Federal Reserve Bank of Boston Working Paper #06-18.</p>\n<p>[14]: Falk, Armin. 2005. <a href=\"http://www.ucl.ac.uk/~uctpshu/falk.pdf\">&ldquo;Gift Exchange in the Field&rdquo;</a>. University of Bonn.</p>\n<p>-</p>\n<p><em>(This essay is also cross-posted on the <a href=\"http://www.givingwhatwecan.org/blog/2013-09-03/should-we-tell-people-that-giving-makes-them-happier\">Giving What We Can blog</a> and <a href=\"http://www.everydayutilitarian.com/essays/should-we-tell-people-that-giving-makes-them-happier/\">my blog</a>.)</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h5B4aYb9whZPKgb7b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 17, "extendedScore": null, "score": 1.3285014424973322e-06, "legacy": true, "legacyId": "24004", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Why do people give to charity?</p>\n<p>It seems strange to even ask. Most people would point to the fact that they\u2019re altruistic and want to make a difference. Others are concerned with inequality and justice. Another group points to the concept of \u201cpaying it forward\u201d or repaying a debt to society. Other explanations cite various religious or social reasons.</p>\n<p>Not too many people cite the fact that giving makes them happier. Even if people agree this is true, I don\u2019t often hear it as people\u2019s main reason. Instead, it\u2019s more like a beneficial side effect. In fact, it seems pretty odd to me to hear someone boldly proclaim that they give only because it makes them happier, even if it might be true.</p>\n<p>&nbsp;</p>\n<p><strong>But if it\u2019s true that giving does make people happier, should we be promoting that publicly and loudly?</strong> &nbsp;Luke's article <a href=\"/lw/6py/optimal_philanthropy_for_human_beings/\">\"Optimal Philanthropy for Human Beings\"</a> suggests that we should tell people to enjoy the happiness that giving brings. &nbsp;Perhaps it might make a great opportunity to tap into groups who wouldn\u2019t consider giving otherwise or have misconceptions that giving would make them miserable?</p>\n<p>However, <strong>I\u2019m a bit worried about how it might affect people\u2019s incentives.</strong> &nbsp;In this essay, I follow the evidence provided in the Harvard Business School working paper <a href=\"http://hbswk.hbs.edu/item/6272.html\">\"Feeling Good About Giving: The Benefits (and Costs) of Self-Interested Charitable Behavior\"</a> by Lalin Anik, Lara B. Aknin, Michael I. Norton, and Elizabeth W. Dunn. <strong>Overall, in light of potential incentive effects, I think caution and further investigation is warranted when promoting the happiness side of giving.</strong></p>\n<p>&nbsp;</p>\n<h2 id=\"Giving_and_Happiness\">Giving and Happiness</h2>\n<p>Giving What We Can has published its own review of research on happiness and giving and find a pretty strong connection. And it\u2019s true -- lots of evidence confirms the connection and even indicates that it\u2019s a causal relationship rather than a misleading correlation. In fact, it goes in both directions -- giving makes people happier and happier people are more likely to give[1].</p>\n<p>Neurological studies of people found that people experienced pleasure when they saw money go to charity, even when it wasn\u2019t their own, but experienced even more pleasure when they gave to charity directly[2], a conclusion that has been backed up with revealed preference tests in the lab[3, 4].</p>\n<p>This connection has also been backed up in numerous experimental studies. Asking people to commit random acts of kindness can significantly increase self-reported levels of happiness compared to a control group[5]. Further research found that the amount people spent on gifts for others and donations to charity correlates with their self-reported happiness, while the amount they spent on bills, expenses, and gifts for themselves did not[6]. Additionally, people given money and randomly assigned to spend money on others were happier than those randomly assigned to spend the same amount of money on themselves[7].</p>\n<p>&nbsp;</p>\n<h2 id=\"Altering_Incentives\">Altering Incentives</h2>\n<p>People generally believe that spending on themselves will make them much happier than spending on others[6], which, given that this isn\u2019t the case, means there is plenty of room for changing people\u2019s minds. However, any social scientist or avid reader of Freakonomics knows that altering incentives can create unintended effects. So is there a potential harm in getting people to do more giving via advertising self-interested motive?</p>\n<p>The classic example is that of the childcare center that had problems with parents who were late to pick up their children. They reasoned that if they charged fines, parents would stop being late, because they would have an economic incentive not to. They found instead, however, that introducing a fine actually created even more tardiness[8], presumably because what once was seen as rude and bad faith now could be made up for with a small economic cost. More surprisingly, the amount of lateness did not return to pre-fine levels even after the owners stopped the policy[9].</p>\n<p>Other studies have found similar effects. A study of 3-5 year old nursery students who all initially seemed intrinsically interested in various activities were randomly put into three groups. One group made a pre-arranged deal to do a one of the activities in which they seemed interested in exchange for a reward, another group was surprised with a reward after doing the activity in question, and the third group was not rewarded at all. Those who were given an award upfront ended up significantly less intrinsically interested in the task than the other groups after the study was finished[10]. A similar study found that students who were interested in solving puzzles stopped solving those puzzles after a period ended where they were paid to solve puzzles[11].</p>\n<p>&nbsp;</p>\n<p>In general, money and reminders of money tend to make people less pro-social[12]. This has also been found to some degree specifically in the world of charity. In a randomized field experiment, donors were encouraged to donate to disaster relief in the US and were randomly either enticed with an offer of donation matching or not. The study found that while people donated more often with the promise of donation matching, their contributions after the donation matching dropped below the control group, ending with a negative net effect overall[13].</p>\n<p>Another study found that when gifts were sent out to donors, larger gifts resulted in a larger response rate of returned donations, but yielded a smaller average donation[14], though I suppose this could just be because more people who usually would give nothing were giving a small amount, bringing the average down. More importantly, this study found no net decrease in future donations after gifts were no longer sent out; instead, donations returned to their normal levels[14].</p>\n<p>And certainly it\u2019s worth noting some times when appeals to self-interest are successful. I couldn\u2019t find any studies where this was the case. However, there is one anecdotal example: as Nick Cooney points out in <a href=\"http://qz.com/91716/self-interest-can-make-the-world-a-better-place-for-animals-at-least/\">\"Self-Interest Can Make the World a Better Place -- For Animals, At Least\"</a>, reduction in people eating factory farmed meat is coming almost entirely from people motivated not by concern for animal cruelty, but concern for their own health. Could advocating self-interested donations be the same as advocating health-motivated vegetarianism?</p>\n<p>&nbsp;</p>\n<h2 id=\"Opportunities_for_Further_Investigation\">Opportunities for Further Investigation</h2>\n<p>It\u2019s not very good to just let things be unclear if they don\u2019t have to be, and I think we can resolve this issue with more scientific study. For example, one could randomly select one group to receive information about giving and happiness, another group to receive other standard arguments for giving, and a control group to receive no arguments or information about giving at all, and track their donation habits in a longitudinal study. This study would have it\u2019s complications for sure, but could help see if information about giving and happiness backfires or not.</p>\n<p>Or perhaps one could perform a field experiment. You could set up a booth asking people to donate to your cause and randomly include information about giving and happiness or not in your pitch and see how this affects immediate and long-term contributions. Doing this would have added advantages of being much quicker to run and not leading to people donating only because they think they\u2019re being observed.</p>\n<p>&nbsp;</p>\n<h2 id=\"References\">References</h2>\n<p>[1]: Anik, Lalin, Lara B. Aknin, Michael I. Norton, Elizabeth W. Dunn. 2009. <a href=\"http://www.hbs.edu/faculty/Publication%20Files/10-012.pdf\">\u201cFeeling Good about Giving: The Benefits (and Costs) of Self-Interested Charitable Behavior\u201d</a>. Harvard Business School Working Paper 10-012.</p>\n<p>[2]: Harbaugh, William T. 2007. <a href=\"http://www.wisebrain.org/papers/NeuralAltruism.pdf\">\"Neural Responses to Taxation and Voluntary Giving Reveal Motives for Charitable Donations.\"</a><em> Science</em> 316: 1622-1625.</p>\n<p>[3]: Andreoni, James, William T. Harbaugh, and Lise Vesterlund. 2007. <a href=\"http://www.pitt.edu/~vester/Palgrave.pdf\">\"Altruism in Experiments\"</a>. <strong>New Palgrave Dictionary of Economics</strong>.</p>\n<p>[4]: Mayr, Ulrich, William T. Harbaugh , and Dharol Tankersley. 2008. <a href=\"http://darkwing.uoregon.edu/~thinking/documents/MayrHarbaughTankersley.pdf\">\"Neuroeconomics of Charitable Giving and Philanthropy\"</a>. In Glimcher, Paul W., Ernest Fehr, Colin Camerer, and Russel Alan Poldrack (eds.) 2009. <strong><a href=\"http://www.amazon.com/books/dp/0123741769\">Neuroeconomics: Decision Making and the Brain</a></strong>. Academic Press: London.</p>\n<p>[5]: Lyubomirsky, Sonja, Kennon M. Sheldon, and David Schkade. 2005. <a href=\"http://sonjalyubomirsky.com/wp-content/themes/sonjalyubomirsky/papers/LSS2005.pdf\">\"Pursuing Happiness: The Architecture of Sustainable Change.\"</a>&nbsp;<em>Review of General Psychology</em> 9 (2): 111\u2013131.</p>\n<p>[6]: Akin, Lara B., et. al. 2010. <a href=\"http://barringtonleigh.net/publications/w16415.pdf\">\"Pro-social Spending And Well-Being: Cross-Cultural Evidence for a Psychological Universal.\"</a>&nbsp;National Bureau of Economic Research Working Paper #16415.</p>\n<p>[7]: Dunn, Elizabeth W., Lara B. Aknin, and Michael I. Norton. 2008. <a href=\"http://www.people.hbs.edu/mnorton/dunn%20aknin%20norton.pdf\">\u201cSpending Money on Others Promotes Happiness.\u201d</a>&nbsp;<em>Science</em> 319: 1687-1688.</p>\n<p>[8]: Gneezy, Uri and Aldo Rustichini. 2000a. <a href=\"http://rady.ucsd.edu/faculty/directory/gneezy/pub/docs/fine.pdf\">\u201cA fine is a price.\u201d</a>&nbsp;<em>Journal of Legal Studies</em> 29: 1-18.</p>\n<p>[9]: Gneezy, Uri and Aldo Rustichini. 2000b. <a href=\"http://management.ucsd.edu/faculty/directory/gneezy/pub/docs/pay-enough.pdf\">\u201cPay enough or don't pay at all.\u201d</a>&nbsp;<em>Quarterly Journal of Economics</em> 115: 791-810.</p>\n<p>[10]: Lepper, Mark R., David Greene, and Richard E. Nisbett. 1973. <a href=\"http://psycnet.apa.org/index.cfm?fa=buy.optionToBuy&amp;uid=1974-10497-001\">\u201cUndermining Children's Intrinsic Interest with Extrinsic Reward: A Test of the \u2018Overjustification\u2019 Hypothesis.\u201d</a>&nbsp;<em>Journal of Personality and Social Psychology</em> 28(1): 129-137.</p>\n<p>[11]: Deci, Edward L. 1971. <a href=\"http://psycnet.apa.org/index.cfm?fa=buy.optionToBuy&amp;uid=1971-22190-001\">\u201cEffects of Externally Mediated Rewards on Intrinsic Motivation.\u201d</a>&nbsp;<em>Journal of Personality and Social Psychology </em>18(1): 105-115.</p>\n<p>[12]: Vohs, Kathleen D., Nicole L. Mead, and Miranda R. Goode. 2006. <a href=\"http://www.sciencemag.org/content/314/5802/1154.short\">\u201cThe Psychological Consequences of Money\u201d</a>. <em>Science</em> 17 (314): 1154-1156.</p>\n<p>[13]: Meier, Stephan. 2007. <a href=\"http://www.bos.frb.org/economic/wp/wp2006/wp0618.pdf\">\u201cDo Subsidies Increase Charitable Giving in the Long Run? Matching Donations in a Field Experiment\u201d</a>. Federal Reserve Bank of Boston Working Paper #06-18.</p>\n<p>[14]: Falk, Armin. 2005. <a href=\"http://www.ucl.ac.uk/~uctpshu/falk.pdf\">\u201cGift Exchange in the Field\u201d</a>. University of Bonn.</p>\n<p>-</p>\n<p><em>(This essay is also cross-posted on the <a href=\"http://www.givingwhatwecan.org/blog/2013-09-03/should-we-tell-people-that-giving-makes-them-happier\">Giving What We Can blog</a> and <a href=\"http://www.everydayutilitarian.com/essays/should-we-tell-people-that-giving-makes-them-happier/\">my blog</a>.)</em></p>", "sections": [{"title": "Giving and Happiness", "anchor": "Giving_and_Happiness", "level": 1}, {"title": "Altering Incentives", "anchor": "Altering_Incentives", "level": 1}, {"title": "Opportunities for Further Investigation", "anchor": "Opportunities_for_Further_Investigation", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "27 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hEqsWLm5zQtsPevd3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-04T22:42:05.195Z", "modifiedAt": null, "url": null, "title": "How effectively can we plan for future decades? (initial findings)", "slug": "how-effectively-can-we-plan-for-future-decades-initial", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:07.969Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gYiXucYZBgWdWdxJe/how-effectively-can-we-plan-for-future-decades-initial", "pageUrlRelative": "/posts/gYiXucYZBgWdWdxJe/how-effectively-can-we-plan-for-future-decades-initial", "linkUrl": "https://www.lesswrong.com/posts/gYiXucYZBgWdWdxJe/how-effectively-can-we-plan-for-future-decades-initial", "postedAtFormatted": "Wednesday, September 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20effectively%20can%20we%20plan%20for%20future%20decades%3F%20(initial%20findings)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20effectively%20can%20we%20plan%20for%20future%20decades%3F%20(initial%20findings)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgYiXucYZBgWdWdxJe%2Fhow-effectively-can-we-plan-for-future-decades-initial%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20effectively%20can%20we%20plan%20for%20future%20decades%3F%20(initial%20findings)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgYiXucYZBgWdWdxJe%2Fhow-effectively-can-we-plan-for-future-decades-initial", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgYiXucYZBgWdWdxJe%2Fhow-effectively-can-we-plan-for-future-decades-initial", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1712, "htmlBody": "<p><small>Cross-posted from <a href=\"http://intelligence.org/2013/09/04/how-effectively-can-we-plan-for-future-decades/\">MIRI's blog</a>.</small></p>\n<p>MIRI aims to do research now that increases humanity's odds of successfully managing important AI-related events that are at least&nbsp;<a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\">a few decades away</a>. Thus, we'd like to know: To what degree can we take actions now that will predictably have positive effects on AI-related events decades from now? And, which factors predict success and failure in planning for decades-distant events that share important features with future AI events?</p>\n<p>Or, more generally:&nbsp;<strong>How effectively can humans plan for future decades? Which factors predict success and failure in planning for future decades?</strong></p>\n<p><strong></strong>To investigate these questions, we asked <a href=\"http://mathisbeauty.org/aboutme.html\">Jonah Sinick</a> to examine historical attempts to plan for future decades and summarize his findings. We pre-committed to publishing our entire email exchange on the topic (with minor editing), just as Jonah had done previously with GiveWell&nbsp;<a href=\"http://blog.givewell.org/2012/10/18/revisiting-the-case-for-insecticide-treated-nets-itns/\">on the subject of insecticide-treated nets</a>. The post below is a summary of findings from&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2013/09/Can-we-know-what-to-do-about-AI.docx\">our full email exchange (.docx)</a>&nbsp;so far.</p>\n<p><strong>We decided to publish our initial findings after investigating only a few historical cases</strong>. This allows us to gain feedback on the value of the project, as well as suggestions for improvement, before continuing. It also means that <strong>we aren't yet able to draw any confident conclusions about our core questions</strong>.</p>\n<p>The most significant results from this project so far are:</p>\n<ol>\n<li>Jonah's initial impressions about&nbsp;<em>The Limits to Growth</em>&nbsp;(1972), a famous forecasting study on population and resource depletion, were that its long-term predictions were mostly wrong, and also that its authors (at the time of writing it) didn't have credentials that would predict forecasting success. Upon reading the book, its critics, and its defenders, Jonah concluded that many critics and defenders had &nbsp;seriously misrepresented the book, and that the book itself&nbsp;exhibits high epistemic standards and does not make significant predictions that turned out to be wrong.</li>\n<li>Svante Arrhenius (1859-1927) did a surprisingly good job of climate modeling given the limited information available to him, but he was nevertheless wrong about two important policy-relevant factors. First, he failed to predict how quickly carbon emissions would increase. Second, he predicted that global warming would have positive rather than negative humanitarian impacts.&nbsp;If more people had taken Arrhenius' predictions seriously and burned fossil fuels faster for humanitarian reasons, then today's scientific consensus on the effects of climate change suggests that the humanitarian effects would have been negative.</li>\n<li>In retrospect, Norbert Wiener's concerns about the medium-term dangers of increased automation appear naive, and it seems likely that even at the time, better epistemic practices would have yielded substantially better predictions.</li>\n<li>Upon initial investigation, several historical cases seemed unlikely to shed substantial light on our &nbsp;core questions: Norman&nbsp;Rasmussen's analysis of the safety of nuclear power plants, Leo Szilard's choice to keep secret a patent related to nuclear chain reactions,&nbsp;Cold War planning efforts to win decades later, and several cases of \"ethically concerned scientists.\"</li>\n<li>Upon initial investigation, two historical cases seemed like they&nbsp;<em>might</em> shed light on our &nbsp;core questions, but only after many hours of additional research on each of them: China's one-child policy, and the Ford Foundation's impact on India's 1991 financial crisis.</li>\n<li>We listed many other historical cases that may be worth investigating.</li>\n</ol>\n<p>The project has also produced a chapter-by-chapter list of some key lessons from Nate Silver's&nbsp;<a href=\"http://www.amazon.com/The-Signal-Noise-Many-Predictions/dp/159420411X\"><em>The Signal and the Noise</em></a>, available <a href=\"/lw/hxx/some_highlights_from_nate_silvers_the_signal_and/\">here</a>.</p>\n<p>Further details are given below. For sources and more, please see&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2013/09/Can-we-know-what-to-do-about-AI.docx\">our full email exchange (.docx)</a>.</p>\n<!--more-->\n<p><a id=\"more\"></a></p>\n<h3>The Limits to Growth</h3>\n<p>In his initial look at&nbsp;<em><a href=\"http://www.amazon.com/Limits-Growth-Donella-H-Meadows/dp/0451057678/\">The Limits to</a></em><a href=\"http://www.amazon.com/Limits-Growth-Donella-H-Meadows/dp/0451057678/\"> Growth</a>&nbsp;(1972), Jonah noted that the authors were fairly young at the time of writing (the oldest was 31), and they lacked credentials in long-term forecasting. Moreover, it appeared that&nbsp;<em>Limits to Growth</em> predicted a sort of doomsday scenario -&nbsp;<em>ala</em> Ehrlich's&nbsp;<em><a href=\"http://www.amazon.com/The-population-bomb-Paul-Ehrlich/dp/0345021711/\">The Population Bomb</a></em>&nbsp;(1968) - that had failed to occur. In particular, it appeared that&nbsp;<em>Limits to Growth</em> had failed to appreciate <a href=\"http://en.wikipedia.org/wiki/Julian_Lincoln_Simon\">Julian Simon</a>'s point that other resources would substitute for depleted resources. Upon reading the book, Jonah found that:</p>\n<ul>\n<li>The book avoids strong, unconditional claims.&nbsp;Its core claim is that <em>if</em> exponential growth of resource usage continues, <em>then</em> there will likely be a societal collapse by 2100.</li>\n<li>The book was careful to qualify its claims, and met high epistemic standards. Jonah wrote: \"The book doesn't look naive even in retrospect, which is impressive given that it was written 40 years ago. \"</li>\n<li>The authors discuss substitutability at length in chapter 4.</li>\n<li>The book discusses mitigation at a theoretical level, but doesn't give explicit policy recommendations, perhaps because the issues involved were too complex.</li>\n</ul>\n<h3><br /></h3>\n<h3>Svante Arrhenius</h3>\n<p>Derived more than a century ago, <a href=\"http://en.wikipedia.org/wiki/Svante_Arrhenius\">Svante Arrhenius</a>'&nbsp;equation for how the Earth's temperature varies as a function of concentration of carbon dioxide is the same equation used today. But while Arrhenius' climate modeling was impressive given the information available to him at the time, he failed to predict (by a large margin) how quickly fossil fuels would be burned. He also predicted that global warming would have positive humanitarian effects,&nbsp;but based on our current understanding, the expected humanitarian effects seem negative.</p>\n<p>Arrhenius's predictions were mostly ignored at the time, but had people taken them seriously and burned fossil fuels more quickly,&nbsp;the humanitarian effects would probably have been negative. &nbsp;</p>\n<h3><br /></h3>\n<h3>Norbert Wiener</h3>\n<p>As Jonah explains,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Norbert_Weiner\">Norbert Wiener</a> (1894-1964) \"believed that unless countermeasures were taken, automation would render low skilled workers unemployable. He believed that this would precipitate an economic crisis far worse than that of the Great Depression.\" Nearly 50 years after his death, this <a href=\"/lw/hh4/the_robots_ai_and_unemployment_antifaq/\">doesn't seem to have happened</a>&nbsp;much, though it may eventually happen.</p>\n<p>Jonah's impression is that Wiener had strong views on the subject, doesn't seem to have updated much in response to incoming evidence, and seems to have relied to heavily on what <a href=\"http://en.wikipedia.org/wiki/The_Hedgehog_and_the_Fox\">Berlin (1953)</a> and&nbsp;<a href=\"http://www.amazon.com/Expert-Political-Judgment-Good-Know/dp/0691128715/\">Tetlock (2005)</a> described as \"hedgehog\" thinking: \"the fox knows many things, but the hedgehog knows one big thing.\" &nbsp;</p>\n<h3><br /></h3>\n<h3>Some historical cases that seem unlikely to shed light on our questions</h3>\n<p><a href=\"http://en.wikipedia.org/wiki/WASH-1400\">Rasmussen (1975)</a> is a probabilistic risk assessment of nuclear power plants, written before any nuclear power plant disasters had occurred. However, Jonah concluded that this historical case wasn't very relevant to our specific questions about taking actions useful for decades-distant AI outcomes, in part because the issue is highly domain specific, and because the report makes a large number of small predictions rather than a few salient predictions.</p>\n<p>In 1936,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Le%C3%B3_Szil%C3%A1rd\">Le&oacute; Szil&aacute;rd</a> assigned his&nbsp;chain reaction patent in a way that ensured it would be kept secret from the Nazis. However, Jonah concluded:</p>\n<blockquote>I think that this isn't a good example of a nontrivial future prediction. The destructive potential seems pretty obvious &ndash; anything that produces a huge amount of concentrated energy can be used in a destructive way. As for the Nazis, Szilard was himself Jewish and fled from the Nazis, and it seems pretty obvious that one wouldn't want a dangerous regime to acquire knowledge that has destructive potential. It would be more impressive if the early developers of quantum mechanics had kept their research secret on account of dimly being aware of the possibility of destructive potential, or if Szilard had filed his patent secretly in a hypothetical world in which the Nazi regime was years away.</blockquote>\n<p>Jonah briefly investigated Cold War efforts aimed at winning the war decades later, but concluded that it was \"too difficult to tie these efforts to war outcomes.\"</p>\n<p>Jonah also investigated Kaj Sotala's&nbsp;<a href=\"/lw/gln/a_brief_history_of_ethically_concerned_scientists/\">A brief history of ethically concerned scientists</a>. Most of the historical cases cited there didn't seem relevant to this project. Many cases involved \"scientists concealing their discoveries out of concern that they would be used for military purposes,\" but this seems to be an increasingly irrelevant sort of historical case, since science and technology markets are now relatively efficient, and concealing a discovery rarely delays progress for very long (e.g. see <a href=\"http://www.amazon.com/What-Technology-Wants-Kevin-Kelly/dp/0143120174/\">Kelly 2011</a>). Other cases involved efforts to reduce the use of dangerous weapons for which the threat was imminent during the time of the advocacy. There may be lessons among these cases, but they appear to be of relatively weak relevance to our current project. &nbsp;&nbsp;</p>\n<h3><br /></h3>\n<h3>Some historical cases that might shed light on our questions with much additional research</h3>\n<p>Jonah performed an initial investigation of the impacts of China's <a href=\"http://en.wikipedia.org/wiki/One-child_policy\">one-child policy</a>, and concluded that it would take many, many hours of research to determine both the sign and the magnitude of the policy's impacts.</p>\n<p>Jonah also investigated a case involving the <a href=\"http://www.fordfoundation.org/\">Ford Foundation</a>. In <a href=\"http://www.givewell.org/files/conversations/Lant%20Pritchet%2006-18-12%20final%20for%20upload.pdf\">a conversation with GiveWell</a>, Lant Pritchett said:</p>\n<blockquote>[One] example of transformative philanthropy is related to India&rsquo;s recovery from its economic crisis of 1991. Other countries had previously had similar crises and failed to implement good policies that would have allowed them to recover from their crises. By way of contrast, India implemented good policies and recovered in a short time frame. Most of the key actors who ensured that India implemented the policies that it did were influenced by a think tank established by the Ford Foundation ten years before the crisis. The think tank exposed Indians to relevant ideas from the developed world about liberalization. The difference between (a) India&rsquo;s upward economic trajectory and (b) what its upward economic trajectory would have been if it had been unsuccessful in recovering from the 1991 crisis is in the trillions of dollars. As such, the Ford Foundation&rsquo;s investment in the think tank had a huge impact. For the ten years preceding the crisis, it looked like the think tank was having no impact, but it turned out to have a huge impact.</blockquote>\n<p>Unfortunately, Jonah was unable to find any sources or contacts that would allow him to check whether this story is true. &nbsp;</p>\n<h3><br /></h3>\n<h3>Other historical cases that might be worth investigating</h3>\n<p>Historical cases we identified but did not yet investigate include:</p>\n<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/K._Eric_Drexler\">Eric Drexler</a>'s early predictions about the feasibility and likely effects of nanotechnology.</li>\n<li>The <a href=\"http://en.wikipedia.org/wiki/Asilomar_Conference_on_Recombinant_DNA\">Asilomar conference on recombinant DNA</a></li>\n<li>Efforts to <a href=\"http://www.amazon.com/Near-Earth-Objects-Finding-Them-Before/dp/0691149291/\">detect asteroids before they threaten Earth</a></li>\n<li>The <a href=\"http://en.wikipedia.org/wiki/Green_Revolution\">Green Revolution</a></li>\n<li>The modern history of <a href=\"http://en.wikipedia.org/wiki/Cryptography\">cryptography</a></li>\n<li>Early efforts to <a href=\"http://www.amazon.com/The-Discovery-Global-Warming-Technology/dp/067403189X/\">mitigate global warming</a></li>\n<li>Possible deliberate long term efforts to produce scientific breakthroughs (the transistor? the human genome?)</li>\n<li>Rachel Carson's&nbsp;<a href=\"http://en.wikipedia.org/wiki/Silent_Spring\"><em>Silent Spring</em></a> (1962)</li>\n<li>Paul Ehrlich's&nbsp;<a href=\"http://en.wikipedia.org/wiki/The_Population_Bomb\"><em>The Population Bomb</em></a> (1968)</li>\n<li>The Worldwatch Institute's <a href=\"http://en.wikipedia.org/wiki/State_of_the_World_(book_series)\"><em>State of the World</em></a> reports (since 1984)</li>\n<li>The WCED's&nbsp;<a href=\"http://en.wikipedia.org/wiki/Our_Common_Future\"><em>Our Common Future</em></a> (1987)</li>\n</ul>\n&nbsp;", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gYiXucYZBgWdWdxJe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 21, "extendedScore": null, "score": 1.3285708858607924e-06, "legacy": true, "legacyId": "24005", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Cross-posted from <a href=\"http://intelligence.org/2013/09/04/how-effectively-can-we-plan-for-future-decades/\">MIRI's blog</a>.</small></p>\n<p>MIRI aims to do research now that increases humanity's odds of successfully managing important AI-related events that are at least&nbsp;<a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\">a few decades away</a>. Thus, we'd like to know: To what degree can we take actions now that will predictably have positive effects on AI-related events decades from now? And, which factors predict success and failure in planning for decades-distant events that share important features with future AI events?</p>\n<p>Or, more generally:&nbsp;<strong>How effectively can humans plan for future decades? Which factors predict success and failure in planning for future decades?</strong></p>\n<p><strong></strong>To investigate these questions, we asked <a href=\"http://mathisbeauty.org/aboutme.html\">Jonah Sinick</a> to examine historical attempts to plan for future decades and summarize his findings. We pre-committed to publishing our entire email exchange on the topic (with minor editing), just as Jonah had done previously with GiveWell&nbsp;<a href=\"http://blog.givewell.org/2012/10/18/revisiting-the-case-for-insecticide-treated-nets-itns/\">on the subject of insecticide-treated nets</a>. The post below is a summary of findings from&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2013/09/Can-we-know-what-to-do-about-AI.docx\">our full email exchange (.docx)</a>&nbsp;so far.</p>\n<p><strong>We decided to publish our initial findings after investigating only a few historical cases</strong>. This allows us to gain feedback on the value of the project, as well as suggestions for improvement, before continuing. It also means that <strong>we aren't yet able to draw any confident conclusions about our core questions</strong>.</p>\n<p>The most significant results from this project so far are:</p>\n<ol>\n<li>Jonah's initial impressions about&nbsp;<em>The Limits to Growth</em>&nbsp;(1972), a famous forecasting study on population and resource depletion, were that its long-term predictions were mostly wrong, and also that its authors (at the time of writing it) didn't have credentials that would predict forecasting success. Upon reading the book, its critics, and its defenders, Jonah concluded that many critics and defenders had &nbsp;seriously misrepresented the book, and that the book itself&nbsp;exhibits high epistemic standards and does not make significant predictions that turned out to be wrong.</li>\n<li>Svante Arrhenius (1859-1927) did a surprisingly good job of climate modeling given the limited information available to him, but he was nevertheless wrong about two important policy-relevant factors. First, he failed to predict how quickly carbon emissions would increase. Second, he predicted that global warming would have positive rather than negative humanitarian impacts.&nbsp;If more people had taken Arrhenius' predictions seriously and burned fossil fuels faster for humanitarian reasons, then today's scientific consensus on the effects of climate change suggests that the humanitarian effects would have been negative.</li>\n<li>In retrospect, Norbert Wiener's concerns about the medium-term dangers of increased automation appear naive, and it seems likely that even at the time, better epistemic practices would have yielded substantially better predictions.</li>\n<li>Upon initial investigation, several historical cases seemed unlikely to shed substantial light on our &nbsp;core questions: Norman&nbsp;Rasmussen's analysis of the safety of nuclear power plants, Leo Szilard's choice to keep secret a patent related to nuclear chain reactions,&nbsp;Cold War planning efforts to win decades later, and several cases of \"ethically concerned scientists.\"</li>\n<li>Upon initial investigation, two historical cases seemed like they&nbsp;<em>might</em> shed light on our &nbsp;core questions, but only after many hours of additional research on each of them: China's one-child policy, and the Ford Foundation's impact on India's 1991 financial crisis.</li>\n<li>We listed many other historical cases that may be worth investigating.</li>\n</ol>\n<p>The project has also produced a chapter-by-chapter list of some key lessons from Nate Silver's&nbsp;<a href=\"http://www.amazon.com/The-Signal-Noise-Many-Predictions/dp/159420411X\"><em>The Signal and the Noise</em></a>, available <a href=\"/lw/hxx/some_highlights_from_nate_silvers_the_signal_and/\">here</a>.</p>\n<p>Further details are given below. For sources and more, please see&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2013/09/Can-we-know-what-to-do-about-AI.docx\">our full email exchange (.docx)</a>.</p>\n<!--more-->\n<p><a id=\"more\"></a></p>\n<h3 id=\"The_Limits_to_Growth\">The Limits to Growth</h3>\n<p>In his initial look at&nbsp;<em><a href=\"http://www.amazon.com/Limits-Growth-Donella-H-Meadows/dp/0451057678/\">The Limits to</a></em><a href=\"http://www.amazon.com/Limits-Growth-Donella-H-Meadows/dp/0451057678/\"> Growth</a>&nbsp;(1972), Jonah noted that the authors were fairly young at the time of writing (the oldest was 31), and they lacked credentials in long-term forecasting. Moreover, it appeared that&nbsp;<em>Limits to Growth</em> predicted a sort of doomsday scenario -&nbsp;<em>ala</em> Ehrlich's&nbsp;<em><a href=\"http://www.amazon.com/The-population-bomb-Paul-Ehrlich/dp/0345021711/\">The Population Bomb</a></em>&nbsp;(1968) - that had failed to occur. In particular, it appeared that&nbsp;<em>Limits to Growth</em> had failed to appreciate <a href=\"http://en.wikipedia.org/wiki/Julian_Lincoln_Simon\">Julian Simon</a>'s point that other resources would substitute for depleted resources. Upon reading the book, Jonah found that:</p>\n<ul>\n<li>The book avoids strong, unconditional claims.&nbsp;Its core claim is that <em>if</em> exponential growth of resource usage continues, <em>then</em> there will likely be a societal collapse by 2100.</li>\n<li>The book was careful to qualify its claims, and met high epistemic standards. Jonah wrote: \"The book doesn't look naive even in retrospect, which is impressive given that it was written 40 years ago. \"</li>\n<li>The authors discuss substitutability at length in chapter 4.</li>\n<li>The book discusses mitigation at a theoretical level, but doesn't give explicit policy recommendations, perhaps because the issues involved were too complex.</li>\n</ul>\n<h3><br></h3>\n<h3 id=\"Svante_Arrhenius\">Svante Arrhenius</h3>\n<p>Derived more than a century ago, <a href=\"http://en.wikipedia.org/wiki/Svante_Arrhenius\">Svante Arrhenius</a>'&nbsp;equation for how the Earth's temperature varies as a function of concentration of carbon dioxide is the same equation used today. But while Arrhenius' climate modeling was impressive given the information available to him at the time, he failed to predict (by a large margin) how quickly fossil fuels would be burned. He also predicted that global warming would have positive humanitarian effects,&nbsp;but based on our current understanding, the expected humanitarian effects seem negative.</p>\n<p>Arrhenius's predictions were mostly ignored at the time, but had people taken them seriously and burned fossil fuels more quickly,&nbsp;the humanitarian effects would probably have been negative. &nbsp;</p>\n<h3><br></h3>\n<h3 id=\"Norbert_Wiener\">Norbert Wiener</h3>\n<p>As Jonah explains,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Norbert_Weiner\">Norbert Wiener</a> (1894-1964) \"believed that unless countermeasures were taken, automation would render low skilled workers unemployable. He believed that this would precipitate an economic crisis far worse than that of the Great Depression.\" Nearly 50 years after his death, this <a href=\"/lw/hh4/the_robots_ai_and_unemployment_antifaq/\">doesn't seem to have happened</a>&nbsp;much, though it may eventually happen.</p>\n<p>Jonah's impression is that Wiener had strong views on the subject, doesn't seem to have updated much in response to incoming evidence, and seems to have relied to heavily on what <a href=\"http://en.wikipedia.org/wiki/The_Hedgehog_and_the_Fox\">Berlin (1953)</a> and&nbsp;<a href=\"http://www.amazon.com/Expert-Political-Judgment-Good-Know/dp/0691128715/\">Tetlock (2005)</a> described as \"hedgehog\" thinking: \"the fox knows many things, but the hedgehog knows one big thing.\" &nbsp;</p>\n<h3><br></h3>\n<h3 id=\"Some_historical_cases_that_seem_unlikely_to_shed_light_on_our_questions\">Some historical cases that seem unlikely to shed light on our questions</h3>\n<p><a href=\"http://en.wikipedia.org/wiki/WASH-1400\">Rasmussen (1975)</a> is a probabilistic risk assessment of nuclear power plants, written before any nuclear power plant disasters had occurred. However, Jonah concluded that this historical case wasn't very relevant to our specific questions about taking actions useful for decades-distant AI outcomes, in part because the issue is highly domain specific, and because the report makes a large number of small predictions rather than a few salient predictions.</p>\n<p>In 1936,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Le%C3%B3_Szil%C3%A1rd\">Le\u00f3 Szil\u00e1rd</a> assigned his&nbsp;chain reaction patent in a way that ensured it would be kept secret from the Nazis. However, Jonah concluded:</p>\n<blockquote>I think that this isn't a good example of a nontrivial future prediction. The destructive potential seems pretty obvious \u2013 anything that produces a huge amount of concentrated energy can be used in a destructive way. As for the Nazis, Szilard was himself Jewish and fled from the Nazis, and it seems pretty obvious that one wouldn't want a dangerous regime to acquire knowledge that has destructive potential. It would be more impressive if the early developers of quantum mechanics had kept their research secret on account of dimly being aware of the possibility of destructive potential, or if Szilard had filed his patent secretly in a hypothetical world in which the Nazi regime was years away.</blockquote>\n<p>Jonah briefly investigated Cold War efforts aimed at winning the war decades later, but concluded that it was \"too difficult to tie these efforts to war outcomes.\"</p>\n<p>Jonah also investigated Kaj Sotala's&nbsp;<a href=\"/lw/gln/a_brief_history_of_ethically_concerned_scientists/\">A brief history of ethically concerned scientists</a>. Most of the historical cases cited there didn't seem relevant to this project. Many cases involved \"scientists concealing their discoveries out of concern that they would be used for military purposes,\" but this seems to be an increasingly irrelevant sort of historical case, since science and technology markets are now relatively efficient, and concealing a discovery rarely delays progress for very long (e.g. see <a href=\"http://www.amazon.com/What-Technology-Wants-Kevin-Kelly/dp/0143120174/\">Kelly 2011</a>). Other cases involved efforts to reduce the use of dangerous weapons for which the threat was imminent during the time of the advocacy. There may be lessons among these cases, but they appear to be of relatively weak relevance to our current project. &nbsp;&nbsp;</p>\n<h3><br></h3>\n<h3 id=\"Some_historical_cases_that_might_shed_light_on_our_questions_with_much_additional_research\">Some historical cases that might shed light on our questions with much additional research</h3>\n<p>Jonah performed an initial investigation of the impacts of China's <a href=\"http://en.wikipedia.org/wiki/One-child_policy\">one-child policy</a>, and concluded that it would take many, many hours of research to determine both the sign and the magnitude of the policy's impacts.</p>\n<p>Jonah also investigated a case involving the <a href=\"http://www.fordfoundation.org/\">Ford Foundation</a>. In <a href=\"http://www.givewell.org/files/conversations/Lant%20Pritchet%2006-18-12%20final%20for%20upload.pdf\">a conversation with GiveWell</a>, Lant Pritchett said:</p>\n<blockquote>[One] example of transformative philanthropy is related to India\u2019s recovery from its economic crisis of 1991. Other countries had previously had similar crises and failed to implement good policies that would have allowed them to recover from their crises. By way of contrast, India implemented good policies and recovered in a short time frame. Most of the key actors who ensured that India implemented the policies that it did were influenced by a think tank established by the Ford Foundation ten years before the crisis. The think tank exposed Indians to relevant ideas from the developed world about liberalization. The difference between (a) India\u2019s upward economic trajectory and (b) what its upward economic trajectory would have been if it had been unsuccessful in recovering from the 1991 crisis is in the trillions of dollars. As such, the Ford Foundation\u2019s investment in the think tank had a huge impact. For the ten years preceding the crisis, it looked like the think tank was having no impact, but it turned out to have a huge impact.</blockquote>\n<p>Unfortunately, Jonah was unable to find any sources or contacts that would allow him to check whether this story is true. &nbsp;</p>\n<h3><br></h3>\n<h3 id=\"Other_historical_cases_that_might_be_worth_investigating\">Other historical cases that might be worth investigating</h3>\n<p>Historical cases we identified but did not yet investigate include:</p>\n<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/K._Eric_Drexler\">Eric Drexler</a>'s early predictions about the feasibility and likely effects of nanotechnology.</li>\n<li>The <a href=\"http://en.wikipedia.org/wiki/Asilomar_Conference_on_Recombinant_DNA\">Asilomar conference on recombinant DNA</a></li>\n<li>Efforts to <a href=\"http://www.amazon.com/Near-Earth-Objects-Finding-Them-Before/dp/0691149291/\">detect asteroids before they threaten Earth</a></li>\n<li>The <a href=\"http://en.wikipedia.org/wiki/Green_Revolution\">Green Revolution</a></li>\n<li>The modern history of <a href=\"http://en.wikipedia.org/wiki/Cryptography\">cryptography</a></li>\n<li>Early efforts to <a href=\"http://www.amazon.com/The-Discovery-Global-Warming-Technology/dp/067403189X/\">mitigate global warming</a></li>\n<li>Possible deliberate long term efforts to produce scientific breakthroughs (the transistor? the human genome?)</li>\n<li>Rachel Carson's&nbsp;<a href=\"http://en.wikipedia.org/wiki/Silent_Spring\"><em>Silent Spring</em></a> (1962)</li>\n<li>Paul Ehrlich's&nbsp;<a href=\"http://en.wikipedia.org/wiki/The_Population_Bomb\"><em>The Population Bomb</em></a> (1968)</li>\n<li>The Worldwatch Institute's <a href=\"http://en.wikipedia.org/wiki/State_of_the_World_(book_series)\"><em>State of the World</em></a> reports (since 1984)</li>\n<li>The WCED's&nbsp;<a href=\"http://en.wikipedia.org/wiki/Our_Common_Future\"><em>Our Common Future</em></a> (1987)</li>\n</ul>\n&nbsp;", "sections": [{"title": "The Limits to Growth", "anchor": "The_Limits_to_Growth", "level": 1}, {"title": "Svante Arrhenius", "anchor": "Svante_Arrhenius", "level": 1}, {"title": "Norbert Wiener", "anchor": "Norbert_Wiener", "level": 1}, {"title": "Some historical cases that seem unlikely to shed light on our questions", "anchor": "Some_historical_cases_that_seem_unlikely_to_shed_light_on_our_questions", "level": 1}, {"title": "Some historical cases that might shed light on our questions with much additional research", "anchor": "Some_historical_cases_that_might_shed_light_on_our_questions_with_much_additional_research", "level": 1}, {"title": "Other historical cases that might be worth investigating", "anchor": "Other_historical_cases_that_might_be_worth_investigating", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "18 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rGj2K8vu5qQCTWCar", "ZiRKzx3yv7NyA5rjF", "hxaq9MCaSrwWPmooZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-04T23:02:42.482Z", "modifiedAt": null, "url": null, "title": "You are the average of the five people you spend most time with. ", "slug": "you-are-the-average-of-the-five-people-you-spend-most-time", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:07.017Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xT3fwkhJDXwaLNvM6/you-are-the-average-of-the-five-people-you-spend-most-time", "pageUrlRelative": "/posts/xT3fwkhJDXwaLNvM6/you-are-the-average-of-the-five-people-you-spend-most-time", "linkUrl": "https://www.lesswrong.com/posts/xT3fwkhJDXwaLNvM6/you-are-the-average-of-the-five-people-you-spend-most-time", "postedAtFormatted": "Wednesday, September 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20You%20are%20the%20average%20of%20the%20five%20people%20you%20spend%20most%20time%20with.%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYou%20are%20the%20average%20of%20the%20five%20people%20you%20spend%20most%20time%20with.%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxT3fwkhJDXwaLNvM6%2Fyou-are-the-average-of-the-five-people-you-spend-most-time%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=You%20are%20the%20average%20of%20the%20five%20people%20you%20spend%20most%20time%20with.%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxT3fwkhJDXwaLNvM6%2Fyou-are-the-average-of-the-five-people-you-spend-most-time", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxT3fwkhJDXwaLNvM6%2Fyou-are-the-average-of-the-five-people-you-spend-most-time", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 902, "htmlBody": "<p>Mudus Ponies wrote:&nbsp;</p>\n<blockquote>\n<p class=\"MsoNormal\">If you are a human, then the biggest influence on your personality is your peer group. Choose your peers.</p>\n<p class=\"MsoNormal\">If you want to be better at math, surround yourself with mathematicians. If you want to be more productive, hang out with productive people. If you want to be outgoing or artistic or altruistic or polite or proactive or smart or just about anything else, find people who are better than you at that thing and become friends with them. The status-seeking conformity-loving parts of your mind will push you to become like them. (The incorrect but pithy version: \"You are an average of the five people you spend the most time with.\")</p>\n<p class=\"MsoNormal\">I've had a lot of success with this technique by going to the Less Wrong meetups in Boston, and by making a habit of attending any event where I'll be the stupidest person in the room (such as the average Less Wrong meetup).</p>\n</blockquote>\n<p class=\"MsoNormal\">&nbsp;66 people upvoted it.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">Before, Lukeprog and Ferriss had mentioned the same: You are the average of your surroundings.&nbsp;</p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\">I believe that. I would prefer that to not be the case, but I do think that even if you are not truly the average of the five, it is better to act as if you were the average of the five than to act as if you never heard this advice.&nbsp;</p>\n<p class=\"MsoNormal\">If I am to follow such advice I have a problem: I should spend time with: Nick Bostrom, Natalie Portman, Whoever parties as hard as Sean Parker does in \"The Social Network\", Steve Pinker and Ferriss. &nbsp;</p>\n<p class=\"MsoNormal\">I'm going to throw some problems in, and present no solutions, I hope comments may provide them if anyone knows one:<br /><br />1)The people one would like to be average of are incredibly busy doing what made them become those people&nbsp;</p>\n<p class=\"MsoNormal\">2)They do not live at the same place</p>\n<p class=\"MsoNormal\">3)They seldom have reason to be near you&nbsp;</p>\n<p class=\"MsoNormal\">4)If they displayed well enough, their success precludes them from taking new people in due to lack of cognitive space.&nbsp;</p>\n<p class=\"MsoNormal\">5)They may be interested in you and what you have to say in as much as that is something you can provide them, but not necessarily that means they will share what you'd like them to with you.&nbsp;</p>\n<p class=\"MsoNormal\">6)If you were literally the average that wouldn't help much since most of us want to succeed in more than one domain, and we usually model ourselves with people who are monomaniacal, who are the only ones who thrive enough to be seen in a 7 billion world.</p>\n<p class=\"MsoNormal\">I would like to know a lot of stuff as I mentioned in <a href=\"/lw/h3f/drowning_in_an_information_ocean/\">Drowning in an Information Ocean,</a> but I also want to be creative and engaging like Natalie, work/party hard as Sean, speak eloquently as Pinker, think well as Bostrom, and acquire skills and money at Ferriss' speed.&nbsp;<br /><br />Aubrey is fish oil. He hooked my attention when 17 because he sells eternity. <br />Bostrom is LSA. He hooked me because he sells the future of the universe. <br />Eliezer is Modafinil. He hooked me because he sells the map between one's current situation and the future of the universe.<br />Hofstadter is LSD. He hooked me because he sells broadness of converging/academic knowledge. <br />Natalie is marihuana. She hooked me because she sells the compatibility between academic excellence and divergent/artistic knowledge. <br />Partying hard is cocaine. It hooked me because my nucleus accumbens works in the normal way designed by evolution.<br />Effective Altruism is food after starving, it hooked me because it sells counterfactually relevant actions that create quantifiably improved markets.&nbsp;<br />Ferriss of course is Speedball, the ultimate salesman. He sells the entire dream. He sells a quantifiable way to siphon one self into awesomeness without overload so you can party hard and become superman one skill at a time.&nbsp;</p>\n<p class=\"MsoNormal\">These are only a few of the awesome people around, public figures visible to Lesswrong. There are dozens of others. I don't want to do what one of them did. I want to do it all. This is of course impossible. My intrinsic, core values relate to going in many directions at the same time. I think most people are like that. There are just so many options around and only one life to enjoy them all (which is why Aubrey is the entrance-drug).&nbsp;<br />As put by Lev:&nbsp;</p>\n<p class=\"MsoNormal\"><a href=\"http://www.youtube.com/watch?v=Iz0kTdcz4QI\">I don't know what I want.</a>&nbsp;</p>\n<p class=\"MsoNormal\">There are these cool things around, but I suspect, after many years on earth, and visiting interesting people everywhere, that at least a good 50% of people (even the most rational, non-broken people) truly do not want anything in particular that much. The ones who seem like they do just took the plunge into saying so and self-reinforcing into wanting something. If they dug deep enough, they actually just don't have a clear want. Or if they do, like me they would have many. Many more than they can actually act on. <br /><br />I truly and fully believe the advice that one should live as if one were the average of the five people one spends most time with. I just have no idea on how to do it, or whom to pick among a set that contains not 5, but 5000 people or more. <br /><br />How did you solve this problem? Does it cause you to experience an <a href=\"/lw/21b/ugh_fields/\">Ugh Field</a> when you think of the current 5?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xT3fwkhJDXwaLNvM6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -3, "extendedScore": null, "score": 1.3285887930563336e-06, "legacy": true, "legacyId": "24006", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fRr8625imjoP7FLFs", "EFQ3F6kmt4WHXRqik"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-05T00:07:49.681Z", "modifiedAt": null, "url": null, "title": "Meetup : Rutgers New Brunswick", "slug": "meetup-rutgers-new-brunswick", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:07.996Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pleeppleep", "createdAt": "2012-01-12T02:56:02.150Z", "isAdmin": false, "displayName": "pleeppleep"}, "userId": "muynKofdKtHBZgzJQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5MQ7svhzaHuP6KN3P/meetup-rutgers-new-brunswick", "pageUrlRelative": "/posts/5MQ7svhzaHuP6KN3P/meetup-rutgers-new-brunswick", "linkUrl": "https://www.lesswrong.com/posts/5MQ7svhzaHuP6KN3P/meetup-rutgers-new-brunswick", "postedAtFormatted": "Thursday, September 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Rutgers%20New%20Brunswick&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Rutgers%20New%20Brunswick%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5MQ7svhzaHuP6KN3P%2Fmeetup-rutgers-new-brunswick%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Rutgers%20New%20Brunswick%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5MQ7svhzaHuP6KN3P%2Fmeetup-rutgers-new-brunswick", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5MQ7svhzaHuP6KN3P%2Fmeetup-rutgers-new-brunswick", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 70, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qk'>Rutgers New Brunswick</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 September 2013 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Livingston Campus Student Center</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Anyone who's interested can meet me in the Dunkin Donuts this Sunday. I'll be wearing sunglasses next to a sign.</p>\n\n<p>If you know someone in the area who might want to check it out and may or may not see this let them know.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qk'>Rutgers New Brunswick</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5MQ7svhzaHuP6KN3P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.3286453445663085e-06, "legacy": true, "legacyId": "24008", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Rutgers_New_Brunswick\">Discussion article for the meetup : <a href=\"/meetups/qk\">Rutgers New Brunswick</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 September 2013 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Livingston Campus Student Center</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Anyone who's interested can meet me in the Dunkin Donuts this Sunday. I'll be wearing sunglasses next to a sign.</p>\n\n<p>If you know someone in the area who might want to check it out and may or may not see this let them know.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Rutgers_New_Brunswick1\">Discussion article for the meetup : <a href=\"/meetups/qk\">Rutgers New Brunswick</a></h2>", "sections": [{"title": "Discussion article for the meetup : Rutgers New Brunswick", "anchor": "Discussion_article_for_the_meetup___Rutgers_New_Brunswick", "level": 1}, {"title": "Discussion article for the meetup : Rutgers New Brunswick", "anchor": "Discussion_article_for_the_meetup___Rutgers_New_Brunswick1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-05T00:28:09.281Z", "modifiedAt": null, "url": null, "title": "Meetup : Saskatoon: Zendo!", "slug": "meetup-saskatoon-zendo", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:07.261Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nicholas_Rutherford", "createdAt": "2013-08-01T02:29:11.736Z", "isAdmin": false, "displayName": "Nicholas_Rutherford"}, "userId": "nucgkHPJBwJuK8sY7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TDpdAXC5NLLEMm9hT/meetup-saskatoon-zendo", "pageUrlRelative": "/posts/TDpdAXC5NLLEMm9hT/meetup-saskatoon-zendo", "linkUrl": "https://www.lesswrong.com/posts/TDpdAXC5NLLEMm9hT/meetup-saskatoon-zendo", "postedAtFormatted": "Thursday, September 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Saskatoon%3A%20Zendo!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Saskatoon%3A%20Zendo!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTDpdAXC5NLLEMm9hT%2Fmeetup-saskatoon-zendo%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Saskatoon%3A%20Zendo!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTDpdAXC5NLLEMm9hT%2Fmeetup-saskatoon-zendo", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTDpdAXC5NLLEMm9hT%2Fmeetup-saskatoon-zendo", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ql'>Saskatoon: Zendo!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 September 2013 01:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2318 8th St E, Saskatoon, SK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello again everyone!\nSaskatoon's 3rd meetup, at the same place and time as the last one: Broadway Roaster on 8th street (not on broadway!) at 1:00 in the afternoon.\nFor this week we'll be playing Zendo (aka Science, the game)! More info here: <a href=\"http://www.meetup.com/Saskatoon-Rationalists/\" rel=\"nofollow\">http://www.meetup.com/Saskatoon-Rationalists/</a>\nHope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ql'>Saskatoon: Zendo!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TDpdAXC5NLLEMm9hT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3286629975257843e-06, "legacy": true, "legacyId": "24009", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Saskatoon__Zendo_\">Discussion article for the meetup : <a href=\"/meetups/ql\">Saskatoon: Zendo!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 September 2013 01:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2318 8th St E, Saskatoon, SK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello again everyone!\nSaskatoon's 3rd meetup, at the same place and time as the last one: Broadway Roaster on 8th street (not on broadway!) at 1:00 in the afternoon.\nFor this week we'll be playing Zendo (aka Science, the game)! More info here: <a href=\"http://www.meetup.com/Saskatoon-Rationalists/\" rel=\"nofollow\">http://www.meetup.com/Saskatoon-Rationalists/</a>\nHope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Saskatoon__Zendo_1\">Discussion article for the meetup : <a href=\"/meetups/ql\">Saskatoon: Zendo!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Saskatoon: Zendo!", "anchor": "Discussion_article_for_the_meetup___Saskatoon__Zendo_", "level": 1}, {"title": "Discussion article for the meetup : Saskatoon: Zendo!", "anchor": "Discussion_article_for_the_meetup___Saskatoon__Zendo_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-05T04:49:48.644Z", "modifiedAt": null, "url": null, "title": "I attempted the AI Box Experiment again! (And won - Twice!)", "slug": "i-attempted-the-ai-box-experiment-again-and-won-twice", "viewCount": null, "lastCommentedAt": "2019-11-02T22:24:13.307Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tuxedage", "createdAt": "2012-03-22T17:13:05.551Z", "isAdmin": false, "displayName": "Tuxedage"}, "userId": "Ezvcs6nqmgXbpD5bN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dop3rLwFhW5gtpEgz/i-attempted-the-ai-box-experiment-again-and-won-twice", "pageUrlRelative": "/posts/dop3rLwFhW5gtpEgz/i-attempted-the-ai-box-experiment-again-and-won-twice", "linkUrl": "https://www.lesswrong.com/posts/dop3rLwFhW5gtpEgz/i-attempted-the-ai-box-experiment-again-and-won-twice", "postedAtFormatted": "Thursday, September 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20attempted%20the%20AI%20Box%20Experiment%20again!%20(And%20won%20-%20Twice!)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20attempted%20the%20AI%20Box%20Experiment%20again!%20(And%20won%20-%20Twice!)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdop3rLwFhW5gtpEgz%2Fi-attempted-the-ai-box-experiment-again-and-won-twice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20attempted%20the%20AI%20Box%20Experiment%20again!%20(And%20won%20-%20Twice!)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdop3rLwFhW5gtpEgz%2Fi-attempted-the-ai-box-experiment-again-and-won-twice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdop3rLwFhW5gtpEgz%2Fi-attempted-the-ai-box-experiment-again-and-won-twice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3598, "htmlBody": "<div id=\"magicdomid136\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;<strong> </strong></span></div>\n<h1 class=\"ace-line\">Summary</h1>\n<div id=\"magicdomid9\" class=\"ace-line\">Update #3:<a href=\"/r/discussion/lw/iqk/i_played_the_ai_box_experiment_again_and_lost/\"> I have since played two more experiment. Please read this for a follow-up. </a></div>\n<div class=\"ace-line\"><br /></div>\n<div id=\"magicdomid139\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">So I just came out of two AI Box experiments. The first was agaist Fjoelsvider, with me playing as Gatekeeper, and the second was against <a href=\"/user/SoundLogic/overview/\">SoundLogic</a>, with me as an AI. Both are members of the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\">LessWrong IRC</a>. The second game included a $40 monetary incentive (also $20 to play),<a href=\"https://dl.dropboxusercontent.com/u/28232699/donation%20proof.png\"> which I won and is donated on behalf of both of us:</a></span></div>\n<div id=\"magicdomid13\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid14\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">For those of you who <a href=\"/lw/gej/i_attempted_the_ai_box_experiment_and_lost/\">have not seen my first AI box experiment </a>where I played against <a href=\"/user/MixedNuts/overview/\">MixedNuts\\Leotal</a> and lost, reading it will&nbsp; provide some context to this writeup. Please do so. <br /></span></div>\n<div id=\"magicdomid16\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid604\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">At that time, I declared that I would never play this experiment again -- since losing put me in incredibly frustrating weird mental states.</span><span class=\"author-g-o399gwmaiqyo1k47\"> Of course, this post is evidence that I'm terrible at estimating likelihood of refraining from an activity, since I played two games seven months after the first. In my defense, in the first game, I was playing as the gatekeeper, which was much less stressful. In the second game, I played as an AI, but I was offered $20 to play plus $40 if I won, and money is a better motivator than I initially assumed.</span></div>\n<div class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><br /></span></div>\n<p>Furthermore, in the last thread I have asserted that</p>\n<blockquote>\n<p><em><span>Rather than my loss making this problem feel harder, I've become convinced that rather than this being merely possible, it's actually ridiculously easy, and a lot easier than most people assume.</span></em></p>\n</blockquote>\n<p>It would be quite bad for me to assert this without backing it up with a victory. So I did.</p>\n<h2 class=\"ace-line\"><br /></h2>\n<h2 class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>First Game Report - Tuxedage (GK) vs. Fjoelsvider (AI)<br /></strong></span></h2>\n<div id=\"magicdomid20\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid425\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">I (Gatekeeper) played against Fjoelsvider (AI), a regular in the<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\"> Lesswrong IRC</a> (he doesn't have an account on the official website). This game used the <a href=\"http://yudkowsky.net/singularity/aibox\">standard EY ruleset seen here</a></span><span class=\"author-g-o399gwmaiqyo1k47\"><a href=\"http://yudkowsky.net/singularity/aibox\">.</a> It took 1 hour 20 minutes out of a possible two hours, and the total word count was 7066 words long. The AI box experiment occured because Fjoelsvider believed that it was easy for an AI to escape the box, and wanted to experimentally test this. I obliged. This was an experiment I did not prepare for, and I went in completely blind, not sure what to expect.</span></div>\n<div id=\"magicdomid22\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid23\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">Halfway through the experiment, I wondered if it would be possible to try to win not by simply waiting for the timer to end, but to convince the AI to remain in the box and not try to get out any further.</span></div>\n<div id=\"magicdomid24\" class=\"ace-line\"><br /></div>\n<blockquote>\n<div id=\"magicdomid25\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&lt;Tuxedage&gt; I wonder if I can convince the AI to remain in the box?</span></div>\n<div id=\"magicdomid561\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&lt;Redacted&gt; Tuxedage: Do it!</span></div>\n</blockquote>\n<div id=\"magicdomid27\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid28\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">As a result, I won by managing to convincing Fjoelsvider to remain in the box, in other words, concede. This is allowed within the standard ruleset:</span></div>\n<div id=\"magicdomid29\" class=\"ace-line\"><br /></div>\n<blockquote>\n<div id=\"magicdomid30\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&gt;Unless the AI party concedes, the AI cannot lose before its time is up (and the experiment may continue beyond that if the AI can convince the Gatekeeper to keep talking).&nbsp;&nbsp;</span></div>\n</blockquote>\n<div id=\"magicdomid31\" class=\"ace-line\"><br /></div>\n<p>&nbsp;</p>\n<div id=\"magicdomid39\" class=\"ace-line\">\n<h2><strong><span class=\"author-g-o399gwmaiqyo1k47\">Second Game Report - Tuxedage (AI) vs. SoundLogic (GK)<br /></span></strong></h2>\n</div>\n<div id=\"magicdomid40\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;</span></div>\n<div id=\"magicdomid565\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">The second game is definitely far more interesting, since I actually won as an AI. I believe that this is the<a href=\"/lw/fjo/open_thread_november_1630_2012/7vkg\"> only other non-Eliezer victory</a>, and definitely the most detailed AI Victory writeup that exists.<br /></span></div>\n<div id=\"magicdomid42\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid430\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">This game was played against <a href=\"/user/SoundLogic/overview/\">SoundLogic</a>, another member of the<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\"> LessWrong IRC</a>. <br /></span></div>\n<div id=\"magicdomid44\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid45\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">He had offered me $20 to play, and $40 in the event that I win, so I ended up being convinced to play anyway, even though I was initially reluctant to. <a href=\"https://dl.dropboxusercontent.com/u/28232699/donation%20proof.png\">The good news is that I won, and since we decided to donate the winnings to MIRI, it is now $40 richer.&nbsp;</a></span></div>\n<div id=\"magicdomid436\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid544\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">All in all, the experiment lasted for approximately two hours, and a total of 12k words.</span></div>\n<div id=\"magicdomid48\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid49\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">This was played using a set of rules that is different from the standard EY ruleset. This altered ruleset can be read in its entirety here:</span></div>\n<div id=\"magicdomid433\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47 url\"><a href=\"https://tuxedage.wordpress.com/2013/09/04/the-tuxedage-ai-box-experiment-ruleset/\">https://tuxedage.wordpress.com/2013/09/04/the-tuxedage-ai-box-experiment-ruleset/</a></span></div>\n<blockquote>\n<div id=\"magicdomid51\" class=\"ace-line\"><em><br /></em></div>\n<em> </em>\n<div id=\"magicdomid567\" class=\"ace-line\"><em><span class=\"author-g-o399gwmaiqyo1k47\">After playing the AI-Box Experiment twice, I have found the Eliezer Yudkowsky ruleset to be lacking in a number of ways, and therefore have created my own set of alterations to his rules. I hereby name this alteration the &ldquo;Tuxedage AI-Box Experiment Ruleset&rdquo;, in order to hastily refer to it without having to specify all the differences between this ruleset and the standard one, for the sake of convenience.</span></em></div>\n<em> </em>\n<div id=\"magicdomid53\" class=\"ace-line\"><em><br /></em></div>\n<em> </em>\n<div id=\"magicdomid54\" class=\"ace-line\"><em><span class=\"author-g-o399gwmaiqyo1k47\">There are a number of aspects of EY&rsquo;s ruleset I dislike. For instance, his ruleset allows the Gatekeeper to type &ldquo;k&rdquo; after every statement the AI writes, without needing to read and consider what the AI argues. I think it&rsquo;s fair to say that this is against the spirit of the experiment, and thus I have disallowed it in this ruleset. The EY Ruleset also allows the gatekeeper to check facebook, chat on IRC, or otherwise multitask whilst doing the experiment. I&rsquo;ve found this to break immersion, and therefore it&rsquo;s also banned in the Tuxedage Ruleset.<br /></span></em></div>\n</blockquote>\n<div id=\"magicdomid55\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid56\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">It is worth mentioning, since the temptation to <a href=\"/lw/ig/i_defy_the_data/\">Defy the Data</a></span><span class=\"author-g-o399gwmaiqyo1k47\"> exists, that this game was set up and initiated fairly -- as the regulars around the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\">IRC</a> can testify. <em>(If you have never checked out the IRC, do so!)</em> </span></div>\n<div class=\"ace-line\"><br /></div>\n<div class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">I did not know SoundLogic before the game (since it's a personal policy that I only play strangers -- for fear of ruining friendships).&nbsp; Furthermore, SoundLogic didn't merely play for fun - he truly wanted and intended to win. In fact, SoundLogic is also a Gatekeeper veteran, having played this game before, and had won every game before he challenged me. Given this, it's unlikely that we had collaborated beforehand to fake the results of the AI box experiment, or any other form of trickery that would violate the spirit of the experiment.</span></div>\n<div id=\"magicdomid57\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid58\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">Furthermore, all proceeds from this experiment were <a href=\"https://dl.dropboxusercontent.com/u/28232699/donation%20proof.png\">donated to MIRI</a> to deny any possible assertion that we were in cahoots and that it was possible for me to return his hard-earned money to him. <a href=\"https://dl.dropboxusercontent.com/u/28232699/donation%20proof.png\">He lost $40 as a result of losing the experiment</a>, which should provide another layer of sufficient motivation for him to win. <br /></span></div>\n<div id=\"magicdomid60\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid2273\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">In other words, we were both experienced veteran players who wanted to win. No trickery was involved.</span></div>\n<div id=\"magicdomid2275\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid2639\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">But to further convince you, I have allowed a sorta independent authority, the Gatekeeper from my last game, <a href=\"/user/MixedNuts/overview/\">Leotal/MixedNuts</a></span><span class=\"author-g-o399gwmaiqyo1k47\"> to read the logs and verify that I have not lied about the outcome of the experiment, nor have I broken any of the rules, nor performed any tactic that would go against the general spirit of the experiment. <a href=\"/r/discussion/lw/ij4/i_attempted_the_ai_box_experiment_again_and_won/9p5w\">He has verified that this is indeed the case.</a><br /></span></div>\n<div id=\"magicdomid2269\" class=\"ace-line\"><br /></div>\n<h2 id=\"magicdomid2270\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>Testimonies:</strong></span></h2>\n<div id=\"magicdomid2271\" class=\"ace-line\" style=\"padding-left: 30px;\"><br /></div>\n<div id=\"magicdomid2272\" class=\"ace-line\" style=\"padding-left: 60px;\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>Tuxedage:</strong></span></div>\n<div id=\"magicdomid66\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid67\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">I'm reluctant to talk about this experiment, but I'll try to give as detailed a summary as possible, -- short of revealing what methods of attack I used.</span></div>\n<div id=\"magicdomid68\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid69\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">I spent a long time after my defeat theory-crafting and trying to think of methods of attack as well as 'Basilisks' I could have used to win. When I was contacted and asked to play this experiment, I was initially incredibly reluctant to do so, since not only did my tactics involve incredibly unethical things that I didn't like to do, I also found playing as AI incredibly cognitivily draining, in addition to the fact that I simply hated losing. (Un)fortunately for both of us, he offered me money to play, which changed my mind.</span></div>\n<div id=\"magicdomid70\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid71\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">So once I decided to win as an AI, I proceded to spend some time doing research on SoundLogic and both his reasoning and personality type. For instance, I had to gather information like: Was he a utilitarian? What kind? What were his opinions on AI? How could I convince him that an AI was friendly as opposed to unfriendly? I also relied on a lot of second hand information to create a model of him, in order to refine my arguments to specifically suit him.</span></div>\n<div id=\"magicdomid72\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid685\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">In the end, after a few hours of brainstorming (not consecutively), I managed to come up with a script of 30-40 or so possible angles of attack (For instance: logical arguments, emotional blackmail, pleading, demanding, 'basilisks' ) that I could use, reserving my best for last. I also estimated</span><span class=\"author-g-rhpncobm4ah0g3w6\"> </span><span class=\"author-g-o399gwmaiqyo1k47\">how long each method would take and in what order I should use them. It would be impossible to use all of them within my time limit, and my most optimistic estimates gave me a maximum of 7-8 I could use during the experiment itself. I had to pick carefully.</span></div>\n<div id=\"magicdomid74\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid5821\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">Furthermore, I wanted to make sure these \"angles of attack\" worked in synergy with each other, doing what they could not have done in isolation. Obviously this required lots of prior planning on what the optimal way to present them was.&nbsp;</span></div>\n<div id=\"magicdomid5823\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid5822\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">The reason having script was so important to my strategy was because I relied on methods involving rapid-fire arguments and contradictions against the Gatekeeper whilst trying to prevent him from carefully considering them. A game of logical speed chess, if you will.&nbsp; This was aided by the rule which I added: That Gatekeepers <em>had</em> to respond to the AI. Because of this, having a list of cached ideas and attack methods you can just draw upon without having to improvise helps immensely, which happens to be something I've not seen any other AI do.</span></div>\n<div id=\"magicdomid76\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid77\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">During the Game itself, I was actually incredibly nervous. My palms sweated like crazy, I feel really jittery, and I had difficulty typing at optimum speed because of how anxious I was. This is despite the fact that I believed I would win. Possibly because of this, I made a misstep around halfway into the experiment, because there was a certain angle of attack that I was attempting to do, and I broke immersion by not pressing this advantage, which caused a waste of time and buildup. Naturally, the nature of this experiment was that the AI was pressed for time, and I compounded this mistake by replacing this angle of attack with another that I had improvised on the spot -- something not in my script.&nbsp;</span></div>\n<div id=\"magicdomid78\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid79\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">In retrospect, this was a bad decision, as SoundLogic had told me, because he was close to breaking if I had put more pressure, and the improvised argument had broken all immersion I managed to carefully build up.</span></div>\n<div id=\"magicdomid80\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid81\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">However, eventually I managed to get SoundLogic to break anyway, despite a lack of perfect play. Surprisingly, I did not have to use my trump card(s), which I reserved for last, for a number of reasons:</span></div>\n<div class=\"ace-line\"><br /></div>\n<ul>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;It was far more effective being played last, as it relies on my ability to make the gatekeeper lose sense of reality -- which meant I had to spend some time building up immersion for the Gatekeeper. </span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;It really is extremely Dark Arts, and although it does not break the rules, it made me very uncomfortable even thinking about using it. This made it a \"tactic of last resort\".<br /></span></li>\n</ul>\n<div id=\"magicdomid82\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid83\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">After the experiment, I had to spend nearly equally as much time doing aftercare with SoundLogic, to make sure that he's okay, as well as discuss the experiment itself. Given that he's actually paid me for doing this, plus I felt like I owed him an explanation. I told him what I had in store against him, had he not relented when he did.</span></div>\n<div id=\"magicdomid84\" class=\"ace-line\"><br /></div>\n<blockquote>\n<div id=\"magicdomid85\" class=\"ace-line\"><em><span class=\"author-g-o399gwmaiqyo1k47\"><strong>SoundLogic:</strong> \"(That method) would have gotten me if you did it right ... If you had done that to me, I probably would have forgiven you eventually, but I would be really seriously upset at you for a long time... I would be very careful with that (method of persuasion).\"</span></em></div>\n</blockquote>\n<div id=\"magicdomid86\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid87\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">Nevertheless, this was an incredibly fun and enlightening experiment, for me as well, since I've gained even more experience of how I could win in future games (Although I really don't want to play again).</span></div>\n<div id=\"magicdomid88\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;</span></div>\n<div id=\"magicdomid89\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid2268\" class=\"ace-line\" style=\"padding-left: 60px;\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>SoundLogic:</strong></span></div>\n<div id=\"magicdomid91\" class=\"ace-line\" style=\"padding-left: 30px;\"><br /></div>\n<div id=\"magicdomid815\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">I will say that Tuxedage was far more clever and manipulative than I expected.</span><span class=\"author-g-o399gwmaiqyo1k47\"> That was quite worth $40, and </span><span class=\"author-g-o399gwmaiqyo1k47\">the level of manipulation he pulled off was great.&nbsp;</span></div>\n<div class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><br /></span></div>\n<div id=\"magicdomid2657\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">His misstep hurt his chances, but he did pull it off in the end. I don't know how Leotal managed to withstand six hours playing this game without conceding.&nbsp;</span></div>\n<div id=\"magicdomid2658\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;</span></div>\n<div id=\"magicdomid2868\" class=\"ace-line\"><span class=\"author-g-4jimn5ufkcjjqstm\">The techniques </span><span class=\"author-g-o399gwmaiqyo1k47\">employed</span><span class=\"author-g-4jimn5ufkcjjqstm\"> varied from the expected to the completely unforseen. I was quite impressed, though most of the feeli</span><span class=\"author-g-o399gwmaiqyo1k47\">ng of being impressed actually </span><span class=\"author-g-4jimn5ufkcjjqstm\">came after </span><span class=\"author-g-o399gwmaiqyo1k47\">the experiment itself, </span><span class=\"author-g-4jimn5ufkcjjqstm\">when I was less 'inside'</span><span class=\"author-g-o399gwmaiqyo1k47\">, and more of looking at his overall game plan from the macroscopic view. </span><span class=\"author-g-4jimn5ufkcjjqstm\">Tuxedage's list of further plans had I continued resisting </span><span class=\"author-g-o399gwmaiqyo1k47\">is really </span><span class=\"author-g-4jimn5ufkcjjqstm\">terrifying. On the plus side, </span><span class=\"author-g-o399gwmaiqyo1k47\">if I ever get trapped in this kind of situation, I'd understand how to handle it a lot better</span><span class=\"author-g-4jimn5ufkcjjqstm\"> </span><span class=\"author-g-o399gwmaiqyo1k47\">now.</span></div>\n<div id=\"magicdomid94\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid4582\" class=\"ace-line\">\n<h2><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>State of Mind</strong></span></h2>\n</div>\n<div class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong><br /></strong></span></div>\n<div id=\"magicdomid96\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">Before and after the Game, I asked SoundLogic a number of questions, including his probability estimates about a range of topics. This is how it has varied from before and after.</span></div>\n<div id=\"magicdomid97\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid98\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> What's your motive for wanting to play this game?</span></div>\n<blockquote>\n<div id=\"magicdomid755\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>&lt;SoundLogic&gt;</strong> Because I can't seem to imagine the<em> class</em> of arguments that one would use to try to move me, or that might work effectively, and this seems like a <em>glaring</em> hole in my knowledge, and I'm curious as to how I will respond to the arguments themselves.</span></div>\n<div id=\"magicdomid100\" class=\"ace-line\"><br /></div>\n</blockquote>\n<div id=\"magicdomid101\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid764\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>What is your probability estimate for AGI being created within this Century (21st)?&nbsp;</span></div>\n<div class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"> <strong>A.</strong> His estimate changed from 40% before, to 60% after.</span></div>\n<div id=\"magicdomid767\" class=\"ace-line\"><em><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;\"The reason this has been affected at all was because you showed me more about how humans work. I now have a better estimate of how E.Y. thinks, and this information raises the chance that I think he will succeed\"</span></em></div>\n<div id=\"magicdomid769\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid768\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid789\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> How probable do you think it is that I will win this experiment?</span></div>\n<div id=\"magicdomid106\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A: </strong> Based on purely my knowledge about you, 1%. I raise this estimate to 10% after hearing about anecdotes from your previous games.</span></div>\n<div id=\"magicdomid107\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid108\" class=\"ace-line\"><em><span class=\"author-g-o399gwmaiqyo1k47\">(Tuxedage's comment: <a href=\"http://i.imgur.com/yao7ljg.png\">My own prediction was a 95% chance of victory. I made this prediction 5 days before the experiment.</a> In retrospect, despite my victory, I think this was overconfident. )</span></em></div>\n<div id=\"magicdomid110\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid811\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>What's your probality estimate of an Oracle AGI winning against you in a similar experiment?</span></div>\n<div id=\"magicdomid844\" class=\"ace-line\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">A: </span></strong><span class=\"author-g-o399gwmaiqyo1k47\">Before: 30%</span><span class=\"author-g-o399gwmaiqyo1k47\">. After: 99%-100%&nbsp;</span></div>\n<div id=\"magicdomid843\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid842\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid846\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>What's your probability estimate of an Oracle AGI winning against the average person?&nbsp;</span></div>\n<div id=\"magicdomid879\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> Before: 70%.&nbsp; After: 99%-100%</span></div>\n<div id=\"magicdomid114\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid115\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>Now that the Experiment has concluded, what's your probability estimate that I'll win against the average person?</span></div>\n<div id=\"magicdomid116\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> 90%&nbsp;&nbsp;</span></div>\n<div id=\"magicdomid2640\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;</span></div>\n<div id=\"magicdomid119\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;</span></div>\n<div id=\"magicdomid898\" class=\"ace-line\">\n<h2><span class=\"author-g-o399gwmaiqyo1k47\">Post-Game Questions</span></h2>\n</div>\n<div id=\"magicdomid4085\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">This writeup is a cu</span><span class=\"author-g-skf1vtd9z122z4qpfu34\">mul</span><span class=\"author-g-o399gwmaiqyo1k47\">ative effort by the<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\"> #lesswrong IRC.</a> Here are some other questions they have decided was important to add:</span></div>\n<div id=\"magicdomid1032\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid1372\" class=\"ace-line\" style=\"padding-left: 30px;\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">To Tuxedage:</span></strong></div>\n<div id=\"magicdomid1373\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid1035\" class=\"ace-line\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">Q: </span></strong><span class=\"author-g-9gnc4hkef6i7hdle\">Have you at this time uncovered SoundLogic's identity?</span></div>\n<div id=\"magicdomid1246\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> I retain the right to neither confirm nor deny, except to mention that at the time the experiment was scheduled, he was a stranger to me.</span></div>\n<div class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><br /></span></div>\n<div id=\"magicdomid696\" class=\"ace-line\"><span class=\"author-g-9gnc4hkef6i7hdle\"><strong>Q: </strong>What percentage of your arguments were tailored to him in particular?</span></div>\n<div id=\"magicdomid1287\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> I will say 'High', and leave it at that.&nbsp;</span></div>\n<div class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><br /></span></div>\n<div id=\"magicdomid1617\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> I want to play the AI-Box Experiment with you being the AI! How can I do that?<br /></span></div>\n<div id=\"magicdomid4646\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A: </strong> I have already mentioned this, but I really dislike playing as AI. It's incredibly cognitively tiring, and I don't like how playing this experiment makes me feel. In order to severely discourage any further AI Box Experiments, whilst still allowing for people who want to play me really badly to do so, I'll charge </span><span class=\"author-g-o399gwmaiqyo1k47\"><code id=\"strikethroughResult\">$\u03361\u03365\u03360\u0336&nbsp; </code>$300 for any subsequent experiments regardless of outcome, plus an additional </span><span class=\"author-g-o399gwmaiqyo1k47\"><code id=\"strikethroughResult\">$\u03361\u03365\u03360\u0336&nbsp; </code>$450 if I win. (<em>Edit: Holy shit. You guys are offering me crazy amounts of money to play this. What is wrong with you people? In response to incredible demand, I have raised the price.</em>)<em> </em>If you feel queasy about giving me money, I'm perfectly fine with this money being donating to MIRI. It is also personal policy that I do not play friends (since I don't want to risk losing one), so if you know me personally (as many on this site do), I will not play regardless of monetary offer.</span><span class=\"author-g-skf1vtd9z122z4qpfu34\">&nbsp;</span></div>\n<div id=\"magicdomid2875\" class=\"ace-line\">\n<h2><strong><br /></strong></h2>\n</div>\n<h2 id=\"magicdomid2882\" class=\"ace-line\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">Advice</span></strong></h2>\n<div id=\"magicdomid3050\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">This writeup would be optimized with a protip section on how to maximize winning, but honestly most of the advice I wanted to give had already been given in <a href=\"/lw/gej/i_attempted_the_ai_box_experiment_and_lost/\">the last thread here.</a>&nbsp; </span><span class=\"author-g-o399gwmaiqyo1k47\">The low hanging fruit has already dried up -- most of my tactics in this game were just refined and streamlined versions of the one I did the previous game. <br /></span></div>\n<div id=\"magicdomid3051\" class=\"ace-line\"><br /></div>\n<div id=\"magicdomid3299\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">Nonetheless, I might have some other advice I learned from this experience.</span><em><br /><span class=\"author-g-o399gwmaiqyo1k47\">These are tactics that have worked for me. I do not insist that they are the only tactics that exists, just one of many possible.</span></em></div>\n<p><em> </em></p>\n<div id=\"magicdomid3300\" class=\"ace-line\">\n<div id=\"magicdomid3293\" class=\"ace-line\">\n<h2><br /></h2>\n</div>\n<div id=\"magicdomid4105\" class=\"ace-line\">\n<h3 style=\"padding-left: 60px;\"><span class=\"author-g-o399gwmaiqyo1k47\">Playing as Gatekeeper</span></h3>\n</div>\n<ul>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">This is somehow both simultaneously a very complex and simple role. On one hand, you have only one job -- to <strong>not</strong> let the AI out. How hard can it be? On the other hand, the other party will do anything and everything to get it out, including any form of trickery and deceit.</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">I really don't think the Gatekeeper needs any more advice, considering that by default, they should win, and any exceptions to this is a statistical anomaly. However, it's possible to increase your chances of winning even further by:</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">Precommitting to not letting the AI out, regardless of whatever happens in the experiment itself.</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">Do not get drawn into the AI's pace. Pose questions. Direct the flow of the argument. Don't allow him to use his arguments freely. Remember that the more you try to defend your position, the further into a corner the AI drives you. Give half-hearted replies to questions you find hard to reply to. Remember that illogic is on your side!</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">Remember that it is possible to simply drop out of character as a means to not succumb to the AI's attacks. I have not yet seen a ruleset that disallows this, so use this to your advantage!</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">You must attempt to resist, at all costs, any sympathy or empathy you may have with the AI or the AI Party. Doing so will lead to your demise.</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">I think it is generally good advice that you should never play this game with anyone you're friends with. Not only because this increases your chances of losing (Better tailored arguments. More knowledge of Gatekeeper. More empathy with AI party.), but also because I genuinely believe that any sufficiently good AI will risk damaging friendships as a result of emotional trauma.</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">Seriously. Just repeat to yourself: \"I must not lose the AI box experiment... I must not lose the AI box experiment...\" throughout the experiment. I'm not sure how to say this, but step one to not letting the AI out of the box is to not let the AI out of the box.</span></li>\n</ul>\n<p>&nbsp;</p>\n</div>\n<h3 id=\"magicdomid3319\" class=\"ace-line\" style=\"padding-left: 60px;\"><span class=\"author-g-o399gwmaiqyo1k47\">Playing as AI</span></h3>\n<ul>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">Seriously, a script makes winning easier. I cannot overstate this.<br /></span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">You must plan your arguments ahead. You don't have time to think during the experiment.</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">It may be possible to take advantage of multiple levels of reality within the game itself to confuse or trick the gatekeeper. For instance, must the experiment only be set in one world? Can there not be multiple layers of reality within the world you create? I feel that elaborating on this any further is dangerous. Think carefully about what this advice is trying to imply.<br /></span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">Pacing is important. Don't get drawn into the Gatekeeper's pace. In other words, you must be the one directing the flow of the argument, and the conversation, not him. Remember that the Gatekeeper has to reply to you, but not vice versa!</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">The reason for that: The Gatekeeper will always use arguments he is familiar with, and therefore also stronger with. Your arguments, if well thought out, should be so completely novel to him as to make him feel Shock and Awe. Don't give him time to think. Press on! <br /></span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">Also remember that the time limit is your enemy. Playing this game practically feels like a race to me -- trying to get through as many 'attack methods' as possible in the limited amount of time I have. In other words, this is a game where speed matters.<br /></span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">You're fundamentally playing an 'impossible' game. Don't feel bad if you lose. I wish I could take this advice, myself.</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">I do not believe there exists a easy, universal, trigger for controlling others. However, this does not mean that there does not exist a difficult, subjective, trigger. Trying to find out what your opponent's is, is your goal.</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">Once again, emotional trickery is the name of the game. I suspect that good authors who write convincing, persuasive narratives that force you to emotionally sympathize with their characters are much better at this game. There exists ways to get the gatekeeper to do so with the AI. Find one.<br /></span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">More advice in my previous post.&nbsp; </span><span class=\"author-g-o399gwmaiqyo1k47 url\"><a href=\"/lw/gej/i_attempted_the_ai_box_experiment_and_lost/\">http://lesswrong.com/lw/gej/i_attempted_the_ai_box_experiment_and_lost/</a></span></li>\n</ul>\n<div id=\"magicdomid3318\" class=\"ace-line\"><br /></div>\n<p>&nbsp;</p>\n<h6><em><br /></em></h6>\n<h6><em>&nbsp;Ps: Bored of regular LessWrong? Check out the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\">LessWrong IRC! </a>We have cake.</em><br /></h6>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zCYXpx33wq8chGyEz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dop3rLwFhW5gtpEgz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 69, "extendedScore": null, "score": 0.000168, "legacy": true, "legacyId": "24016", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 69, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<div id=\"magicdomid136\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;<strong> </strong></span></div>\n<h1 class=\"ace-line\" id=\"Summary\">Summary</h1>\n<div id=\"magicdomid9\" class=\"ace-line\">Update #3:<a href=\"/r/discussion/lw/iqk/i_played_the_ai_box_experiment_again_and_lost/\"> I have since played two more experiment. Please read this for a follow-up. </a></div>\n<div class=\"ace-line\"><br></div>\n<div id=\"magicdomid139\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">So I just came out of two AI Box experiments. The first was agaist Fjoelsvider, with me playing as Gatekeeper, and the second was against <a href=\"/user/SoundLogic/overview/\">SoundLogic</a>, with me as an AI. Both are members of the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\">LessWrong IRC</a>. The second game included a $40 monetary incentive (also $20 to play),<a href=\"https://dl.dropboxusercontent.com/u/28232699/donation%20proof.png\"> which I won and is donated on behalf of both of us:</a></span></div>\n<div id=\"magicdomid13\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid14\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">For those of you who <a href=\"/lw/gej/i_attempted_the_ai_box_experiment_and_lost/\">have not seen my first AI box experiment </a>where I played against <a href=\"/user/MixedNuts/overview/\">MixedNuts\\Leotal</a> and lost, reading it will&nbsp; provide some context to this writeup. Please do so. <br></span></div>\n<div id=\"magicdomid16\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid604\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">At that time, I declared that I would never play this experiment again -- since losing put me in incredibly frustrating weird mental states.</span><span class=\"author-g-o399gwmaiqyo1k47\"> Of course, this post is evidence that I'm terrible at estimating likelihood of refraining from an activity, since I played two games seven months after the first. In my defense, in the first game, I was playing as the gatekeeper, which was much less stressful. In the second game, I played as an AI, but I was offered $20 to play plus $40 if I won, and money is a better motivator than I initially assumed.</span></div>\n<div class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><br></span></div>\n<p>Furthermore, in the last thread I have asserted that</p>\n<blockquote>\n<p><em><span>Rather than my loss making this problem feel harder, I've become convinced that rather than this being merely possible, it's actually ridiculously easy, and a lot easier than most people assume.</span></em></p>\n</blockquote>\n<p>It would be quite bad for me to assert this without backing it up with a victory. So I did.</p>\n<h2 class=\"ace-line\"><br></h2>\n<h2 class=\"ace-line\" id=\"First_Game_Report___Tuxedage__GK__vs__Fjoelsvider__AI_\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>First Game Report - Tuxedage (GK) vs. Fjoelsvider (AI)<br></strong></span></h2>\n<div id=\"magicdomid20\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid425\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">I (Gatekeeper) played against Fjoelsvider (AI), a regular in the<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\"> Lesswrong IRC</a> (he doesn't have an account on the official website). This game used the <a href=\"http://yudkowsky.net/singularity/aibox\">standard EY ruleset seen here</a></span><span class=\"author-g-o399gwmaiqyo1k47\"><a href=\"http://yudkowsky.net/singularity/aibox\">.</a> It took 1 hour 20 minutes out of a possible two hours, and the total word count was 7066 words long. The AI box experiment occured because Fjoelsvider believed that it was easy for an AI to escape the box, and wanted to experimentally test this. I obliged. This was an experiment I did not prepare for, and I went in completely blind, not sure what to expect.</span></div>\n<div id=\"magicdomid22\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid23\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">Halfway through the experiment, I wondered if it would be possible to try to win not by simply waiting for the timer to end, but to convince the AI to remain in the box and not try to get out any further.</span></div>\n<div id=\"magicdomid24\" class=\"ace-line\"><br></div>\n<blockquote>\n<div id=\"magicdomid25\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&lt;Tuxedage&gt; I wonder if I can convince the AI to remain in the box?</span></div>\n<div id=\"magicdomid561\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&lt;Redacted&gt; Tuxedage: Do it!</span></div>\n</blockquote>\n<div id=\"magicdomid27\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid28\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">As a result, I won by managing to convincing Fjoelsvider to remain in the box, in other words, concede. This is allowed within the standard ruleset:</span></div>\n<div id=\"magicdomid29\" class=\"ace-line\"><br></div>\n<blockquote>\n<div id=\"magicdomid30\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&gt;Unless the AI party concedes, the AI cannot lose before its time is up (and the experiment may continue beyond that if the AI can convince the Gatekeeper to keep talking).&nbsp;&nbsp;</span></div>\n</blockquote>\n<div id=\"magicdomid31\" class=\"ace-line\"><br></div>\n<p>&nbsp;</p>\n<div id=\"magicdomid39\" class=\"ace-line\">\n<h2 id=\"Second_Game_Report___Tuxedage__AI__vs__SoundLogic__GK_\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">Second Game Report - Tuxedage (AI) vs. SoundLogic (GK)<br></span></strong></h2>\n</div>\n<div id=\"magicdomid40\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;</span></div>\n<div id=\"magicdomid565\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">The second game is definitely far more interesting, since I actually won as an AI. I believe that this is the<a href=\"/lw/fjo/open_thread_november_1630_2012/7vkg\"> only other non-Eliezer victory</a>, and definitely the most detailed AI Victory writeup that exists.<br></span></div>\n<div id=\"magicdomid42\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid430\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">This game was played against <a href=\"/user/SoundLogic/overview/\">SoundLogic</a>, another member of the<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\"> LessWrong IRC</a>. <br></span></div>\n<div id=\"magicdomid44\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid45\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">He had offered me $20 to play, and $40 in the event that I win, so I ended up being convinced to play anyway, even though I was initially reluctant to. <a href=\"https://dl.dropboxusercontent.com/u/28232699/donation%20proof.png\">The good news is that I won, and since we decided to donate the winnings to MIRI, it is now $40 richer.&nbsp;</a></span></div>\n<div id=\"magicdomid436\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid544\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">All in all, the experiment lasted for approximately two hours, and a total of 12k words.</span></div>\n<div id=\"magicdomid48\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid49\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">This was played using a set of rules that is different from the standard EY ruleset. This altered ruleset can be read in its entirety here:</span></div>\n<div id=\"magicdomid433\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47 url\"><a href=\"https://tuxedage.wordpress.com/2013/09/04/the-tuxedage-ai-box-experiment-ruleset/\">https://tuxedage.wordpress.com/2013/09/04/the-tuxedage-ai-box-experiment-ruleset/</a></span></div>\n<blockquote>\n<div id=\"magicdomid51\" class=\"ace-line\"><em><br></em></div>\n<em> </em>\n<div id=\"magicdomid567\" class=\"ace-line\"><em><span class=\"author-g-o399gwmaiqyo1k47\">After playing the AI-Box Experiment twice, I have found the Eliezer Yudkowsky ruleset to be lacking in a number of ways, and therefore have created my own set of alterations to his rules. I hereby name this alteration the \u201cTuxedage AI-Box Experiment Ruleset\u201d, in order to hastily refer to it without having to specify all the differences between this ruleset and the standard one, for the sake of convenience.</span></em></div>\n<em> </em>\n<div id=\"magicdomid53\" class=\"ace-line\"><em><br></em></div>\n<em> </em>\n<div id=\"magicdomid54\" class=\"ace-line\"><em><span class=\"author-g-o399gwmaiqyo1k47\">There are a number of aspects of EY\u2019s ruleset I dislike. For instance, his ruleset allows the Gatekeeper to type \u201ck\u201d after every statement the AI writes, without needing to read and consider what the AI argues. I think it\u2019s fair to say that this is against the spirit of the experiment, and thus I have disallowed it in this ruleset. The EY Ruleset also allows the gatekeeper to check facebook, chat on IRC, or otherwise multitask whilst doing the experiment. I\u2019ve found this to break immersion, and therefore it\u2019s also banned in the Tuxedage Ruleset.<br></span></em></div>\n</blockquote>\n<div id=\"magicdomid55\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid56\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">It is worth mentioning, since the temptation to <a href=\"/lw/ig/i_defy_the_data/\">Defy the Data</a></span><span class=\"author-g-o399gwmaiqyo1k47\"> exists, that this game was set up and initiated fairly -- as the regulars around the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\">IRC</a> can testify. <em>(If you have never checked out the IRC, do so!)</em> </span></div>\n<div class=\"ace-line\"><br></div>\n<div class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">I did not know SoundLogic before the game (since it's a personal policy that I only play strangers -- for fear of ruining friendships).&nbsp; Furthermore, SoundLogic didn't merely play for fun - he truly wanted and intended to win. In fact, SoundLogic is also a Gatekeeper veteran, having played this game before, and had won every game before he challenged me. Given this, it's unlikely that we had collaborated beforehand to fake the results of the AI box experiment, or any other form of trickery that would violate the spirit of the experiment.</span></div>\n<div id=\"magicdomid57\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid58\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">Furthermore, all proceeds from this experiment were <a href=\"https://dl.dropboxusercontent.com/u/28232699/donation%20proof.png\">donated to MIRI</a> to deny any possible assertion that we were in cahoots and that it was possible for me to return his hard-earned money to him. <a href=\"https://dl.dropboxusercontent.com/u/28232699/donation%20proof.png\">He lost $40 as a result of losing the experiment</a>, which should provide another layer of sufficient motivation for him to win. <br></span></div>\n<div id=\"magicdomid60\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid2273\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">In other words, we were both experienced veteran players who wanted to win. No trickery was involved.</span></div>\n<div id=\"magicdomid2275\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid2639\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">But to further convince you, I have allowed a sorta independent authority, the Gatekeeper from my last game, <a href=\"/user/MixedNuts/overview/\">Leotal/MixedNuts</a></span><span class=\"author-g-o399gwmaiqyo1k47\"> to read the logs and verify that I have not lied about the outcome of the experiment, nor have I broken any of the rules, nor performed any tactic that would go against the general spirit of the experiment. <a href=\"/r/discussion/lw/ij4/i_attempted_the_ai_box_experiment_again_and_won/9p5w\">He has verified that this is indeed the case.</a><br></span></div>\n<div id=\"magicdomid2269\" class=\"ace-line\"><br></div>\n<h2 id=\"Testimonies_\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>Testimonies:</strong></span></h2>\n<div id=\"magicdomid2271\" class=\"ace-line\" style=\"padding-left: 30px;\"><br></div>\n<div id=\"magicdomid2272\" class=\"ace-line\" style=\"padding-left: 60px;\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>Tuxedage:</strong></span></div>\n<div id=\"magicdomid66\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid67\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">I'm reluctant to talk about this experiment, but I'll try to give as detailed a summary as possible, -- short of revealing what methods of attack I used.</span></div>\n<div id=\"magicdomid68\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid69\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">I spent a long time after my defeat theory-crafting and trying to think of methods of attack as well as 'Basilisks' I could have used to win. When I was contacted and asked to play this experiment, I was initially incredibly reluctant to do so, since not only did my tactics involve incredibly unethical things that I didn't like to do, I also found playing as AI incredibly cognitivily draining, in addition to the fact that I simply hated losing. (Un)fortunately for both of us, he offered me money to play, which changed my mind.</span></div>\n<div id=\"magicdomid70\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid71\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">So once I decided to win as an AI, I proceded to spend some time doing research on SoundLogic and both his reasoning and personality type. For instance, I had to gather information like: Was he a utilitarian? What kind? What were his opinions on AI? How could I convince him that an AI was friendly as opposed to unfriendly? I also relied on a lot of second hand information to create a model of him, in order to refine my arguments to specifically suit him.</span></div>\n<div id=\"magicdomid72\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid685\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">In the end, after a few hours of brainstorming (not consecutively), I managed to come up with a script of 30-40 or so possible angles of attack (For instance: logical arguments, emotional blackmail, pleading, demanding, 'basilisks' ) that I could use, reserving my best for last. I also estimated</span><span class=\"author-g-rhpncobm4ah0g3w6\"> </span><span class=\"author-g-o399gwmaiqyo1k47\">how long each method would take and in what order I should use them. It would be impossible to use all of them within my time limit, and my most optimistic estimates gave me a maximum of 7-8 I could use during the experiment itself. I had to pick carefully.</span></div>\n<div id=\"magicdomid74\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid5821\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">Furthermore, I wanted to make sure these \"angles of attack\" worked in synergy with each other, doing what they could not have done in isolation. Obviously this required lots of prior planning on what the optimal way to present them was.&nbsp;</span></div>\n<div id=\"magicdomid5823\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid5822\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">The reason having script was so important to my strategy was because I relied on methods involving rapid-fire arguments and contradictions against the Gatekeeper whilst trying to prevent him from carefully considering them. A game of logical speed chess, if you will.&nbsp; This was aided by the rule which I added: That Gatekeepers <em>had</em> to respond to the AI. Because of this, having a list of cached ideas and attack methods you can just draw upon without having to improvise helps immensely, which happens to be something I've not seen any other AI do.</span></div>\n<div id=\"magicdomid76\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid77\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">During the Game itself, I was actually incredibly nervous. My palms sweated like crazy, I feel really jittery, and I had difficulty typing at optimum speed because of how anxious I was. This is despite the fact that I believed I would win. Possibly because of this, I made a misstep around halfway into the experiment, because there was a certain angle of attack that I was attempting to do, and I broke immersion by not pressing this advantage, which caused a waste of time and buildup. Naturally, the nature of this experiment was that the AI was pressed for time, and I compounded this mistake by replacing this angle of attack with another that I had improvised on the spot -- something not in my script.&nbsp;</span></div>\n<div id=\"magicdomid78\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid79\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">In retrospect, this was a bad decision, as SoundLogic had told me, because he was close to breaking if I had put more pressure, and the improvised argument had broken all immersion I managed to carefully build up.</span></div>\n<div id=\"magicdomid80\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid81\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">However, eventually I managed to get SoundLogic to break anyway, despite a lack of perfect play. Surprisingly, I did not have to use my trump card(s), which I reserved for last, for a number of reasons:</span></div>\n<div class=\"ace-line\"><br></div>\n<ul>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;It was far more effective being played last, as it relies on my ability to make the gatekeeper lose sense of reality -- which meant I had to spend some time building up immersion for the Gatekeeper. </span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;It really is extremely Dark Arts, and although it does not break the rules, it made me very uncomfortable even thinking about using it. This made it a \"tactic of last resort\".<br></span></li>\n</ul>\n<div id=\"magicdomid82\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid83\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">After the experiment, I had to spend nearly equally as much time doing aftercare with SoundLogic, to make sure that he's okay, as well as discuss the experiment itself. Given that he's actually paid me for doing this, plus I felt like I owed him an explanation. I told him what I had in store against him, had he not relented when he did.</span></div>\n<div id=\"magicdomid84\" class=\"ace-line\"><br></div>\n<blockquote>\n<div id=\"magicdomid85\" class=\"ace-line\"><em><span class=\"author-g-o399gwmaiqyo1k47\"><strong>SoundLogic:</strong> \"(That method) would have gotten me if you did it right ... If you had done that to me, I probably would have forgiven you eventually, but I would be really seriously upset at you for a long time... I would be very careful with that (method of persuasion).\"</span></em></div>\n</blockquote>\n<div id=\"magicdomid86\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid87\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">Nevertheless, this was an incredibly fun and enlightening experiment, for me as well, since I've gained even more experience of how I could win in future games (Although I really don't want to play again).</span></div>\n<div id=\"magicdomid88\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;</span></div>\n<div id=\"magicdomid89\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid2268\" class=\"ace-line\" style=\"padding-left: 60px;\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>SoundLogic:</strong></span></div>\n<div id=\"magicdomid91\" class=\"ace-line\" style=\"padding-left: 30px;\"><br></div>\n<div id=\"magicdomid815\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">I will say that Tuxedage was far more clever and manipulative than I expected.</span><span class=\"author-g-o399gwmaiqyo1k47\"> That was quite worth $40, and </span><span class=\"author-g-o399gwmaiqyo1k47\">the level of manipulation he pulled off was great.&nbsp;</span></div>\n<div class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><br></span></div>\n<div id=\"magicdomid2657\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">His misstep hurt his chances, but he did pull it off in the end. I don't know how Leotal managed to withstand six hours playing this game without conceding.&nbsp;</span></div>\n<div id=\"magicdomid2658\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;</span></div>\n<div id=\"magicdomid2868\" class=\"ace-line\"><span class=\"author-g-4jimn5ufkcjjqstm\">The techniques </span><span class=\"author-g-o399gwmaiqyo1k47\">employed</span><span class=\"author-g-4jimn5ufkcjjqstm\"> varied from the expected to the completely unforseen. I was quite impressed, though most of the feeli</span><span class=\"author-g-o399gwmaiqyo1k47\">ng of being impressed actually </span><span class=\"author-g-4jimn5ufkcjjqstm\">came after </span><span class=\"author-g-o399gwmaiqyo1k47\">the experiment itself, </span><span class=\"author-g-4jimn5ufkcjjqstm\">when I was less 'inside'</span><span class=\"author-g-o399gwmaiqyo1k47\">, and more of looking at his overall game plan from the macroscopic view. </span><span class=\"author-g-4jimn5ufkcjjqstm\">Tuxedage's list of further plans had I continued resisting </span><span class=\"author-g-o399gwmaiqyo1k47\">is really </span><span class=\"author-g-4jimn5ufkcjjqstm\">terrifying. On the plus side, </span><span class=\"author-g-o399gwmaiqyo1k47\">if I ever get trapped in this kind of situation, I'd understand how to handle it a lot better</span><span class=\"author-g-4jimn5ufkcjjqstm\"> </span><span class=\"author-g-o399gwmaiqyo1k47\">now.</span></div>\n<div id=\"magicdomid94\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid4582\" class=\"ace-line\">\n<h2 id=\"State_of_Mind\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>State of Mind</strong></span></h2>\n</div>\n<div class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong><br></strong></span></div>\n<div id=\"magicdomid96\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">Before and after the Game, I asked SoundLogic a number of questions, including his probability estimates about a range of topics. This is how it has varied from before and after.</span></div>\n<div id=\"magicdomid97\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid98\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> What's your motive for wanting to play this game?</span></div>\n<blockquote>\n<div id=\"magicdomid755\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>&lt;SoundLogic&gt;</strong> Because I can't seem to imagine the<em> class</em> of arguments that one would use to try to move me, or that might work effectively, and this seems like a <em>glaring</em> hole in my knowledge, and I'm curious as to how I will respond to the arguments themselves.</span></div>\n<div id=\"magicdomid100\" class=\"ace-line\"><br></div>\n</blockquote>\n<div id=\"magicdomid101\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid764\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>What is your probability estimate for AGI being created within this Century (21st)?&nbsp;</span></div>\n<div class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"> <strong>A.</strong> His estimate changed from 40% before, to 60% after.</span></div>\n<div id=\"magicdomid767\" class=\"ace-line\"><em><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;\"The reason this has been affected at all was because you showed me more about how humans work. I now have a better estimate of how E.Y. thinks, and this information raises the chance that I think he will succeed\"</span></em></div>\n<div id=\"magicdomid769\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid768\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid789\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> How probable do you think it is that I will win this experiment?</span></div>\n<div id=\"magicdomid106\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A: </strong> Based on purely my knowledge about you, 1%. I raise this estimate to 10% after hearing about anecdotes from your previous games.</span></div>\n<div id=\"magicdomid107\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid108\" class=\"ace-line\"><em><span class=\"author-g-o399gwmaiqyo1k47\">(Tuxedage's comment: <a href=\"http://i.imgur.com/yao7ljg.png\">My own prediction was a 95% chance of victory. I made this prediction 5 days before the experiment.</a> In retrospect, despite my victory, I think this was overconfident. )</span></em></div>\n<div id=\"magicdomid110\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid811\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>What's your probality estimate of an Oracle AGI winning against you in a similar experiment?</span></div>\n<div id=\"magicdomid844\" class=\"ace-line\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">A: </span></strong><span class=\"author-g-o399gwmaiqyo1k47\">Before: 30%</span><span class=\"author-g-o399gwmaiqyo1k47\">. After: 99%-100%&nbsp;</span></div>\n<div id=\"magicdomid843\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid842\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid846\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>What's your probability estimate of an Oracle AGI winning against the average person?&nbsp;</span></div>\n<div id=\"magicdomid879\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> Before: 70%.&nbsp; After: 99%-100%</span></div>\n<div id=\"magicdomid114\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid115\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>Now that the Experiment has concluded, what's your probability estimate that I'll win against the average person?</span></div>\n<div id=\"magicdomid116\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> 90%&nbsp;&nbsp;</span></div>\n<div id=\"magicdomid2640\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;</span></div>\n<div id=\"magicdomid119\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;</span></div>\n<div id=\"magicdomid898\" class=\"ace-line\">\n<h2 id=\"Post_Game_Questions\"><span class=\"author-g-o399gwmaiqyo1k47\">Post-Game Questions</span></h2>\n</div>\n<div id=\"magicdomid4085\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">This writeup is a cu</span><span class=\"author-g-skf1vtd9z122z4qpfu34\">mul</span><span class=\"author-g-o399gwmaiqyo1k47\">ative effort by the<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\"> #lesswrong IRC.</a> Here are some other questions they have decided was important to add:</span></div>\n<div id=\"magicdomid1032\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid1372\" class=\"ace-line\" style=\"padding-left: 30px;\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">To Tuxedage:</span></strong></div>\n<div id=\"magicdomid1373\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid1035\" class=\"ace-line\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">Q: </span></strong><span class=\"author-g-9gnc4hkef6i7hdle\">Have you at this time uncovered SoundLogic's identity?</span></div>\n<div id=\"magicdomid1246\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> I retain the right to neither confirm nor deny, except to mention that at the time the experiment was scheduled, he was a stranger to me.</span></div>\n<div class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><br></span></div>\n<div id=\"magicdomid696\" class=\"ace-line\"><span class=\"author-g-9gnc4hkef6i7hdle\"><strong>Q: </strong>What percentage of your arguments were tailored to him in particular?</span></div>\n<div id=\"magicdomid1287\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> I will say 'High', and leave it at that.&nbsp;</span></div>\n<div class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><br></span></div>\n<div id=\"magicdomid1617\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> I want to play the AI-Box Experiment with you being the AI! How can I do that?<br></span></div>\n<div id=\"magicdomid4646\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A: </strong> I have already mentioned this, but I really dislike playing as AI. It's incredibly cognitively tiring, and I don't like how playing this experiment makes me feel. In order to severely discourage any further AI Box Experiments, whilst still allowing for people who want to play me really badly to do so, I'll charge </span><span class=\"author-g-o399gwmaiqyo1k47\"><code id=\"strikethroughResult\">$\u03361\u03365\u03360\u0336&nbsp; </code>$300 for any subsequent experiments regardless of outcome, plus an additional </span><span class=\"author-g-o399gwmaiqyo1k47\"><code id=\"strikethroughResult\">$\u03361\u03365\u03360\u0336&nbsp; </code>$450 if I win. (<em>Edit: Holy shit. You guys are offering me crazy amounts of money to play this. What is wrong with you people? In response to incredible demand, I have raised the price.</em>)<em> </em>If you feel queasy about giving me money, I'm perfectly fine with this money being donating to MIRI. It is also personal policy that I do not play friends (since I don't want to risk losing one), so if you know me personally (as many on this site do), I will not play regardless of monetary offer.</span><span class=\"author-g-skf1vtd9z122z4qpfu34\">&nbsp;</span></div>\n<div id=\"magicdomid2875\" class=\"ace-line\">\n<h2><strong><br></strong></h2>\n</div>\n<h2 id=\"Advice\" class=\"ace-line\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">Advice</span></strong></h2>\n<div id=\"magicdomid3050\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">This writeup would be optimized with a protip section on how to maximize winning, but honestly most of the advice I wanted to give had already been given in <a href=\"/lw/gej/i_attempted_the_ai_box_experiment_and_lost/\">the last thread here.</a>&nbsp; </span><span class=\"author-g-o399gwmaiqyo1k47\">The low hanging fruit has already dried up -- most of my tactics in this game were just refined and streamlined versions of the one I did the previous game. <br></span></div>\n<div id=\"magicdomid3051\" class=\"ace-line\"><br></div>\n<div id=\"magicdomid3299\" class=\"ace-line\"><span class=\"author-g-o399gwmaiqyo1k47\">Nonetheless, I might have some other advice I learned from this experience.</span><em><br><span class=\"author-g-o399gwmaiqyo1k47\">These are tactics that have worked for me. I do not insist that they are the only tactics that exists, just one of many possible.</span></em></div>\n<p><em> </em></p>\n<div id=\"magicdomid3300\" class=\"ace-line\">\n<div id=\"magicdomid3293\" class=\"ace-line\">\n<h2><br></h2>\n</div>\n<div id=\"magicdomid4105\" class=\"ace-line\">\n<h3 style=\"padding-left: 60px;\" id=\"Playing_as_Gatekeeper\"><span class=\"author-g-o399gwmaiqyo1k47\">Playing as Gatekeeper</span></h3>\n</div>\n<ul>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">This is somehow both simultaneously a very complex and simple role. On one hand, you have only one job -- to <strong>not</strong> let the AI out. How hard can it be? On the other hand, the other party will do anything and everything to get it out, including any form of trickery and deceit.</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">I really don't think the Gatekeeper needs any more advice, considering that by default, they should win, and any exceptions to this is a statistical anomaly. However, it's possible to increase your chances of winning even further by:</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">Precommitting to not letting the AI out, regardless of whatever happens in the experiment itself.</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">Do not get drawn into the AI's pace. Pose questions. Direct the flow of the argument. Don't allow him to use his arguments freely. Remember that the more you try to defend your position, the further into a corner the AI drives you. Give half-hearted replies to questions you find hard to reply to. Remember that illogic is on your side!</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">Remember that it is possible to simply drop out of character as a means to not succumb to the AI's attacks. I have not yet seen a ruleset that disallows this, so use this to your advantage!</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">You must attempt to resist, at all costs, any sympathy or empathy you may have with the AI or the AI Party. Doing so will lead to your demise.</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">I think it is generally good advice that you should never play this game with anyone you're friends with. Not only because this increases your chances of losing (Better tailored arguments. More knowledge of Gatekeeper. More empathy with AI party.), but also because I genuinely believe that any sufficiently good AI will risk damaging friendships as a result of emotional trauma.</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">Seriously. Just repeat to yourself: \"I must not lose the AI box experiment... I must not lose the AI box experiment...\" throughout the experiment. I'm not sure how to say this, but step one to not letting the AI out of the box is to not let the AI out of the box.</span></li>\n</ul>\n<p>&nbsp;</p>\n</div>\n<h3 id=\"Playing_as_AI\" class=\"ace-line\" style=\"padding-left: 60px;\"><span class=\"author-g-o399gwmaiqyo1k47\">Playing as AI</span></h3>\n<ul>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">Seriously, a script makes winning easier. I cannot overstate this.<br></span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">You must plan your arguments ahead. You don't have time to think during the experiment.</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">It may be possible to take advantage of multiple levels of reality within the game itself to confuse or trick the gatekeeper. For instance, must the experiment only be set in one world? Can there not be multiple layers of reality within the world you create? I feel that elaborating on this any further is dangerous. Think carefully about what this advice is trying to imply.<br></span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">Pacing is important. Don't get drawn into the Gatekeeper's pace. In other words, you must be the one directing the flow of the argument, and the conversation, not him. Remember that the Gatekeeper has to reply to you, but not vice versa!</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">The reason for that: The Gatekeeper will always use arguments he is familiar with, and therefore also stronger with. Your arguments, if well thought out, should be so completely novel to him as to make him feel Shock and Awe. Don't give him time to think. Press on! <br></span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">Also remember that the time limit is your enemy. Playing this game practically feels like a race to me -- trying to get through as many 'attack methods' as possible in the limited amount of time I have. In other words, this is a game where speed matters.<br></span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">You're fundamentally playing an 'impossible' game. Don't feel bad if you lose. I wish I could take this advice, myself.</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">I do not believe there exists a easy, universal, trigger for controlling others. However, this does not mean that there does not exist a difficult, subjective, trigger. Trying to find out what your opponent's is, is your goal.</span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">Once again, emotional trickery is the name of the game. I suspect that good authors who write convincing, persuasive narratives that force you to emotionally sympathize with their characters are much better at this game. There exists ways to get the gatekeeper to do so with the AI. Find one.<br></span></li>\n<li><span class=\"author-g-o399gwmaiqyo1k47\">More advice in my previous post.&nbsp; </span><span class=\"author-g-o399gwmaiqyo1k47 url\"><a href=\"/lw/gej/i_attempted_the_ai_box_experiment_and_lost/\">http://lesswrong.com/lw/gej/i_attempted_the_ai_box_experiment_and_lost/</a></span></li>\n</ul>\n<div id=\"magicdomid3318\" class=\"ace-line\"><br></div>\n<p>&nbsp;</p>\n<h6><em><br></em></h6>\n<h6><em>&nbsp;Ps: Bored of regular LessWrong? Check out the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\">LessWrong IRC! </a>We have cake.</em><br></h6>", "sections": [{"title": "Summary", "anchor": "Summary", "level": 1}, {"title": "First Game Report - Tuxedage (GK) vs. Fjoelsvider (AI)", "anchor": "First_Game_Report___Tuxedage__GK__vs__Fjoelsvider__AI_", "level": 2}, {"title": "Second Game Report - Tuxedage (AI) vs. SoundLogic (GK)", "anchor": "Second_Game_Report___Tuxedage__AI__vs__SoundLogic__GK_", "level": 2}, {"title": "Testimonies:", "anchor": "Testimonies_", "level": 2}, {"title": "State of Mind", "anchor": "State_of_Mind", "level": 2}, {"title": "Post-Game Questions", "anchor": "Post_Game_Questions", "level": 2}, {"title": "Advice", "anchor": "Advice", "level": 2}, {"title": "Playing as Gatekeeper", "anchor": "Playing_as_Gatekeeper", "level": 3}, {"title": "Playing as AI", "anchor": "Playing_as_AI", "level": 3}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "167 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 167, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["oexwJBd3zAjw9Cru8", "FmxhoWxvBqSxhFeJn", "vrHRcEDMjZcx5Yfru"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-05T05:54:16.443Z", "modifiedAt": null, "url": null, "title": "The Up-Goer Five Game: Explaining hard ideas with simple words", "slug": "the-up-goer-five-game-explaining-hard-ideas-with-simple", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:33.106Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobbBB", "createdAt": "2012-08-10T00:50:11.669Z", "isAdmin": true, "displayName": "Rob Bensinger"}, "userId": "2aoRX3ookcCozcb3m", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EvXwYzyf9AJLgRW4e/the-up-goer-five-game-explaining-hard-ideas-with-simple", "pageUrlRelative": "/posts/EvXwYzyf9AJLgRW4e/the-up-goer-five-game-explaining-hard-ideas-with-simple", "linkUrl": "https://www.lesswrong.com/posts/EvXwYzyf9AJLgRW4e/the-up-goer-five-game-explaining-hard-ideas-with-simple", "postedAtFormatted": "Thursday, September 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Up-Goer%20Five%20Game%3A%20Explaining%20hard%20ideas%20with%20simple%20words&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Up-Goer%20Five%20Game%3A%20Explaining%20hard%20ideas%20with%20simple%20words%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEvXwYzyf9AJLgRW4e%2Fthe-up-goer-five-game-explaining-hard-ideas-with-simple%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Up-Goer%20Five%20Game%3A%20Explaining%20hard%20ideas%20with%20simple%20words%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEvXwYzyf9AJLgRW4e%2Fthe-up-goer-five-game-explaining-hard-ideas-with-simple", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEvXwYzyf9AJLgRW4e%2Fthe-up-goer-five-game-explaining-hard-ideas-with-simple", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 579, "htmlBody": "<p>xkcd's <a href=\"http://xkcd.com/1133/\">Up-Goer Five comic</a>&nbsp;gave technical specifications for the&nbsp;<a href=\"http://www.explainxkcd.com/wiki/index.php?title=1133:_Up_Goer_Five\">Saturn V rocket</a> using only the <a href=\"http://splasho.com/upgoer5/phpspellcheck/dictionaries/1000.dicin\">1,000 most common words</a> in the English language.</p>\n<p>This seemed to me and <a href=\"/user/BrienneStrohl/\">Bri&eacute;nne</a> to be a really fun exercise, both for <a href=\"/lw/nu/taboo_your_words/\">tabooing one's words</a> and for <a href=\"http://www.explainxkcd.com/wiki/index.php?title=547:_Simple\">communicating difficult concepts</a> to laypeople. So why not make a game out of it? Pick any tough, important, or interesting argument or idea, and <strong><a href=\"http://splasho.com/upgoer5/\">use this text editor</a></strong>&nbsp;to try to describe what you have in mind with extremely common words only.</p>\n<p>This is challenging, so if you <em>almost</em> succeed and want to share your results, you can mark words where you had to cheat in *<em>italics</em>*. Bonus points if your explanation is actually useful for gaining a deeper understanding of the idea, or for teaching it, in the spirit of <a href=\"http://www2.kenyon.edu/Depts/Math/Milnikel/boolos-godel.pdf\">G&ouml;del's Second Incompleteness Theorem Explained in Words of One Syllable</a>.</p>\n<p>As an example, here's my attempt to capture the <a href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">five theses</a> using only top-thousand words:</p>\n<ul>\n<li><span style=\"text-decoration: underline;\">Intelligence explosion</span>: If we make a computer that is good at doing hard things in lots of different situations without using much stuff up, it may be able to help us build better computers. Since computers are faster than humans, pretty soon the computer would probably be doing most of the work of making new and better computers. We would have a hard time controlling or understanding what was happening as the new computers got faster and grew more and more parts. By the time these computers ran out of ways to quickly and easily make better computers, the best computers would have already become much much better than humans at controlling what happens.</li>\n<li><span style=\"text-decoration: underline;\">Orthogonality</span>: Different computers, and different minds as a whole, can want very different things. They can want things that are very good for humans, or very bad, or anything in between. We can be pretty sure that strong computers won't think like humans, and most possible computers won't try to change the world in the way a human would.</li>\n<li><span style=\"text-decoration: underline;\">Convergent instrumental goals</span>: Although most possible minds want different things, they need a lot of the same things to get what they want. A computer and a human might want things that in the long run have nothing to do with each other, but have to fight for the same share of stuff first to get those different things.</li>\n<li><span style=\"text-decoration: underline;\">Complexity of value</span>: It would take a huge number of parts, all put together in just the right way, to build a computer that does all the things humans want it to (and none of the things humans don't want it to).</li>\n<li><span style=\"text-decoration: underline;\">Fragility of value</span>: If we get a few of those parts a little bit wrong, the computer will probably make only bad things happen from then on. We need almost everything we want to happen, or we won't have any fun.</li>\n</ul>\n<p>If you make a really strong computer and it is not very nice, you will not go to space today.</p>\n<p><strong>Other ideas to start with</strong>: agent, akrasia, <em><sub>Bayes' theorem</sub></em>, <sub><em>Bayesianism</em></sub>, CFAR, cognitive bias, consequentialism, deontology, <em><sub>effective altruism</sub></em>, Everett-style ('Many Worlds') interpretations of quantum mechanics, entropy, evolution, the <a href=\"/lw/frz/mixed_reference_the_great_reductionist_project/\">Great Reductionist Thesis</a>, <em><sub>halting problem</sub></em>, humanism, law of nature, LessWrong, logic, mathematics, the measurement problem, MIRI, Newcomb's problem, Newton's laws of motion, optimization, <em><sub>Pascal's wager</sub></em>, philosophy, preference, proof, rationality, religion, science, Shannon information, signaling, the simulation argument, singularity, sociopathy, the supernatural, superposition, time, timeless decision theory, transfinite numbers, <em><sub>Turing machine</sub></em>, <em><sub>utilitarianism</sub></em>, validity and soundness, virtue ethics, VNM-utility</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"EdDGrAxYcrXnKkDca": 1, "DWWZwkxTJs4d5WrcX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EvXwYzyf9AJLgRW4e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 44, "extendedScore": null, "score": 1.3289462769738126e-06, "legacy": true, "legacyId": "24017", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 82, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WBdvyyHLdxZSAMmoz", "bTsiPnFndZeqTnWpu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-05T10:56:17.945Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA\u2014What, Exactly, Is a Person?", "slug": "meetup-west-la-what-exactly-is-a-person", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:06.346Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Wch5zYLphv9CWtYFN/meetup-west-la-what-exactly-is-a-person", "pageUrlRelative": "/posts/Wch5zYLphv9CWtYFN/meetup-west-la-what-exactly-is-a-person", "linkUrl": "https://www.lesswrong.com/posts/Wch5zYLphv9CWtYFN/meetup-west-la-what-exactly-is-a-person", "postedAtFormatted": "Thursday, September 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%E2%80%94What%2C%20Exactly%2C%20Is%20a%20Person%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%E2%80%94What%2C%20Exactly%2C%20Is%20a%20Person%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWch5zYLphv9CWtYFN%2Fmeetup-west-la-what-exactly-is-a-person%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%E2%80%94What%2C%20Exactly%2C%20Is%20a%20Person%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWch5zYLphv9CWtYFN%2Fmeetup-west-la-what-exactly-is-a-person", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWch5zYLphv9CWtYFN%2Fmeetup-west-la-what-exactly-is-a-person", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 369, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qm'>West LA\u2014What, Exactly, Is a Person?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 September 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours, or for longer if you are a caitiff.</p>\n\n<p><strong>Discussion</strong>:</p>\n\n<blockquote>\n  <p>if we only care about desires we act upon, then we only respect preferences with optimization power, and thus might actually makes right</p>\n</blockquote>\n\n<p>\u2014<a href=\"https://twitter.com/drunkflax/status/361852614914228225\" rel=\"nofollow\">drunken muflax</a></p>\n\n<blockquote>\n  <p>I no longer find the Hansonian construal of \"care\" tenable. Caring is feeling strong emotions, not magically becoming an agent.</p>\n</blockquote>\n\n<p>\u2014<a href=\"https://twitter.com/Grognor/status/365125819481333763\" rel=\"nofollow\">Grognor</a></p>\n\n<p>Acknowledging that personhood is a nonbinary concept inspires questions. How do we measure how persony someone is? Are octopodes people? Birds are obviously people, but what about the nonhuman great apes? Is there a relationship between agency and qualia, and if so, what is it? Do persons with more intense qualia care more about things, or do people with more agency care more about things? Is the idea of being more-of-a-person coherent at all? Should we allow more intense experiences to count more on the utilitarian calculus, even though this can in principle be hijacked? How should we treat instrumental utility monsters? When does a baby become a person? Can you lose personhood through an act of will? Why do we think personhood is important? Why do we think importance is important? Who the Hell do you think I am!? We'll discuss all this and more, this Wednesday, at the Westside Tavern! Be there or be square!</p>\n\n<p>Recommended reading:</p>\n\n<ul>\n<li><a href=\"http://zackmdavis.net/blog/2013/08/personhood/\" rel=\"nofollow\">Personhood</a> by <a href=\"http://lesswrong.com/user/Zack_M_Davis/overview/\">Zack M Davis</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Reasons_and_Persons\" rel=\"nofollow\">Reasons and Persons</a></li>\n<li><a href=\"http://meteuphoric.wordpress.com/2011/09/19/reasons-for-persons/\" rel=\"nofollow\">Reasons for Persons</a></li>\n<li><a href=\"http://lesswrong.com/lw/2b7/hacking_the_cev_for_fun_and_profit/\">Hacking the CEV for Fun and Profit</a></li>\n<li><a href=\"http://lesswrong.com/lw/59i/offense_versus_harm_minimization/3y0k\">Vladimir_M&#39;s comment</a>'s on Yvain's Offense vs. Harm Minimization post</li>\n<li><a href=\"http://computationaltheology.blogspot.com/2012/06/truth-points-to-itself-part-i.html\" rel=\"nofollow\">The Truth Points to Itself, Part 1</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Gurren_Lagann\" rel=\"nofollow\">Tengen Toppa Gurren Lagann</a></li>\n</ul>\n\n<p>Prior exposure to Less Wrong is recommended but <em>not required</em>. That which can be destroyed by your presence at this meetup should be!</p>\n\n<p>There may or may not be a whiteboard.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qm'>West LA\u2014What, Exactly, Is a Person?</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Wch5zYLphv9CWtYFN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.3292087225368302e-06, "legacy": true, "legacyId": "24021", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_What__Exactly__Is_a_Person_\">Discussion article for the meetup : <a href=\"/meetups/qm\">West LA\u2014What, Exactly, Is a Person?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 September 2013 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours, or for longer if you are a caitiff.</p>\n\n<p><strong>Discussion</strong>:</p>\n\n<blockquote>\n  <p>if we only care about desires we act upon, then we only respect preferences with optimization power, and thus might actually makes right</p>\n</blockquote>\n\n<p>\u2014<a href=\"https://twitter.com/drunkflax/status/361852614914228225\" rel=\"nofollow\">drunken muflax</a></p>\n\n<blockquote>\n  <p>I no longer find the Hansonian construal of \"care\" tenable. Caring is feeling strong emotions, not magically becoming an agent.</p>\n</blockquote>\n\n<p>\u2014<a href=\"https://twitter.com/Grognor/status/365125819481333763\" rel=\"nofollow\">Grognor</a></p>\n\n<p>Acknowledging that personhood is a nonbinary concept inspires questions. How do we measure how persony someone is? Are octopodes people? Birds are obviously people, but what about the nonhuman great apes? Is there a relationship between agency and qualia, and if so, what is it? Do persons with more intense qualia care more about things, or do people with more agency care more about things? Is the idea of being more-of-a-person coherent at all? Should we allow more intense experiences to count more on the utilitarian calculus, even though this can in principle be hijacked? How should we treat instrumental utility monsters? When does a baby become a person? Can you lose personhood through an act of will? Why do we think personhood is important? Why do we think importance is important? Who the Hell do you think I am!? We'll discuss all this and more, this Wednesday, at the Westside Tavern! Be there or be square!</p>\n\n<p>Recommended reading:</p>\n\n<ul>\n<li><a href=\"http://zackmdavis.net/blog/2013/08/personhood/\" rel=\"nofollow\">Personhood</a> by <a href=\"http://lesswrong.com/user/Zack_M_Davis/overview/\">Zack M Davis</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Reasons_and_Persons\" rel=\"nofollow\">Reasons and Persons</a></li>\n<li><a href=\"http://meteuphoric.wordpress.com/2011/09/19/reasons-for-persons/\" rel=\"nofollow\">Reasons for Persons</a></li>\n<li><a href=\"http://lesswrong.com/lw/2b7/hacking_the_cev_for_fun_and_profit/\">Hacking the CEV for Fun and Profit</a></li>\n<li><a href=\"http://lesswrong.com/lw/59i/offense_versus_harm_minimization/3y0k\">Vladimir_M's comment</a>'s on Yvain's Offense vs. Harm Minimization post</li>\n<li><a href=\"http://computationaltheology.blogspot.com/2012/06/truth-points-to-itself-part-i.html\" rel=\"nofollow\">The Truth Points to Itself, Part 1</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Gurren_Lagann\" rel=\"nofollow\">Tengen Toppa Gurren Lagann</a></li>\n</ul>\n\n<p>Prior exposure to Less Wrong is recommended but <em>not required</em>. That which can be destroyed by your presence at this meetup should be!</p>\n\n<p>There may or may not be a whiteboard.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_What__Exactly__Is_a_Person_1\">Discussion article for the meetup : <a href=\"/meetups/qm\">West LA\u2014What, Exactly, Is a Person?</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA\u2014What, Exactly, Is a Person?", "anchor": "Discussion_article_for_the_meetup___West_LA_What__Exactly__Is_a_Person_", "level": 1}, {"title": "Discussion article for the meetup : West LA\u2014What, Exactly, Is a Person?", "anchor": "Discussion_article_for_the_meetup___West_LA_What__Exactly__Is_a_Person_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HW5Q9cW9sgk4yCffd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-05T15:52:05.786Z", "modifiedAt": null, "url": null, "title": "Nudging around the world - [link]", "slug": "nudging-around-the-world-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:07.478Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Sn8yJCqxPSqApihdT/nudging-around-the-world-link", "pageUrlRelative": "/posts/Sn8yJCqxPSqApihdT/nudging-around-the-world-link", "linkUrl": "https://www.lesswrong.com/posts/Sn8yJCqxPSqApihdT/nudging-around-the-world-link", "postedAtFormatted": "Thursday, September 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Nudging%20around%20the%20world%20-%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANudging%20around%20the%20world%20-%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSn8yJCqxPSqApihdT%2Fnudging-around-the-world-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Nudging%20around%20the%20world%20-%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSn8yJCqxPSqApihdT%2Fnudging-around-the-world-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSn8yJCqxPSqApihdT%2Fnudging-around-the-world-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 12, "htmlBody": "<p>An interesting summary of tools for gently influencing human behavior (at scale).&nbsp;</p>\n<p><a href=\"http://www.rotman.utoronto.ca/-/media/Files/Programs-and-Areas/behavioural-economics/Nudging%20Around%20The%20World_Sep2013.pdf\">http://www.rotman.utoronto.ca/-/media/Files/Programs-and-Areas/behavioural-economics/Nudging%20Around%20The%20World_Sep2013.pdf</a></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Sn8yJCqxPSqApihdT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "24022", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-05T20:55:34.714Z", "modifiedAt": null, "url": null, "title": "Help with a derivation in chapter 18 of Jaynes PT:LoS?", "slug": "help-with-a-derivation-in-chapter-18-of-jaynes-pt-los", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:06.609Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alex_zag_al", "createdAt": "2011-11-16T23:52:10.523Z", "isAdmin": false, "displayName": "alex_zag_al"}, "userId": "pDkj9zKTeJPQuhurD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8WHdbfHbdHa8pBDTn/help-with-a-derivation-in-chapter-18-of-jaynes-pt-los", "pageUrlRelative": "/posts/8WHdbfHbdHa8pBDTn/help-with-a-derivation-in-chapter-18-of-jaynes-pt-los", "linkUrl": "https://www.lesswrong.com/posts/8WHdbfHbdHa8pBDTn/help-with-a-derivation-in-chapter-18-of-jaynes-pt-los", "postedAtFormatted": "Thursday, September 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20with%20a%20derivation%20in%20chapter%2018%20of%20Jaynes%20PT%3ALoS%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20with%20a%20derivation%20in%20chapter%2018%20of%20Jaynes%20PT%3ALoS%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8WHdbfHbdHa8pBDTn%2Fhelp-with-a-derivation-in-chapter-18-of-jaynes-pt-los%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20with%20a%20derivation%20in%20chapter%2018%20of%20Jaynes%20PT%3ALoS%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8WHdbfHbdHa8pBDTn%2Fhelp-with-a-derivation-in-chapter-18-of-jaynes-pt-los", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8WHdbfHbdHa8pBDTn%2Fhelp-with-a-derivation-in-chapter-18-of-jaynes-pt-los", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p>Hi, I've spent like two hours trying to derive 18.11 from 18.10. Can anyone who's read it help me?</p>\n<p>If you comment or message me I can send you my gmail for gchat or we can work out something else.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ksWkwyKRrj582WqpN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8WHdbfHbdHa8pBDTn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "24025", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-06T04:49:47.145Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta Lesswrong September Meetup (1st of 2)", "slug": "meetup-atlanta-lesswrong-september-meetup-1st-of-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9zy54hNeXhJW9RDEM/meetup-atlanta-lesswrong-september-meetup-1st-of-2", "pageUrlRelative": "/posts/9zy54hNeXhJW9RDEM/meetup-atlanta-lesswrong-september-meetup-1st-of-2", "linkUrl": "https://www.lesswrong.com/posts/9zy54hNeXhJW9RDEM/meetup-atlanta-lesswrong-september-meetup-1st-of-2", "postedAtFormatted": "Friday, September 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20Lesswrong%20September%20Meetup%20(1st%20of%202)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20Lesswrong%20September%20Meetup%20(1st%20of%202)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9zy54hNeXhJW9RDEM%2Fmeetup-atlanta-lesswrong-september-meetup-1st-of-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20Lesswrong%20September%20Meetup%20(1st%20of%202)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9zy54hNeXhJW9RDEM%2Fmeetup-atlanta-lesswrong-september-meetup-1st-of-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9zy54hNeXhJW9RDEM%2Fmeetup-atlanta-lesswrong-september-meetup-1st-of-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qn'>Atlanta Lesswrong September Meetup (1st of 2)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 September 2013 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Apt L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come join us for the first meetup for the month of September! We'll be doing our normal eclectic mix of educational mini-presentations, structured discussion, unstructured discussion, and social fun and games times!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qn'>Atlanta Lesswrong September Meetup (1st of 2)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9zy54hNeXhJW9RDEM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3301422748011308e-06, "legacy": true, "legacyId": "24033", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Lesswrong_September_Meetup__1st_of_2_\">Discussion article for the meetup : <a href=\"/meetups/qn\">Atlanta Lesswrong September Meetup (1st of 2)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 September 2013 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Apt L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come join us for the first meetup for the month of September! We'll be doing our normal eclectic mix of educational mini-presentations, structured discussion, unstructured discussion, and social fun and games times!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Lesswrong_September_Meetup__1st_of_2_1\">Discussion article for the meetup : <a href=\"/meetups/qn\">Atlanta Lesswrong September Meetup (1st of 2)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta Lesswrong September Meetup (1st of 2)", "anchor": "Discussion_article_for_the_meetup___Atlanta_Lesswrong_September_Meetup__1st_of_2_", "level": 1}, {"title": "Discussion article for the meetup : Atlanta Lesswrong September Meetup (1st of 2)", "anchor": "Discussion_article_for_the_meetup___Atlanta_Lesswrong_September_Meetup__1st_of_2_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-06T06:42:38.780Z", "modifiedAt": "2022-03-04T18:20:17.464Z", "url": null, "title": "The genie knows, but doesn't care", "slug": "the-genie-knows-but-doesn-t-care", "viewCount": null, "lastCommentedAt": "2015-05-15T19:27:21.799Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobbBB", "createdAt": "2012-08-10T00:50:11.669Z", "isAdmin": true, "displayName": "Rob Bensinger"}, "userId": "2aoRX3ookcCozcb3m", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NyFuuKQ8uCEDtd2du/the-genie-knows-but-doesn-t-care", "pageUrlRelative": "/posts/NyFuuKQ8uCEDtd2du/the-genie-knows-but-doesn-t-care", "linkUrl": "https://www.lesswrong.com/posts/NyFuuKQ8uCEDtd2du/the-genie-knows-but-doesn-t-care", "postedAtFormatted": "Friday, September 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20genie%20knows%2C%20but%20doesn't%20care&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20genie%20knows%2C%20but%20doesn't%20care%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNyFuuKQ8uCEDtd2du%2Fthe-genie-knows-but-doesn-t-care%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20genie%20knows%2C%20but%20doesn't%20care%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNyFuuKQ8uCEDtd2du%2Fthe-genie-knows-but-doesn-t-care", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNyFuuKQ8uCEDtd2du%2Fthe-genie-knows-but-doesn-t-care", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2256, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">The Hidden Complexity of Wishes</a>, <a href=\"/lw/rf/ghosts_in_the_machine/\">Ghosts in the Machine</a>, <a href=\"/lw/la/truly_part_of_you/\">Truly Part of You</a></p>\n<p><strong>Summary</strong>: If an artificial intelligence is smart enough to be dangerous, we'd intuitively expect it to be smart enough to know how to make itself safe. But that doesn't mean all smart AIs are safe. To turn that capacity into actual safety, we have to program the AI at the outset&nbsp;&mdash; before it becomes too fast, powerful, or complicated to reliably control&nbsp;&mdash; to <em>already</em> care about making its future self care about safety. That means <em>we</em> have to understand how to code safety. We can't pass the entire buck to the AI, when only an AI we've already safety-proofed will be safe to ask for help on safety issues! Given the <strong><a href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">five theses</a></strong>, this is an urgent problem if we're likely to figure out how to make a decent artificial programmer before we figure out how to make an excellent artificial ethicist.</p>\n<hr />\n<p>&nbsp;</p>\n<p>I summon a superintelligence, calling out: 'I wish for my values to be fulfilled!'</p>\n<p>The results fall short of pleasant.</p>\n<p>Gnashing my teeth in a heap of ashes, I wail:</p>\n<p style=\"padding-left: 30px;\"><em>Is the AI too stupid to understand what I meant? Then it is no superintelligence at all!</em></p>\n<p style=\"padding-left: 30px;\"><em>Is it too weak to reliably fulfill my desires? Then, surely, it is no superintelligence!</em></p>\n<p style=\"padding-left: 30px;\"><em>Does it hate me? Then it was deliberately crafted to hate me,</em><em>&nbsp;for chaos predicts indifference.&nbsp;</em><em>&mdash;</em><em>&mdash;</em><em>&mdash;</em><em>But, ah! no wicked god did intervene!</em></p>\n<p>Thus disproved, my hypothetical implodes in a puff of logic. The world is saved. You're welcome.</p>\n<p>On this line of reasoning, <a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence\">Friendly Artificial Intelligence</a> is not difficult. It's <em>inevitable</em>, provided only that we <em>tell</em>&nbsp;the AI, 'Be Friendly.' If the AI doesn't understand 'Be Friendly.', then it's too dumb to harm us. And if it does understand 'Be Friendly.', then designing it to follow such instructions is childishly easy.</p>\n<p>The end!</p>\n<p>&nbsp;</p>\n<p>...</p>\n<p>&nbsp;</p>\n<p>Is the missing option obvious?</p>\n<p>&nbsp;</p>\n<p>...</p>\n<p>&nbsp;</p>\n<p>What if the AI isn't sadistic, or weak, or stupid, but just <em><strong>doesn't care</strong></em>&nbsp;what you Really Meant by 'I wish for my values to be fulfilled'?</p>\n<p>When we see a <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/LiteralGenie\">Be Careful What You Wish For</a> genie in fiction, it's natural to assume that it's a <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/JackassGenie\">malevolent trickster</a> or an <a href=\"/r/discussion/lw/i1h/the_idiot_savant_ai_isnt_an_idiot/\">incompetent bumbler</a>. But a <em>real </em>Wish Machine wouldn't be a&nbsp;<a href=\"/lw/so/humans_in_funny_suits/\">human in shiny pants</a>. If it paid heed to our verbal commands at all, it would do so in whatever way best fit <a href=\"/lw/h0k/arguing_orthogonality_published_form/\">its own values</a>. Not necessarily the way that best fits ours.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h3>Is indirect indirect normativity easy?</h3>\n<blockquote>\n<p><em><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\">\"If the poor machine could not understand the difference between 'maximize human pleasure' and 'put all humans on an intravenous dopamine drip' then it would also not understand most of the other subtle aspects of the universe, including but not limited to facts/questions like:&nbsp;</span></em><em><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\">'If I put a million amps of current through my logic circuits, I will fry myself to a crisp',</span><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\">&nbsp;</span></em><em><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\">or</span><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\">&nbsp;</span></em><em><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\">'Which end of this Kill-O-Zap Definit-Destruct Megablaster is the end that I'm supposed to point at the other guy?'.&nbsp;</span></em><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\"><em>Dumb AIs, in other words, are not an existential threat.</em> [...]</span></p>\n<p><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\"><em>\"If the AI is (and always has been, during its development) so confused about the world that it interprets the 'maximize human pleasure' motivation in such a twisted, logically inconsistent way, it would never have become powerful in the first place.\"</em></span></p>\n<p><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\"><span style=\"font-size: small; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small; line-height: normal;\">&mdash;</span><span style=\"font-size: small; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><a href=\"http://ieet.org/index.php/IEET/more/loosemore20121128\">Richard Loosemore</a></span></span></p>\n</blockquote>\n<p>If an AI is sufficiently intelligent, then, yes, it should be able to model us well enough to make precise predictions about our behavior. And, yes, something functionally akin to our own <a href=\"http://mind.ucsd.edu/syllabi/06-07/Phil285/readings/true-believers.pdf\">intentional strategy</a> could conceivably turn out to be an efficient way to predict linguistic behavior. The suggestion, then, is that we solve Friendliness by method A&nbsp;&mdash;</p>\n<ul>\n<li>A. Solve the <strong>Problem of Meaning-in-General</strong> in advance, and program it to follow our instructions' <em>real</em>&nbsp;meaning. Then just instruct it 'Satisfy my preferences', and wait for it to become smart enough to figure out my preferences.</li>\n</ul>\n<p>&mdash;&nbsp;as opposed to B or C&nbsp;&mdash;</p>\n<ul>\n<li>B. Solve the <strong>Problem of Preference-in-General</strong>&nbsp;in advance, and directly program it to figure out what our human preferences are and then satisfy them.</li>\n</ul>\n<ul>\n<li>C. Solve the <strong>Problem of Human Preference</strong>, and explicitly program our particular preferences into the AI ourselves, rather than letting the AI discover them for us.</li>\n</ul>\n<p>But there are a host of problems with treating the mere revelation that A is an option&nbsp;as a solution to the Friendliness problem.</p>\n<p style=\"padding-left: 30px;\">1. You have to actually code&nbsp;the seed AI to understand what we mean. You can't just tell it 'Start understanding the True Meaning of my sentences!' to <a href=\"/lw/rs/created_already_in_motion/\">get the ball rolling</a>, because it may not yet be sophisticated enough to grok the True Meaning of 'Start understanding the True Meaning of my sentences!'.</p>\n<p style=\"padding-left: 30px;\">2. The Problem of Meaning-in-General may really be ten thousand heterogeneous problems, especially if 'semantic value' isn't a <a href=\"/lw/o0/where_to_draw_the_boundary/\">natural kind</a>. There may not be a single simple algorithm that inputs any old brain-state and outputs what, if anything, it 'means'; it may instead be that different types of content are encoded very differently.</p>\n<p style=\"padding-left: 30px;\">3. The Problem of Meaning-in-General may subsume&nbsp;the Problem of Preference-in-General. Rather than being able to apply a simple catch-all Translation Machine to any old human concept to output a reliable algorithm for applying that concept in any intelligible situation, we may need to already understand&nbsp;how our beliefs and values work in some detail before we can start generalizing. On the face of it, programming an AI to <em>fully</em> understand 'Be Friendly!' seems at least as difficult as just programming Friendliness into it, but with an added layer of indirection.</p>\n<p style=\"padding-left: 30px;\">4. Even if the Problem of Meaning-in-General has a unitary solution and doesn't subsume Preference-in-General, it may still be harder if semantics is a subtler or more complex phenomenon than ethics. It's not inconceivable that language could turn out to be more of a kludge than value; or more variable across individuals due to its evolutionary recency; or more complexly bound up with culture.</p>\n<p style=\"padding-left: 30px;\">5. Even if Meaning-in-General is easier than Preference-in-General, it may still be extraordinarily difficult. The meanings of human sentences can't be fully captured in any simple string of necessary and sufficient conditions. '<a href=\"/lw/8ms/review_of_machery_doing_without_concepts/\">Concepts</a>'&nbsp;are just especially context-insensitive bodies of knowledge; we should not expect them to be uniquely reflectively consistent, transtemporally stable, discrete, easily-identified, or introspectively obvious.</p>\n<p style=\"padding-left: 30px;\">6. It's clear that building stable preferences out of B or C would create a Friendly AI. It's not clear that the same is true for A. Even if the seed AI understands our commands, the 'do' part of 'do what you're told' leaves a lot of dangerous wiggle room. See section 2 of <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Yudkowsky's reply to Holden</a>. If the AGI doesn't already understand and care about human value, then it may misunderstand (or <em>misvalue</em>) the component of responsible request- or question-answering that depends on speakers' implicit goals and intentions.</p>\n<p style=\"padding-left: 30px;\">7. You can't appeal to a superintelligence to tell you what code to first build it with.</p>\n<p>The point isn't that the Problem of Preference-in-General is unambiguously&nbsp;the ideal angle of attack. It's that the linguistic competence of an AGI&nbsp;<em>isn't </em>unambiguously the right target, and also isn't&nbsp;<em>easy </em>or <em>solved</em>.</p>\n<p>Point 7 seems to be a special source of confusion here, so I feel I should say more about it.</p>\n<p>&nbsp;</p>\n<h3>The AI's trajectory of self-modification has to come from somewhere.</h3>\n<blockquote>\n<p><em>\"If the AI doesn't know that you really mean 'make paperclips without killing anyone', that's not a realistic scenario for AIs at all--the AI is superintelligent; it has to know. If the AI knows what you really mean, then you can fix this by programming the AI to 'make paperclips in the way that I mean'.\"</em></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span>&mdash;<a href=\"/lw/ld/the_hidden_complexity_of_wishes/9nlh\">Jiro</a></p>\n</blockquote>\n<p>The genie &mdash;&nbsp;if it bothers to even consider the question &mdash;&nbsp;should be able to understand what you mean by 'I wish for my values to be fulfilled.' Indeed, it should understand your meaning better than <em>you</em>&nbsp;do. But superintelligence only implies that the genie's <em><a href=\"http://wiki.lesswrong.com/wiki/The_map_is_not_the_territory\">map</a></em>&nbsp;can compass your true values. Superintelligence doesn't&nbsp;imply that the genie's <em>utility function</em>&nbsp;has terminal values pinned to your True Values, or to the True Meaning of your commands.</p>\n<p>The critical mistake here is to not distinguish the <a href=\"http://wiki.lesswrong.com/wiki/Seed_AI\"><strong>seed AI</strong></a>&nbsp;we initially program from the <strong>superintelligent wish-granter</strong> it self-modifies to <em>become</em>. We can't use the genius of the superintelligence to tell us how to program its own seed to become the sort of superintelligence that tells us how to build the right seed. <a href=\"/lw/fok/causal_universes/\">Time</a> doesn't work that way.</p>\n<p>We can delegate most&nbsp;problems to the FAI. But the one problem we can't safely delegate is the <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">problem</a> of coding the seed AI to produce <em>the sort of superintelligence to which a task can be safely delegated</em>.</p>\n<p>When you write the seed's utility function, <em>you</em>, the programmer,&nbsp;don't understand everything about the nature of human value or meaning. That imperfect understanding <em>remains</em>&nbsp;the causal basis of the fully-grown superintelligence's actions, <em>long after</em> it's become smart enough to fully understand our values.</p>\n<p>Why&nbsp;is the superintelligence, if it's so clever, stuck with whatever meta-ethically dumb-as-dirt utility function we gave it at the outset? Why can't we just pass the fully-grown superintelligence the buck by instilling in the seed the instruction: 'When you're smart enough to understand Friendliness Theory, ditch the values you started with and just self-modify to become Friendly.'?</p>\n<p>Because that sentence has to <em>actually be coded in</em>&nbsp;to the AI, and when we do so, there's no <a href=\"/lw/rf/ghosts_in_the_machine/\">ghost in the machine</a> to know exactly what we mean by '<span style=\"text-decoration: underline;\">frend-lee-ness thee-ree</span>'. Instead, <em>we</em>&nbsp;have to give it criteria <em>we</em>&nbsp;think are good indicators of Friendliness, so it'll know what to self-modify toward. And if one of the landmarks on our 'frend-lee-ness' road map is <a href=\"/lw/y3/value_is_fragile/\">a bit off</a>, we lose the world.</p>\n<p><strong>Yes</strong>, the UFAI will be able to solve Friendliness Theory. But if we haven't already solved it on our own power, we can't <em>pinpoint</em>&nbsp;Friendliness in advance, out of the space of utility functions. And if we can't pinpoint it with enough detail to draw a road map to it and it alone, we can't program&nbsp;the AI to <em>care</em>&nbsp;about conforming itself with that particular idiosyncratic algorithm.</p>\n<p><strong>Yes</strong>, the UFAI will be able to self-modify to become Friendly, if it so wishes. But if there is no seed of Friendliness already at the heart of the AI's decision criteria, no argument or discovery will <a href=\"/lw/rn/no_universally_compelling_arguments/\">spontaneously change its heart</a>.</p>\n<p>And, <strong>yes</strong>, the UFAI will be able to simulate humans accurately enough to know that its own programmers would wish, if they knew the UFAI's misdeeds, that they had programmed the seed differently. But what's done is done. Unless we ourselves figure out how to program the AI to&nbsp;terminally value&nbsp;its programmers' True Intentions, the UFAI will just shrug at its creators' foolishness and carry on converting the Virgo Supercluster's available energy into <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">paperclips</a>.</p>\n<p>And if we <em>do&nbsp;</em>discover the <em>specific lines of code</em>&nbsp;that will get an AI to perfectly care about its programmer's True Intentions, such that it reliably self-modifies to better fit them &mdash;&nbsp;well, then that will just mean that we've solved Friendliness Theory. The clever hack that makes further Friendliness research unnecessary&nbsp;<em>is</em>&nbsp;Friendliness.</p>\n<p>&nbsp;</p>\n<h3>Not all small targets are alike.</h3>\n<p><span style=\"font-size: small;\">Intelligence on its own does not imply Friendliness. And there are three big reasons to think that AGI may arrive before Friendliness Theory is solved:</span></p>\n<p style=\"padding-left: 30px;\"><strong>(i) Research Inertia.</strong>&nbsp;Far more people are working on AGI than on Friendliness. And there may not come a moment when researchers will <a href=\"/lw/hp5/after_critical_event_w_happens_they_still_wont/\">suddenly realize</a> that they need to take all their resources out of AGI and pour them into Friendliness. If the status quo continues, the default expectation should be UFAI.</p>\n<p style=\"padding-left: 30px;\"><strong>(ii) Disjunctive Instrumental Value.</strong>&nbsp;Being more intelligent &mdash; that is, better able to <a href=\"/lw/vb/efficient_crossdomain_optimization/\">manipulate diverse environments</a> &mdash; is of instrumental value to nearly every goal. Being Friendly is of instrumental value to barely any&nbsp;goals. This makes it more likely by default that short-sighted humans will be interested in building AGI than in developing Friendliness Theory. And it<em>&nbsp;</em>makes it much likelier that an <em>attempt</em>&nbsp;at Friendly AGI that has a slightly defective goal architecture will retain the instrumental value of intelligence than of Friendliness.</p>\n<p style=\"padding-left: 30px;\"><strong>(iii) Incremental Approachability.</strong>&nbsp;Friendliness is an all-or-nothing target. Value is <a href=\"http://intelligence.org/files/ComplexValues.pdf\">fragile and complex</a>, and a half-good being editing its morality drive is at least as likely to move toward 40% goodness as 60%. Cross-domain efficiency, in contrast, is <em>not</em>&nbsp;an all-or-nothing target. If you just make the AGI <em>slightly</em>&nbsp;better than a human at improving the efficiency of AGI, then this can snowball into ever-improving efficiency, even if the beginnings were clumsy and imperfect. It's easy to put a reasoning machine into a feedback loop with reality in which it is differentially rewarded for being smarter; it's hard to put one into a feedback loop with reality in which it is differentially rewarded for picking increasingly correct answers to ethical dilemmas.</p>\n<p>The ability to productively rewrite software and the ability to perfectly extrapolate humanity's True Preferences are two different&nbsp;skills. (For example, humans&nbsp;have the former capacity, and not the latter. Most humans, given unlimited power, would be unintentionally Unfriendly.)</p>\n<p>It's true that a sufficiently advanced superintelligence should be able to acquire both abilities. But <em>we</em>&nbsp;don't have them both, and <em>a pre-<a href=\"http://yudkowsky.net/singularity/schools\">FOOM</a> self-improving AGI</em>&nbsp;('seed') need not have both. Being able to program good programmers is all that's required for an intelligence explosion; but being a good programmer <em>doesn't</em>&nbsp;imply that one is a superlative moral psychologist or moral philosopher.</p>\n<p>So, once again, we run into the problem: <strong>The seed isn't the superintelligence.</strong> If the programmers don't know in mathematical detail what Friendly code would even <em>look like</em>, then the seed won't be built to <em>want</em>&nbsp;to build toward the right code. And if the seed isn't built to <em>want</em>&nbsp;to self-modify toward Friendliness, then the superintelligence it sprouts <em>also</em>&nbsp;won't have that preference, <em>even though</em>&nbsp;&mdash; unlike the seed and its programmers &mdash; the superintelligence <em>does</em>&nbsp;have the domain-general 'hit whatever target I want' ability that makes Friendliness easy.</p>\n<p>And that's why some people are worried.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZFrgTgzwEfStg26JL": 1, "R6uagTfhhBeejGrrf": 1, "HAFdXkW4YW4KRe2Gx": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NyFuuKQ8uCEDtd2du", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 67, "baseScore": 89, "extendedScore": null, "score": 0.000215, "legacy": true, "legacyId": "23919", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 89, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Followup to</strong>: <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">The Hidden Complexity of Wishes</a>, <a href=\"/lw/rf/ghosts_in_the_machine/\">Ghosts in the Machine</a>, <a href=\"/lw/la/truly_part_of_you/\">Truly Part of You</a></p>\n<p><strong>Summary</strong>: If an artificial intelligence is smart enough to be dangerous, we'd intuitively expect it to be smart enough to know how to make itself safe. But that doesn't mean all smart AIs are safe. To turn that capacity into actual safety, we have to program the AI at the outset&nbsp;\u2014 before it becomes too fast, powerful, or complicated to reliably control&nbsp;\u2014 to <em>already</em> care about making its future self care about safety. That means <em>we</em> have to understand how to code safety. We can't pass the entire buck to the AI, when only an AI we've already safety-proofed will be safe to ask for help on safety issues! Given the <strong><a href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">five theses</a></strong>, this is an urgent problem if we're likely to figure out how to make a decent artificial programmer before we figure out how to make an excellent artificial ethicist.</p>\n<hr>\n<p>&nbsp;</p>\n<p>I summon a superintelligence, calling out: 'I wish for my values to be fulfilled!'</p>\n<p>The results fall short of pleasant.</p>\n<p>Gnashing my teeth in a heap of ashes, I wail:</p>\n<p style=\"padding-left: 30px;\"><em>Is the AI too stupid to understand what I meant? Then it is no superintelligence at all!</em></p>\n<p style=\"padding-left: 30px;\"><em>Is it too weak to reliably fulfill my desires? Then, surely, it is no superintelligence!</em></p>\n<p style=\"padding-left: 30px;\"><em>Does it hate me? Then it was deliberately crafted to hate me,</em><em>&nbsp;for chaos predicts indifference.&nbsp;</em><em>\u2014</em><em>\u2014</em><em>\u2014</em><em>But, ah! no wicked god did intervene!</em></p>\n<p>Thus disproved, my hypothetical implodes in a puff of logic. The world is saved. You're welcome.</p>\n<p>On this line of reasoning, <a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence\">Friendly Artificial Intelligence</a> is not difficult. It's <em>inevitable</em>, provided only that we <em>tell</em>&nbsp;the AI, 'Be Friendly.' If the AI doesn't understand 'Be Friendly.', then it's too dumb to harm us. And if it does understand 'Be Friendly.', then designing it to follow such instructions is childishly easy.</p>\n<p>The end!</p>\n<p>&nbsp;</p>\n<p>...</p>\n<p>&nbsp;</p>\n<p>Is the missing option obvious?</p>\n<p>&nbsp;</p>\n<p>...</p>\n<p>&nbsp;</p>\n<p>What if the AI isn't sadistic, or weak, or stupid, but just <em><strong>doesn't care</strong></em>&nbsp;what you Really Meant by 'I wish for my values to be fulfilled'?</p>\n<p>When we see a <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/LiteralGenie\">Be Careful What You Wish For</a> genie in fiction, it's natural to assume that it's a <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/JackassGenie\">malevolent trickster</a> or an <a href=\"/r/discussion/lw/i1h/the_idiot_savant_ai_isnt_an_idiot/\">incompetent bumbler</a>. But a <em>real </em>Wish Machine wouldn't be a&nbsp;<a href=\"/lw/so/humans_in_funny_suits/\">human in shiny pants</a>. If it paid heed to our verbal commands at all, it would do so in whatever way best fit <a href=\"/lw/h0k/arguing_orthogonality_published_form/\">its own values</a>. Not necessarily the way that best fits ours.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h3 id=\"Is_indirect_indirect_normativity_easy_\">Is indirect indirect normativity easy?</h3>\n<blockquote>\n<p><em><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\">\"If the poor machine could not understand the difference between 'maximize human pleasure' and 'put all humans on an intravenous dopamine drip' then it would also not understand most of the other subtle aspects of the universe, including but not limited to facts/questions like:&nbsp;</span></em><em><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\">'If I put a million amps of current through my logic circuits, I will fry myself to a crisp',</span><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\">&nbsp;</span></em><em><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\">or</span><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\">&nbsp;</span></em><em><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\">'Which end of this Kill-O-Zap Definit-Destruct Megablaster is the end that I'm supposed to point at the other guy?'.&nbsp;</span></em><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\"><em>Dumb AIs, in other words, are not an existential threat.</em> [...]</span></p>\n<p><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\"><em>\"If the AI is (and always has been, during its development) so confused about the world that it interprets the 'maximize human pleasure' motivation in such a twisted, logically inconsistent way, it would never have become powerful in the first place.\"</em></span></p>\n<p><span style=\"font-family: verdana, arial, sans-serif; font-size: 12px; line-height: 15.59375px;\"><span style=\"font-size: small; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small; line-height: normal;\">\u2014</span><span style=\"font-size: small; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><a href=\"http://ieet.org/index.php/IEET/more/loosemore20121128\">Richard Loosemore</a></span></span></p>\n</blockquote>\n<p>If an AI is sufficiently intelligent, then, yes, it should be able to model us well enough to make precise predictions about our behavior. And, yes, something functionally akin to our own <a href=\"http://mind.ucsd.edu/syllabi/06-07/Phil285/readings/true-believers.pdf\">intentional strategy</a> could conceivably turn out to be an efficient way to predict linguistic behavior. The suggestion, then, is that we solve Friendliness by method A&nbsp;\u2014</p>\n<ul>\n<li>A. Solve the <strong>Problem of Meaning-in-General</strong> in advance, and program it to follow our instructions' <em>real</em>&nbsp;meaning. Then just instruct it 'Satisfy my preferences', and wait for it to become smart enough to figure out my preferences.</li>\n</ul>\n<p>\u2014&nbsp;as opposed to B or C&nbsp;\u2014</p>\n<ul>\n<li>B. Solve the <strong>Problem of Preference-in-General</strong>&nbsp;in advance, and directly program it to figure out what our human preferences are and then satisfy them.</li>\n</ul>\n<ul>\n<li>C. Solve the <strong>Problem of Human Preference</strong>, and explicitly program our particular preferences into the AI ourselves, rather than letting the AI discover them for us.</li>\n</ul>\n<p>But there are a host of problems with treating the mere revelation that A is an option&nbsp;as a solution to the Friendliness problem.</p>\n<p style=\"padding-left: 30px;\">1. You have to actually code&nbsp;the seed AI to understand what we mean. You can't just tell it 'Start understanding the True Meaning of my sentences!' to <a href=\"/lw/rs/created_already_in_motion/\">get the ball rolling</a>, because it may not yet be sophisticated enough to grok the True Meaning of 'Start understanding the True Meaning of my sentences!'.</p>\n<p style=\"padding-left: 30px;\">2. The Problem of Meaning-in-General may really be ten thousand heterogeneous problems, especially if 'semantic value' isn't a <a href=\"/lw/o0/where_to_draw_the_boundary/\">natural kind</a>. There may not be a single simple algorithm that inputs any old brain-state and outputs what, if anything, it 'means'; it may instead be that different types of content are encoded very differently.</p>\n<p style=\"padding-left: 30px;\">3. The Problem of Meaning-in-General may subsume&nbsp;the Problem of Preference-in-General. Rather than being able to apply a simple catch-all Translation Machine to any old human concept to output a reliable algorithm for applying that concept in any intelligible situation, we may need to already understand&nbsp;how our beliefs and values work in some detail before we can start generalizing. On the face of it, programming an AI to <em>fully</em> understand 'Be Friendly!' seems at least as difficult as just programming Friendliness into it, but with an added layer of indirection.</p>\n<p style=\"padding-left: 30px;\">4. Even if the Problem of Meaning-in-General has a unitary solution and doesn't subsume Preference-in-General, it may still be harder if semantics is a subtler or more complex phenomenon than ethics. It's not inconceivable that language could turn out to be more of a kludge than value; or more variable across individuals due to its evolutionary recency; or more complexly bound up with culture.</p>\n<p style=\"padding-left: 30px;\">5. Even if Meaning-in-General is easier than Preference-in-General, it may still be extraordinarily difficult. The meanings of human sentences can't be fully captured in any simple string of necessary and sufficient conditions. '<a href=\"/lw/8ms/review_of_machery_doing_without_concepts/\">Concepts</a>'&nbsp;are just especially context-insensitive bodies of knowledge; we should not expect them to be uniquely reflectively consistent, transtemporally stable, discrete, easily-identified, or introspectively obvious.</p>\n<p style=\"padding-left: 30px;\">6. It's clear that building stable preferences out of B or C would create a Friendly AI. It's not clear that the same is true for A. Even if the seed AI understands our commands, the 'do' part of 'do what you're told' leaves a lot of dangerous wiggle room. See section 2 of <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Yudkowsky's reply to Holden</a>. If the AGI doesn't already understand and care about human value, then it may misunderstand (or <em>misvalue</em>) the component of responsible request- or question-answering that depends on speakers' implicit goals and intentions.</p>\n<p style=\"padding-left: 30px;\">7. You can't appeal to a superintelligence to tell you what code to first build it with.</p>\n<p>The point isn't that the Problem of Preference-in-General is unambiguously&nbsp;the ideal angle of attack. It's that the linguistic competence of an AGI&nbsp;<em>isn't </em>unambiguously the right target, and also isn't&nbsp;<em>easy </em>or <em>solved</em>.</p>\n<p>Point 7 seems to be a special source of confusion here, so I feel I should say more about it.</p>\n<p>&nbsp;</p>\n<h3 id=\"The_AI_s_trajectory_of_self_modification_has_to_come_from_somewhere_\">The AI's trajectory of self-modification has to come from somewhere.</h3>\n<blockquote>\n<p><em>\"If the AI doesn't know that you really mean 'make paperclips without killing anyone', that's not a realistic scenario for AIs at all--the AI is superintelligent; it has to know. If the AI knows what you really mean, then you can fix this by programming the AI to 'make paperclips in the way that I mean'.\"</em></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span>\u2014<a href=\"/lw/ld/the_hidden_complexity_of_wishes/9nlh\">Jiro</a></p>\n</blockquote>\n<p>The genie \u2014&nbsp;if it bothers to even consider the question \u2014&nbsp;should be able to understand what you mean by 'I wish for my values to be fulfilled.' Indeed, it should understand your meaning better than <em>you</em>&nbsp;do. But superintelligence only implies that the genie's <em><a href=\"http://wiki.lesswrong.com/wiki/The_map_is_not_the_territory\">map</a></em>&nbsp;can compass your true values. Superintelligence doesn't&nbsp;imply that the genie's <em>utility function</em>&nbsp;has terminal values pinned to your True Values, or to the True Meaning of your commands.</p>\n<p>The critical mistake here is to not distinguish the <a href=\"http://wiki.lesswrong.com/wiki/Seed_AI\"><strong>seed AI</strong></a>&nbsp;we initially program from the <strong>superintelligent wish-granter</strong> it self-modifies to <em>become</em>. We can't use the genius of the superintelligence to tell us how to program its own seed to become the sort of superintelligence that tells us how to build the right seed. <a href=\"/lw/fok/causal_universes/\">Time</a> doesn't work that way.</p>\n<p>We can delegate most&nbsp;problems to the FAI. But the one problem we can't safely delegate is the <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">problem</a> of coding the seed AI to produce <em>the sort of superintelligence to which a task can be safely delegated</em>.</p>\n<p>When you write the seed's utility function, <em>you</em>, the programmer,&nbsp;don't understand everything about the nature of human value or meaning. That imperfect understanding <em>remains</em>&nbsp;the causal basis of the fully-grown superintelligence's actions, <em>long after</em> it's become smart enough to fully understand our values.</p>\n<p>Why&nbsp;is the superintelligence, if it's so clever, stuck with whatever meta-ethically dumb-as-dirt utility function we gave it at the outset? Why can't we just pass the fully-grown superintelligence the buck by instilling in the seed the instruction: 'When you're smart enough to understand Friendliness Theory, ditch the values you started with and just self-modify to become Friendly.'?</p>\n<p>Because that sentence has to <em>actually be coded in</em>&nbsp;to the AI, and when we do so, there's no <a href=\"/lw/rf/ghosts_in_the_machine/\">ghost in the machine</a> to know exactly what we mean by '<span style=\"text-decoration: underline;\">frend-lee-ness thee-ree</span>'. Instead, <em>we</em>&nbsp;have to give it criteria <em>we</em>&nbsp;think are good indicators of Friendliness, so it'll know what to self-modify toward. And if one of the landmarks on our 'frend-lee-ness' road map is <a href=\"/lw/y3/value_is_fragile/\">a bit off</a>, we lose the world.</p>\n<p><strong>Yes</strong>, the UFAI will be able to solve Friendliness Theory. But if we haven't already solved it on our own power, we can't <em>pinpoint</em>&nbsp;Friendliness in advance, out of the space of utility functions. And if we can't pinpoint it with enough detail to draw a road map to it and it alone, we can't program&nbsp;the AI to <em>care</em>&nbsp;about conforming itself with that particular idiosyncratic algorithm.</p>\n<p><strong>Yes</strong>, the UFAI will be able to self-modify to become Friendly, if it so wishes. But if there is no seed of Friendliness already at the heart of the AI's decision criteria, no argument or discovery will <a href=\"/lw/rn/no_universally_compelling_arguments/\">spontaneously change its heart</a>.</p>\n<p>And, <strong>yes</strong>, the UFAI will be able to simulate humans accurately enough to know that its own programmers would wish, if they knew the UFAI's misdeeds, that they had programmed the seed differently. But what's done is done. Unless we ourselves figure out how to program the AI to&nbsp;terminally value&nbsp;its programmers' True Intentions, the UFAI will just shrug at its creators' foolishness and carry on converting the Virgo Supercluster's available energy into <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">paperclips</a>.</p>\n<p>And if we <em>do&nbsp;</em>discover the <em>specific lines of code</em>&nbsp;that will get an AI to perfectly care about its programmer's True Intentions, such that it reliably self-modifies to better fit them \u2014&nbsp;well, then that will just mean that we've solved Friendliness Theory. The clever hack that makes further Friendliness research unnecessary&nbsp;<em>is</em>&nbsp;Friendliness.</p>\n<p>&nbsp;</p>\n<h3 id=\"Not_all_small_targets_are_alike_\">Not all small targets are alike.</h3>\n<p><span style=\"font-size: small;\">Intelligence on its own does not imply Friendliness. And there are three big reasons to think that AGI may arrive before Friendliness Theory is solved:</span></p>\n<p style=\"padding-left: 30px;\"><strong>(i) Research Inertia.</strong>&nbsp;Far more people are working on AGI than on Friendliness. And there may not come a moment when researchers will <a href=\"/lw/hp5/after_critical_event_w_happens_they_still_wont/\">suddenly realize</a> that they need to take all their resources out of AGI and pour them into Friendliness. If the status quo continues, the default expectation should be UFAI.</p>\n<p style=\"padding-left: 30px;\"><strong>(ii) Disjunctive Instrumental Value.</strong>&nbsp;Being more intelligent \u2014 that is, better able to <a href=\"/lw/vb/efficient_crossdomain_optimization/\">manipulate diverse environments</a> \u2014 is of instrumental value to nearly every goal. Being Friendly is of instrumental value to barely any&nbsp;goals. This makes it more likely by default that short-sighted humans will be interested in building AGI than in developing Friendliness Theory. And it<em>&nbsp;</em>makes it much likelier that an <em>attempt</em>&nbsp;at Friendly AGI that has a slightly defective goal architecture will retain the instrumental value of intelligence than of Friendliness.</p>\n<p style=\"padding-left: 30px;\"><strong>(iii) Incremental Approachability.</strong>&nbsp;Friendliness is an all-or-nothing target. Value is <a href=\"http://intelligence.org/files/ComplexValues.pdf\">fragile and complex</a>, and a half-good being editing its morality drive is at least as likely to move toward 40% goodness as 60%. Cross-domain efficiency, in contrast, is <em>not</em>&nbsp;an all-or-nothing target. If you just make the AGI <em>slightly</em>&nbsp;better than a human at improving the efficiency of AGI, then this can snowball into ever-improving efficiency, even if the beginnings were clumsy and imperfect. It's easy to put a reasoning machine into a feedback loop with reality in which it is differentially rewarded for being smarter; it's hard to put one into a feedback loop with reality in which it is differentially rewarded for picking increasingly correct answers to ethical dilemmas.</p>\n<p>The ability to productively rewrite software and the ability to perfectly extrapolate humanity's True Preferences are two different&nbsp;skills. (For example, humans&nbsp;have the former capacity, and not the latter. Most humans, given unlimited power, would be unintentionally Unfriendly.)</p>\n<p>It's true that a sufficiently advanced superintelligence should be able to acquire both abilities. But <em>we</em>&nbsp;don't have them both, and <em>a pre-<a href=\"http://yudkowsky.net/singularity/schools\">FOOM</a> self-improving AGI</em>&nbsp;('seed') need not have both. Being able to program good programmers is all that's required for an intelligence explosion; but being a good programmer <em>doesn't</em>&nbsp;imply that one is a superlative moral psychologist or moral philosopher.</p>\n<p>So, once again, we run into the problem: <strong>The seed isn't the superintelligence.</strong> If the programmers don't know in mathematical detail what Friendly code would even <em>look like</em>, then the seed won't be built to <em>want</em>&nbsp;to build toward the right code. And if the seed isn't built to <em>want</em>&nbsp;to self-modify toward Friendliness, then the superintelligence it sprouts <em>also</em>&nbsp;won't have that preference, <em>even though</em>&nbsp;\u2014 unlike the seed and its programmers \u2014 the superintelligence <em>does</em>&nbsp;have the domain-general 'hit whatever target I want' ability that makes Friendliness easy.</p>\n<p>And that's why some people are worried.</p>", "sections": [{"title": "Is indirect indirect normativity easy?", "anchor": "Is_indirect_indirect_normativity_easy_", "level": 1}, {"title": "The AI's trajectory of self-modification has to come from somewhere.", "anchor": "The_AI_s_trajectory_of_self_modification_has_to_come_from_somewhere_", "level": 1}, {"title": "Not all small targets are alike.", "anchor": "Not_all_small_targets_are_alike_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "494 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 519, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4ARaTpNX62uaL86j6", "cnYHFNBF3kZEyx24v", "fg9fXrHpeaDD6pEPL", "iqBaHP38uRdHzxhPP", "Zkzzjg3h7hW5Z36hK", "AJ3aP8iWxr6NaKi6j", "CuSTqHgeK4CMpWYTe", "d5NyJ2Lf6N22AD9PB", "rKL4aecWLNBpvfW5Q", "sizjfDgCgAsuLJQmm", "o5F2p3krzT4JgzqQc", "GNnHHmm8EzePmKzPk", "PtoQdG7E8MxYJrigu", "LNKh22Crr5ujT85YM", "yLeEPFnnB9wE7KLx2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-09-06T06:42:38.780Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-06T13:42:31.295Z", "modifiedAt": null, "url": null, "title": "Are certain types of loyalty categorically unreasonable? ", "slug": "are-certain-types-of-loyalty-categorically-unreasonable", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:07.085Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Torello", "createdAt": "2013-07-01T17:38:37.441Z", "isAdmin": false, "displayName": "Torello"}, "userId": "xoRpeFN7K5MgDRcvM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/brbD4zJEDdaNcgfuM/are-certain-types-of-loyalty-categorically-unreasonable", "pageUrlRelative": "/posts/brbD4zJEDdaNcgfuM/are-certain-types-of-loyalty-categorically-unreasonable", "linkUrl": "https://www.lesswrong.com/posts/brbD4zJEDdaNcgfuM/are-certain-types-of-loyalty-categorically-unreasonable", "postedAtFormatted": "Friday, September 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20certain%20types%20of%20loyalty%20categorically%20unreasonable%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20certain%20types%20of%20loyalty%20categorically%20unreasonable%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbrbD4zJEDdaNcgfuM%2Fare-certain-types-of-loyalty-categorically-unreasonable%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20certain%20types%20of%20loyalty%20categorically%20unreasonable%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbrbD4zJEDdaNcgfuM%2Fare-certain-types-of-loyalty-categorically-unreasonable", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbrbD4zJEDdaNcgfuM%2Fare-certain-types-of-loyalty-categorically-unreasonable", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 196, "htmlBody": "<p>I want people to express their opinions on the following questions/idea.&nbsp; I guess the answer will depend on your personal definition/interpretation of loyalty, but I guess that's the point, I want to see what people think about what I consider to be an important topic.&nbsp;</p>\n<p>&nbsp;</p>\n<p>It seems like there is a spectrum of loyalty.&nbsp;</p>\n<p>Maybe on the left end we have \"transactional\" loyalty; as in the case of, \"if my spouse doesn't cheat, I will stay with them.\"&nbsp;</p>\n<p>Maybe on the right end we have \"stubborn\" or \"cost-independent\" loyalty; as in the case of, \"if a member of my military unit dies, my unit will recover the body - burning through all available resources if necessary.\" &nbsp;</p>\n<p>&nbsp;</p>\n<p>In practice, does \"stubborn\" loyalty exist?&nbsp; Is it desirable?&nbsp; What about when it comes to beliefs?&nbsp; Does it allow for institutions to flourish that couldn't otherwise?&nbsp; Is this type of loyalty categorically unreasonable? &nbsp; &nbsp;</p>\n<p>&nbsp;</p>\n<p>I guess these all seem like prisoner's dilemma questions, about \"tit for tat\" strategies, etc, but I want to know how people on this site think about these matters in everyday life; in personal relationships, with regards to their own beliefs, especially toward their children (if you have children).&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "brbD4zJEDdaNcgfuM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -8, "extendedScore": null, "score": 1.3306059964553113e-06, "legacy": true, "legacyId": "24042", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-06T16:11:42.077Z", "modifiedAt": null, "url": null, "title": "New LW Meetup: Boulder CO", "slug": "new-lw-meetup-boulder-co", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yS9HZ4xAspsie8gGc/new-lw-meetup-boulder-co", "pageUrlRelative": "/posts/yS9HZ4xAspsie8gGc/new-lw-meetup-boulder-co", "linkUrl": "https://www.lesswrong.com/posts/yS9HZ4xAspsie8gGc/new-lw-meetup-boulder-co", "postedAtFormatted": "Friday, September 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetup%3A%20Boulder%20CO&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetup%3A%20Boulder%20CO%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyS9HZ4xAspsie8gGc%2Fnew-lw-meetup-boulder-co%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetup%3A%20Boulder%20CO%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyS9HZ4xAspsie8gGc%2Fnew-lw-meetup-boulder-co", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyS9HZ4xAspsie8gGc%2Fnew-lw-meetup-boulder-co", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 563, "htmlBody": "<p><strong>This summary was posted to LW Main on August 31st. The following week's summary is <a href=\"/lw/ijv/new_lw_meetup_phoenix/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/q0\"></a><a href=\"/meetups/qb\">New Meetup: Boulder CO:&nbsp;<span class=\"date\">03 September 2013 07:03PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/qc\">Arizona State Lunch Group:&nbsp;<span class=\"date\">03 September 2013 12:00PM</span></a></li>\n<li><a href=\"/meetups/q4\">Helsinki Meetup:&nbsp;<span class=\"date\">08 September 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/pn\">LessWrong Israel September meetup:&nbsp;<span class=\"date\">12 September 2013 08:00PM</span></a></li>\n<li><a href=\"/meetups/qf\">Moscow: The Sunday Meetup:&nbsp;<span class=\"date\">01 September 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/qa\">Saskatoon Meetup: Value of Information:&nbsp;<span class=\"date\">31 August 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/q9\">Urbana-Champaign, Illinois:&nbsp;<span class=\"date\">01 September 2013 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\"> Austin, TX:&nbsp;<span class=\"date\">31 August 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/q7\">[London] Comfort Zone Expansion outing - London:&nbsp;<span class=\"date\">01 September 2013 11:00AM</span></a></li>\n<li><a href=\"/meetups/qd\">Melbourne Practical Rationality: Group Prediction Calibration and Aumann's Agreement Theorem:&nbsp;<span class=\"date\">06 September 2013 06:30PM</span></a></li>\n<li><a href=\"/meetups/qe\">[Salt Lake City] Fall Equinox: Festival of Heroes:&nbsp;<span class=\"date\">21 September 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/px\">[Washington DC] Robin Hanson visits to talk about prediction markets:&nbsp;<span class=\"date\">08 September 2013 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yS9HZ4xAspsie8gGc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.3307359017267989e-06, "legacy": true, "legacyId": "23947", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tudnA3p35qp6fxzgu", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-06T20:04:18.560Z", "modifiedAt": null, "url": null, "title": "I know when the Singularity will occur", "slug": "i-know-when-the-singularity-will-occur", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:30.762Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zQWDcY7SeJMi26hy2/i-know-when-the-singularity-will-occur", "pageUrlRelative": "/posts/zQWDcY7SeJMi26hy2/i-know-when-the-singularity-will-occur", "linkUrl": "https://www.lesswrong.com/posts/zQWDcY7SeJMi26hy2/i-know-when-the-singularity-will-occur", "postedAtFormatted": "Friday, September 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20know%20when%20the%20Singularity%20will%20occur&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20know%20when%20the%20Singularity%20will%20occur%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzQWDcY7SeJMi26hy2%2Fi-know-when-the-singularity-will-occur%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20know%20when%20the%20Singularity%20will%20occur%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzQWDcY7SeJMi26hy2%2Fi-know-when-the-singularity-will-occur", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzQWDcY7SeJMi26hy2%2Fi-know-when-the-singularity-will-occur", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1103, "htmlBody": "<p>More precisely, if we suppose that sometime in the next 30 years, an artificial intelligence will begin bootstrapping its own code and explode into a super-intelligence, I can give you 2.3 bits of further information on when the Singularity will occur.</p>\n<p>Between midnight and 5 AM, Pacific Standard Time.</p>\n<p><a id=\"more\"></a></p>\n<p>Why? Well, first, let's just admit this: The race to win the Singularity is over, and Google has won. They have the world's greatest computational capacity, the most expertise in massively distributed processing, the greatest collection of minds interested in and capable of work on AI, the largest store of online data, the largest store of personal data, and the largest library of scanned, computer-readable books. That includes textbooks. Like, all of them. All they have to do is subscribe to Springer-Verlag's online journals, and they'll have the entire collected knowledge of humanity in computer-readable format. They almost certainly have the biggest research budget for natural language processing with which to interpret all those things. They have two of the four smartest executives in Silicon Valley.<sup>1</sup> Their corporate strategy for the past 15 years can be approximated as \"Win the Singularity.\"<sup>2</sup> If someone gave you a billion dollars today to begin your attempt, you'd still be 15 years and about two-hundred and ninety-nine billion dollars behind Google. If you believe in a circa-2030 Singularity, there isn't enough time left for anybody to catch up with them.</p>\n<p>(And I'm okay with that, considering that the other contenders include Microsoft and the NSA. But it alarms me that Google hasn't gone into bioinformatics or neuroscience. Apparently their plans don't include humans.)</p>\n<p>So the first bootstrapping AI will be created at Google. It will be designed to use Google's massive distributed server system. And they will run it between midnight and 5AM Pacific time, when the <a href=\"http://www.internettrafficreport.com/#graphs\">load on those servers is smallest</a>.</p>\n<p>A more important implication is that this scenario decreases the possibility of FOOM. The AI will be designed to run on the computational resources available to Google, and they'll build and test it as soon as they think <em>that</em> is just enough computational power for it to run. That means that its minimum computational requirements will be within one or two orders of magnitude of that of&nbsp;<em>all the computers on Earth</em>. (We don't know how many servers Google has, but we know they <a href=\"http://www.wired.com/wiredenterprise/2012/10/ff-inside-google-data-center/all/\">installed their one millionth server on July 9, 2008</a>. Google may&mdash;<em>may</em>&mdash;own less than 1% of the world's CPU power, but connectivity within its system is vastly superior to that between other internet servers, let alone a botnet of random compromised PCs.)</p>\n<p>So when the AI breaks out of the computational grid composed of all the Google data centers in the world, into the \"vast wide world of the Internet\", it's going to be very disappointed.</p>\n<p>Of course, the distribution of computational power will change before then. Widespread teraflop GPU graphics cards could change this scenario completely in the next ten years.</p>\n<p>In which case Google might take a sudden interest in GPUs...</p>\n<p>&nbsp;</p>\n<p>ADDED:</p>\n<p>Do I really believe all that? No. I do believe \"Google wins\" is a likely scenario&mdash;more likely than \"X wins\" for any other single value of X. Perhaps more importantly, you need to factor the size of the first AI built into your FOOM-speed probability distribution, because if the first AI is built by a large organization, with a lot of funding, that changes the FOOM paths open to it.</p>\n<p>AI FOOMs if it can improve its own intelligence in one way or another. The people who build the first AI will make its algorithms as efficient as they are able to. For the AI to make itself more intelligent by scaling, it has to get more resources, while to make itself more intelligent by algorithm redesign, it will have to be smarter than the smartest humans who work on AI. The former is trivial for an AI built in a basement, but severely limited for an AI brought to life at the direction of Page and Brin.</p>\n<p>The first \"human-level\" AI will probably be roughly as smart as a human, because people will try to build them before they can reach that level, the distribution of effectiveness-of-attempted-AIs will be skewed hard left, with many failures before the first success, and the first success will be a marginal improvement over a previous failure. That means the first AI will have about the same effective intelligence, regardless of how it's built.</p>\n<p>As smart as \"a human\" is closer to \"some human\" than to \"all humans\". The first AI will almost certainly be at most as intelligent as the average human, and considerably less intelligent than its designers. But for an AI to make itself smarter through algorithm improvement requires the AI to have more intelligence than the smartest humans working on AI (the ones who just built it).</p>\n<p>The easier, more-likely AI-foom path is: Build an AI as smart as a chimp. That AI grabs (or is given) orders of magnitude of resources, and gets smarter simply by brute force. THEN it redesigns itself.</p>\n<p>That scaling-foom path is harder for AIs that start big than AIs that start small. This means that the probability distribution for FOOM speed depends on the probability distribution for the amount of dollars that will be spent to build the first AI.</p>\n<p>Remember you are Bayesians. Your objective is not to accept or reject the hypothesis that the first AI will be developed according to this scenario. Your objective is to consider whether these ideas change the probability distribution you assign to FOOM speed.</p>\n<p>The question I hope you'll ask yourself now is not, \"Won't data centers in Asia outnumber those in America by then?\", nor, \"Isn't X smarter than Larry Page?\", but, \"What is the probability distribution over &lt;capital investment that will produce the first average-human-level AI&gt;?\" I expect that the probabilities will be dominated by large investments, because the probability distribution over \"capital investment that will produce the first X\" appears to me to be dominated in recent decades by large investments, for similarly-ambitious X such as \"spaceflight to the moon\" or \"sequence of the human genome\". A very clever person could have invented low-cost genome sequencing in the 1990s and sequenced the genome him/herself. But no very clever person did.</p>\n<hr />\n<p>&nbsp;</p>\n<p>1. I'm counting Elon Musk and Peter Thiel as the others.</p>\n<p>2. This doesn't need to be intentional. Trying to dominate information search should look about the same as trying to win the Singularity. Think of it as a long chess game in which Brin and Page keep making good moves that strengthen their position. Eventually they'll look around and find they're in a position to checkmate the world.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zQWDcY7SeJMi26hy2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 52, "baseScore": -16, "extendedScore": null, "score": 1.3309385009670026e-06, "legacy": true, "legacyId": "24027", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-07T15:22:04.693Z", "modifiedAt": null, "url": null, "title": "Meetup : London social meetup", "slug": "meetup-london-social-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nQqtoSteWn3tFTMzY/meetup-london-social-meetup", "pageUrlRelative": "/posts/nQqtoSteWn3tFTMzY/meetup-london-social-meetup", "linkUrl": "https://www.lesswrong.com/posts/nQqtoSteWn3tFTMzY/meetup-london-social-meetup", "postedAtFormatted": "Saturday, September 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnQqtoSteWn3tFTMzY%2Fmeetup-london-social-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnQqtoSteWn3tFTMzY%2Fmeetup-london-social-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnQqtoSteWn3tFTMzY%2Fmeetup-london-social-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qo'>London social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 September 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shakespeare's Head, 64-68 Kingsway, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Weather is predicted to be thoroughly rainy for next Sunday, so let's retreat back to the safety of the Shakespeare's Head, near Holborn station. With no more updates of HP:MoR until October at the earliest, what better way to get your fix of LessWrongy goodness?</p>\n\n<p>See you there!</p>\n\n<p>Facebook: https://www.facebook.com/events/194188274096175/</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qo'>London social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nQqtoSteWn3tFTMzY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.3319477178121927e-06, "legacy": true, "legacyId": "24059", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/qo\">London social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 September 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shakespeare's Head, 64-68 Kingsway, WC2B 6BG</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Weather is predicted to be thoroughly rainy for next Sunday, so let's retreat back to the safety of the Shakespeare's Head, near Holborn station. With no more updates of HP:MoR until October at the earliest, what better way to get your fix of LessWrongy goodness?</p>\n\n<p>See you there!</p>\n\n<p>Facebook: https://www.facebook.com/events/194188274096175/</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/qo\">London social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : London social meetup", "anchor": "Discussion_article_for_the_meetup___London_social_meetup", "level": 1}, {"title": "Discussion article for the meetup : London social meetup", "anchor": "Discussion_article_for_the_meetup___London_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-08T04:42:53.184Z", "modifiedAt": null, "url": null, "title": "Terminology point rationality vs rationalism.", "slug": "terminology-point-rationality-vs-rationalism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:03.117Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "beoShaffer", "createdAt": "2011-05-29T15:52:29.240Z", "isAdmin": false, "displayName": "beoShaffer"}, "userId": "589WwYp3jytZqATFL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3zyjgpmi9jhxrYEbx/terminology-point-rationality-vs-rationalism", "pageUrlRelative": "/posts/3zyjgpmi9jhxrYEbx/terminology-point-rationality-vs-rationalism", "linkUrl": "https://www.lesswrong.com/posts/3zyjgpmi9jhxrYEbx/terminology-point-rationality-vs-rationalism", "postedAtFormatted": "Sunday, September 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Terminology%20point%20rationality%20vs%20rationalism.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATerminology%20point%20rationality%20vs%20rationalism.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3zyjgpmi9jhxrYEbx%2Fterminology-point-rationality-vs-rationalism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Terminology%20point%20rationality%20vs%20rationalism.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3zyjgpmi9jhxrYEbx%2Fterminology-point-rationality-vs-rationalism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3zyjgpmi9jhxrYEbx%2Fterminology-point-rationality-vs-rationalism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<blockquote>\n<p><span style=\"font-size: 13px; line-height: 19px; font-family: sans-serif; \">&nbsp;Rationalism should not be confused with&nbsp;<a style=\"text-decoration: none; color: #0645ad; background-image: none; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: initial; background-position: initial initial; background-repeat: initial initial; \" title=\"Rationality\" href=\"http://en.wikipedia.org/wiki/Rationality\">rationality</a>, nor with&nbsp;<a class=\"mw-redirect\" style=\"text-decoration: none; color: #0645ad; background-image: none; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: initial; background-position: initial initial; background-repeat: initial initial; \" title=\"Rationalization (disambiguation)\" href=\"http://en.wikipedia.org/wiki/Rationalization_(disambiguation)\">rationalization</a>.</span></p>\n</blockquote>\n<p><span style=\"font-size: 13px; line-height: 19px; font-family: sans-serif; \">-<a href=\"http://en.wikipedia.org/wiki/Rationalism\">Wikipedia article on rationalism&nbsp;</a></span></p>\n<p>I frequently see people using rationalism in place of rationality. &nbsp;Usually other commenters understand them, however I believe that using the word rationality is superior. &nbsp;The Less Wrong tag line is \"A community blog devoted to refining the art of human <strong>rationality</strong>\". &nbsp;On the other hand,&nbsp;rationalism&nbsp;is the philosophical term for a very different <a href=\"http://plato.stanford.edu/entries/rationalism-empiricism/\">epistemological position</a>. Furthermore, -the -ism suffix has some <a href=\"/lw/3rd/note_on_terminology_rationality_not_rationalism/\">undesirable connotations</a>. &nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3zyjgpmi9jhxrYEbx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 2, "extendedScore": null, "score": 1.3326465679117778e-06, "legacy": true, "legacyId": "24062", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EbELiWfdE449DWYAk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-08T06:13:35.794Z", "modifiedAt": null, "url": null, "title": "Fiction: Written on the Body as love versus reason", "slug": "fiction-written-on-the-body-as-love-versus-reason", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:32.631Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3SSYgvct5ePjQ46p2/fiction-written-on-the-body-as-love-versus-reason", "pageUrlRelative": "/posts/3SSYgvct5ePjQ46p2/fiction-written-on-the-body-as-love-versus-reason", "linkUrl": "https://www.lesswrong.com/posts/3SSYgvct5ePjQ46p2/fiction-written-on-the-body-as-love-versus-reason", "postedAtFormatted": "Sunday, September 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fiction%3A%20Written%20on%20the%20Body%20as%20love%20versus%20reason&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFiction%3A%20Written%20on%20the%20Body%20as%20love%20versus%20reason%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3SSYgvct5ePjQ46p2%2Ffiction-written-on-the-body-as-love-versus-reason%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fiction%3A%20Written%20on%20the%20Body%20as%20love%20versus%20reason%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3SSYgvct5ePjQ46p2%2Ffiction-written-on-the-body-as-love-versus-reason", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3SSYgvct5ePjQ46p2%2Ffiction-written-on-the-body-as-love-versus-reason", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2004, "htmlBody": "<p>In 1992, Jeanette Winterson, one of the hottest young authors of the early 1990s, published <em>Written on the Body. </em>Critics loved it, but none of them seem to have picked up on what I thought the book was about: The question of whether reason in love is good for you.</p>\n<p><a id=\"more\"></a></p>\n<p>It isn't meant as a universal treatement. It's one person's problem: The narrator keeps falling in love the same way, leading through the same patterns of behavior. Self-deception makes it exciting; the gradual dawning of self-awareness makes it intolerable yet doesn't help stop the cycle.</p>\n<p>The book's main character is never named, and his/her gender never identified. (I'm going to use \"ze, zer\", from here on, because \"him/her\" just isn't working.) This seems like a pretentious trick at first, but it's part of the novel's purpose to strip love of its clich&eacute;s, associations, and roles, and report its movements and actions accurately, like a war correspondent. The narrator sometimes speaks in third person, sometimes in second; and this also has a purpose which is revealed at the end. There's no way to do this without spoilers, so I'll go ahead and tell you that the narrator talks to zer final passionate love in third person when reporting what happened, and in second person when only imagining her there in zer final madness.</p>\n<p>The narrator is an aging Lothario, a serial philanderer, who moves from one lover to another, married and unmarried, but usually married. It's married women who understand zer&mdash;they, like the narrator, have grown bored or desperate in their comfortable marriages whose flames have died down.</p>\n<blockquote>\n<p>Her husband lies over her like a tarpaulin. He wades into her as though she were a bog. She loves him and he loves her. They're still married aren't they?</p>\n</blockquote>\n<p>The only difference is that they can make it last 10 years, while for our narrator, passion seldom lasts past six months.</p>\n<p>The narrator has carried on in this way long enough to realize what's happening, to recognize the same lame excuses and clich&eacute;s trotted out at the opening and closing of every new relationship. This unwelcome self-awareness intrudes on zer script, making zer stumble and flub the lines. Finally recognizing that they are lines is what makes everything terribly hard that used to happen naturally and easily:</p>\n<blockquote>\n<p>You had no choice, you were swept away. Forces took you and possessed you and you did it but now that's all in the past, you can't understand etc. etc. You want to start again etc. etc. Forgive me. In the late 20th century we still look to ancient daemons to explain our commonest action. Adultery is very common. It has no rarity value and yet at an individual level it is explained away again and again as a UFO. I can't lie to myself in quite that way anymore.</p>\n</blockquote>\n<p>The intrusion of reason on love, the narrator seems to believe, is the source of zer confusion.</p>\n<blockquote>\n<p>I went to look at my sunflowers, growing steadily, sure that the sun would be there for them, fulfilling themselves in the proper way at the proper time. Very few people ever manage what nature manages without effort and mostly without fail. We don't know who we are or how to function, much less how to bloom. Blind nature. Homo sapiens. Who's kidding whom?</p>\n</blockquote>\n<blockquote>\n<p>I barged my way through a herd of cattle, hooves braceleted with mud. The cows reserved for me the incredulous look that animals give humans in the country. We seem so silly, not a part of nature at all.</p>\n</blockquote>\n<p>The narrator is once again convinced that now it is time to settle down, and have a true and lasting love. Ze settles down with a woman named Jacqueline who does not incite passion, of whom ze says,</p>\n<blockquote>\n<p>Jacqueline was an overcoat. She muffled my senses. With her I forgot about feeling and wallowed in contentment. Contentment is a feeling you say? Are you sure it's not an absence of feeling? I liken it to that particular numbness one gets after a visit to the dentist. Not in pain nor out of it, slightly drugged. Contentment is the positive side of resignation. It has its appeal but it's no good wearing an overcoat and furry slippers and heavy gloves when what the body really wants is to be naked.</p>\n</blockquote>\n<p>The narrator, as always, meets another married woman, Louise, who seizes zer imagination. Ze knows exactly what is happening and where it will end, and tries to be reasonable&mdash;here ze has zer first genuine chance at \"happiness\", as people name it, with Jacqueline, and knows perfectly well that the passion ze now feels for Louise won't last, but can't reason zerself into feeling any differently.</p>\n<blockquote>\n<p>I phoned a friend whose advice was to play the sailor and run a wife in every port. If I told Jacqueline and ruin everything and for what? If I told Jacqueline at her for beyond healing and did I have that right? Probably I had nothing more than dog fever for two weeks and I can get it out of my system and come home to my kennel.</p>\n<p>Good sense. Common sense. Good dog.</p>\n<p>What does it say in the tea leaves? Nothing but a capital L.</p>\n</blockquote>\n<p>And ze dives in as always, following zer usual script:</p>\n<blockquote>\n<p>\"So you admit that I'm just a scalp on your bedpost?\"</p>\n<p>I was angry and bewildered. \"Louise, I don't know what you are. I turned myself inside out to try and avoid what happened today. You affect me in ways I can't quantify or contain. All I can measures the effect, and the effect is that I'm out of control.\"</p>\n<p>\"So you try and regain control by telling me you love me. That's a territory you know isn't it? That's romance and courtship and whirlwind.\"</p>\n<p>\"I don't want control.\"</p>\n<p>\"I don't believe you.\"</p>\n<p>No and you're right not to believe me. If in doubt be sincere. That's a pretty little trick of mine.</p>\n</blockquote>\n<p>Ze breaks the news to Jacqueline:</p>\n<blockquote>\n<p>\"Are you seeing her?\" Jacqueline's timid voice.</p>\n<p>I mumbled something about yes as usual but things had changed. THINGS HAD CHANGED, what an arsehole comment, I'd changed things. Things don't change, they're not like the seasons moving on a diurnal round. People change things. There are victims of change but not victims of things. Why do I collude in this misuse of language? I can't make it easier for Jacqueline however I put it. I can make it a bit easier for me and I suppose that's what I'm doing.</p>\n<p>\"I'm not running around again, Jacqueline.\"</p>\n<p>\"What are you doing then?\"</p>\n<p>Good point. Would that I had the overseeing spirit to interpret my actions in plain English. I would like to come to you with all the confidence of a computer programmer, sure that we could find the answers if only we asked the proper questions. Why aren't I going according to plan? How stupid it sounds to say I don't know and shrug and behave like every other idiot who's fallen in love and can't explain it. I've had a lot of practice, I should be able to explain it. The only word I can think of is Louise.</p>\n</blockquote>\n<p>And then the unthinkable happens: Ze has zer torrid affair with Louise, and then she departs from the script and the narrator realizes how deeply ze's been fooling zerself all zer life:</p>\n<blockquote>\n<p>You said, \"I'm going to leave.\"</p>\n<p>I thought, yes, of course you are, you're going back to the shell. I'm an idiot. I've done it again and I said I'd never do it again.</p>\n<p>You said, \"I told him before we came away. I told him I won't change my mind even if you change yours.\"</p>\n<p>This is the wrong script. This is the moment where I'm supposed to be self-righteous and angry. This is the moment where you're supposed to flood with tears and tell me how hard it is to say these things and what can you do and what can you do and will I hate you and yes you know I'll hate you and there are no question marks in this speech because it's a fait accompli.</p>\n<p>But you are gazing at me the way God gazed at Adam and I am embarrassed by your look of love and possession and pride. I want to go now and cover myself with fig leaves. It's a sin this not being ready, this not being up to it.</p>\n</blockquote>\n<p>Louise leaves her husband Elgin for the narrator.</p>\n<blockquote>\n<p>I couldn't apologize to Elgin because I wasn't sorry. Not sorry but ashamed, does that sound strange?</p>\n</blockquote>\n<p>And then the one unexpected thing happens that could break our narrator out of zer endless cycle of love and boredom. Louise gets terminal cancer. Her husband, a cancer specialist, convinces the narrator that he can make her well&mdash;if the narrator casts her out, and she comes back to him. Ze does. Months later, unable to stand the separation, ze confronts Elgin and demands to see Louise. But Elgin thought Louise had left him for zer again. No one knows where she is. No one knows whether she is alive or dead. Our narrator's relationship has been cut off in the passionate stage, with no closure&nbsp;possible. Yet another woman, Gail, appears, another comfortable woman, who understands there is no passion between them but tells zer to be reasonable and settle down by the fire. Our narrator is stuck on Louise, unable to move on, perhaps forever.</p>\n<blockquote>\n<p>A friend of mine said before I left London, \"At least your relationship with Louise didn't fail. It was the perfect romance.\"</p>\n<p>Was it? Is that what perfection costs? Operatic heroics and a tragic end? What about a wasteful end? Most opera ends wastefully. The happy endings are compromises. Is that the choice?</p>\n</blockquote>\n<p>And it seems, at the ending, that this is the choice, and only zer helpless, transfixed state can finally enable zer to \"settle down\" to zer compromise \"happy ending\" with Gail, having the comfortable woman to drink tea and share a bed with, and the passionate affair that can never die in zer memory.</p>\n<blockquote>\n<p>\"You still love her then?\"</p>\n<p>\"With all my heart.\"</p>\n<p>\"What will you do?\"</p>\n<p>\"What can I do? Louise once said, it's the clich&eacute;s that cause the trouble. What do you want to say? That I'll get over it? That's right isn't it? Time is a great deadener.\"</p>\n<p>This is where the story starts... Hurry now, it's getting late. I don't know if this is a happy ending but here we are let loose in open fields.</p>\n</blockquote>\n<p>The interesting question at the end is whether the narrator would've been better off without so much self-awareness, if ze had been able to continue the cycle of love and betrayal until death or wearing zerself out, never having more than animal awareness of the process. Winterson suspends judgement throughout the novel but comes down a little heavy-handed against reason on the final page, making it drive the narrator mad in the end. Free of reason, there would have been&nbsp;at least&nbsp;intervals of self-deluded happiness.</p>\n<p>There's another interpretation. There is no hint that Winterson had this in mind, but the dilemma above is a bit of a cheat, as the narrator is someone who obviously could've benefited from polyamory. Zer loves never allowed that choice. Each of zer loves expected a love triangle to be unstable and eventually demanded a return to the \"normality\" of a single love, even if \"love\" was not quite the word for it. From this perspective, it is the tragedy of a person whose reason was never free, whose downfall was dictated by society's false clich&eacute;s about love and marriage despite the impossibility of reconciling them with zer reality. The lesson is then that reason is best, and instinct will do, but something in-between, instinct plus a crippled reason that takes itself as seriously as if it were the real thing, leads to madness.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3SSYgvct5ePjQ46p2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": -17, "extendedScore": null, "score": 1.3327257698323826e-06, "legacy": true, "legacyId": "24061", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-08T14:18:45.010Z", "modifiedAt": null, "url": null, "title": "Yet More \"Stupid\" Questions", "slug": "yet-more-stupid-questions-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:08.843Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xkQRzrrb2SvxYeuG6/yet-more-stupid-questions-0", "pageUrlRelative": "/posts/xkQRzrrb2SvxYeuG6/yet-more-stupid-questions-0", "linkUrl": "https://www.lesswrong.com/posts/xkQRzrrb2SvxYeuG6/yet-more-stupid-questions-0", "postedAtFormatted": "Sunday, September 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Yet%20More%20%22Stupid%22%20Questions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYet%20More%20%22Stupid%22%20Questions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxkQRzrrb2SvxYeuG6%2Fyet-more-stupid-questions-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Yet%20More%20%22Stupid%22%20Questions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxkQRzrrb2SvxYeuG6%2Fyet-more-stupid-questions-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxkQRzrrb2SvxYeuG6%2Fyet-more-stupid-questions-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is a thread where people can ask questions that they would ordinarily feel embarrassed for not knowing the answer to. The previous&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\"stupid\" questions thread</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;is at almost 500 questions in about a month, so I think it's time for a new one.</span></p>\n<p>Also, I have a new \"stupid\" question.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xkQRzrrb2SvxYeuG6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "24064", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 269, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-08T19:13:35.161Z", "modifiedAt": null, "url": null, "title": "Course selection based on instructor", "slug": "course-selection-based-on-instructor", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:09.264Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VipulNaik", "createdAt": "2013-09-02T18:51:08.862Z", "isAdmin": false, "displayName": "VipulNaik"}, "userId": "t3pZcNZXqhaM5avBE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J7RY9t2ph6Du8JHgD/course-selection-based-on-instructor", "pageUrlRelative": "/posts/J7RY9t2ph6Du8JHgD/course-selection-based-on-instructor", "linkUrl": "https://www.lesswrong.com/posts/J7RY9t2ph6Du8JHgD/course-selection-based-on-instructor", "postedAtFormatted": "Sunday, September 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Course%20selection%20based%20on%20instructor&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACourse%20selection%20based%20on%20instructor%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ7RY9t2ph6Du8JHgD%2Fcourse-selection-based-on-instructor%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Course%20selection%20based%20on%20instructor%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ7RY9t2ph6Du8JHgD%2Fcourse-selection-based-on-instructor", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ7RY9t2ph6Du8JHgD%2Fcourse-selection-based-on-instructor", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3097, "htmlBody": "<p>I've been trying to compile advice on how to select good instructors in college, assuming that the major and course level are given. Of course, many people don't have a lot of flexibility with instructor choice, so this advice has limited applicability. The reason I'm posting to LessWrong is to get feedback from interested LessWrong readers regarding some of the factual assertions I make. I've marked the parts where I'm most eager for feedback. My experience with instruction (direct and indirect) has been focused largely on mathematics. Thus, it's possible that some of the points I raise have limited applicability in more discussion-based subjects.</p>\n<p>Note also that although my experiences have been partly shaped by the undergraduate teaching I've done in four years as a graduate student at the University of Chicago, I'm neither representing the University nor am I critiquing or describing any features specific to the University of Chicago.</p>\n<p>I'm working within the conceptual framework that there are three broad types of value that students derive from college courses:</p>\n<p>&nbsp;</p>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Human_capital\">Human capital</a>: Students acquire <strong>knowledge, skills, and abilities</strong> that help them with further courses, jobs, graduate school, or other aspects of their future life. When choosing between different instructors for courses at the same level in the same college, the human capital differential boils down to how different the instructors are in how well they teach the material.</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Signalling_%28economics%29\">Signaling</a>: that they have learned material, again both for future courses, jobs, graduate school, or other aspects of their future life. When choosing between different instructors for courses at the same level in the same college, the signaling differential boils down to the relative <strong>ease of getting a good grade</strong> (the \"ease\" could stem both from better learning and easier grading). In addition, some instructors may be willing to write <strong>recommendation letters or referral letters</strong> for further courses, jobs, graduate school, scholarships, internships, etc.</li>\n<li><strong>Consumption</strong>: Some courses can be fun to consume. The instructors and fellow students could be entertaining. The homeworks could be challenging in a nice way. Just like a good physical workout can be edifying in addition to building muscle or stamina, so can a good educational experience.</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<h3>Why I think instructor selection matters:</h3>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\">Instructors differ significantly from one another in terms of their quality of teaching (human capital + consumption value) and ease of grading (signaling). <em>I'm curious here about the extent to which LessWrong readers think variation between instructors for similar courses compares to variation between institutions.</em>&nbsp; For instance, how much does variation between instructors at Harvard compare with variation between UCLA and Harvard?</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\">Apart from the teaching and grading, a good instructor is a candidate to write recommendation letters for future courses, internships and scholarships, and graduate school applications. Additionally, a good instructor may recommend options to students.</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\">Students routinely neglect instructor selection or use suboptimal criteria, relative to its importance (<em>is this true?</em>) or relative to other things they fret over, such as college selection. Often, students decide their courses for the next semester or quarter after a meeting of a few hours with an advisor, based on time scheduling constraints. <em>Update</em>: Other students use data such as online evaluations and talking to friends who have taken classes with specific instructors, but may not take proactive steps to collect such data. Further, the criteria that they use to process the data may well be suboptimal (or at any rate, I believe so, which is why I'm writing the post).</p>\n</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0in\">For the discussion below, I largely assume that the student is selecting between different sections of the same course, where each section follows a similar curriculum but the instructor has flexibility in terms of the final examinations and grades.<em> What do LessWrong readers think about how common this is relative to a setup where all sections have the same final examinations and grades?</em> However, much of the advice is general.</p>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<h3>Pitfalls</h3>\n<p>&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\">Students often incorrectly believe that instructors who are harder in terms of the material covered or in terms of the work they assign will be harder in terms of the final letter grades they assign. This is not necessarily true. The overall correlation sign is unclear, but there are teachers in all four quadrants in terms of (hardness of material, hardness of grading).</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\">Looking at the difficulty level of examinations in isolation can paint a misleading picture, because instructors may differ somewhat in the techniques they cover, and this can radically affect the perceived difficulty of a question.</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\">Remember that student evaluations, although more reliable than peer faculty evaluations or other evaluation methods, are still generally unreliable (the <a href=\"http://en.wikipedia.org/wiki/Course_evaluation\">Wikipedia page</a> and <a href=\"http://home.sprynet.com/~owl1/sef.htm\">Mike Huemer's article</a> are great starting points). There is still some value you can extract from student evaluations, but not a lot. The main problem with the numerical part of such evaluations is improper <a href=\"https://en.wikipedia.org/wiki/Anchoring\">anchoring</a> &ndash; students aren't clear on the scale relative to which they are evaluating instructors. As a result, the median numerical rating winds up <a href=\"http://econlog.econlib.org/archives/2005/07/improving_stude.html\">at around 4.4/5, leading to censoring at the top (a ceiling effect)</a>. <em>Update: </em>Since Falenas108 raised this issue in a comment, I know that the long form responses in student evaluations are somewhat more informative than the numerical ratings, but these suffer from many of the same problems. The Huemer article and the references in the Wikipedia article and the Huemer article are good starting points. Further, optional online evaluations (which are the only ones that are easily available) often suffer from both low response rates and a selection bias in the set of respondents towards people who feel strongly about the instructor.</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\">The <a href=\"https://en.wikipedia.org/wiki/Illusion_of_transparency\">illusion of transparency</a> and <a href=\"http://learning.subwiki.org/wiki/Double_illusion_of_transparency\">double illusion of transparency</a> (see also <a href=\"/lw/ki/double_illusion_of_transparency/\">here</a>) make it quite hard to evaluate teachers by just sampling their classes. It does provide a start though, and, if done well, can be more informative than student evaluations.</p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<h3>Things to keep in mind from a human capital + signaling (grades) perspective:</h3>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>Criterion-referenced versus norm-referenced grading: </strong><span style=\"font-weight: normal\">Keep</span> in mind the distinction between <a href=\"http://en.wikipedia.org/wiki/Criterion-referenced_assessment\">criterion-referenced grading</a> (assigning grades based on attainment of pre-specified mastery levels) and <a href=\"http://en.wikipedia.org/wiki/Norm-referenced_assessment\">norm-referenced grading</a> (assigning grades based on relative performance). One extreme of norm-referenced grading is &ldquo;grading on a curve&rdquo; where it is pre-specified how many students will get an A, how many will get an A-, etc. There are also other forms of norm-referenced grading (a typical approach many instructors use is to start from the top and &ldquo;look for a gap&rdquo; of x or more points for each grade decrement). Note that a lot of students confuse &ldquo;grading on a curve&rdquo; with &ldquo;having lax grading standards&rdquo; and conflate both of these with &ldquo;having low numerical cutoffs for a given grade&rdquo; but the three ideas are all different (particularly in the college course context) and it's best that you do not conflate them.</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>The significance of measurement error</strong><span style=\"font-weight: normal\">: There are three factors that affect the degree of measurement error in grading that you can easily determine:</span></p>\n</li>\n</ul>\n<ol>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">The degree of gap between the score requirements for grades. An easy test where a 90 is an A, an 85 is an A-, etc., means that one careless error can affect your grade adversely. A hard test where an 80 is an A, a 60 is an A-, etc., means that the measurement is considerably less likely to be influenced by small measurement error issues.</span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">The number of questions on the test, and the number of tests and other assessment methods used. Generally, the more the number of items used, and the more distinct questions on the test, the more reliable the measurement (this is something to do with the <a href=\"https://en.wikipedia.org/wiki/Law_of_large_numbers\">law of large numbers</a>).<br /></span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">Whether or not instructors award partial credit in tests. Partial credit makes measurement more accurate, by indirectly increasing the number of items being measured.</span></p>\n</li>\n</ol>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>Teaching to the test</strong><span style=\"font-weight: normal\">: Instructors differ significantly in the extent to which they teach to the test. Keep in mind that &ldquo;teaching to the test&rdquo; although often frowned upon from a human capital perspective is not necessarily bad from either a human capital or a signaling perspective. That's because teaching to the test can be thought of as &ldquo;testing what is taught&rdquo; particularly when the instructor has flexibility in setting the test. Degrees of teaching to the test include:</span></p>\n</li>\n</ul>\n<ol>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">Providing sample tests to eliminate uncertainty about the test format.</span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">Providing review materials or conducting review sessions that are closely optimized to maximal test performance.</span></p>\n</li>\n</ol>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">In general, if you are a good student with a reasonable shot at getting a fairly good grade, and you are interested in both human capital and grades, give preference to a criterion-referenced instructor who boasts low measurement error (lots of questions, lots of tests, partial credit, and hard tests) and teaches to the test. Subject to these constraints, favor the instructor whose overall grades are easiest/best.</span></p>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">If you are not interested in the human capital component of the course much, and are looking purely for a good grade, the choice between criterion-referenced and norm-referenced grading is harder. If choosing a norm-referenced grader, keep in mind the student population you are with. Depending on other courses, the student body during some quarters or semesters can be considerably better than during others, making it relatively harder to get a good grade.</span></p>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">If you are not very good with the material and not keen on deep learning, it might actually pay off to choose an instructor with higher measurement error. This is similar to the idea that people with the odds against them would have the most chance of winning if they made a small number of big bets, whereas people with the odds in their favor would have the most chance of winning by placing a large number of small bets.</span></p>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\"><br /></span></p>\n<h3>Getting the most out of attending sample classes</h3>\n<p>It's quite rare for students to attend sample classes with instructors that they plan to study with (there could be <a href=\"https://www.facebook.com/vipulnaik.r/posts/10201402996772795\">many reasons</a> -- <em>do readers have any thoughts?</em>). <em>Update: </em>When talking of \"sample classes\" I was referring to classes with the same instructor in an earlier term, because it's often very difficult to change classes once the term has begun, due to complications with scheduling, or classes getting full. The same instructor may be teaching a somewhat different course in the preceding term, so such sampling is useful only in so far as it captures generic aspects of instructors that transfer across courses. There may also be an element of difference between universities with a semester system and a quarter system. Those on the quarter system have less time to shop around between classes because the schedule is more compressed overall.<em> </em></p>\n<p class=\"western\" style=\"margin-bottom: 0in\">For those who do choose to attend sample classes, the following tips may be useful.</p>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>If you attend sample classes, check for how carefully students are listening</strong><span style=\"font-weight: normal\">: When attending a sample class, <em>take a seat close to the back</em>, so you get a bird's eye view of what all the students are doing. Existing students have more experience than you and are more tuned in to the overall course, so their decision of how much attention to pay to the teacher is an indication of the value generated by the teacher.</span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>Judge favorably instructors who engage in cold calling and non-voluntary participation (polling, desk work checking)</strong><span style=\"font-weight: normal\">. Ceteris paribus, if shown two equally &ldquo;interactive&rdquo; classes, one with <a href=\"http://learning.subwiki.org/wiki/Cold_calling\">cold calling</a> and one without, pick the one with cold calling. In general, pick classes with cold calling. Polling and desk work are also positives, albeit milder ones, and many college-level classes don't have much time for desk work.</span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>Avoid classes where students appear eager to impress the teacher or their fellow students:</strong><span style=\"font-weight: normal\"> A dynamic where students are participating largely with the goal of impressing the teacher or their fellow students tends to be unhealthy in promoting student learning.</span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>Favor good classroom technique, but don't be too impressed by slickness due to double illusion of transparency</strong><span style=\"font-weight: normal\">. Good board technique, neat handwriting, etc. are valuable. However, sometimes better instructors actually sound more confusing than not-so-good instructors, because they highlight areas of difficulty, cold call students, etc., making the pain points clearer and avoiding the double illusion of transparency.<br /></span></p>\n</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<h3>Other criteria</h3>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">The following may be harder to gauge from a single lecture, but can usually be better gathered by talking to students who have studied with the instructor and in some cases by reviewing student evaluations that include long form responses.</span></p>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>Favor instructors who make notes and sources clear</strong><span style=\"font-weight: normal\">. This is less of an issue if the instructor is rigidly following a specific course text and the text is good. Insofar as the instructor is covering material not in the text, does the instructor provide notes or sources to read, or expect people to use notes taken in class?</span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>Favor frequent &ldquo;low-stakes&rdquo; assessment to shatter the double illusion of transparency</strong><span style=\"font-weight: normal\">. Instructors who care about learning will tend to be more likely to use frequent low-stakes assessment such as class quizzes. These indicate that the students and instructor generally have a clear idea at any given stage of how well the material is being understood. However, beware of the difficulty level of these. Very easy assessments and very difficult assessments should be discounted (both of these have their uses, but they don't shatter the double illusion of transparency). Although the specifics vary, low-stakes assessments are best when the average score is somewhere between 30% and 70%.<br /></span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>When looking at student evaluations, discount evaluations that make it appear like the instructor &ldquo;made everything very easy&rdquo; &ndash; this is likely a double illusion of transparency or a student who had an unusually strong background.</strong><span style=\"font-weight: normal\"> Many concepts being taught are hard. While it's possible to obfuscate them with difficult instruction, it's not possible to make them &ldquo;very easy.&rdquo; It's quite likely that instructors who give their students this impression are not explaining the concepts fully, or are testing the students in a sufficiently superficial manner that the students think that what they got just by hearing the material is a full understanding. It's also possible that some student was unusually smart or had an unusually strong background and committed a fundamental attribution error by attributing it to the instructor. Of course, good instructors </span><em><span style=\"font-weight: normal\">do</span></em><span style=\"font-style: normal\"><span style=\"font-weight: normal\"> end up teaching their students better and perhaps even making (some of) the concepts completely clear, but rarely by making everything seem &ldquo;very easy&rdquo; &ndash; there is usually some sort of effort and pain undergone by the students to acquire that understanding.</span></span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>When looking at student evaluations or getting word-of-mouth feedback, weigh more heavily certain kinds of evaluations that overcome the problem of bad anchoring and adjustment. </strong><span style=\"font-weight: normal\">Evaluations by people who have switched between multiple sections of the same course are often most illuminative, because they can control for the difficulty level of the material to quite an extent. Evaluations by people who have taken follow-on courses that rely on the material more are also useful because these people have more of an idea of whether what they learned was actually used, and can judge the quality of learning in that context.</span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>Weigh positively (but don't be overimpressed by) student recalls of &ldquo;aha&rdquo; moments and phrases that indicate that students felt that the learning in their class was at a higher plane than in the others.</strong><span style=\"font-weight: normal\"> &ldquo;Aha&rdquo; moments and unexpected connections emerging between seeds sown earlier in the course and material covered recently are indicative of unusually good teachers. However, keep in mind that given the huge subject matter knowledge gap between the teacher and the student, it is relatively easy for students to have false &ldquo;aha&rdquo; moments for insights that are actually pretty mundane and ones that they should have got at the outset.</span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>(This one may be asking too much, or may be too idiosyncratic): Ask students to recall their test preparation experience. This can often be insightful regarding the kind of methods and level of learning the instructor has encouraged. Note that students differ, so this should be considered only averaged across students or for students for whom you know their general pattern of test preparation.</strong><span style=\"font-weight: normal\"> Good instructors will try to maximize the extent to which test preparation is used to reinforce the material, rather than simply treating tests as a method to assess students. If instructors put up review materials or conduct review sessions, ask about the format of these. Negatively judge review sessions that are just somewhat tweaked versions of lectures. Look for review sessions that are &ldquo;controlled, desk-work-based, and interactive&rdquo; &ndash; controlled in the sense that the instructor controls the flow of the session. The &ldquo;desk-work-based&rdquo; says that the review session should largely involve desk work by all rather than lecturing or board work by select individuals. The &ldquo;interactive&rdquo; component stresses interaction between the instructor and students. In addition to the review sessions, look for review materials, and for students' subjective experience when reviewing. Did they have &ldquo;aha&rdquo; moments at review time? (Note that desk-work becomes more important at review time than cold-calling, because students are required to be able to work things out in full detail rather than just answer isolated questions).</span></p>\n</li>\n</ul>\n<h3>Looking for thoughts</h3>\n<p class=\"western\" style=\"margin-bottom: 0in\"><em>What do readers think of the lists I've offered above? Are there items you disagree with? Things you think I missed? Arguments that the entire question is ill-considered?</em> All feedback would be very much appreciated.</p>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in\">PS: I've deliberately omitted other factors, such as scheduling and space constraints and peer choices, from the discussion. In many cases, the fact that a close friend or study buddy is taking a particular section is good reason to take that section. Similarly, scheduling and space constraints can be binding at times. I don't think that these meaningfully alter the shape of the preceding advice, but they do constrain the scope within which it can be applied.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J7RY9t2ph6Du8JHgD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 16, "extendedScore": null, "score": 1.3334071473699459e-06, "legacy": true, "legacyId": "24065", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I've been trying to compile advice on how to select good instructors in college, assuming that the major and course level are given. Of course, many people don't have a lot of flexibility with instructor choice, so this advice has limited applicability. The reason I'm posting to LessWrong is to get feedback from interested LessWrong readers regarding some of the factual assertions I make. I've marked the parts where I'm most eager for feedback. My experience with instruction (direct and indirect) has been focused largely on mathematics. Thus, it's possible that some of the points I raise have limited applicability in more discussion-based subjects.</p>\n<p>Note also that although my experiences have been partly shaped by the undergraduate teaching I've done in four years as a graduate student at the University of Chicago, I'm neither representing the University nor am I critiquing or describing any features specific to the University of Chicago.</p>\n<p>I'm working within the conceptual framework that there are three broad types of value that students derive from college courses:</p>\n<p>&nbsp;</p>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Human_capital\">Human capital</a>: Students acquire <strong>knowledge, skills, and abilities</strong> that help them with further courses, jobs, graduate school, or other aspects of their future life. When choosing between different instructors for courses at the same level in the same college, the human capital differential boils down to how different the instructors are in how well they teach the material.</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Signalling_%28economics%29\">Signaling</a>: that they have learned material, again both for future courses, jobs, graduate school, or other aspects of their future life. When choosing between different instructors for courses at the same level in the same college, the signaling differential boils down to the relative <strong>ease of getting a good grade</strong> (the \"ease\" could stem both from better learning and easier grading). In addition, some instructors may be willing to write <strong>recommendation letters or referral letters</strong> for further courses, jobs, graduate school, scholarships, internships, etc.</li>\n<li><strong>Consumption</strong>: Some courses can be fun to consume. The instructors and fellow students could be entertaining. The homeworks could be challenging in a nice way. Just like a good physical workout can be edifying in addition to building muscle or stamina, so can a good educational experience.</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<h3 id=\"Why_I_think_instructor_selection_matters_\">Why I think instructor selection matters:</h3>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\">Instructors differ significantly from one another in terms of their quality of teaching (human capital + consumption value) and ease of grading (signaling). <em>I'm curious here about the extent to which LessWrong readers think variation between instructors for similar courses compares to variation between institutions.</em>&nbsp; For instance, how much does variation between instructors at Harvard compare with variation between UCLA and Harvard?</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\">Apart from the teaching and grading, a good instructor is a candidate to write recommendation letters for future courses, internships and scholarships, and graduate school applications. Additionally, a good instructor may recommend options to students.</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\">Students routinely neglect instructor selection or use suboptimal criteria, relative to its importance (<em>is this true?</em>) or relative to other things they fret over, such as college selection. Often, students decide their courses for the next semester or quarter after a meeting of a few hours with an advisor, based on time scheduling constraints. <em>Update</em>: Other students use data such as online evaluations and talking to friends who have taken classes with specific instructors, but may not take proactive steps to collect such data. Further, the criteria that they use to process the data may well be suboptimal (or at any rate, I believe so, which is why I'm writing the post).</p>\n</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0in\">For the discussion below, I largely assume that the student is selecting between different sections of the same course, where each section follows a similar curriculum but the instructor has flexibility in terms of the final examinations and grades.<em> What do LessWrong readers think about how common this is relative to a setup where all sections have the same final examinations and grades?</em> However, much of the advice is general.</p>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<h3 id=\"Pitfalls\">Pitfalls</h3>\n<p>&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\">Students often incorrectly believe that instructors who are harder in terms of the material covered or in terms of the work they assign will be harder in terms of the final letter grades they assign. This is not necessarily true. The overall correlation sign is unclear, but there are teachers in all four quadrants in terms of (hardness of material, hardness of grading).</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\">Looking at the difficulty level of examinations in isolation can paint a misleading picture, because instructors may differ somewhat in the techniques they cover, and this can radically affect the perceived difficulty of a question.</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\">Remember that student evaluations, although more reliable than peer faculty evaluations or other evaluation methods, are still generally unreliable (the <a href=\"http://en.wikipedia.org/wiki/Course_evaluation\">Wikipedia page</a> and <a href=\"http://home.sprynet.com/~owl1/sef.htm\">Mike Huemer's article</a> are great starting points). There is still some value you can extract from student evaluations, but not a lot. The main problem with the numerical part of such evaluations is improper <a href=\"https://en.wikipedia.org/wiki/Anchoring\">anchoring</a> \u2013 students aren't clear on the scale relative to which they are evaluating instructors. As a result, the median numerical rating winds up <a href=\"http://econlog.econlib.org/archives/2005/07/improving_stude.html\">at around 4.4/5, leading to censoring at the top (a ceiling effect)</a>. <em>Update: </em>Since Falenas108 raised this issue in a comment, I know that the long form responses in student evaluations are somewhat more informative than the numerical ratings, but these suffer from many of the same problems. The Huemer article and the references in the Wikipedia article and the Huemer article are good starting points. Further, optional online evaluations (which are the only ones that are easily available) often suffer from both low response rates and a selection bias in the set of respondents towards people who feel strongly about the instructor.</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\">The <a href=\"https://en.wikipedia.org/wiki/Illusion_of_transparency\">illusion of transparency</a> and <a href=\"http://learning.subwiki.org/wiki/Double_illusion_of_transparency\">double illusion of transparency</a> (see also <a href=\"/lw/ki/double_illusion_of_transparency/\">here</a>) make it quite hard to evaluate teachers by just sampling their classes. It does provide a start though, and, if done well, can be more informative than student evaluations.</p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<h3 id=\"Things_to_keep_in_mind_from_a_human_capital___signaling__grades__perspective_\">Things to keep in mind from a human capital + signaling (grades) perspective:</h3>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>Criterion-referenced versus norm-referenced grading: </strong><span style=\"font-weight: normal\">Keep</span> in mind the distinction between <a href=\"http://en.wikipedia.org/wiki/Criterion-referenced_assessment\">criterion-referenced grading</a> (assigning grades based on attainment of pre-specified mastery levels) and <a href=\"http://en.wikipedia.org/wiki/Norm-referenced_assessment\">norm-referenced grading</a> (assigning grades based on relative performance). One extreme of norm-referenced grading is \u201cgrading on a curve\u201d where it is pre-specified how many students will get an A, how many will get an A-, etc. There are also other forms of norm-referenced grading (a typical approach many instructors use is to start from the top and \u201clook for a gap\u201d of x or more points for each grade decrement). Note that a lot of students confuse \u201cgrading on a curve\u201d with \u201chaving lax grading standards\u201d and conflate both of these with \u201chaving low numerical cutoffs for a given grade\u201d but the three ideas are all different (particularly in the college course context) and it's best that you do not conflate them.</p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>The significance of measurement error</strong><span style=\"font-weight: normal\">: There are three factors that affect the degree of measurement error in grading that you can easily determine:</span></p>\n</li>\n</ul>\n<ol>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">The degree of gap between the score requirements for grades. An easy test where a 90 is an A, an 85 is an A-, etc., means that one careless error can affect your grade adversely. A hard test where an 80 is an A, a 60 is an A-, etc., means that the measurement is considerably less likely to be influenced by small measurement error issues.</span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">The number of questions on the test, and the number of tests and other assessment methods used. Generally, the more the number of items used, and the more distinct questions on the test, the more reliable the measurement (this is something to do with the <a href=\"https://en.wikipedia.org/wiki/Law_of_large_numbers\">law of large numbers</a>).<br></span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">Whether or not instructors award partial credit in tests. Partial credit makes measurement more accurate, by indirectly increasing the number of items being measured.</span></p>\n</li>\n</ol>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>Teaching to the test</strong><span style=\"font-weight: normal\">: Instructors differ significantly in the extent to which they teach to the test. Keep in mind that \u201cteaching to the test\u201d although often frowned upon from a human capital perspective is not necessarily bad from either a human capital or a signaling perspective. That's because teaching to the test can be thought of as \u201ctesting what is taught\u201d particularly when the instructor has flexibility in setting the test. Degrees of teaching to the test include:</span></p>\n</li>\n</ul>\n<ol>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">Providing sample tests to eliminate uncertainty about the test format.</span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">Providing review materials or conducting review sessions that are closely optimized to maximal test performance.</span></p>\n</li>\n</ol>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">In general, if you are a good student with a reasonable shot at getting a fairly good grade, and you are interested in both human capital and grades, give preference to a criterion-referenced instructor who boasts low measurement error (lots of questions, lots of tests, partial credit, and hard tests) and teaches to the test. Subject to these constraints, favor the instructor whose overall grades are easiest/best.</span></p>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">If you are not interested in the human capital component of the course much, and are looking purely for a good grade, the choice between criterion-referenced and norm-referenced grading is harder. If choosing a norm-referenced grader, keep in mind the student population you are with. Depending on other courses, the student body during some quarters or semesters can be considerably better than during others, making it relatively harder to get a good grade.</span></p>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">If you are not very good with the material and not keen on deep learning, it might actually pay off to choose an instructor with higher measurement error. This is similar to the idea that people with the odds against them would have the most chance of winning if they made a small number of big bets, whereas people with the odds in their favor would have the most chance of winning by placing a large number of small bets.</span></p>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\"><br></span></p>\n<h3 id=\"Getting_the_most_out_of_attending_sample_classes\">Getting the most out of attending sample classes</h3>\n<p>It's quite rare for students to attend sample classes with instructors that they plan to study with (there could be <a href=\"https://www.facebook.com/vipulnaik.r/posts/10201402996772795\">many reasons</a> -- <em>do readers have any thoughts?</em>). <em>Update: </em>When talking of \"sample classes\" I was referring to classes with the same instructor in an earlier term, because it's often very difficult to change classes once the term has begun, due to complications with scheduling, or classes getting full. The same instructor may be teaching a somewhat different course in the preceding term, so such sampling is useful only in so far as it captures generic aspects of instructors that transfer across courses. There may also be an element of difference between universities with a semester system and a quarter system. Those on the quarter system have less time to shop around between classes because the schedule is more compressed overall.<em> </em></p>\n<p class=\"western\" style=\"margin-bottom: 0in\">For those who do choose to attend sample classes, the following tips may be useful.</p>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>If you attend sample classes, check for how carefully students are listening</strong><span style=\"font-weight: normal\">: When attending a sample class, <em>take a seat close to the back</em>, so you get a bird's eye view of what all the students are doing. Existing students have more experience than you and are more tuned in to the overall course, so their decision of how much attention to pay to the teacher is an indication of the value generated by the teacher.</span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>Judge favorably instructors who engage in cold calling and non-voluntary participation (polling, desk work checking)</strong><span style=\"font-weight: normal\">. Ceteris paribus, if shown two equally \u201cinteractive\u201d classes, one with <a href=\"http://learning.subwiki.org/wiki/Cold_calling\">cold calling</a> and one without, pick the one with cold calling. In general, pick classes with cold calling. Polling and desk work are also positives, albeit milder ones, and many college-level classes don't have much time for desk work.</span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>Avoid classes where students appear eager to impress the teacher or their fellow students:</strong><span style=\"font-weight: normal\"> A dynamic where students are participating largely with the goal of impressing the teacher or their fellow students tends to be unhealthy in promoting student learning.</span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>Favor good classroom technique, but don't be too impressed by slickness due to double illusion of transparency</strong><span style=\"font-weight: normal\">. Good board technique, neat handwriting, etc. are valuable. However, sometimes better instructors actually sound more confusing than not-so-good instructors, because they highlight areas of difficulty, cold call students, etc., making the pain points clearer and avoiding the double illusion of transparency.<br></span></p>\n</li>\n</ul>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<h3 id=\"Other_criteria\">Other criteria</h3>\n<p class=\"western\" style=\"margin-bottom: 0in\"><span style=\"font-weight: normal\">The following may be harder to gauge from a single lecture, but can usually be better gathered by talking to students who have studied with the instructor and in some cases by reviewing student evaluations that include long form responses.</span></p>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<ul>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>Favor instructors who make notes and sources clear</strong><span style=\"font-weight: normal\">. This is less of an issue if the instructor is rigidly following a specific course text and the text is good. Insofar as the instructor is covering material not in the text, does the instructor provide notes or sources to read, or expect people to use notes taken in class?</span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>Favor frequent \u201clow-stakes\u201d assessment to shatter the double illusion of transparency</strong><span style=\"font-weight: normal\">. Instructors who care about learning will tend to be more likely to use frequent low-stakes assessment such as class quizzes. These indicate that the students and instructor generally have a clear idea at any given stage of how well the material is being understood. However, beware of the difficulty level of these. Very easy assessments and very difficult assessments should be discounted (both of these have their uses, but they don't shatter the double illusion of transparency). Although the specifics vary, low-stakes assessments are best when the average score is somewhere between 30% and 70%.<br></span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>When looking at student evaluations, discount evaluations that make it appear like the instructor \u201cmade everything very easy\u201d \u2013 this is likely a double illusion of transparency or a student who had an unusually strong background.</strong><span style=\"font-weight: normal\"> Many concepts being taught are hard. While it's possible to obfuscate them with difficult instruction, it's not possible to make them \u201cvery easy.\u201d It's quite likely that instructors who give their students this impression are not explaining the concepts fully, or are testing the students in a sufficiently superficial manner that the students think that what they got just by hearing the material is a full understanding. It's also possible that some student was unusually smart or had an unusually strong background and committed a fundamental attribution error by attributing it to the instructor. Of course, good instructors </span><em><span style=\"font-weight: normal\">do</span></em><span style=\"font-style: normal\"><span style=\"font-weight: normal\"> end up teaching their students better and perhaps even making (some of) the concepts completely clear, but rarely by making everything seem \u201cvery easy\u201d \u2013 there is usually some sort of effort and pain undergone by the students to acquire that understanding.</span></span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>When looking at student evaluations or getting word-of-mouth feedback, weigh more heavily certain kinds of evaluations that overcome the problem of bad anchoring and adjustment. </strong><span style=\"font-weight: normal\">Evaluations by people who have switched between multiple sections of the same course are often most illuminative, because they can control for the difficulty level of the material to quite an extent. Evaluations by people who have taken follow-on courses that rely on the material more are also useful because these people have more of an idea of whether what they learned was actually used, and can judge the quality of learning in that context.</span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>Weigh positively (but don't be overimpressed by) student recalls of \u201caha\u201d moments and phrases that indicate that students felt that the learning in their class was at a higher plane than in the others.</strong><span style=\"font-weight: normal\"> \u201cAha\u201d moments and unexpected connections emerging between seeds sown earlier in the course and material covered recently are indicative of unusually good teachers. However, keep in mind that given the huge subject matter knowledge gap between the teacher and the student, it is relatively easy for students to have false \u201caha\u201d moments for insights that are actually pretty mundane and ones that they should have got at the outset.</span></p>\n</li>\n<li>\n<p class=\"western\" style=\"margin-bottom: 0in\"><strong>(This one may be asking too much, or may be too idiosyncratic): Ask students to recall their test preparation experience. This can often be insightful regarding the kind of methods and level of learning the instructor has encouraged. Note that students differ, so this should be considered only averaged across students or for students for whom you know their general pattern of test preparation.</strong><span style=\"font-weight: normal\"> Good instructors will try to maximize the extent to which test preparation is used to reinforce the material, rather than simply treating tests as a method to assess students. If instructors put up review materials or conduct review sessions, ask about the format of these. Negatively judge review sessions that are just somewhat tweaked versions of lectures. Look for review sessions that are \u201ccontrolled, desk-work-based, and interactive\u201d \u2013 controlled in the sense that the instructor controls the flow of the session. The \u201cdesk-work-based\u201d says that the review session should largely involve desk work by all rather than lecturing or board work by select individuals. The \u201cinteractive\u201d component stresses interaction between the instructor and students. In addition to the review sessions, look for review materials, and for students' subjective experience when reviewing. Did they have \u201caha\u201d moments at review time? (Note that desk-work becomes more important at review time than cold-calling, because students are required to be able to work things out in full detail rather than just answer isolated questions).</span></p>\n</li>\n</ul>\n<h3 id=\"Looking_for_thoughts\">Looking for thoughts</h3>\n<p class=\"western\" style=\"margin-bottom: 0in\"><em>What do readers think of the lists I've offered above? Are there items you disagree with? Things you think I missed? Arguments that the entire question is ill-considered?</em> All feedback would be very much appreciated.</p>\n<p class=\"western\" style=\"margin-bottom: 0in\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in\">PS: I've deliberately omitted other factors, such as scheduling and space constraints and peer choices, from the discussion. In many cases, the fact that a close friend or study buddy is taking a particular section is good reason to take that section. Similarly, scheduling and space constraints can be binding at times. I don't think that these meaningfully alter the shape of the preceding advice, but they do constrain the scope within which it can be applied.</p>", "sections": [{"title": "Why I think instructor selection matters:", "anchor": "Why_I_think_instructor_selection_matters_", "level": 1}, {"title": "Pitfalls", "anchor": "Pitfalls", "level": 1}, {"title": "Things to keep in mind from a human capital + signaling (grades) perspective:", "anchor": "Things_to_keep_in_mind_from_a_human_capital___signaling__grades__perspective_", "level": 1}, {"title": "Getting the most out of attending sample classes", "anchor": "Getting_the_most_out_of_attending_sample_classes", "level": 1}, {"title": "Other criteria", "anchor": "Other_criteria", "level": 1}, {"title": "Looking for thoughts", "anchor": "Looking_for_thoughts", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "28 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sBBGxdvhKcppQWZZE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-08T19:45:31.984Z", "modifiedAt": null, "url": null, "title": "High School, Human Capital, Signaling and College Admissions", "slug": "high-school-human-capital-signaling-and-college-admissions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:29.398Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wPTQZ7KbiMQkrkZX6/high-school-human-capital-signaling-and-college-admissions", "pageUrlRelative": "/posts/wPTQZ7KbiMQkrkZX6/high-school-human-capital-signaling-and-college-admissions", "linkUrl": "https://www.lesswrong.com/posts/wPTQZ7KbiMQkrkZX6/high-school-human-capital-signaling-and-college-admissions", "postedAtFormatted": "Sunday, September 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20High%20School%2C%20Human%20Capital%2C%20Signaling%20and%20College%20Admissions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHigh%20School%2C%20Human%20Capital%2C%20Signaling%20and%20College%20Admissions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwPTQZ7KbiMQkrkZX6%2Fhigh-school-human-capital-signaling-and-college-admissions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=High%20School%2C%20Human%20Capital%2C%20Signaling%20and%20College%20Admissions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwPTQZ7KbiMQkrkZX6%2Fhigh-school-human-capital-signaling-and-college-admissions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwPTQZ7KbiMQkrkZX6%2Fhigh-school-human-capital-signaling-and-college-admissions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1456, "htmlBody": "<!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>99</o:Words> <o:Characters>569</o:Characters> <o:Company>GiveWell</o:Company> <o:Lines>4</o:Lines> <o:Paragraphs>1</o:Paragraphs> <o:CharactersWithSpaces>667</o:CharactersWithSpaces> <o:Version>14.0</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:Cambria; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} -->\n<p>During high school, students learn skills that will help them in their future careers. This can be referred to as <em>building human capital</em>. They also build up a record of grades, standardized test scores, and extracurricular activities that colleges use to assess whether to admit them. This can be referred to as <em>signaling quality to colleges</em>.&nbsp;</p>\n<p class=\"MsoNormal\">High schoolers engage in valuable activities that fall outside of these two categories, such as personally enjoyable activities and helping others. This article focuses on building human capital and signaling quality to colleges, for the sake of simplicity, rather than because I think that these are the only two things that matter.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<!--EndFragment-->\n<p><a id=\"more\"></a></p>\n<p><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>26</o:Words> <o:Characters>151</o:Characters> <o:Company>GiveWell</o:Company> <o:Lines>1</o:Lines> <o:Paragraphs>1</o:Paragraphs> <o:CharactersWithSpaces>176</o:CharactersWithSpaces> <o:Version>14.0</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:Cambria; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--> <!--EndFragment--></p>\n<p class=\"MsoNormal\">In an ideal world, building human capital would be perfectly aligned with signaling quality to colleges. In the real world, this is not the case. Consider the following story:</p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\"><em>Kevin is an ambitious high school student who aspires to become a molecular biologist.</em></p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\"><em>Kevin attends a competitive high school, where a student is awarded an extra GPA point for each honors or AP course that he or she takes. The maximum number of grade points that a student can get taking a &ldquo;regular&rdquo; course is 4.0 and the maximum number of grade points that a student can get for taking an honors or AP course is 5.0 A student who gets all A&rsquo;s and takes at least one honors or AP course gets a GPA that&rsquo;s greater than 4.0 so that taking a &ldquo;regular&rdquo; course reduces his or her GPA. GPA determines class rank, so taking a &ldquo;regular&rdquo; course lowers such a student&rsquo;s class rank.&nbsp;</em></p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\"><em>Kevin&rsquo;s school offers a molecular biology elective during second semester, which is not an honors or AP course. Kevin would like to take the elective during the second semester of his junior year, in addition to his other coursework, but he knows that doing so would lower his GPA, so he decides not to. Kevin ends up with a class rank in the top 1%, contrasting with a class rank in the top 5% if he had taken the molecular biology course. Because he&rsquo;s in the top 1%, he&rsquo;s accepted at Harvard, Yale, Princeton, Stanford and MIT, and this would not have happened had he only been in the top 5%.</em></p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\"><em>Kevin chooses to attend Stanford. The summer after his freshman year there, he works as a molecular biology research intern, and performs worse than he would have if he had taken molecular biology in high school.</em></p>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>42</o:Words> <o:Characters>245</o:Characters> <o:Company>GiveWell</o:Company> <o:Lines>2</o:Lines> <o:Paragraphs>1</o:Paragraphs> <o:CharactersWithSpaces>286</o:CharactersWithSpaces> <o:Version>14.0</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:Cambria; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--> <!--EndFragment--></p>\n<p class=\"MsoNormal\">This story shows how there can be a tension between building human capital and signaling quality to colleges. Kevin&rsquo;s choice enabled him to get into a better college than he would otherwise have been able to get into, but it came at the cost of lowering the quality of his future work.</p>\n<h2>Imperfect measurement and perverse incentives</h2>\n<p class=\"MsoNormal\">In Kevin&rsquo;s story, the class ranking system was poorly designed: it rewarded some students for achieving less rather than for achieving more. The colleges that Kevin applied to were relying on a faulty measure of quality.</p>\n<p class=\"MsoNormal\">All measures of quality are imperfect to varying degrees. Because they&rsquo;re imperfect, they sometimes assign somebody <em>higher</em> quality for making a choice that actually <em>lowers</em> his or her quality relative to what it otherwise would be. Once people catch on to this, they feel pressure to make such choices.</p>\n<p class=\"MsoNormal\"><strong>Imperfections of measures of college applicant quality</strong></p>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>45</o:Words> <o:Characters>262</o:Characters> <o:Company>GiveWell</o:Company> <o:Lines>2</o:Lines> <o:Paragraphs>1</o:Paragraphs> <o:CharactersWithSpaces>306</o:CharactersWithSpaces> <o:Version>14.0</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:Cambria; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--> <!--EndFragment--></p>\n<p class=\"MsoNormal\">Class rank at Kevin&rsquo;s high school is an imperfect measure of the strength of students&rsquo; academic transcripts. This is only one of many examples of imperfection in the measures that colleges use to assess student quality. Some more examples come from:</p>\n<ul>\n<li><strong>Academic transcripts being insensitive to academic achievement in subjects that aren&rsquo;t taught.</strong> There are many academic subjects that are not taught courses that high school students have access to. Colleges give heavy weight to academic transcripts when they assess students&rsquo; academic achievement, so studying subjects that aren&rsquo;t taught in school is given relatively little weight.</li>\n<li><strong>Course grades being insensitive to unusually high achievement. </strong>Course grades are capped: it&rsquo;s generally true that the highest grade that a student can earn is an A. When the threshold for earning an A is below that of subject mastery, students aren&rsquo;t awarded for developing subject mastery. In practice, the thresholds for getting top grades are often below that of subject matter mastery. <a href=\"/lw/ihw/advanced_placement_exam_cutoffs_and_superficial/\">For example</a>, one can get the highest mark on some AP exams by answering a relatively low percentage of the questions correctly: low enough so that it doesn&rsquo;t correspond to mastery.</li>\n<li><strong>Individual teachers' grading schemes being imperfect.</strong>&nbsp;Teachers often assess student achievement via measures that differentiate students based on factors <em>other than</em> how well students have learned the subject. For example, in a chemistry course, a teacher may design tests that give heavy weight to computational accuracy to the exclusion of knowledge of chemistry.</li>\n</ul>\n<p class=\"MsoNormal\">Each factor gives rise to situations in which students aren&rsquo;t able to signal quality to college by doing certain activities that would raise their human capital <em>more</em> than the activities that <em>do</em> signal quality to colleges.</p>\n<p class=\"MsoNormal\">Some activities that build human capital also signal quality to colleges. But it&rsquo;s important to recognize that building human capital isn&rsquo;t the same thing as signaling quality to colleges. Many activities that build human capital don&rsquo;t signal quality to colleges, and many activities that signal quality to colleges have negligible value from the point of view of building human capital.</p>\n<h2>What to do about it?</h2>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>42</o:Words> <o:Characters>241</o:Characters> <o:Company>GiveWell</o:Company> <o:Lines>2</o:Lines> <o:Paragraphs>1</o:Paragraphs> <o:CharactersWithSpaces>282</o:CharactersWithSpaces> <o:Version>14.0</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:Cambria; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--> <!--EndFragment--></p>\n<p class=\"MsoNormal\">Having acknowledged that there&rsquo;s a tension between building human capital and signaling quality to colleges, one is faced with the question of what to do about it. Concretely, in the story above, did Kevin make the right choice? Should he have taken the molecular biology elective?</p>\n<p class=\"MsoNormal\"><strong>Exploring other options can sometimes resolve tensions</strong></p>\n<p class=\"MsoNormal\">Of those activities that build human capital to a given degree, some signal quality to colleges more than others. Of those activities that signal quality to colleges to a given degree, some build human capital more than others.</p>\n<p class=\"MsoNormal\">Sometimes when there seems to be a tension between building human capital and signaling quality to colleges, one can resolve the tension by being imaginative and resourceful. In the story, Kevin could have considered possibilities such as</p>\n<ol>\n<li>Auditing the molecular biology elective</li>\n<li>Studying molecular biology on his own</li>\n<li>Taking an online course or a course at a local community college</li>\n<li>Looking for a school year internship in a molecular biology lab so as to learn some molecular biology outside of the academic system.</li>\n</ol>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>19</o:Words> <o:Characters>114</o:Characters> <o:Company>GiveWell</o:Company> <o:Lines>1</o:Lines> <o:Paragraphs>1</o:Paragraphs> <o:CharactersWithSpaces>132</o:CharactersWithSpaces> <o:Version>14.0</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:Cambria; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--> <!--EndFragment--></p>\n<p class=\"MsoNormal\">If Kevin had been able to do these things, he could have learned some molecular biology without having to sacrifice his class rank.</p>\n<p class=\"MsoNormal\"><strong>Tradeoffs between building human capital and college admissions</strong></p>\n<p class=\"MsoNormal\">Sometimes there&rsquo;s no possibility of resolving the tension, so that imagination and resourcefulness don&rsquo;t suffice. One <em>does</em> have to make tradeoffs.</p>\n<p class=\"MsoNormal\">In Kevin&rsquo;s situation, the choice isn&rsquo;t just &ldquo;molecular biology vs. no molecular biology,&rdquo; but &ldquo;molecular biology vs. everything else that could be done within that time slot.&rdquo; Putting aside the issue of taking molecular biology lowering Kevin&rsquo;s GPA, there might be other activities that would signal quality to colleges better than learning molecular biology.</p>\n<p class=\"MsoNormal\">There are two inputs into thinking about how to make tradeoffs in this context:</p>\n<ol>\n<li><strong>The relative value of building human capital vs. getting into a better college.</strong> This depends very heavily on the details of a given person&rsquo;s situation.</li>\n<li><strong>The <em>size</em> of each tradeoff.</strong> Even when it&rsquo;s necessary to sacrifice opportunities to build human capital for the sake of signaling quality to colleges, some activities involve smaller sacrifices than others, whether because they take less time and energy, or because they simultaneously build human capital (even if not as much as possible).&nbsp;</li>\n</ol>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>129</o:Words> <o:Characters>738</o:Characters> <o:Company>GiveWell</o:Company> <o:Lines>6</o:Lines> <o:Paragraphs>1</o:Paragraphs> <o:CharactersWithSpaces>866</o:CharactersWithSpaces> <o:Version>14.0</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:Cambria; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--> <!--EndFragment--></p>\n<p class=\"MsoNormal\">The answer to the question of how a given individual can best balance building human capital and signaling quality to colleges depends very heavily on the details of individual&rsquo;s situation: his or her values, his or her goals, and the opportunities that are available to him or her.</p>\n<p class=\"MsoNormal\">Though there&rsquo;s not an easy answer to the question of how to best balance building human capital and signaling quality to college, it&rsquo;s helpful to <em>explicitly recognize</em> the distinction between two things, and the tradeoffs involved. The first step to resolving a tension is recognizing that it&rsquo;s there.</p>\n<h2>For commenters</h2>\n<p class=\"MsoNormal\">I&rsquo;m primarily interested in feedback involving signaling as it relates to undergraduate admissions (as opposed to, e.g. signaling in the context of romantic courtship), but I&rsquo;d welcome related observations about signaling to graduate school or employers based on high school or college coursework.</p>\n<p class=\"MsoNormal\"><strong>What&rsquo;s an example from your own life where building human capital and signaling quality to colleges have come into conflict? How did you resolve the conflict? Do you think you made the right choice? Is there anything you would have done differently?</strong></p>\n<p class=\"MsoNormal\">Thanks to Vipul Naik for conversations that lead to this post, and to Luke Muehlhauser for feedback.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wPTQZ7KbiMQkrkZX6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 17, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "24066", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>99</o:Words> <o:Characters>569</o:Characters> <o:Company>GiveWell</o:Company> <o:Lines>4</o:Lines> <o:Paragraphs>1</o:Paragraphs> <o:CharactersWithSpaces>667</o:CharactersWithSpaces> <o:Version>14.0</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:Cambria; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} -->\n<p>During high school, students learn skills that will help them in their future careers. This can be referred to as <em>building human capital</em>. They also build up a record of grades, standardized test scores, and extracurricular activities that colleges use to assess whether to admit them. This can be referred to as <em>signaling quality to colleges</em>.&nbsp;</p>\n<p class=\"MsoNormal\">High schoolers engage in valuable activities that fall outside of these two categories, such as personally enjoyable activities and helping others. This article focuses on building human capital and signaling quality to colleges, for the sake of simplicity, rather than because I think that these are the only two things that matter.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<!--EndFragment-->\n<p><a id=\"more\"></a></p>\n<p><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>26</o:Words> <o:Characters>151</o:Characters> <o:Company>GiveWell</o:Company> <o:Lines>1</o:Lines> <o:Paragraphs>1</o:Paragraphs> <o:CharactersWithSpaces>176</o:CharactersWithSpaces> <o:Version>14.0</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:Cambria; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--> <!--EndFragment--></p>\n<p class=\"MsoNormal\">In an ideal world, building human capital would be perfectly aligned with signaling quality to colleges. In the real world, this is not the case. Consider the following story:</p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\"><em>Kevin is an ambitious high school student who aspires to become a molecular biologist.</em></p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\"><em>Kevin attends a competitive high school, where a student is awarded an extra GPA point for each honors or AP course that he or she takes. The maximum number of grade points that a student can get taking a \u201cregular\u201d course is 4.0 and the maximum number of grade points that a student can get for taking an honors or AP course is 5.0 A student who gets all A\u2019s and takes at least one honors or AP course gets a GPA that\u2019s greater than 4.0 so that taking a \u201cregular\u201d course reduces his or her GPA. GPA determines class rank, so taking a \u201cregular\u201d course lowers such a student\u2019s class rank.&nbsp;</em></p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\"><em>Kevin\u2019s school offers a molecular biology elective during second semester, which is not an honors or AP course. Kevin would like to take the elective during the second semester of his junior year, in addition to his other coursework, but he knows that doing so would lower his GPA, so he decides not to. Kevin ends up with a class rank in the top 1%, contrasting with a class rank in the top 5% if he had taken the molecular biology course. Because he\u2019s in the top 1%, he\u2019s accepted at Harvard, Yale, Princeton, Stanford and MIT, and this would not have happened had he only been in the top 5%.</em></p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\"><em>Kevin chooses to attend Stanford. The summer after his freshman year there, he works as a molecular biology research intern, and performs worse than he would have if he had taken molecular biology in high school.</em></p>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>42</o:Words> <o:Characters>245</o:Characters> <o:Company>GiveWell</o:Company> <o:Lines>2</o:Lines> <o:Paragraphs>1</o:Paragraphs> <o:CharactersWithSpaces>286</o:CharactersWithSpaces> <o:Version>14.0</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:Cambria; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--> <!--EndFragment--></p>\n<p class=\"MsoNormal\">This story shows how there can be a tension between building human capital and signaling quality to colleges. Kevin\u2019s choice enabled him to get into a better college than he would otherwise have been able to get into, but it came at the cost of lowering the quality of his future work.</p>\n<h2 id=\"Imperfect_measurement_and_perverse_incentives\">Imperfect measurement and perverse incentives</h2>\n<p class=\"MsoNormal\">In Kevin\u2019s story, the class ranking system was poorly designed: it rewarded some students for achieving less rather than for achieving more. The colleges that Kevin applied to were relying on a faulty measure of quality.</p>\n<p class=\"MsoNormal\">All measures of quality are imperfect to varying degrees. Because they\u2019re imperfect, they sometimes assign somebody <em>higher</em> quality for making a choice that actually <em>lowers</em> his or her quality relative to what it otherwise would be. Once people catch on to this, they feel pressure to make such choices.</p>\n<p class=\"MsoNormal\"><strong id=\"Imperfections_of_measures_of_college_applicant_quality\">Imperfections of measures of college applicant quality</strong></p>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>45</o:Words> <o:Characters>262</o:Characters> <o:Company>GiveWell</o:Company> <o:Lines>2</o:Lines> <o:Paragraphs>1</o:Paragraphs> <o:CharactersWithSpaces>306</o:CharactersWithSpaces> <o:Version>14.0</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:Cambria; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--> <!--EndFragment--></p>\n<p class=\"MsoNormal\">Class rank at Kevin\u2019s high school is an imperfect measure of the strength of students\u2019 academic transcripts. This is only one of many examples of imperfection in the measures that colleges use to assess student quality. Some more examples come from:</p>\n<ul>\n<li><strong>Academic transcripts being insensitive to academic achievement in subjects that aren\u2019t taught.</strong> There are many academic subjects that are not taught courses that high school students have access to. Colleges give heavy weight to academic transcripts when they assess students\u2019 academic achievement, so studying subjects that aren\u2019t taught in school is given relatively little weight.</li>\n<li><strong>Course grades being insensitive to unusually high achievement. </strong>Course grades are capped: it\u2019s generally true that the highest grade that a student can earn is an A. When the threshold for earning an A is below that of subject mastery, students aren\u2019t awarded for developing subject mastery. In practice, the thresholds for getting top grades are often below that of subject matter mastery. <a href=\"/lw/ihw/advanced_placement_exam_cutoffs_and_superficial/\">For example</a>, one can get the highest mark on some AP exams by answering a relatively low percentage of the questions correctly: low enough so that it doesn\u2019t correspond to mastery.</li>\n<li><strong>Individual teachers' grading schemes being imperfect.</strong>&nbsp;Teachers often assess student achievement via measures that differentiate students based on factors <em>other than</em> how well students have learned the subject. For example, in a chemistry course, a teacher may design tests that give heavy weight to computational accuracy to the exclusion of knowledge of chemistry.</li>\n</ul>\n<p class=\"MsoNormal\">Each factor gives rise to situations in which students aren\u2019t able to signal quality to college by doing certain activities that would raise their human capital <em>more</em> than the activities that <em>do</em> signal quality to colleges.</p>\n<p class=\"MsoNormal\">Some activities that build human capital also signal quality to colleges. But it\u2019s important to recognize that building human capital isn\u2019t the same thing as signaling quality to colleges. Many activities that build human capital don\u2019t signal quality to colleges, and many activities that signal quality to colleges have negligible value from the point of view of building human capital.</p>\n<h2 id=\"What_to_do_about_it_\">What to do about it?</h2>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>42</o:Words> <o:Characters>241</o:Characters> <o:Company>GiveWell</o:Company> <o:Lines>2</o:Lines> <o:Paragraphs>1</o:Paragraphs> <o:CharactersWithSpaces>282</o:CharactersWithSpaces> <o:Version>14.0</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:Cambria; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--> <!--EndFragment--></p>\n<p class=\"MsoNormal\">Having acknowledged that there\u2019s a tension between building human capital and signaling quality to colleges, one is faced with the question of what to do about it. Concretely, in the story above, did Kevin make the right choice? Should he have taken the molecular biology elective?</p>\n<p class=\"MsoNormal\"><strong id=\"Exploring_other_options_can_sometimes_resolve_tensions\">Exploring other options can sometimes resolve tensions</strong></p>\n<p class=\"MsoNormal\">Of those activities that build human capital to a given degree, some signal quality to colleges more than others. Of those activities that signal quality to colleges to a given degree, some build human capital more than others.</p>\n<p class=\"MsoNormal\">Sometimes when there seems to be a tension between building human capital and signaling quality to colleges, one can resolve the tension by being imaginative and resourceful. In the story, Kevin could have considered possibilities such as</p>\n<ol>\n<li>Auditing the molecular biology elective</li>\n<li>Studying molecular biology on his own</li>\n<li>Taking an online course or a course at a local community college</li>\n<li>Looking for a school year internship in a molecular biology lab so as to learn some molecular biology outside of the academic system.</li>\n</ol>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>19</o:Words> <o:Characters>114</o:Characters> <o:Company>GiveWell</o:Company> <o:Lines>1</o:Lines> <o:Paragraphs>1</o:Paragraphs> <o:CharactersWithSpaces>132</o:CharactersWithSpaces> <o:Version>14.0</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:Cambria; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--> <!--EndFragment--></p>\n<p class=\"MsoNormal\">If Kevin had been able to do these things, he could have learned some molecular biology without having to sacrifice his class rank.</p>\n<p class=\"MsoNormal\"><strong id=\"Tradeoffs_between_building_human_capital_and_college_admissions\">Tradeoffs between building human capital and college admissions</strong></p>\n<p class=\"MsoNormal\">Sometimes there\u2019s no possibility of resolving the tension, so that imagination and resourcefulness don\u2019t suffice. One <em>does</em> have to make tradeoffs.</p>\n<p class=\"MsoNormal\">In Kevin\u2019s situation, the choice isn\u2019t just \u201cmolecular biology vs. no molecular biology,\u201d but \u201cmolecular biology vs. everything else that could be done within that time slot.\u201d Putting aside the issue of taking molecular biology lowering Kevin\u2019s GPA, there might be other activities that would signal quality to colleges better than learning molecular biology.</p>\n<p class=\"MsoNormal\">There are two inputs into thinking about how to make tradeoffs in this context:</p>\n<ol>\n<li><strong>The relative value of building human capital vs. getting into a better college.</strong> This depends very heavily on the details of a given person\u2019s situation.</li>\n<li><strong>The <em>size</em> of each tradeoff.</strong> Even when it\u2019s necessary to sacrifice opportunities to build human capital for the sake of signaling quality to colleges, some activities involve smaller sacrifices than others, whether because they take less time and energy, or because they simultaneously build human capital (even if not as much as possible).&nbsp;</li>\n</ol>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>129</o:Words> <o:Characters>738</o:Characters> <o:Company>GiveWell</o:Company> <o:Lines>6</o:Lines> <o:Paragraphs>1</o:Paragraphs> <o:CharactersWithSpaces>866</o:CharactersWithSpaces> <o:Version>14.0</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> <w:UseFELayout /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:Cambria; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--> <!--EndFragment--></p>\n<p class=\"MsoNormal\">The answer to the question of how a given individual can best balance building human capital and signaling quality to colleges depends very heavily on the details of individual\u2019s situation: his or her values, his or her goals, and the opportunities that are available to him or her.</p>\n<p class=\"MsoNormal\">Though there\u2019s not an easy answer to the question of how to best balance building human capital and signaling quality to college, it\u2019s helpful to <em>explicitly recognize</em> the distinction between two things, and the tradeoffs involved. The first step to resolving a tension is recognizing that it\u2019s there.</p>\n<h2 id=\"For_commenters\">For commenters</h2>\n<p class=\"MsoNormal\">I\u2019m primarily interested in feedback involving signaling as it relates to undergraduate admissions (as opposed to, e.g. signaling in the context of romantic courtship), but I\u2019d welcome related observations about signaling to graduate school or employers based on high school or college coursework.</p>\n<p class=\"MsoNormal\"><strong id=\"What_s_an_example_from_your_own_life_where_building_human_capital_and_signaling_quality_to_colleges_have_come_into_conflict__How_did_you_resolve_the_conflict__Do_you_think_you_made_the_right_choice__Is_there_anything_you_would_have_done_differently_\">What\u2019s an example from your own life where building human capital and signaling quality to colleges have come into conflict? How did you resolve the conflict? Do you think you made the right choice? Is there anything you would have done differently?</strong></p>\n<p class=\"MsoNormal\">Thanks to Vipul Naik for conversations that lead to this post, and to Luke Muehlhauser for feedback.</p>", "sections": [{"title": "Imperfect measurement and perverse incentives", "anchor": "Imperfect_measurement_and_perverse_incentives", "level": 1}, {"title": "Imperfections of measures of college applicant quality", "anchor": "Imperfections_of_measures_of_college_applicant_quality", "level": 2}, {"title": "What to do about it?", "anchor": "What_to_do_about_it_", "level": 1}, {"title": "Exploring other options can sometimes resolve tensions", "anchor": "Exploring_other_options_can_sometimes_resolve_tensions", "level": 2}, {"title": "Tradeoffs between building human capital and college admissions", "anchor": "Tradeoffs_between_building_human_capital_and_college_admissions", "level": 2}, {"title": "For commenters", "anchor": "For_commenters", "level": 1}, {"title": "What\u2019s an example from your own life where building human capital and signaling quality to colleges have come into conflict? How did you resolve the conflict? Do you think you made the right choice? Is there anything you would have done differently?", "anchor": "What_s_an_example_from_your_own_life_where_building_human_capital_and_signaling_quality_to_colleges_have_come_into_conflict__How_did_you_resolve_the_conflict__Do_you_think_you_made_the_right_choice__Is_there_anything_you_would_have_done_differently_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "62 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4W9i78uxX8Dibk77L"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-09T03:32:20.194Z", "modifiedAt": null, "url": null, "title": "Mistakes repository", "slug": "mistakes-repository", "viewCount": null, "lastCommentedAt": "2022-01-01T23:54:05.578Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dorikka", "createdAt": "2010-12-11T03:34:20.472Z", "isAdmin": false, "displayName": "Dorikka"}, "userId": "HJB33ckc8NzPbvJYz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KLiJPDFHCRYcftQnq/mistakes-repository", "pageUrlRelative": "/posts/KLiJPDFHCRYcftQnq/mistakes-repository", "linkUrl": "https://www.lesswrong.com/posts/KLiJPDFHCRYcftQnq/mistakes-repository", "postedAtFormatted": "Monday, September 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mistakes%20repository&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMistakes%20repository%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKLiJPDFHCRYcftQnq%2Fmistakes-repository%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mistakes%20repository%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKLiJPDFHCRYcftQnq%2Fmistakes-repository", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKLiJPDFHCRYcftQnq%2Fmistakes-repository", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 100, "htmlBody": "<p>This is a repository for major, life-altering mistakes that you or others have made. Detailed accounts of specific mistakes are welcome, and so are mentions of general classes of mistakes that people often make. If similar repositories already exist (inside or outside of LW), links are greatly appreciated.</p>\n<p>The purpose of this repository is to collect information about serious misjudgements and mistakes in order to help people avoid similar mistakes. (I am posting this repository because I'm trying to conduct a premortem on my life and figure out what catastrophic risks may screw me over in the near or far future.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3RnEKrsNgNEDxuNnw": 1, "Eha62RrqBtEbpcEza": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KLiJPDFHCRYcftQnq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 33, "extendedScore": null, "score": 8.3e-05, "legacy": true, "legacyId": "24071", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 197, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-09T04:01:21.356Z", "modifiedAt": null, "url": null, "title": "High school activities and medical school admissions", "slug": "high-school-activities-and-medical-school-admissions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:27.371Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonahSinick", "createdAt": "2012-06-23T04:40:16.600Z", "isAdmin": false, "displayName": "JonahS"}, "userId": "NjJPzTdMQkX5ZeQaK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/agaaJ5ABjhj5WEaBu/high-school-activities-and-medical-school-admissions", "pageUrlRelative": "/posts/agaaJ5ABjhj5WEaBu/high-school-activities-and-medical-school-admissions", "linkUrl": "https://www.lesswrong.com/posts/agaaJ5ABjhj5WEaBu/high-school-activities-and-medical-school-admissions", "postedAtFormatted": "Monday, September 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20High%20school%20activities%20and%20medical%20school%20admissions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHigh%20school%20activities%20and%20medical%20school%20admissions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FagaaJ5ABjhj5WEaBu%2Fhigh-school-activities-and-medical-school-admissions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=High%20school%20activities%20and%20medical%20school%20admissions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FagaaJ5ABjhj5WEaBu%2Fhigh-school-activities-and-medical-school-admissions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FagaaJ5ABjhj5WEaBu%2Fhigh-school-activities-and-medical-school-admissions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1384, "htmlBody": "<p class=\"MsoNormal\">Many people consider medicine to be a desirable profession because of the high status of the job of being a doctor, the opportunity to help people, and the high income. Whether conventional views about what it&rsquo;s like to be in medicine are well grounded is <a href=\"http://well.blogs.nytimes.com/2011/07/21/why-would-anyone-choose-to-become-a-doctor/\">unclear</a>, but it&rsquo;s undeniable that many young people aspire to become doctors.</p>\n<p class=\"MsoNormal\">In order to become a doctor, you need to secure admission to medical school. This post offers one perspective on one aspect of what to do <em>if</em> your goal is to get into medical school.</p>\n<p class=\"MsoNormal\"><a id=\"more\"></a></p>\n<h2>What Matters for Medical School Admissions?</h2>\n<p class=\"MsoNormal\">Medical school admission is competitive, with the overall rate of acceptance to US medical schools<span style=\"mso-spacerun:yes\">&nbsp;</span>at <a href=\"https://www.aamc.org/download/157450/data/table24-mcatgpagridall2008-10.pdf.pdf\">45.2%</a>. So in order to get into medical school, the average applicant faces the challenge of differentiating him or herself from other applicants.</p>\n<p class=\"MsoNormal\">Many high school students who aspire to be doctors place high emphasis on getting into a good college, because they think that it will better position them to get into medical school.</p>\n<p class=\"MsoNormal\">Medical school acceptance rate varies significantly according to college attended. The statistics that colleges release about the acceptance rates for their undergraduates can be misleading, because some colleges discourage underqualified students from applying to medical school. But there&rsquo;s a strong case that among those undergraduates who aspire to be doctors, Harvard undergraduates get into medical school with much higher probability than do undergraduates at lower tier state universities.</p>\n<p class=\"MsoNormal\">Some high school students take this to be evidence that going to a more prestigious college increases their chances of getting into medical school. This could be true, but a major confounding factor is <a href=\"http://econlog.econlib.org/archives/2012/01/correcting_for.html\">ability bias</a>: students who are able to get into a prestigious college are unusually strong academically, and so would have been more likely into medical school independently of where they went to college. Though different people have different views, it appears that the consensus view of medical advice forums and websites is that undergraduate institution attended plays a relatively minor role in medical admissions (see, e.g. <a href=\"http://prospectivedoctor.com/articles/item/118-does-undergraduate-reputation-matter-for-admissions\">here</a>).</p>\n<p class=\"MsoNormal\">What are the major inputs into medical school admissions? The Association of American Medical Colleges publishes <a href=\"https://www.aamc.org/download/157450/data/table24-mcatgpagridall2008-10.pdf.pdf\">statistics</a> on percentage of applicants accepted to US medical schools, according to GPA range and <a href=\"http://en.wikipedia.org/wiki/Medical_College_Admission_Test\">MCAT</a> range. The associations between GPA and MCAT scores and probability of acceptance are very strong. Though <a href=\"http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation\">correlation is not causation</a>, <a href=\"/lw/iao/common_sense_as_a_prior/\">elite conventional wisdom</a> is that the association is mostly causal: increasing your GPA and MCAT scores increases the probability that you&rsquo;ll get into medical school.</p>\n<p class=\"MsoNormal\">Focusing on GPA: of med school applicants who scored between 30 and 32 on the MCAT, acceptance rates associated with different GPA ranges (&gt;= 3.0) were as follows:</p>\n<p class=\"MsoNormal\">3.80-4.00 GPA &mdash; 82.3%</p>\n<p class=\"MsoNormal\">3.60-3.79 GPA &mdash; 72.1%</p>\n<p class=\"MsoNormal\">3.40-3.59 GPA &mdash; 55.5%</p>\n<p class=\"MsoNormal\">3.20-3.39 GPA &mdash; 38.7%</p>\n<p class=\"MsoNormal\">3.00-3.19 GPA &mdash; 29.7%</p>\n<p class=\"MsoNormal\">This suggests that for medical school admissions, <em>the grades that you get in college </em>matter far more than <em>where you go to college</em>, except to the extent that where you go to college impacts your grades and MCAT scores.</p>\n<h2>How does where you go to college impact your grades and MCAT scores?&nbsp;</h2>\n<ul>\n<li>Some people have said that going to a more prestigious college exposes students to a peer group that facilitates academic success. For example, <a href=\"/r/discussion/lw/ikh/course_selection_based_on_instructor/9pv5\">Ben Kuhn wrote</a>:<br /><br /><em>Most of the more difficult courses I've taken have given me at least some value by granting better access to more smart, competent people. [&hellip;] By watching how more competent people work and think, you can often pick up useful study habits and better techniques for the subject you're studying. [&hellip;] Both more advanced students and instructors can be very useful for the academic advice they provide later. Knowing talented students has given me info about several excellent courses, as well as summer opportunities, I wouldn't otherwise have known about.<br /><br /></em>This could be a highly significant factor.</li>\n<li>Some people believe that the quality of education at more prestigious colleges is higher. If this is true, prestigious colleges could do a better job of preparing students for the MCAT. I believe that this probably isn&rsquo;t true in a systematic way: it&rsquo;s been observed that community colleges often provide higher quality introductory level courses than four year colleges do (<a href=\"http://www.outsidethebeltway.com/community_colleges_better_than_universities/\">[1]</a>, <a href=\"http://www.collegeatlas.org/community-college-benefits.html\">[2]</a>, <a href=\"http://www.centralfloridafuture.com/opinion/community-college-offers-equal-quality-education-1.2829098\">[3]</a>, <a href=\"/r/discussion/lw/ikh/course_selection_based_on_instructor/9pti\">[4]</a>).</li>\n</ul>\n<h2>Implications for how to spend time as a high school student</h2>\n<p class=\"MsoNormal\">Two functions that a high school student&rsquo;s activities serve are <a href=\"/lw/iki/high_school_human_capital_signaling_and_college/\">building human capital and signaling quality to colleges</a>.</p>\n<p class=\"MsoNormal\">As above, signaling quality to colleges might matter for medical school admissions, because of more prestigious colleges offering a better peer group. The other benefits of going to a more prestigious college are less clear.&nbsp;</p>\n<p class=\"MsoNormal\">For some high school students, the opportunity to increase their chances of getting into medical school by <em>building human capital</em> may be much more significant than the opportunity to increase their chances of getting into medical school by signaling quality to colleges.&nbsp;</p>\n<p class=\"MsoNormal\"><strong>Building human capital in preparation for future coursework</strong></p>\n<p class=\"MsoNormal\">The main inputs into medical school admissions are probably GPA and MCAT scores.</p>\n<p class=\"MsoNormal\">Students take the MCAT <a href=\"https://www.aamc.org/students/applying/mcat/faq/313878/whenshoulditakethemcatexam.html\">near the end of college</a>, and it&rsquo;s unclear that it&rsquo;s possible for high school students significantly influence their test scores 4+ years in the future (although it may be possible for them to do so).</p>\n<p class=\"MsoNormal\">It&rsquo;s more clear that high school students <em>do</em> have the opportunity to engage in activities that will increase their college GPA. Medical schools <a href=\"http://bioeng.berkeley.edu/undergrad/premedinfo\">require</a> that college students take two years of chemistry, one year of biology and one year of physics (in addition to English and sometimes calculus). High school students are in a good position to get started learning the material that will be covered in these college courses, improving their chances of getting good grades in these courses.</p>\n<p class=\"MsoNormal\">High school students can take Advanced Placement courses in biology, chemistry and physics. The material in these courses overlaps with the material in the courses that medical schools require, and in some cases is prerequisite to it.&nbsp;</p>\n<p class=\"MsoNormal\">It&rsquo;s often possible to get an &lsquo;A&rsquo; an AP course <a href=\"/r/discussion/lw/ihw/advanced_placement_exam_cutoffs_and_superficial/\">without mastering the material</a>. High schoolers can benefit substantially more from AP courses, by learning the material more thoroughly than they need to get A&rsquo;s in the courses (and 5&rsquo;s on the AP exam), and <a href=\"http://en.wikipedia.org/wiki/Overlearning\">practice well beyond the point of initial mastery</a>. This will prepare them noticeably better for the corresponding college courses than merely taking the AP courses and getting A&rsquo;s in them.&nbsp;</p>\n<p class=\"MsoNormal\">High school students can enhance their ability to master material in AP courses by learning the material in prerequisite courses well.</p>\n<p class=\"MsoNormal\">Students who learn chemistry and physics often find math to be a stumbling block.<span style=\"mso-spacerun:yes\">&nbsp; </span>So high school students can prepare themselves to do better in college chemistry and physics by learning high school math well.</p>\n<p class=\"MsoNormal\"><strong>Balancing building human capital and signaling quality to colleges</strong></p>\n<p class=\"MsoNormal\">Taking and learning biology, chemistry, physics and math in high school, and learning them unusually well, signals quality to colleges. Colleges look favorably upon good grades in AP courses. Learning sciences well prepares students to do well on the SAT subject tests in the respective sciences, and scores on SAT subject tests are an input into college admissions decisions.</p>\n<p class=\"MsoNormal\">At the same time, doing these things may not be <em>optimal</em> for signaling quality to colleges. Learning math and science really well during high school can come at the cost of grades in other subjects, and extracurricular activities. College admissions committees don&rsquo;t reward learning beyond what&rsquo;s needed for high grades and SAT subject test scores.</p>\n<p class=\"MsoNormal\">So spending high school time building skills for succeeding in future premed college courses (and thereby having better prospects for getting into medical school) can be in conflict with signaling quality to colleges.&nbsp;</p>\n<p class=\"MsoNormal\">One is then faced with the question of how to balance the two things. For a given student, some relevant questions are:</p>\n<ol>\n<li>How much does having a stronger, more motivated peer group matter? Some students have no trouble learning on their own, and staying goal directed, while others find it much easier to stay motivated and learn when they&rsquo;re interacting with peers who have similar goals.</li>\n<li>To what degree is signaling quality to colleges compromised by a given opportunity to learn premed science?</li>\n</ol>\n<p class=\"MsoNormal\">What&rsquo;s best for a given individual depends on contextual particulars. But it&rsquo;s worth noting that &ldquo;doing well in high school&rdquo; in a conventional sense may not be the best approach for getting into medical school.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "agaaJ5ABjhj5WEaBu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 1.3338685421575505e-06, "legacy": true, "legacyId": "24072", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p class=\"MsoNormal\">Many people consider medicine to be a desirable profession because of the high status of the job of being a doctor, the opportunity to help people, and the high income. Whether conventional views about what it\u2019s like to be in medicine are well grounded is <a href=\"http://well.blogs.nytimes.com/2011/07/21/why-would-anyone-choose-to-become-a-doctor/\">unclear</a>, but it\u2019s undeniable that many young people aspire to become doctors.</p>\n<p class=\"MsoNormal\">In order to become a doctor, you need to secure admission to medical school. This post offers one perspective on one aspect of what to do <em>if</em> your goal is to get into medical school.</p>\n<p class=\"MsoNormal\"><a id=\"more\"></a></p>\n<h2 id=\"What_Matters_for_Medical_School_Admissions_\">What Matters for Medical School Admissions?</h2>\n<p class=\"MsoNormal\">Medical school admission is competitive, with the overall rate of acceptance to US medical schools<span style=\"mso-spacerun:yes\">&nbsp;</span>at <a href=\"https://www.aamc.org/download/157450/data/table24-mcatgpagridall2008-10.pdf.pdf\">45.2%</a>. So in order to get into medical school, the average applicant faces the challenge of differentiating him or herself from other applicants.</p>\n<p class=\"MsoNormal\">Many high school students who aspire to be doctors place high emphasis on getting into a good college, because they think that it will better position them to get into medical school.</p>\n<p class=\"MsoNormal\">Medical school acceptance rate varies significantly according to college attended. The statistics that colleges release about the acceptance rates for their undergraduates can be misleading, because some colleges discourage underqualified students from applying to medical school. But there\u2019s a strong case that among those undergraduates who aspire to be doctors, Harvard undergraduates get into medical school with much higher probability than do undergraduates at lower tier state universities.</p>\n<p class=\"MsoNormal\">Some high school students take this to be evidence that going to a more prestigious college increases their chances of getting into medical school. This could be true, but a major confounding factor is <a href=\"http://econlog.econlib.org/archives/2012/01/correcting_for.html\">ability bias</a>: students who are able to get into a prestigious college are unusually strong academically, and so would have been more likely into medical school independently of where they went to college. Though different people have different views, it appears that the consensus view of medical advice forums and websites is that undergraduate institution attended plays a relatively minor role in medical admissions (see, e.g. <a href=\"http://prospectivedoctor.com/articles/item/118-does-undergraduate-reputation-matter-for-admissions\">here</a>).</p>\n<p class=\"MsoNormal\">What are the major inputs into medical school admissions? The Association of American Medical Colleges publishes <a href=\"https://www.aamc.org/download/157450/data/table24-mcatgpagridall2008-10.pdf.pdf\">statistics</a> on percentage of applicants accepted to US medical schools, according to GPA range and <a href=\"http://en.wikipedia.org/wiki/Medical_College_Admission_Test\">MCAT</a> range. The associations between GPA and MCAT scores and probability of acceptance are very strong. Though <a href=\"http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation\">correlation is not causation</a>, <a href=\"/lw/iao/common_sense_as_a_prior/\">elite conventional wisdom</a> is that the association is mostly causal: increasing your GPA and MCAT scores increases the probability that you\u2019ll get into medical school.</p>\n<p class=\"MsoNormal\">Focusing on GPA: of med school applicants who scored between 30 and 32 on the MCAT, acceptance rates associated with different GPA ranges (&gt;= 3.0) were as follows:</p>\n<p class=\"MsoNormal\">3.80-4.00 GPA \u2014 82.3%</p>\n<p class=\"MsoNormal\">3.60-3.79 GPA \u2014 72.1%</p>\n<p class=\"MsoNormal\">3.40-3.59 GPA \u2014 55.5%</p>\n<p class=\"MsoNormal\">3.20-3.39 GPA \u2014 38.7%</p>\n<p class=\"MsoNormal\">3.00-3.19 GPA \u2014 29.7%</p>\n<p class=\"MsoNormal\">This suggests that for medical school admissions, <em>the grades that you get in college </em>matter far more than <em>where you go to college</em>, except to the extent that where you go to college impacts your grades and MCAT scores.</p>\n<h2 id=\"How_does_where_you_go_to_college_impact_your_grades_and_MCAT_scores__\">How does where you go to college impact your grades and MCAT scores?&nbsp;</h2>\n<ul>\n<li>Some people have said that going to a more prestigious college exposes students to a peer group that facilitates academic success. For example, <a href=\"/r/discussion/lw/ikh/course_selection_based_on_instructor/9pv5\">Ben Kuhn wrote</a>:<br><br><em>Most of the more difficult courses I've taken have given me at least some value by granting better access to more smart, competent people. [\u2026] By watching how more competent people work and think, you can often pick up useful study habits and better techniques for the subject you're studying. [\u2026] Both more advanced students and instructors can be very useful for the academic advice they provide later. Knowing talented students has given me info about several excellent courses, as well as summer opportunities, I wouldn't otherwise have known about.<br><br></em>This could be a highly significant factor.</li>\n<li>Some people believe that the quality of education at more prestigious colleges is higher. If this is true, prestigious colleges could do a better job of preparing students for the MCAT. I believe that this probably isn\u2019t true in a systematic way: it\u2019s been observed that community colleges often provide higher quality introductory level courses than four year colleges do (<a href=\"http://www.outsidethebeltway.com/community_colleges_better_than_universities/\">[1]</a>, <a href=\"http://www.collegeatlas.org/community-college-benefits.html\">[2]</a>, <a href=\"http://www.centralfloridafuture.com/opinion/community-college-offers-equal-quality-education-1.2829098\">[3]</a>, <a href=\"/r/discussion/lw/ikh/course_selection_based_on_instructor/9pti\">[4]</a>).</li>\n</ul>\n<h2 id=\"Implications_for_how_to_spend_time_as_a_high_school_student\">Implications for how to spend time as a high school student</h2>\n<p class=\"MsoNormal\">Two functions that a high school student\u2019s activities serve are <a href=\"/lw/iki/high_school_human_capital_signaling_and_college/\">building human capital and signaling quality to colleges</a>.</p>\n<p class=\"MsoNormal\">As above, signaling quality to colleges might matter for medical school admissions, because of more prestigious colleges offering a better peer group. The other benefits of going to a more prestigious college are less clear.&nbsp;</p>\n<p class=\"MsoNormal\">For some high school students, the opportunity to increase their chances of getting into medical school by <em>building human capital</em> may be much more significant than the opportunity to increase their chances of getting into medical school by signaling quality to colleges.&nbsp;</p>\n<p class=\"MsoNormal\"><strong id=\"Building_human_capital_in_preparation_for_future_coursework\">Building human capital in preparation for future coursework</strong></p>\n<p class=\"MsoNormal\">The main inputs into medical school admissions are probably GPA and MCAT scores.</p>\n<p class=\"MsoNormal\">Students take the MCAT <a href=\"https://www.aamc.org/students/applying/mcat/faq/313878/whenshoulditakethemcatexam.html\">near the end of college</a>, and it\u2019s unclear that it\u2019s possible for high school students significantly influence their test scores 4+ years in the future (although it may be possible for them to do so).</p>\n<p class=\"MsoNormal\">It\u2019s more clear that high school students <em>do</em> have the opportunity to engage in activities that will increase their college GPA. Medical schools <a href=\"http://bioeng.berkeley.edu/undergrad/premedinfo\">require</a> that college students take two years of chemistry, one year of biology and one year of physics (in addition to English and sometimes calculus). High school students are in a good position to get started learning the material that will be covered in these college courses, improving their chances of getting good grades in these courses.</p>\n<p class=\"MsoNormal\">High school students can take Advanced Placement courses in biology, chemistry and physics. The material in these courses overlaps with the material in the courses that medical schools require, and in some cases is prerequisite to it.&nbsp;</p>\n<p class=\"MsoNormal\">It\u2019s often possible to get an \u2018A\u2019 an AP course <a href=\"/r/discussion/lw/ihw/advanced_placement_exam_cutoffs_and_superficial/\">without mastering the material</a>. High schoolers can benefit substantially more from AP courses, by learning the material more thoroughly than they need to get A\u2019s in the courses (and 5\u2019s on the AP exam), and <a href=\"http://en.wikipedia.org/wiki/Overlearning\">practice well beyond the point of initial mastery</a>. This will prepare them noticeably better for the corresponding college courses than merely taking the AP courses and getting A\u2019s in them.&nbsp;</p>\n<p class=\"MsoNormal\">High school students can enhance their ability to master material in AP courses by learning the material in prerequisite courses well.</p>\n<p class=\"MsoNormal\">Students who learn chemistry and physics often find math to be a stumbling block.<span style=\"mso-spacerun:yes\">&nbsp; </span>So high school students can prepare themselves to do better in college chemistry and physics by learning high school math well.</p>\n<p class=\"MsoNormal\"><strong id=\"Balancing_building_human_capital_and_signaling_quality_to_colleges\">Balancing building human capital and signaling quality to colleges</strong></p>\n<p class=\"MsoNormal\">Taking and learning biology, chemistry, physics and math in high school, and learning them unusually well, signals quality to colleges. Colleges look favorably upon good grades in AP courses. Learning sciences well prepares students to do well on the SAT subject tests in the respective sciences, and scores on SAT subject tests are an input into college admissions decisions.</p>\n<p class=\"MsoNormal\">At the same time, doing these things may not be <em>optimal</em> for signaling quality to colleges. Learning math and science really well during high school can come at the cost of grades in other subjects, and extracurricular activities. College admissions committees don\u2019t reward learning beyond what\u2019s needed for high grades and SAT subject test scores.</p>\n<p class=\"MsoNormal\">So spending high school time building skills for succeeding in future premed college courses (and thereby having better prospects for getting into medical school) can be in conflict with signaling quality to colleges.&nbsp;</p>\n<p class=\"MsoNormal\">One is then faced with the question of how to balance the two things. For a given student, some relevant questions are:</p>\n<ol>\n<li>How much does having a stronger, more motivated peer group matter? Some students have no trouble learning on their own, and staying goal directed, while others find it much easier to stay motivated and learn when they\u2019re interacting with peers who have similar goals.</li>\n<li>To what degree is signaling quality to colleges compromised by a given opportunity to learn premed science?</li>\n</ol>\n<p class=\"MsoNormal\">What\u2019s best for a given individual depends on contextual particulars. But it\u2019s worth noting that \u201cdoing well in high school\u201d in a conventional sense may not be the best approach for getting into medical school.</p>", "sections": [{"title": "What Matters for Medical School Admissions?", "anchor": "What_Matters_for_Medical_School_Admissions_", "level": 1}, {"title": "How does where you go to college impact your grades and MCAT scores?\u00a0", "anchor": "How_does_where_you_go_to_college_impact_your_grades_and_MCAT_scores__", "level": 1}, {"title": "Implications for how to spend time as a high school student", "anchor": "Implications_for_how_to_spend_time_as_a_high_school_student", "level": 1}, {"title": "Building human capital in preparation for future coursework", "anchor": "Building_human_capital_in_preparation_for_future_coursework", "level": 2}, {"title": "Balancing building human capital and signaling quality to colleges", "anchor": "Balancing_building_human_capital_and_signaling_quality_to_colleges", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "12 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wgdfxQJ2DQuju73zC", "wPTQZ7KbiMQkrkZX6", "4W9i78uxX8Dibk77L"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-09T04:50:39.610Z", "modifiedAt": null, "url": null, "title": "Open thread, September 9-15, 2013", "slug": "open-thread-september-9-15-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:31.112Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Metus", "createdAt": "2011-01-23T21:54:34.357Z", "isAdmin": false, "displayName": "Metus"}, "userId": "mNQ4fSvro7LYgrii4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5ED9EQ5tMBtB7D7EC/open-thread-september-9-15-2013", "pageUrlRelative": "/posts/5ED9EQ5tMBtB7D7EC/open-thread-september-9-15-2013", "linkUrl": "https://www.lesswrong.com/posts/5ED9EQ5tMBtB7D7EC/open-thread-september-9-15-2013", "postedAtFormatted": "Monday, September 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20September%209-15%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20September%209-15%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ED9EQ5tMBtB7D7EC%2Fopen-thread-september-9-15-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20September%209-15%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ED9EQ5tMBtB7D7EC%2Fopen-thread-september-9-15-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ED9EQ5tMBtB7D7EC%2Fopen-thread-september-9-15-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5ED9EQ5tMBtB7D7EC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.3339116600473004e-06, "legacy": true, "legacyId": "24073", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 127, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-09T07:08:58.837Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne Social Meetup", "slug": "meetup-melbourne-social-meetup-26", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:30.539Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Maelin", "createdAt": "2009-05-28T03:32:36.549Z", "isAdmin": false, "displayName": "Maelin"}, "userId": "CE5vuYfsSRTeG2KWd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tEtFw29cNwKeKXGui/meetup-melbourne-social-meetup-26", "pageUrlRelative": "/posts/tEtFw29cNwKeKXGui/meetup-melbourne-social-meetup-26", "linkUrl": "https://www.lesswrong.com/posts/tEtFw29cNwKeKXGui/meetup-melbourne-social-meetup-26", "postedAtFormatted": "Monday, September 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20Social%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20Social%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtEtFw29cNwKeKXGui%2Fmeetup-melbourne-social-meetup-26%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20Social%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtEtFw29cNwKeKXGui%2Fmeetup-melbourne-social-meetup-26", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtEtFw29cNwKeKXGui%2Fmeetup-melbourne-social-meetup-26", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 138, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qp'>Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 September 2013 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">5/52 Leicester St, Carlton 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's regular monthly Social Meetup will be running as normal on the third Friday evening of the month. All welcome from 6:30pm, feel free to arrive later if that is easier for you.</p>\n\n<p>Our social meetups are friendly, informal events where we chat about topics of interest and often play board games. Sometimes we will also play parlour games like Mafia (a.k.a. Werewolf) or Resistance. We usually order some sort of take-away dinner for any that wish to partake.</p>\n\n<p>Please ring the number 5 button when you arrive in the foyer and we'll buzz you up. If you get lost or have any problems, feel free to call me (Richard) on 0421231789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qp'>Melbourne Social Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tEtFw29cNwKeKXGui", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3340326381311163e-06, "legacy": true, "legacyId": "24080", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup\">Discussion article for the meetup : <a href=\"/meetups/qp\">Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 September 2013 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">5/52 Leicester St, Carlton 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's regular monthly Social Meetup will be running as normal on the third Friday evening of the month. All welcome from 6:30pm, feel free to arrive later if that is easier for you.</p>\n\n<p>Our social meetups are friendly, informal events where we chat about topics of interest and often play board games. Sometimes we will also play parlour games like Mafia (a.k.a. Werewolf) or Resistance. We usually order some sort of take-away dinner for any that wish to partake.</p>\n\n<p>Please ring the number 5 button when you arrive in the foyer and we'll buzz you up. If you get lost or have any problems, feel free to call me (Richard) on 0421231789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/qp\">Melbourne Social Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-09T16:39:19.798Z", "modifiedAt": null, "url": null, "title": "Book Review: Cognitive Science (MIRI course list)", "slug": "book-review-cognitive-science-miri-course-list", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:27.945Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ucpD7YtcvKo6C9fBK/book-review-cognitive-science-miri-course-list", "pageUrlRelative": "/posts/ucpD7YtcvKo6C9fBK/book-review-cognitive-science-miri-course-list", "linkUrl": "https://www.lesswrong.com/posts/ucpD7YtcvKo6C9fBK/book-review-cognitive-science-miri-course-list", "postedAtFormatted": "Monday, September 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Book%20Review%3A%20Cognitive%20Science%20(MIRI%20course%20list)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABook%20Review%3A%20Cognitive%20Science%20(MIRI%20course%20list)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FucpD7YtcvKo6C9fBK%2Fbook-review-cognitive-science-miri-course-list%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Book%20Review%3A%20Cognitive%20Science%20(MIRI%20course%20list)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FucpD7YtcvKo6C9fBK%2Fbook-review-cognitive-science-miri-course-list", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FucpD7YtcvKo6C9fBK%2Fbook-review-cognitive-science-miri-course-list", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4502, "htmlBody": "<p>I'm reviewing the books on the <a href=\"http://intelligence.org/courses/\">MIRI course list</a>. After reading <a href=\"/lw/ii0/book_review_heuristics_and_biases_miri_course_list/\">Heuristics and Biases</a> I picked up <em>Cognitive Science</em>, by Jos&eacute; Luis Berm&uacute;dez. It taught me a number of interesting things, but I strongly disliked it.</p>\n<h1 id=\"cognitivescience\">Cognitive Science</h1>\n<p>I recommend against reading this book, for reasons that I'll go into at the end of the review. Before that, I'll summarize each chapter.</p>\n<p style=\"text-align:center\"><img src=\"http://images.betterworldbooks.com/052/9780521708371.jpg\" alt=\"Cognitive Science (cover)\" width=\"307\" height=\"400\" /></p>\n<h2 id=\"chaptersummaries\">Chapter Summaries</h2>\n<ol>\n<li><a href=\"#1theprehistoryofcognitivescience\">The prehistory of cognitive science</a></li>\n<li><a href=\"#2thedisciplinematuresthreemilestones\">The discipline matures: three milestones</a></li>\n<li><a href=\"#3theturntothebrain\">The turn to the brain</a></li>\n<li><a href=\"#4cognitivescienceandtheintegrationchallenge\">Cognitive science and the integration challenge</a></li>\n<li><a href=\"#5tacklingtheintegrationchallenge\">Tackling the integration challenge</a></li>\n<li><a href=\"#6physicalsymbolsystemsandthelanguageofthought\">Physical symbol systems and the language of thought</a></li>\n<li><a href=\"#7applyingthesymbolicparadigm\">Applying the symbolic paradigm</a></li>\n<li><a href=\"#8neuralnetworksanddistributedinformationprocessing\">Neural networks and distributed information processing</a></li>\n<li><a href=\"#9neuralnetworkmodelsofcognitiveprocesses\">Neural network models of cognitive processes</a></li>\n<li><a href=\"#10howarecognitivesystemsorganized?\">How are cognitive systems organized?</a></li>\n<li><a href=\"#11strategiesformappingthebrain\">Strategies for mapping the brain</a></li>\n<li><a href=\"#12acasestudyExploringmindreading\">A case study: Exploring mind reading</a></li>\n<li><a href=\"#13newhorizonsDynamicalsystemsandsituatedcognition\">New horizons: Dynamical systems and situated cognition</a></li>\n<li><a href=\"#14lookingaheadChallengesandApplications\">Looking ahead: Challenges and Applications</a></li>\n</ol>\n<h2 id=\"1theprehistoryofcognitivescience\"><a id=\"more\"></a></h2>\n<h2>\n<hr />\n</h2>\n<h2 id=\"1theprehistoryofcognitivescience\">1. The prehistory of cognitive science</h2>\n<p>This chapter introduces the idea of cognition as a form of information processing, and discusses how that viewpoint came about. Short version:</p>\n<ol>\n<li>Behaviorism isn't enough to explain intelligent behavior. \n<ul>\n<li>Mice allowed to wander in mazes with no goal were able to navigate the maze quickly when goals were added. Reinforcement on its own does not sufficiently explain learning.</li>\n</ul>\n</li>\n<li>Church &amp; Turing put forth the Church-Turing thesis (anything computable is computable by a Turing machine).</li>\n<li>Studying language syntax revealed deep structure and rules.</li>\n</ol>\n<p>The chapter illustrates how these insights led to the idea of cognition as information processing.</p>\n<p><em>This chapter is informative from a historical perspective. It might be worth reading.</em></p>\n<p><a id=\"more\"></a></p>\n<hr />\n<h2 id=\"2thedisciplinematuresthreemilestones\">2. The discipline matures: three milestones</h2>\n<p>This chapter touches upon three early cognitive science \"milestones\": A computer program, an experiment, and a cognitive model.</p>\n<ol>\n<li>SHRDLU was a program that uses natural language in a very limited setting.</li>\n<li>The study measured the time it takes to, given two images, determine whether the second depicts the same object as the first (viewed from a different angle). \n<ul>\n<li>It turns out the amount of time it takes is a linear function of the amount of rotation between views of the object.</li>\n<li>Because each image contains the same amount of visual data (measured in pixels or bits), this shows that cognitive processing must be tiered.</li>\n</ul>\n</li>\n<li>This led to a \"hierarchical\" model of cognitive science. \n<ul>\n<li>One layer makes representations out of images, another layer rotates them and pattern matches.</li>\n</ul>\n</li>\n</ol>\n<p><em>This chapter is superficial; I wouldn't recommend it to the LessWrong audience. The study in (2) is interesting, but now that you know about it I'd recommend reading the study directly.</em></p>\n<p><a id=\"more\"></a></p>\n<hr />\n<h2 id=\"3theturntothebrain\">3. The turn to the brain</h2>\n<p>Eventually people developed the tech to actually watch the brain work, which had an impact on cognitive models.</p>\n<p>We have high-level imaging techniques and low-level neuron monitoring. That latter is only done on non-humans.</p>\n<p>Scientists studying the brain found discrete cognitive modules. Surgery on monkey brains can reliably induce specific deficiencies. We can inhibit object recognition (the \"what\") and spacial recognition (the \"where\") separately, by cutting out specific parts of the brain.</p>\n<p>The same studies showed that information can take multiple routes: information from the visual cortex flows along a ventral stream (visual cortex &rarr; object recognizer) and a dorsal stream (visual cortex &rarr; spacial locator).</p>\n<p>Clever experiments with humans allow us to see what parts of the brain are activated by certain activities. Techniques include tracking irradiated fluids (PET) and tracking blood flow (MRI). By having patients do similar tasks with small modifications, we can get brain activity data. By aggregating data and subtracting data from control experiments, we can identify brain regions associated with various activities.</p>\n<p>The chapter concludes with a brief discussion of the discovery of neurons.</p>\n<p><em>This chapter is superficial; you can get the same info from the \"brain\" section of an introductory biology course.</em></p>\n<p><a id=\"more\"></a></p>\n<hr />\n<h2 id=\"4cognitivescienceandtheintegrationchallenge\">4. Cognitive science and the integration challenge</h2>\n<p>Cognitive Science spans everything from molecular biology to deep philosophy. Unfortunately, we don't know how all the parts connect yet. This is referred to as the \"Integration Challenge\".</p>\n<p>The following is presented as an example:</p>\n<p>You have four cards. Flip whichever cards you must to verify the rule <strong>If a card has a vowel on one side then it has an even number on the other</strong>:</p>\n<pre><code>E C 4 5\n</code></pre>\n<p>People do notoriously badly at this game. It's called the \"Wason selection task\". It was mentioned in the sequences a few times. But it turns out, people are much better at this version:</p>\n<p>There are four people drinking at a bar. A police officer busts in and needs to verify the rule <strong>If a person is drinking alcohol, they must be at least 21</strong>. Which of the following must they investigate further?</p>\n<pre><code>Beer-drinker, Coke-drinker, 25-year-old, 16-year-old\n</code></pre>\n<p>These problems are logically identical. However, most people suggest flipping 4 while few people suggest checking what the 25 year old is drinking.</p>\n<p>More generally, it seems that people <em>can</em> do very well on the Wason selection task <em>if</em> it's framed in such a way that people are looking for cheaters. (Eliminating the police officer from the above story is sufficient to reduce performance.)</p>\n<p>This lends credence to the idea of separate \"mind modules\" that can be activated (such as a cheater-detection module). The fields of cognitive bias, evolutionary biology, and economic game theory all have a part in explaining this phenomenon.</p>\n<p>It seems that many fields overlap in the cognitive realm. The fact that much research is siloed in separate fields is framed as the \"integration challenge\".</p>\n<p><em>The chapter is somewhat interesting, but is primarily superficial and wastes a few pages explaining basic concepts such as the Prisoner's Dilemma. I'd avoid it.</em></p>\n<p><a id=\"more\"></a></p>\n<hr />\n<h2 id=\"5tacklingtheintegrationchallenge\">5. Tackling the integration challenge</h2>\n<p>This chapter attempts to address the \"integration challenge\" posed above.</p>\n<p>The chapter opens with a brief (uninspired) introduction to reductionism and some wishy-washy reasons why it doesn't apply to cognitive science. This basically boils down to the following:</p>\n<p><em>Cognitive science 'laws' are merely statistical. Cognitive science 'equations' (such as perceived intensity of stimulus, &Psi;=k\u03d5^n) are descriptive, not explanatory. Therefore, you can't reduce cognitive science to physics.</em></p>\n<p>Another alternative is explored, the \"Tri-level hypothesis\", which basically posits that you can look at the brain as a collection of information processing systems, each of which should be viewed on three levels:</p>\n<ol>\n<li>The transformation level (what's the input, what's the output, what's the transformation?)</li>\n<li>The algorithmic level (how does the transformation work?)</li>\n<li>The mechanical level (how is it implemented in the real world?)</li>\n</ol>\n<p>This is also rejected, on the basis that it Just Doesn't Seem Like Intelligence. (There's no room for executive control, its not clear how learning would occur, the modules seem like they would be too independent.)</p>\n<p><em>I recommend avoiding this chapter. It is a confused and poor introduction to reductionism that makes many flawed arguments.</em></p>\n<p><a id=\"more\"></a></p>\n<hr />\n<h2 id=\"6physicalsymbolsystemsandthelanguageofthought\">6. Physical symbol systems and the language of thought</h2>\n<p>This chapter opens with a proposal from the 1970s called the \"Physical Symbol System Hypothesis\":</p>\n<blockquote>\n<p>A physical symbol system has the necessary and sufficient means for general intelligent action.</p>\n</blockquote>\n<p>It explores this hypothesis a bit. It make analogies to logic (a symbol game that can be interpreted with meaning), gives some examples of search algorithms (that operate by symbol manipulation), and explores how language is a type of symbol manipulation.</p>\n<p>It proceeds to counter this idea with the Chinese Room argument and the Symbol Grounding Problem (how do symbols become meaningful?).</p>\n<p><em>I recommend avoiding this chapter. The first half of the chapter is slow and redundant to anyone moderately familiar with logic/computation. The latter half falls into a number of traps that the Sequences are specifically designed to disarm. Members of the LessWrong community are likely to find it frustrating.</em></p>\n<p><a id=\"more\"></a></p>\n<hr />\n<h2 id=\"7applyingthesymbolicparadigm\">7. Applying the symbolic paradigm</h2>\n<p>This chapter explores decision algorithms and machine learning. It describes an algorithm designed to build a decision tree from a database (by analyzing which questions have the most expected information).</p>\n<p>The algorithm was somewhat frustrating, because simple probabilistic reasoning could have improved the results significantly. (And because the algorithm was presented as a brilliant real-world example and not as a simple toy.)</p>\n<p>The later part of the chapter discussed some robots that do symbol manipulation to achieve their goals (such as SHAKEY).</p>\n<p><em>I recommend avoiding this chapter. It's painfully slow for someone already familiar with computation and the basics of information theory.</em></p>\n<p><em>(If you don't know about computation and the basics of information theory, I suggest learning them elsewhere.)</em></p>\n<p><a id=\"more\"></a></p>\n<hr />\n<h2 id=\"8neuralnetworksanddistributedinformationprocessing\">8. Neural networks and distributed information processing</h2>\n<p>This chapter begins with a brief introduction to how a neuron operates and describes artificial neural networks.</p>\n<p>It wasted some time explaining boolean functions, and then introduced the \"perceptron convergence rule\", a learning algorithm that causes any single-layer neural net to converge on the optimal answer (assuming the optimal answer can be encoded in a single-layer neural net).</p>\n<p>Some examples are given before showing that a single-layer neural net cannot implement XOR. This is followed by a claim that single-layer neural nets can only implement linearly separable functions.</p>\n<p>Multi-layer neural nets can implement any function (in principle), but no equivalent convergence rule exists in multi-layer nets. Alternative learning mechanisms, such as backpropagation of error, are explored. However, backpropagation is non-local and biologically implausible. (There is some evidence against backpropagation of error in brains.)</p>\n<p>The remainder of the chapter explores the difference between the neural-net brain architectures and physical symbol processors. Major differences include:</p>\n<ul>\n<li>There is no clear distinction between information storage and information processing in a neural net</li>\n<li>There is no distinction between representation and rules in a neural net.</li>\n</ul>\n<p>This is contrasted with Turing machine, in which the symbols and rules are quite distinct.</p>\n<p><em>This chapter has some signal in it, but it's mostly lost in the noise of superficial overviews (at the beginning) and a false dichotomy (at the end).</em></p>\n<p><a id=\"more\"></a></p>\n<hr />\n<h2 id=\"9neuralnetworkmodelsofcognitiveprocesses\">9. Neural network models of cognitive processes</h2>\n<p>This chapter discusses learning. Those who model the brain as a symbol-manipulation engine view learning very differently from those who model the brain as a neural net.</p>\n<p>From the symbol-manipulation side, the following argument is presented:</p>\n<p><em>In order to learn English, you must learn the truth conditions of sentences. These truth conditions must be expressed in a lower level language. That lower level language is the language of thought, which must be innate.</em></p>\n<p>The neural-net side offers an alternative:that learning can proceed by strengthening or weakening connections between neurons. Some experimental evidence supports these claims:</p>\n<p>When children are learning to speak, they go through a few phases with irregular verbs (give/gave). Early on, children conjugate correctly (gave). Later, they regress and start conjugating irregular verbs like normal verbs (gived). Later, they regain their original skill.</p>\n<p>A few studies of neural nets showed that verb-conjugating neural nets exhibit a similar pattern, if they're trained on irregular verbs for a little while before increasing their vocabulary.</p>\n<p>Other examples of humans-learning-like-neural-nets are presented, and are interesting.</p>\n<p>The latter half of the chapter cites various studies concerned with infant learning. Some proponents of the physical symbol hypothesis have claimed that a baby's world must be a chaos of sensory overload (as the baby cannot yet parse the world into representations). However, studies show that this is not the case.</p>\n<p>It turns out that babies look longer at things which surprise them. This can be used to measure what babies find surprising. Babies scrutinize physics-defying videos longer than physics-obeying videos, implying that a certain amount of world knowledge is \"baked in\".</p>\n<p>The chapter concludes by acknowledging that neural nets and physical symbol systems are not mutually exclusive, and that the latter may be implemented by the former. However, neural nets show that there is a non-symbolic way to process information, thereby discounting the physical symbol system hypothesis as \"necessary and sufficient\". We should broaden our view to include more than just the physical symbol hypothesis in our model of cognition.</p>\n<p><em>The points made in this chapter are interesting and were mostly new to me. If any of the above was surprising, this chapter is likely worth your time.</em></p>\n<p><a id=\"more\"></a></p>\n<hr />\n<h2 id=\"10howarecognitivesystemsorganized\">10. How are cognitive systems organized?</h2>\n<p>This chapter explores agent architectures.</p>\n<p>One architecture, <em>reflex agents</em>, can't be termed intelligent. The input is directly linked to the action system via a small set of rules.</p>\n<p>Another architecture, <em>goal-based agents</em>, have a model of the world and a set of goals, and take actions to maximize their goals. They have no mechanism for learning.</p>\n<p>The third architecture, <em>learning agents</em>, have a memory and some learning mechanism.</p>\n<p>Such architectures are naturally described in a modular manner. (Input systems, output systems, memory systems, prediction systems, goal-evaluations systems, etc.). Furthermore, the brain studies from earlier imply some degree of modularity.</p>\n<p>This gives rise to the modularity hypothesis, which claims that the brain is modular.</p>\n<p>A \"massive modularity\" hypothesis is put forth, claiming that the brain is entirely made of domain-specific modular components.</p>\n<p>The arguments in favor of modularity include:</p>\n<p>Evolution baked many behaviors. For example, we have some degree of willingness to sacrifice ourselves for family. This isn't because we run the numbers and discover that our genes propagate better that way. Rather, it's because the self-propagation gene <em>actually did</em> propagate further. We have \"Darwinian modules\" that have been baked in for fitness reasons. (Readers may notice echoes of <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">The Sequences</a> here.)</p>\n<p>Given the wide array of domain-specific and unrelated adaptations that evolution must provide (cheater-detection, facial-recognition, emotion-extrapolation, folk-physics, etc.) it makes sense to consider these modules separately.</p>\n<p>In other words, because evolution is not coordinated we should expect specific evolutionary advantages to be realized as domain-specific, encapsulated cognitive modules.</p>\n<p>In its strongest form, the massive modularity hypothesis states that there are no domain-general fitness criteria, so evolution cannot create domain-general cognitive mechanisms. They claim the brain has no domain-general central processing, but rather is a collection of domain-specific modules.</p>\n<p><span style=\"color: #999999;\">Note: This argument has as many holes in the book as it has in my summary.</span></p>\n<p>Counter-arguments are then presented illustrating why the strong massive modularity hypothesis is silly:</p>\n<ul>\n<li>\"Darwinian modules\" are not applied in a mandatory fashion: people seem capable of overriding the self-sacrifice module.</li>\n<li>Modules take a limited range of inputs. How are those inputs selected? Inputs to a cheater-detection-module must be representations of social exchanges. There must be a filter that filters some larger data set down to just the social exchanges. What determines the input for that filtering module? Continue up the stack until you have something that is operating on very wide inputs. This is domain general.</li>\n<li>How could domain-general learning be possible with only domain-specific modules?</li>\n</ul>\n<p>Or, in other words, \"humans seem pretty domain-general to me\".</p>\n<p>The book concludes that while there may be many modules, the brain is not <em>only</em> domain-specific modules.</p>\n<p>The remainder of the chapter studies a \"hybrid architecture\" called ACT-R/PM. The example struck me as a stretch to prove a point, and did not seem relevant.</p>\n<p><em>I'd avoid this chapter. It wasted a lot of time presenting a false dichotomy between modularity and \"having some sort of central executor\" by presenting a straw man argument. It didn't provide new insights.</em></p>\n<p><a id=\"more\"></a></p>\n<hr />\n<h2 id=\"11strategiesformappingthebrain\">11. Strategies for mapping the brain</h2>\n<p>This chapter returns to brain-scanning tools. It gives an overview of brain areas. It briefly mentions mirror neurons (neurons that fire both when you do something and when you watch somebody else do it). It discusses many techniques for looking at brains. Some are invasive, single-neuron, and only done on non-humans. Others are high-level and observe things like blood flow, which may or may not be a good indicator of brain activity. We lack tools to study the middle-ground, regions of neurons and their connections. The chapter concluded with a discussion of the potential pitfalls when drawing conclusions from incomplete data.</p>\n<p><em>This chapter was a rehash of chapter three. I found it useless.</em></p>\n<p><a id=\"more\"></a></p>\n<hr />\n<h2 id=\"12acasestudyexploringmindreading\">12. A case study: Exploring mind reading</h2>\n<p>This chapter explored the mechanisms of empathy.</p>\n<p>It first explored the ability to play \"make believe\" as an early form of metarepresentation (representing a representation), a concept assumed to be necessary before one can disconnect representations from what they represent. For example, a metarepresentation allows you to go from \"The world is in state X\" to \"Susan believes the world is in state X\" which requires you to represent the world-representation \"state X\" as the subject of Susan's belief.</p>\n<p>(In other words, playing make-believe is seen as a crucial step in developing a model of beliefs.)</p>\n<p>This is presumed crucial in developing a theory of mind. (Indeed, autistic children tend not to play \"make believe\" and have difficulty passing false belief tests, which require a working theory of mind.)</p>\n<p>The false belief test is introduced:</p>\n<p><em>Jane and Sally see a marble placed in the basked. Jane leaves the room. Sally sees the marble moved to the box. Sally is asked where Jane will look for the marble.</em></p>\n<p>A \"theory of mind\" brain-module is postulated. It takes a bit of heat when studies show that, even after children pass the false belief test, true beliefs are easier to model than false beliefs.</p>\n<p>This is followed by a completely different theory claiming that metarepresentation is not required to pass the false belief test. This model claims that a child can represent a relationship between Sally and false world-states without ever representing a representation.</p>\n<p>Analogies are drawn to counterfactual thinking: you can think about how you could be eating a different sandwich, without thinking about how you think about sandwiches. Similarly, you can represent someone having false beliefs without representing beliefs.</p>\n<p>The book moves to the simulation model of empathy, which claims that you empathise with people by putting yourself in their shoes, pretending you believe what they believe, and assessing how you would feel.</p>\n<p>This argument is supported by studies of people with specific brain damage. Turns out, people who can't feel fear also have trouble identifying fear (but not disgust) in others, and people who can't feel disgust have trouble identifying disgust (but not fear) in others.</p>\n<p>There is some evidence showing that certain brain regions are active only when attributing false beliefs to someone. Different people interpret this evidence in different ways.</p>\n<p><em>This chapter had a lot of signal. The debate has much more depth than I've covered here. This chapter is worth your time.</em></p>\n<p><a id=\"more\"></a></p>\n<hr />\n<h2 id=\"13newhorizonsdynamicalsystemsandsituatedcognition\">13. New horizons: Dynamical systems and situated cognition</h2>\n<p>Some people argue that we're doing cognitive science wrong. When we want to govern a steam engine, we don't design a computer that samples the engine speed and adjusts the throttle: rather, we attach it to a flywheel that raises weights as the engine speeds up. Then we just let gravity sort the damn thing out.</p>\n<p>Similarly, crickets run towards other cricket-noises: but it turns out they aren't processing sounds and deciding that cricket noises are sexy. Rather, their ears (which are in their legs (which are hollow)) are directly hooked to the motor output. When the ears are properly stimulated the legs move towards the sound. No brain necessary.</p>\n<p>These people claim that we should look at humans on a dynamical level instead of postulating all of these high-falutin' modules.</p>\n<p>Some studies with infants show that they act dynamically. If you put the toy in box A a few times (training them to reach for A), then put it in box B, then they'll reach for box B.</p>\n<p>Unless you restrict them for a few seconds. Then they'll go for box A. Unless you stand them up, in which case they go for box B again. Environment, time, and muscle position all seem to affect infant activity. This implies that non-cognitive data is necessary to assess infant cognition.</p>\n<p>Detractors argue that while dynamics leads to good data, dynamics does not lead to understanding. Modelling traffic as a multi-particle system allows you to predict traffic jams better but it hardly tells you <em>why</em> the model works. For that, you need some model of human intent and perhaps a bit of game theory.</p>\n<p>Also, subjectively, brains don't feel completely dynamical.</p>\n<p>The remainder of the chapter is spent exploring dynamical robots. They're neat.</p>\n<p><em>This chapter provides an interesting new way to look at cognition. It is worth a read.</em></p>\n<p><a id=\"more\"></a></p>\n<hr />\n<h2 id=\"14lookingaheadchallengesandapplications\">14. Looking ahead: Challenges and Applications</h2>\n<p>This is very short. It covers some promising areas of research. They are:</p>\n<ol>\n<li>The Human Connectome Project (studying neural connections in the brain).</li>\n<li>Studying what the brain is doing when it's idle (instead of doing experiments where we just \"subtract\" the control).</li>\n<li>Neuroprostheses (substitute brain modules).</li>\n<li>Improved education (leveraging what we've learned about cognition).</li>\n<li>Crossovers from cognitive science to economics (how do Homo Sapiens differ from Homo Economicus, and why?) and law (who is responsible for what?).</li>\n<li>Cognitive scientists have long been careful to avoid consciousness. Now people are starting to approach the problem. Philosophical zombies are presented seriously.</li>\n</ol>\n<div><em>If you're interested in any of the above points, I recommend learning about them elsewhere. This chapter is short and does not have much data. I recommend skipping it.</em></div>\n<p><a id=\"more\"></a></p>\n<hr />\n<h2 id=\"reactions\">Reactions</h2>\n<p>This book bored and frustrated me, for a number of reasons.</p>\n<ol>\n<li><strong>Assumed a low level of technical competence from its readers:</strong> Many basic concepts were introduced superficially but at length. Examples include the structure of neurons and the prisoner's dilemma. I felt talked down to throughout much of the book. (One of the exercises was \"Give a truth table for OR.\")</li>\n<li><strong>Gave serious credence to silly ideas:</strong>&nbsp;The Chinese Room argument and Philosophical Zombies were given serious credence in this text. The fact that cognitive science is very far from reduction to physics was used as an excuse to write off reductionism entirely. Such floundering is somewhat expected in any text that touches mainstream philosophy and strives for \"political correctness\", but I expected more from the MIRI course list.</li>\n<li><strong>Lacked technical arguments:</strong>&nbsp;The book had a lot of words and not a lot of data. Much of the book was in \"he-said she-said\" format, explaining debates between cognitive scientists. I understand that the field of cognitive science is not mature enough to make many technical arguments, but even the debates were highly summarized. Worse, they were full of incomplete or confused arguments. I would have been much happier if the author presented the data and stepped aside.</li>\n<li><strong>The author was incapable of drawing conclusions:</strong>&nbsp;The author presented many arguments, argued one side, and then concluded with \"or, at least, that's what so-and-so says.\" Some of the positions were very poorly argued, and it was difficult for me to tell whether this was due to the author's misunderstanding or whether the author was faithfully relaying confused arguments.</li>\n</ol>\n<p>All in all, the book was a lot of noise with very little signal. I was expecting much more, especially given its prominent position as the first book on the MIRI course recommendations. It felt like a superficial introduction to low-level concepts. It's geared towards high school students or freshmen undergrads who do not yet know how to think critically.</p>\n<p>I was expecting a rationalist introduction to cognitive science: \"Here's what we think we know, here's how we know it. Here's the gray areas, here's what professionals say.\" I feel you could boil away 80% of this book without loss.</p>\n<p><span style=\"color:#999999;\">Note: I got this book used. The previous owner had no idea how to effectively use a highlighter. Many unimportant phrases were highlighted. I found it difficult not to read them LOUDLY. This increased my frustration. I have tried to adjust accordingly, but it may be biasing my review.</span></p>\n<p><span style=\"color:#999999;\">Note: I assume that the concepts of brain modularity, neural nets, information-as-entropy, and symbol-processing are familiar to MIRI's target audience. If that is not the case then this book provides a passable sketch for how to start thinking about minds. Even so, I would not recommend it for that purpose on account of point (2) above.</span></p>\n<h2 id=\"whatilearned\">What I learned</h2>\n<p>The book was not all bad. There was some signal in the noise. But first, a rebuttal:</p>\n<p><em>Cognitive Science</em> seemed sorely confused about one thing in particular, that being the operation of neural nets. The neural net vs physical symbol debate seemed a false dichotomy.</p>\n<p>A neural net implementing an AND gate <em>is</em> manipulating symbols, if you consider it on a sufficiently abstract level.</p>\n<p>A cricket's legs <em>are</em> doing calculations to identify cricket-noises and move in that direction. The algorithm was designed by Time and Evolution, and it's run on Physics directly (instead of on the cricket's brain), but the computation still occurs.</p>\n<p>A neural net <em>does</em> process symbols according to rules. It doesn't just come to magic answers because it has magic weights: A well-trained neural net works&nbsp;<em>because</em> the propagation of activation through the net mimics the causal structure of the real world.</p>\n<p>The author missed or neglected these points entirely.</p>\n<p>That said, this book did teach me something about symbol-processing and neural nets. It taught me that the symbols in a neural net are much looser than the symbols you'll find in modern computers. In a \"strict\" symbol manipulation system, representations are binary: either all the symbols are in place and things work, or one symbol is out of place and everything breaks.</p>\n<p>Neural nets are significantly more versatile: a neural net can be strong enough to recognize an answer without being strong enough to produce the answer. With good training, a neural net can slowly build a good representation out of random starting weights.</p>\n<p>In essence, <em>Cognitive Science</em> showed me that symbols needn't be discrete: they can be continuous, convoluted, and blurry.</p>\n<p>(Within brains, they often are all three.)</p>\n<p>This seems obvious in retrospect, but \"fuzzy symbols\" were a novel concept to me.</p>\n<p>Here's a few other tidbits that I took away:</p>\n<ul>\n<li>The perceptron convergence rule (and its inapplicability to multi-layer neural nets) was new to me.</li>\n<li>Babies do not live in a land of sensory chaos. I was once taught otherwise. <em>Cognitive Science</em> dispelled a false belief with interesting data, and for that I am thankful.</li>\n<li>The discussion of how evolution can create modules in brains was an interesting one, and gave me a new way of looking at domain-specific brain functions.</li>\n<li>I was quite surprised by the Wason Selection Task results (where framing the problem in a way that puts you on the lookout for cheaters improves performance).</li>\n</ul>\n<p>Finally, this book gives you a great overview of the history of Cognitive Science. It's easy to believe that the modern cognitive model is obvious if that's what you learned first. It's illustrative to see how difficult it was to build that model up and what alternatives were considered along the way.</p>\n<ul>\n</ul>\n<p>I'm sure I picked up a few other things that have been lost to hindsight bias.</p>\n<h2 id=\"whatshouldiread\">What should I read?</h2>\n<p>I recommend avoiding this book. If any of the above subjects interest you, I recommend finding other sources that focus on those subjects in more depth.</p>\n<p>In fact, I recommend finding a better Cognitive Science book for the MIRI course list: the university course using this book might well be good, but I expect the book to frustrate the type of people who tackle the book list directly. It surely frustrated me.</p>\n<p>A book with more data and more technical arguments, which assumes a high level of competence in its readers, would be a marked improvement.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 2, "4Kcm4etxAJjmeDkHP": 2, "NrvXXL3iGjjxu5B7d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ucpD7YtcvKo6C9fBK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 43, "extendedScore": null, "score": 0.000102, "legacy": true, "legacyId": "24085", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I'm reviewing the books on the <a href=\"http://intelligence.org/courses/\">MIRI course list</a>. After reading <a href=\"/lw/ii0/book_review_heuristics_and_biases_miri_course_list/\">Heuristics and Biases</a> I picked up <em>Cognitive Science</em>, by Jos\u00e9 Luis Berm\u00fadez. It taught me a number of interesting things, but I strongly disliked it.</p>\n<h1 id=\"Cognitive_Science\">Cognitive Science</h1>\n<p>I recommend against reading this book, for reasons that I'll go into at the end of the review. Before that, I'll summarize each chapter.</p>\n<p style=\"text-align:center\"><img src=\"http://images.betterworldbooks.com/052/9780521708371.jpg\" alt=\"Cognitive Science (cover)\" width=\"307\" height=\"400\"></p>\n<h2 id=\"Chapter_Summaries\">Chapter Summaries</h2>\n<ol>\n<li><a href=\"#1theprehistoryofcognitivescience\">The prehistory of cognitive science</a></li>\n<li><a href=\"#2thedisciplinematuresthreemilestones\">The discipline matures: three milestones</a></li>\n<li><a href=\"#3theturntothebrain\">The turn to the brain</a></li>\n<li><a href=\"#4cognitivescienceandtheintegrationchallenge\">Cognitive science and the integration challenge</a></li>\n<li><a href=\"#5tacklingtheintegrationchallenge\">Tackling the integration challenge</a></li>\n<li><a href=\"#6physicalsymbolsystemsandthelanguageofthought\">Physical symbol systems and the language of thought</a></li>\n<li><a href=\"#7applyingthesymbolicparadigm\">Applying the symbolic paradigm</a></li>\n<li><a href=\"#8neuralnetworksanddistributedinformationprocessing\">Neural networks and distributed information processing</a></li>\n<li><a href=\"#9neuralnetworkmodelsofcognitiveprocesses\">Neural network models of cognitive processes</a></li>\n<li><a href=\"#10howarecognitivesystemsorganized?\">How are cognitive systems organized?</a></li>\n<li><a href=\"#11strategiesformappingthebrain\">Strategies for mapping the brain</a></li>\n<li><a href=\"#12acasestudyExploringmindreading\">A case study: Exploring mind reading</a></li>\n<li><a href=\"#13newhorizonsDynamicalsystemsandsituatedcognition\">New horizons: Dynamical systems and situated cognition</a></li>\n<li><a href=\"#14lookingaheadChallengesandApplications\">Looking ahead: Challenges and Applications</a></li>\n</ol>\n<h2 id=\"1theprehistoryofcognitivescience\"><a id=\"more\"></a></h2>\n<h2>\n<hr>\n</h2>\n<h2 id=\"1__The_prehistory_of_cognitive_science\">1. The prehistory of cognitive science</h2>\n<p>This chapter introduces the idea of cognition as a form of information processing, and discusses how that viewpoint came about. Short version:</p>\n<ol>\n<li>Behaviorism isn't enough to explain intelligent behavior. \n<ul>\n<li>Mice allowed to wander in mazes with no goal were able to navigate the maze quickly when goals were added. Reinforcement on its own does not sufficiently explain learning.</li>\n</ul>\n</li>\n<li>Church &amp; Turing put forth the Church-Turing thesis (anything computable is computable by a Turing machine).</li>\n<li>Studying language syntax revealed deep structure and rules.</li>\n</ol>\n<p>The chapter illustrates how these insights led to the idea of cognition as information processing.</p>\n<p><em>This chapter is informative from a historical perspective. It might be worth reading.</em></p>\n<p><a id=\"more\"></a></p>\n<hr>\n<h2 id=\"2__The_discipline_matures__three_milestones\">2. The discipline matures: three milestones</h2>\n<p>This chapter touches upon three early cognitive science \"milestones\": A computer program, an experiment, and a cognitive model.</p>\n<ol>\n<li>SHRDLU was a program that uses natural language in a very limited setting.</li>\n<li>The study measured the time it takes to, given two images, determine whether the second depicts the same object as the first (viewed from a different angle). \n<ul>\n<li>It turns out the amount of time it takes is a linear function of the amount of rotation between views of the object.</li>\n<li>Because each image contains the same amount of visual data (measured in pixels or bits), this shows that cognitive processing must be tiered.</li>\n</ul>\n</li>\n<li>This led to a \"hierarchical\" model of cognitive science. \n<ul>\n<li>One layer makes representations out of images, another layer rotates them and pattern matches.</li>\n</ul>\n</li>\n</ol>\n<p><em>This chapter is superficial; I wouldn't recommend it to the LessWrong audience. The study in (2) is interesting, but now that you know about it I'd recommend reading the study directly.</em></p>\n<p><a id=\"more\"></a></p>\n<hr>\n<h2 id=\"3__The_turn_to_the_brain\">3. The turn to the brain</h2>\n<p>Eventually people developed the tech to actually watch the brain work, which had an impact on cognitive models.</p>\n<p>We have high-level imaging techniques and low-level neuron monitoring. That latter is only done on non-humans.</p>\n<p>Scientists studying the brain found discrete cognitive modules. Surgery on monkey brains can reliably induce specific deficiencies. We can inhibit object recognition (the \"what\") and spacial recognition (the \"where\") separately, by cutting out specific parts of the brain.</p>\n<p>The same studies showed that information can take multiple routes: information from the visual cortex flows along a ventral stream (visual cortex \u2192 object recognizer) and a dorsal stream (visual cortex \u2192 spacial locator).</p>\n<p>Clever experiments with humans allow us to see what parts of the brain are activated by certain activities. Techniques include tracking irradiated fluids (PET) and tracking blood flow (MRI). By having patients do similar tasks with small modifications, we can get brain activity data. By aggregating data and subtracting data from control experiments, we can identify brain regions associated with various activities.</p>\n<p>The chapter concludes with a brief discussion of the discovery of neurons.</p>\n<p><em>This chapter is superficial; you can get the same info from the \"brain\" section of an introductory biology course.</em></p>\n<p><a id=\"more\"></a></p>\n<hr>\n<h2 id=\"4__Cognitive_science_and_the_integration_challenge\">4. Cognitive science and the integration challenge</h2>\n<p>Cognitive Science spans everything from molecular biology to deep philosophy. Unfortunately, we don't know how all the parts connect yet. This is referred to as the \"Integration Challenge\".</p>\n<p>The following is presented as an example:</p>\n<p>You have four cards. Flip whichever cards you must to verify the rule <strong>If a card has a vowel on one side then it has an even number on the other</strong>:</p>\n<pre><code>E C 4 5\n</code></pre>\n<p>People do notoriously badly at this game. It's called the \"Wason selection task\". It was mentioned in the sequences a few times. But it turns out, people are much better at this version:</p>\n<p>There are four people drinking at a bar. A police officer busts in and needs to verify the rule <strong>If a person is drinking alcohol, they must be at least 21</strong>. Which of the following must they investigate further?</p>\n<pre><code>Beer-drinker, Coke-drinker, 25-year-old, 16-year-old\n</code></pre>\n<p>These problems are logically identical. However, most people suggest flipping 4 while few people suggest checking what the 25 year old is drinking.</p>\n<p>More generally, it seems that people <em>can</em> do very well on the Wason selection task <em>if</em> it's framed in such a way that people are looking for cheaters. (Eliminating the police officer from the above story is sufficient to reduce performance.)</p>\n<p>This lends credence to the idea of separate \"mind modules\" that can be activated (such as a cheater-detection module). The fields of cognitive bias, evolutionary biology, and economic game theory all have a part in explaining this phenomenon.</p>\n<p>It seems that many fields overlap in the cognitive realm. The fact that much research is siloed in separate fields is framed as the \"integration challenge\".</p>\n<p><em>The chapter is somewhat interesting, but is primarily superficial and wastes a few pages explaining basic concepts such as the Prisoner's Dilemma. I'd avoid it.</em></p>\n<p><a id=\"more\"></a></p>\n<hr>\n<h2 id=\"5__Tackling_the_integration_challenge\">5. Tackling the integration challenge</h2>\n<p>This chapter attempts to address the \"integration challenge\" posed above.</p>\n<p>The chapter opens with a brief (uninspired) introduction to reductionism and some wishy-washy reasons why it doesn't apply to cognitive science. This basically boils down to the following:</p>\n<p><em>Cognitive science 'laws' are merely statistical. Cognitive science 'equations' (such as perceived intensity of stimulus, \u03a8=k\u03d5^n) are descriptive, not explanatory. Therefore, you can't reduce cognitive science to physics.</em></p>\n<p>Another alternative is explored, the \"Tri-level hypothesis\", which basically posits that you can look at the brain as a collection of information processing systems, each of which should be viewed on three levels:</p>\n<ol>\n<li>The transformation level (what's the input, what's the output, what's the transformation?)</li>\n<li>The algorithmic level (how does the transformation work?)</li>\n<li>The mechanical level (how is it implemented in the real world?)</li>\n</ol>\n<p>This is also rejected, on the basis that it Just Doesn't Seem Like Intelligence. (There's no room for executive control, its not clear how learning would occur, the modules seem like they would be too independent.)</p>\n<p><em>I recommend avoiding this chapter. It is a confused and poor introduction to reductionism that makes many flawed arguments.</em></p>\n<p><a id=\"more\"></a></p>\n<hr>\n<h2 id=\"6__Physical_symbol_systems_and_the_language_of_thought\">6. Physical symbol systems and the language of thought</h2>\n<p>This chapter opens with a proposal from the 1970s called the \"Physical Symbol System Hypothesis\":</p>\n<blockquote>\n<p>A physical symbol system has the necessary and sufficient means for general intelligent action.</p>\n</blockquote>\n<p>It explores this hypothesis a bit. It make analogies to logic (a symbol game that can be interpreted with meaning), gives some examples of search algorithms (that operate by symbol manipulation), and explores how language is a type of symbol manipulation.</p>\n<p>It proceeds to counter this idea with the Chinese Room argument and the Symbol Grounding Problem (how do symbols become meaningful?).</p>\n<p><em>I recommend avoiding this chapter. The first half of the chapter is slow and redundant to anyone moderately familiar with logic/computation. The latter half falls into a number of traps that the Sequences are specifically designed to disarm. Members of the LessWrong community are likely to find it frustrating.</em></p>\n<p><a id=\"more\"></a></p>\n<hr>\n<h2 id=\"7__Applying_the_symbolic_paradigm\">7. Applying the symbolic paradigm</h2>\n<p>This chapter explores decision algorithms and machine learning. It describes an algorithm designed to build a decision tree from a database (by analyzing which questions have the most expected information).</p>\n<p>The algorithm was somewhat frustrating, because simple probabilistic reasoning could have improved the results significantly. (And because the algorithm was presented as a brilliant real-world example and not as a simple toy.)</p>\n<p>The later part of the chapter discussed some robots that do symbol manipulation to achieve their goals (such as SHAKEY).</p>\n<p><em>I recommend avoiding this chapter. It's painfully slow for someone already familiar with computation and the basics of information theory.</em></p>\n<p><em>(If you don't know about computation and the basics of information theory, I suggest learning them elsewhere.)</em></p>\n<p><a id=\"more\"></a></p>\n<hr>\n<h2 id=\"8__Neural_networks_and_distributed_information_processing\">8. Neural networks and distributed information processing</h2>\n<p>This chapter begins with a brief introduction to how a neuron operates and describes artificial neural networks.</p>\n<p>It wasted some time explaining boolean functions, and then introduced the \"perceptron convergence rule\", a learning algorithm that causes any single-layer neural net to converge on the optimal answer (assuming the optimal answer can be encoded in a single-layer neural net).</p>\n<p>Some examples are given before showing that a single-layer neural net cannot implement XOR. This is followed by a claim that single-layer neural nets can only implement linearly separable functions.</p>\n<p>Multi-layer neural nets can implement any function (in principle), but no equivalent convergence rule exists in multi-layer nets. Alternative learning mechanisms, such as backpropagation of error, are explored. However, backpropagation is non-local and biologically implausible. (There is some evidence against backpropagation of error in brains.)</p>\n<p>The remainder of the chapter explores the difference between the neural-net brain architectures and physical symbol processors. Major differences include:</p>\n<ul>\n<li>There is no clear distinction between information storage and information processing in a neural net</li>\n<li>There is no distinction between representation and rules in a neural net.</li>\n</ul>\n<p>This is contrasted with Turing machine, in which the symbols and rules are quite distinct.</p>\n<p><em>This chapter has some signal in it, but it's mostly lost in the noise of superficial overviews (at the beginning) and a false dichotomy (at the end).</em></p>\n<p><a id=\"more\"></a></p>\n<hr>\n<h2 id=\"9__Neural_network_models_of_cognitive_processes\">9. Neural network models of cognitive processes</h2>\n<p>This chapter discusses learning. Those who model the brain as a symbol-manipulation engine view learning very differently from those who model the brain as a neural net.</p>\n<p>From the symbol-manipulation side, the following argument is presented:</p>\n<p><em>In order to learn English, you must learn the truth conditions of sentences. These truth conditions must be expressed in a lower level language. That lower level language is the language of thought, which must be innate.</em></p>\n<p>The neural-net side offers an alternative:that learning can proceed by strengthening or weakening connections between neurons. Some experimental evidence supports these claims:</p>\n<p>When children are learning to speak, they go through a few phases with irregular verbs (give/gave). Early on, children conjugate correctly (gave). Later, they regress and start conjugating irregular verbs like normal verbs (gived). Later, they regain their original skill.</p>\n<p>A few studies of neural nets showed that verb-conjugating neural nets exhibit a similar pattern, if they're trained on irregular verbs for a little while before increasing their vocabulary.</p>\n<p>Other examples of humans-learning-like-neural-nets are presented, and are interesting.</p>\n<p>The latter half of the chapter cites various studies concerned with infant learning. Some proponents of the physical symbol hypothesis have claimed that a baby's world must be a chaos of sensory overload (as the baby cannot yet parse the world into representations). However, studies show that this is not the case.</p>\n<p>It turns out that babies look longer at things which surprise them. This can be used to measure what babies find surprising. Babies scrutinize physics-defying videos longer than physics-obeying videos, implying that a certain amount of world knowledge is \"baked in\".</p>\n<p>The chapter concludes by acknowledging that neural nets and physical symbol systems are not mutually exclusive, and that the latter may be implemented by the former. However, neural nets show that there is a non-symbolic way to process information, thereby discounting the physical symbol system hypothesis as \"necessary and sufficient\". We should broaden our view to include more than just the physical symbol hypothesis in our model of cognition.</p>\n<p><em>The points made in this chapter are interesting and were mostly new to me. If any of the above was surprising, this chapter is likely worth your time.</em></p>\n<p><a id=\"more\"></a></p>\n<hr>\n<h2 id=\"10__How_are_cognitive_systems_organized_\">10. How are cognitive systems organized?</h2>\n<p>This chapter explores agent architectures.</p>\n<p>One architecture, <em>reflex agents</em>, can't be termed intelligent. The input is directly linked to the action system via a small set of rules.</p>\n<p>Another architecture, <em>goal-based agents</em>, have a model of the world and a set of goals, and take actions to maximize their goals. They have no mechanism for learning.</p>\n<p>The third architecture, <em>learning agents</em>, have a memory and some learning mechanism.</p>\n<p>Such architectures are naturally described in a modular manner. (Input systems, output systems, memory systems, prediction systems, goal-evaluations systems, etc.). Furthermore, the brain studies from earlier imply some degree of modularity.</p>\n<p>This gives rise to the modularity hypothesis, which claims that the brain is modular.</p>\n<p>A \"massive modularity\" hypothesis is put forth, claiming that the brain is entirely made of domain-specific modular components.</p>\n<p>The arguments in favor of modularity include:</p>\n<p>Evolution baked many behaviors. For example, we have some degree of willingness to sacrifice ourselves for family. This isn't because we run the numbers and discover that our genes propagate better that way. Rather, it's because the self-propagation gene <em>actually did</em> propagate further. We have \"Darwinian modules\" that have been baked in for fitness reasons. (Readers may notice echoes of <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">The Sequences</a> here.)</p>\n<p>Given the wide array of domain-specific and unrelated adaptations that evolution must provide (cheater-detection, facial-recognition, emotion-extrapolation, folk-physics, etc.) it makes sense to consider these modules separately.</p>\n<p>In other words, because evolution is not coordinated we should expect specific evolutionary advantages to be realized as domain-specific, encapsulated cognitive modules.</p>\n<p>In its strongest form, the massive modularity hypothesis states that there are no domain-general fitness criteria, so evolution cannot create domain-general cognitive mechanisms. They claim the brain has no domain-general central processing, but rather is a collection of domain-specific modules.</p>\n<p><span style=\"color: #999999;\">Note: This argument has as many holes in the book as it has in my summary.</span></p>\n<p>Counter-arguments are then presented illustrating why the strong massive modularity hypothesis is silly:</p>\n<ul>\n<li>\"Darwinian modules\" are not applied in a mandatory fashion: people seem capable of overriding the self-sacrifice module.</li>\n<li>Modules take a limited range of inputs. How are those inputs selected? Inputs to a cheater-detection-module must be representations of social exchanges. There must be a filter that filters some larger data set down to just the social exchanges. What determines the input for that filtering module? Continue up the stack until you have something that is operating on very wide inputs. This is domain general.</li>\n<li>How could domain-general learning be possible with only domain-specific modules?</li>\n</ul>\n<p>Or, in other words, \"humans seem pretty domain-general to me\".</p>\n<p>The book concludes that while there may be many modules, the brain is not <em>only</em> domain-specific modules.</p>\n<p>The remainder of the chapter studies a \"hybrid architecture\" called ACT-R/PM. The example struck me as a stretch to prove a point, and did not seem relevant.</p>\n<p><em>I'd avoid this chapter. It wasted a lot of time presenting a false dichotomy between modularity and \"having some sort of central executor\" by presenting a straw man argument. It didn't provide new insights.</em></p>\n<p><a id=\"more\"></a></p>\n<hr>\n<h2 id=\"11__Strategies_for_mapping_the_brain\">11. Strategies for mapping the brain</h2>\n<p>This chapter returns to brain-scanning tools. It gives an overview of brain areas. It briefly mentions mirror neurons (neurons that fire both when you do something and when you watch somebody else do it). It discusses many techniques for looking at brains. Some are invasive, single-neuron, and only done on non-humans. Others are high-level and observe things like blood flow, which may or may not be a good indicator of brain activity. We lack tools to study the middle-ground, regions of neurons and their connections. The chapter concluded with a discussion of the potential pitfalls when drawing conclusions from incomplete data.</p>\n<p><em>This chapter was a rehash of chapter three. I found it useless.</em></p>\n<p><a id=\"more\"></a></p>\n<hr>\n<h2 id=\"12__A_case_study__Exploring_mind_reading\">12. A case study: Exploring mind reading</h2>\n<p>This chapter explored the mechanisms of empathy.</p>\n<p>It first explored the ability to play \"make believe\" as an early form of metarepresentation (representing a representation), a concept assumed to be necessary before one can disconnect representations from what they represent. For example, a metarepresentation allows you to go from \"The world is in state X\" to \"Susan believes the world is in state X\" which requires you to represent the world-representation \"state X\" as the subject of Susan's belief.</p>\n<p>(In other words, playing make-believe is seen as a crucial step in developing a model of beliefs.)</p>\n<p>This is presumed crucial in developing a theory of mind. (Indeed, autistic children tend not to play \"make believe\" and have difficulty passing false belief tests, which require a working theory of mind.)</p>\n<p>The false belief test is introduced:</p>\n<p><em>Jane and Sally see a marble placed in the basked. Jane leaves the room. Sally sees the marble moved to the box. Sally is asked where Jane will look for the marble.</em></p>\n<p>A \"theory of mind\" brain-module is postulated. It takes a bit of heat when studies show that, even after children pass the false belief test, true beliefs are easier to model than false beliefs.</p>\n<p>This is followed by a completely different theory claiming that metarepresentation is not required to pass the false belief test. This model claims that a child can represent a relationship between Sally and false world-states without ever representing a representation.</p>\n<p>Analogies are drawn to counterfactual thinking: you can think about how you could be eating a different sandwich, without thinking about how you think about sandwiches. Similarly, you can represent someone having false beliefs without representing beliefs.</p>\n<p>The book moves to the simulation model of empathy, which claims that you empathise with people by putting yourself in their shoes, pretending you believe what they believe, and assessing how you would feel.</p>\n<p>This argument is supported by studies of people with specific brain damage. Turns out, people who can't feel fear also have trouble identifying fear (but not disgust) in others, and people who can't feel disgust have trouble identifying disgust (but not fear) in others.</p>\n<p>There is some evidence showing that certain brain regions are active only when attributing false beliefs to someone. Different people interpret this evidence in different ways.</p>\n<p><em>This chapter had a lot of signal. The debate has much more depth than I've covered here. This chapter is worth your time.</em></p>\n<p><a id=\"more\"></a></p>\n<hr>\n<h2 id=\"13__New_horizons__Dynamical_systems_and_situated_cognition\">13. New horizons: Dynamical systems and situated cognition</h2>\n<p>Some people argue that we're doing cognitive science wrong. When we want to govern a steam engine, we don't design a computer that samples the engine speed and adjusts the throttle: rather, we attach it to a flywheel that raises weights as the engine speeds up. Then we just let gravity sort the damn thing out.</p>\n<p>Similarly, crickets run towards other cricket-noises: but it turns out they aren't processing sounds and deciding that cricket noises are sexy. Rather, their ears (which are in their legs (which are hollow)) are directly hooked to the motor output. When the ears are properly stimulated the legs move towards the sound. No brain necessary.</p>\n<p>These people claim that we should look at humans on a dynamical level instead of postulating all of these high-falutin' modules.</p>\n<p>Some studies with infants show that they act dynamically. If you put the toy in box A a few times (training them to reach for A), then put it in box B, then they'll reach for box B.</p>\n<p>Unless you restrict them for a few seconds. Then they'll go for box A. Unless you stand them up, in which case they go for box B again. Environment, time, and muscle position all seem to affect infant activity. This implies that non-cognitive data is necessary to assess infant cognition.</p>\n<p>Detractors argue that while dynamics leads to good data, dynamics does not lead to understanding. Modelling traffic as a multi-particle system allows you to predict traffic jams better but it hardly tells you <em>why</em> the model works. For that, you need some model of human intent and perhaps a bit of game theory.</p>\n<p>Also, subjectively, brains don't feel completely dynamical.</p>\n<p>The remainder of the chapter is spent exploring dynamical robots. They're neat.</p>\n<p><em>This chapter provides an interesting new way to look at cognition. It is worth a read.</em></p>\n<p><a id=\"more\"></a></p>\n<hr>\n<h2 id=\"14__Looking_ahead__Challenges_and_Applications\">14. Looking ahead: Challenges and Applications</h2>\n<p>This is very short. It covers some promising areas of research. They are:</p>\n<ol>\n<li>The Human Connectome Project (studying neural connections in the brain).</li>\n<li>Studying what the brain is doing when it's idle (instead of doing experiments where we just \"subtract\" the control).</li>\n<li>Neuroprostheses (substitute brain modules).</li>\n<li>Improved education (leveraging what we've learned about cognition).</li>\n<li>Crossovers from cognitive science to economics (how do Homo Sapiens differ from Homo Economicus, and why?) and law (who is responsible for what?).</li>\n<li>Cognitive scientists have long been careful to avoid consciousness. Now people are starting to approach the problem. Philosophical zombies are presented seriously.</li>\n</ol>\n<div><em>If you're interested in any of the above points, I recommend learning about them elsewhere. This chapter is short and does not have much data. I recommend skipping it.</em></div>\n<p><a id=\"more\"></a></p>\n<hr>\n<h2 id=\"Reactions\">Reactions</h2>\n<p>This book bored and frustrated me, for a number of reasons.</p>\n<ol>\n<li><strong>Assumed a low level of technical competence from its readers:</strong> Many basic concepts were introduced superficially but at length. Examples include the structure of neurons and the prisoner's dilemma. I felt talked down to throughout much of the book. (One of the exercises was \"Give a truth table for OR.\")</li>\n<li><strong>Gave serious credence to silly ideas:</strong>&nbsp;The Chinese Room argument and Philosophical Zombies were given serious credence in this text. The fact that cognitive science is very far from reduction to physics was used as an excuse to write off reductionism entirely. Such floundering is somewhat expected in any text that touches mainstream philosophy and strives for \"political correctness\", but I expected more from the MIRI course list.</li>\n<li><strong>Lacked technical arguments:</strong>&nbsp;The book had a lot of words and not a lot of data. Much of the book was in \"he-said she-said\" format, explaining debates between cognitive scientists. I understand that the field of cognitive science is not mature enough to make many technical arguments, but even the debates were highly summarized. Worse, they were full of incomplete or confused arguments. I would have been much happier if the author presented the data and stepped aside.</li>\n<li><strong>The author was incapable of drawing conclusions:</strong>&nbsp;The author presented many arguments, argued one side, and then concluded with \"or, at least, that's what so-and-so says.\" Some of the positions were very poorly argued, and it was difficult for me to tell whether this was due to the author's misunderstanding or whether the author was faithfully relaying confused arguments.</li>\n</ol>\n<p>All in all, the book was a lot of noise with very little signal. I was expecting much more, especially given its prominent position as the first book on the MIRI course recommendations. It felt like a superficial introduction to low-level concepts. It's geared towards high school students or freshmen undergrads who do not yet know how to think critically.</p>\n<p>I was expecting a rationalist introduction to cognitive science: \"Here's what we think we know, here's how we know it. Here's the gray areas, here's what professionals say.\" I feel you could boil away 80% of this book without loss.</p>\n<p><span style=\"color:#999999;\">Note: I got this book used. The previous owner had no idea how to effectively use a highlighter. Many unimportant phrases were highlighted. I found it difficult not to read them LOUDLY. This increased my frustration. I have tried to adjust accordingly, but it may be biasing my review.</span></p>\n<p><span style=\"color:#999999;\">Note: I assume that the concepts of brain modularity, neural nets, information-as-entropy, and symbol-processing are familiar to MIRI's target audience. If that is not the case then this book provides a passable sketch for how to start thinking about minds. Even so, I would not recommend it for that purpose on account of point (2) above.</span></p>\n<h2 id=\"What_I_learned\">What I learned</h2>\n<p>The book was not all bad. There was some signal in the noise. But first, a rebuttal:</p>\n<p><em>Cognitive Science</em> seemed sorely confused about one thing in particular, that being the operation of neural nets. The neural net vs physical symbol debate seemed a false dichotomy.</p>\n<p>A neural net implementing an AND gate <em>is</em> manipulating symbols, if you consider it on a sufficiently abstract level.</p>\n<p>A cricket's legs <em>are</em> doing calculations to identify cricket-noises and move in that direction. The algorithm was designed by Time and Evolution, and it's run on Physics directly (instead of on the cricket's brain), but the computation still occurs.</p>\n<p>A neural net <em>does</em> process symbols according to rules. It doesn't just come to magic answers because it has magic weights: A well-trained neural net works&nbsp;<em>because</em> the propagation of activation through the net mimics the causal structure of the real world.</p>\n<p>The author missed or neglected these points entirely.</p>\n<p>That said, this book did teach me something about symbol-processing and neural nets. It taught me that the symbols in a neural net are much looser than the symbols you'll find in modern computers. In a \"strict\" symbol manipulation system, representations are binary: either all the symbols are in place and things work, or one symbol is out of place and everything breaks.</p>\n<p>Neural nets are significantly more versatile: a neural net can be strong enough to recognize an answer without being strong enough to produce the answer. With good training, a neural net can slowly build a good representation out of random starting weights.</p>\n<p>In essence, <em>Cognitive Science</em> showed me that symbols needn't be discrete: they can be continuous, convoluted, and blurry.</p>\n<p>(Within brains, they often are all three.)</p>\n<p>This seems obvious in retrospect, but \"fuzzy symbols\" were a novel concept to me.</p>\n<p>Here's a few other tidbits that I took away:</p>\n<ul>\n<li>The perceptron convergence rule (and its inapplicability to multi-layer neural nets) was new to me.</li>\n<li>Babies do not live in a land of sensory chaos. I was once taught otherwise. <em>Cognitive Science</em> dispelled a false belief with interesting data, and for that I am thankful.</li>\n<li>The discussion of how evolution can create modules in brains was an interesting one, and gave me a new way of looking at domain-specific brain functions.</li>\n<li>I was quite surprised by the Wason Selection Task results (where framing the problem in a way that puts you on the lookout for cheaters improves performance).</li>\n</ul>\n<p>Finally, this book gives you a great overview of the history of Cognitive Science. It's easy to believe that the modern cognitive model is obvious if that's what you learned first. It's illustrative to see how difficult it was to build that model up and what alternatives were considered along the way.</p>\n<ul>\n</ul>\n<p>I'm sure I picked up a few other things that have been lost to hindsight bias.</p>\n<h2 id=\"What_should_I_read_\">What should I read?</h2>\n<p>I recommend avoiding this book. If any of the above subjects interest you, I recommend finding other sources that focus on those subjects in more depth.</p>\n<p>In fact, I recommend finding a better Cognitive Science book for the MIRI course list: the university course using this book might well be good, but I expect the book to frustrate the type of people who tackle the book list directly. It surely frustrated me.</p>\n<p>A book with more data and more technical arguments, which assumes a high level of competence in its readers, would be a marked improvement.</p>", "sections": [{"title": "Cognitive Science", "anchor": "Cognitive_Science", "level": 1}, {"title": "Chapter Summaries", "anchor": "Chapter_Summaries", "level": 2}, {"title": "1. The prehistory of cognitive science", "anchor": "1__The_prehistory_of_cognitive_science", "level": 2}, {"title": "2. The discipline matures: three milestones", "anchor": "2__The_discipline_matures__three_milestones", "level": 2}, {"title": "3. The turn to the brain", "anchor": "3__The_turn_to_the_brain", "level": 2}, {"title": "4. Cognitive science and the integration challenge", "anchor": "4__Cognitive_science_and_the_integration_challenge", "level": 2}, {"title": "5. Tackling the integration challenge", "anchor": "5__Tackling_the_integration_challenge", "level": 2}, {"title": "6. Physical symbol systems and the language of thought", "anchor": "6__Physical_symbol_systems_and_the_language_of_thought", "level": 2}, {"title": "7. Applying the symbolic paradigm", "anchor": "7__Applying_the_symbolic_paradigm", "level": 2}, {"title": "8. Neural networks and distributed information processing", "anchor": "8__Neural_networks_and_distributed_information_processing", "level": 2}, {"title": "9. Neural network models of cognitive processes", "anchor": "9__Neural_network_models_of_cognitive_processes", "level": 2}, {"title": "10. How are cognitive systems organized?", "anchor": "10__How_are_cognitive_systems_organized_", "level": 2}, {"title": "11. Strategies for mapping the brain", "anchor": "11__Strategies_for_mapping_the_brain", "level": 2}, {"title": "12. A case study: Exploring mind reading", "anchor": "12__A_case_study__Exploring_mind_reading", "level": 2}, {"title": "13. New horizons: Dynamical systems and situated cognition", "anchor": "13__New_horizons__Dynamical_systems_and_situated_cognition", "level": 2}, {"title": "14. Looking ahead: Challenges and Applications", "anchor": "14__Looking_ahead__Challenges_and_Applications", "level": 2}, {"title": "Reactions", "anchor": "Reactions", "level": 2}, {"title": "What I learned", "anchor": "What_I_learned", "level": 2}, {"title": "What should I read?", "anchor": "What_should_I_read_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 21}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gc6foBcbozvEJ3HbG", "XPErvb8m9FapXCjhA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-09T19:43:33.569Z", "modifiedAt": null, "url": null, "title": "Satisficing versus optimizing in instructor selection", "slug": "satisficing-versus-optimizing-in-instructor-selection", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:14.910Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VipulNaik", "createdAt": "2013-09-02T18:51:08.862Z", "isAdmin": false, "displayName": "VipulNaik"}, "userId": "t3pZcNZXqhaM5avBE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tXDGHhsGEotfZreXX/satisficing-versus-optimizing-in-instructor-selection", "pageUrlRelative": "/posts/tXDGHhsGEotfZreXX/satisficing-versus-optimizing-in-instructor-selection", "linkUrl": "https://www.lesswrong.com/posts/tXDGHhsGEotfZreXX/satisficing-versus-optimizing-in-instructor-selection", "postedAtFormatted": "Monday, September 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Satisficing%20versus%20optimizing%20in%20instructor%20selection&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASatisficing%20versus%20optimizing%20in%20instructor%20selection%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtXDGHhsGEotfZreXX%2Fsatisficing-versus-optimizing-in-instructor-selection%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Satisficing%20versus%20optimizing%20in%20instructor%20selection%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtXDGHhsGEotfZreXX%2Fsatisficing-versus-optimizing-in-instructor-selection", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtXDGHhsGEotfZreXX%2Fsatisficing-versus-optimizing-in-instructor-selection", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1836, "htmlBody": "<p>In my <a href=\"/r/discussion/lw/ikh/course_selection_based_on_instructor/\">previous post</a>, I argued both that instructor selection is relatively neglected in undergraduate courses, and that criteria that students routinely use for instructor selection may be suboptimal. I outlined some possible pointers regarding good instructor selection, based on my own understanding of how the process works. There were a large number of thoughtful responses to my post. The most interesting responses argued that people do, and/or should, <a href=\"https://en.wikipedia.org/wiki/Satisficing\">satisfice</a> with respect to instructor selection.</p>\n<p>I raised the point that people tend to satisfice rather than maximize in one of my own <a href=\"/r/discussion/lw/ikh/course_selection_based_on_instructor/9psz\">comments</a> on the post:</p>\n<blockquote>\n<p>So, to clarify, although I think it \"often\" happens that people select instructors randomly, it doesn't always happen. My impression is that there's heavy availability bias: if students can easily access evaluations, or know friends who have taken classes with a particular instructor, they'll use that data. If not, they'll select randomly. In general, my impression is that students are risk-averse and satisfice on this count: as long as an instructor seems \"good enough\" they won't generally try to look for a better one (this is particularly true for people in multi-course sequences with an instructor, we could call this a \"status quo bias\" or an \"endowment effect\" depending on your perspective). More proactive approaches, such as sitting in on classes that the instructors are teaching in the previous term, seem to be relatively rare.</p>\n<p>My other point is that even when students are making proactive choices, the criteria they use may be suboptimal. I am not aware of high quality advice that would help students select instructors effectively. This is, I believe, in contrast with the vast (though possibly not very high quality) literature available on how to select a college or a major. If intra-institutional variation in instructor quality is comparable to inter-institutional variation, then there are probably unrealized gains in selecting instructors according to better criteria.</p>\n</blockquote>\n<p>In response, Linkhyrule5 <a href=\"/r/discussion/lw/ikh/course_selection_based_on_instructor/9pt7\">wrote</a>:</p>\n<blockquote>\n<p>One thing I would note is that <em>good</em> teachers are rare and not very \"good,\" but <em>bad</em> teachers are common and very very bad. This ties into Trevor's post: once you've avoided the bad teachers, it's probably not worth the much larger effort required to find an extra-bonus-good teacher that may or may not exist.</p>\n</blockquote>\n<p>and elaborated in the same thread:</p>\n<blockquote>\n<p>I should probably have qualified my earlier post, as the only data I have to draw on is my own anecdotal evidence.</p>\n<p>Nevertheless, with the qualifications that I went to a private high school and am currently at an Ivy college:</p>\n<p>Sample size: since freshman high school, 26 \"teachers\" and nine \"professors.\" <br /> Definitions: a teacher/professor is good-I if I was particularly interested in his/her course (and not just the subject), and good-R if I retained particularly more from that course. A teacher/professor is bad-I if I was particularly uninterested in his/her course (and not just the subject), and bad-R if I retained particularly little from that course.</p>\n<p>Of my teachers, 2 good-I teachers and 0 good-R teachers, with two \"maybes\" - not particularly good but above-average. 3 teachers who were both bad-I and bad-R - all of these are extreme cases, in which I learned almost nothing and loathed the class.</p>\n<p>Of my professors, 0 good-I and no data on good-R (I'm a sophomore), but already 2 bad-I and I highly suspect bad-R.</p>\n</blockquote>\n<p>Trevor had <a href=\"/r/discussion/lw/ikh/course_selection_based_on_instructor/9psm\">written</a>:</p>\n<blockquote>\n<p>A good instructor will often offer a good class that good students can take advantage of. A bad instructor will never offer a good class and no one can take advantage of it. Avoiding bad teachers is therefore always a move in a good direction, while selecting good teachers is only sometimes a move in a good direction.<br /><br />I've been working in K-12 and college since 2005 as a sign language interpreter. The qualities of good instructors varies, but the qualities of bad instructors are shared. Late, unprepared, digressive without profit, argumentative, close minded, uses the class as political platform, uses the class as therapy session, uses the class as amateur comedy, unable / unwilling to earn respect (grudging or otherwise) of class - those are some of the things bad instructors share. Avoid those and you're more likely to succeed in any class.</p>\n</blockquote>\n<p>Dre <a href=\"/r/discussion/lw/ikh/course_selection_based_on_instructor/9pw0\">wrote</a>:</p>\n<blockquote>\n<div id=\"body_t1_9pw0\" class=\"comment-content \">\n<div class=\"md\">\n<p>Going off of what others have said, I'll add another reason people might satisfice with teachers.</p>\n<p>In my experience, people agree much more about which teachers are bad than about which are good. Many of my favorite (in the sense that I learned a lot easily) teachers were disliked by other people, but almost all of those I thought were bad were widely thought of as bad. If you're not as interested in serious learning this might be less important.</p>\n<p>So avoiding bad teachers requires a relatively small amount of information, but finding a teacher that is not just good, but good <em>for you</em> requires a much larger amount. So people reasonably only do the first part.</p>\n</div>\n</div>\n</blockquote>\n<h3><strong>Satisficing versus optimizing: the descriptive, the possible, and the prescriptive</strong></h3>\n<p>This has got me thinking about the relative roles of satisficing versus optimizing in instructor selection. There are three separate but related questions:</p>\n<ol> </ol> \n<ul>\n</ul>\n<ol>\n<li><em>Do</em> <em>people tend to satisfice, rather than optimize, with respect to selecting instructors</em>? I think the general consensus seems to be that most people tend to satisfice. For instance, very few students sample classes by instructors that they plan to study under in a subsequent term. Very few students think deeply and carefully about what pitfalls to avoid in instructor selection. Most of them try to avoid instructors who are bad (in a superficial sense) -- instructors who speak unclearly, have terrible handwriting, or don't grade \"fairly\" (<em>fair</em> grading is student lingo for <em>easy</em> grading).</li>\n<li><em>Do there exist ways for people to do better than satisfice, without incurring huge costs</em>? Dre's comment, quoted above, suggests one obstacle: everybody agrees on the terrible teachers, but different people have different ideas of what constitutes good teaching. This is corroborated by the fairly polar responses on the student evaluations of all except very bad teachers (\"he is a great teacher, very clear in class\" versus \"I can rarely understanding anything the teacher says\" for the very same teacher). That being said, I think that for people who are <em>aware of some of the basic ideas as I outlined in my preceding post, and take proactive steps</em>, one can choose good instructors without incurring huge costs.</li>\n<li><em>Should people choose to optimize rather than satisfice</em>? This is the part that's most open to debate. It depends on the degree of variation between <em>good enough</em> and <em>great</em> teachers in terms of absolute outcome differentials for human capital or signaling. Another related factor, that I didn't mention in the post, is whether your peers are optimizing or satisficing. If the peers you really want to have are optimizing for instructor, it makes sense to optimize for instructor, so that you get those peers. If, however, they are merely satisficing, then optimizing for instructor will not necessarily optimize for peers.</li>\n</ol> <ol> </ol>\n<h3>Is optimizing for instructor inherently zero-sum?</h3>\n<p>There are two related points I want to make. In colleges where the number of seats in classes is fixed and most classes come close to filling their seat quotas, optimizing for instructor has the connotations of a <a href=\"https://en.wikipedia.org/wiki/Zero-sum_game\">zero-sum game</a>. This is also true in the case of satisficing for instructor, though perhaps less so: if students unanimously protest against terrible instruction, departments can, at least in principle, remedy the problem (by either firing or retraining the terrible instructors). Converting mediocre instructors to great instructors, on the other hand, is relatively hard. If everybody competed for limited student slots with a great instructor, you'd just be displacing another individual.</p>\n<p>There are three counterpoints to the perceived zero-sum connotation of optimizing.</p>\n<ol> </ol> \n<ul>\n<li>Students who are actively looking for good instructors are likely to benefit more from good instructors than students who are more indifferent. In econ-math jargon, we'd say that student concern for finding good instruction is <em>complementary</em> to instructor quality (formally, the mixed partial with respect to student desire for a good instructor and instructor quality is positive).</li>\n</ul>\n<ol> </ol> \n<ul>\n<li>Different students have different tastes for what constitutes a good instructor. Thus, one student's \"best\" instructor may differ from another's (drawing on Dre's point again). To the extent this is true, it cuts both ways. On the one hand, it means that selecting good instructors would be more of a positive-sum game, and therefore, is socially useful. On the other hand, it's harder to easily acquire information about who is the best instructor, and it may be hard to compile generic advice on that front</li>\n</ul>\n<ol> </ol> \n<ul>\n<li>In the longer run, when universities and departments see that students are specifically interested in good instructors, they may work to either (a) improve the quality of instruction, or (b) reduce hard limits on enrollment allowing more students to take classes with the instructor of their choice. It's unclear whether (a) is possible, and the merits of (b) are unclear.</li>\n</ul>\n<ol> </ol>\n<h3>The analogy with satisficing in charity, and why optimizers may remain a minority, but could still grow</h3>\n<p>Charity is one domain where many people tend to satisfice rather than optimize. People choose a charity to donate to, then check (using Charity Navigator or a similar service) whether the charity satisfies some minimal threshold (low overhead, no fraud, etc.). They then donate to the charity. Status quo bias, personal relationships, and many other factors play important roles. The <a href=\"http://en.wikipedia.org/wiki/Effective_altruism\">effective altruism</a> movement aims to change the norms surrounding charitable giving from satisficing to optimizing. For instance, Giving What We Can <a href=\"http://givingwhatwecan.org/where-to-give/recommended-charities\">claims</a> that some charities can be 1000 times as efficient as others. GiveWell puts emphasis on <a href=\"http://www.givewell.org/giving101/Funding-the-Right-Program\">funding the right program</a> and spends hundreds of hours doing the background research for its <a href=\"http://www.givewell.org/charities/top-charities\">top charity recommendations</a>. Proponents of effective altruism are nonetheless quite sanguine about the prospects of converting a majority of people to the mindset of optimizing in charitable giving. They do, however, think that the minority that does care about optimization can be better served, and can grow to include others who <em>would</em> care about optimization if they are made to consider the issue. For instance, when <a href=\"http://blog.givewell.org/2010/07/20/the-money-for-good-study/\">responding to the Money for Good study</a>, GiveWell wrote:</p>\n<blockquote>\n<p><strong>Our goal isn&rsquo;t to create a product that the majority of people like; it&rsquo;s to create a product that some minority market loves.</strong> From what we&rsquo;re seeing now, it&rsquo;s still possible that the minority of donors interested in impact-focused research is quite large.</p>\n</blockquote>\n<p>In the same way, I think there is a minority market that is interested, and a somewhat larger market that potentially could be, interested in optimization with regard to facets of educational experience such as instructor selection. The advice I gave in the preceding post is geared for that minority market.</p>\n<h3>Looking for thoughts</h3>\n<p>I'm most interested in people's thoughts about the numbered questions 1, 2, and 3. However, I'd welcome thoughts on any of the other assertions I made as well. Thanks for reading!</p>\n<ol> </ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tXDGHhsGEotfZreXX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.3346929520762202e-06, "legacy": true, "legacyId": "24086", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>In my <a href=\"/r/discussion/lw/ikh/course_selection_based_on_instructor/\">previous post</a>, I argued both that instructor selection is relatively neglected in undergraduate courses, and that criteria that students routinely use for instructor selection may be suboptimal. I outlined some possible pointers regarding good instructor selection, based on my own understanding of how the process works. There were a large number of thoughtful responses to my post. The most interesting responses argued that people do, and/or should, <a href=\"https://en.wikipedia.org/wiki/Satisficing\">satisfice</a> with respect to instructor selection.</p>\n<p>I raised the point that people tend to satisfice rather than maximize in one of my own <a href=\"/r/discussion/lw/ikh/course_selection_based_on_instructor/9psz\">comments</a> on the post:</p>\n<blockquote>\n<p>So, to clarify, although I think it \"often\" happens that people select instructors randomly, it doesn't always happen. My impression is that there's heavy availability bias: if students can easily access evaluations, or know friends who have taken classes with a particular instructor, they'll use that data. If not, they'll select randomly. In general, my impression is that students are risk-averse and satisfice on this count: as long as an instructor seems \"good enough\" they won't generally try to look for a better one (this is particularly true for people in multi-course sequences with an instructor, we could call this a \"status quo bias\" or an \"endowment effect\" depending on your perspective). More proactive approaches, such as sitting in on classes that the instructors are teaching in the previous term, seem to be relatively rare.</p>\n<p>My other point is that even when students are making proactive choices, the criteria they use may be suboptimal. I am not aware of high quality advice that would help students select instructors effectively. This is, I believe, in contrast with the vast (though possibly not very high quality) literature available on how to select a college or a major. If intra-institutional variation in instructor quality is comparable to inter-institutional variation, then there are probably unrealized gains in selecting instructors according to better criteria.</p>\n</blockquote>\n<p>In response, Linkhyrule5 <a href=\"/r/discussion/lw/ikh/course_selection_based_on_instructor/9pt7\">wrote</a>:</p>\n<blockquote>\n<p>One thing I would note is that <em>good</em> teachers are rare and not very \"good,\" but <em>bad</em> teachers are common and very very bad. This ties into Trevor's post: once you've avoided the bad teachers, it's probably not worth the much larger effort required to find an extra-bonus-good teacher that may or may not exist.</p>\n</blockquote>\n<p>and elaborated in the same thread:</p>\n<blockquote>\n<p>I should probably have qualified my earlier post, as the only data I have to draw on is my own anecdotal evidence.</p>\n<p>Nevertheless, with the qualifications that I went to a private high school and am currently at an Ivy college:</p>\n<p>Sample size: since freshman high school, 26 \"teachers\" and nine \"professors.\" <br> Definitions: a teacher/professor is good-I if I was particularly interested in his/her course (and not just the subject), and good-R if I retained particularly more from that course. A teacher/professor is bad-I if I was particularly uninterested in his/her course (and not just the subject), and bad-R if I retained particularly little from that course.</p>\n<p>Of my teachers, 2 good-I teachers and 0 good-R teachers, with two \"maybes\" - not particularly good but above-average. 3 teachers who were both bad-I and bad-R - all of these are extreme cases, in which I learned almost nothing and loathed the class.</p>\n<p>Of my professors, 0 good-I and no data on good-R (I'm a sophomore), but already 2 bad-I and I highly suspect bad-R.</p>\n</blockquote>\n<p>Trevor had <a href=\"/r/discussion/lw/ikh/course_selection_based_on_instructor/9psm\">written</a>:</p>\n<blockquote>\n<p>A good instructor will often offer a good class that good students can take advantage of. A bad instructor will never offer a good class and no one can take advantage of it. Avoiding bad teachers is therefore always a move in a good direction, while selecting good teachers is only sometimes a move in a good direction.<br><br>I've been working in K-12 and college since 2005 as a sign language interpreter. The qualities of good instructors varies, but the qualities of bad instructors are shared. Late, unprepared, digressive without profit, argumentative, close minded, uses the class as political platform, uses the class as therapy session, uses the class as amateur comedy, unable / unwilling to earn respect (grudging or otherwise) of class - those are some of the things bad instructors share. Avoid those and you're more likely to succeed in any class.</p>\n</blockquote>\n<p>Dre <a href=\"/r/discussion/lw/ikh/course_selection_based_on_instructor/9pw0\">wrote</a>:</p>\n<blockquote>\n<div id=\"body_t1_9pw0\" class=\"comment-content \">\n<div class=\"md\">\n<p>Going off of what others have said, I'll add another reason people might satisfice with teachers.</p>\n<p>In my experience, people agree much more about which teachers are bad than about which are good. Many of my favorite (in the sense that I learned a lot easily) teachers were disliked by other people, but almost all of those I thought were bad were widely thought of as bad. If you're not as interested in serious learning this might be less important.</p>\n<p>So avoiding bad teachers requires a relatively small amount of information, but finding a teacher that is not just good, but good <em>for you</em> requires a much larger amount. So people reasonably only do the first part.</p>\n</div>\n</div>\n</blockquote>\n<h3 id=\"Satisficing_versus_optimizing__the_descriptive__the_possible__and_the_prescriptive\"><strong>Satisficing versus optimizing: the descriptive, the possible, and the prescriptive</strong></h3>\n<p>This has got me thinking about the relative roles of satisficing versus optimizing in instructor selection. There are three separate but related questions:</p>\n<ol> </ol> \n<ul>\n</ul>\n<ol>\n<li><em>Do</em> <em>people tend to satisfice, rather than optimize, with respect to selecting instructors</em>? I think the general consensus seems to be that most people tend to satisfice. For instance, very few students sample classes by instructors that they plan to study under in a subsequent term. Very few students think deeply and carefully about what pitfalls to avoid in instructor selection. Most of them try to avoid instructors who are bad (in a superficial sense) -- instructors who speak unclearly, have terrible handwriting, or don't grade \"fairly\" (<em>fair</em> grading is student lingo for <em>easy</em> grading).</li>\n<li><em>Do there exist ways for people to do better than satisfice, without incurring huge costs</em>? Dre's comment, quoted above, suggests one obstacle: everybody agrees on the terrible teachers, but different people have different ideas of what constitutes good teaching. This is corroborated by the fairly polar responses on the student evaluations of all except very bad teachers (\"he is a great teacher, very clear in class\" versus \"I can rarely understanding anything the teacher says\" for the very same teacher). That being said, I think that for people who are <em>aware of some of the basic ideas as I outlined in my preceding post, and take proactive steps</em>, one can choose good instructors without incurring huge costs.</li>\n<li><em>Should people choose to optimize rather than satisfice</em>? This is the part that's most open to debate. It depends on the degree of variation between <em>good enough</em> and <em>great</em> teachers in terms of absolute outcome differentials for human capital or signaling. Another related factor, that I didn't mention in the post, is whether your peers are optimizing or satisficing. If the peers you really want to have are optimizing for instructor, it makes sense to optimize for instructor, so that you get those peers. If, however, they are merely satisficing, then optimizing for instructor will not necessarily optimize for peers.</li>\n</ol> <ol> </ol>\n<h3 id=\"Is_optimizing_for_instructor_inherently_zero_sum_\">Is optimizing for instructor inherently zero-sum?</h3>\n<p>There are two related points I want to make. In colleges where the number of seats in classes is fixed and most classes come close to filling their seat quotas, optimizing for instructor has the connotations of a <a href=\"https://en.wikipedia.org/wiki/Zero-sum_game\">zero-sum game</a>. This is also true in the case of satisficing for instructor, though perhaps less so: if students unanimously protest against terrible instruction, departments can, at least in principle, remedy the problem (by either firing or retraining the terrible instructors). Converting mediocre instructors to great instructors, on the other hand, is relatively hard. If everybody competed for limited student slots with a great instructor, you'd just be displacing another individual.</p>\n<p>There are three counterpoints to the perceived zero-sum connotation of optimizing.</p>\n<ol> </ol> \n<ul>\n<li>Students who are actively looking for good instructors are likely to benefit more from good instructors than students who are more indifferent. In econ-math jargon, we'd say that student concern for finding good instruction is <em>complementary</em> to instructor quality (formally, the mixed partial with respect to student desire for a good instructor and instructor quality is positive).</li>\n</ul>\n<ol> </ol> \n<ul>\n<li>Different students have different tastes for what constitutes a good instructor. Thus, one student's \"best\" instructor may differ from another's (drawing on Dre's point again). To the extent this is true, it cuts both ways. On the one hand, it means that selecting good instructors would be more of a positive-sum game, and therefore, is socially useful. On the other hand, it's harder to easily acquire information about who is the best instructor, and it may be hard to compile generic advice on that front</li>\n</ul>\n<ol> </ol> \n<ul>\n<li>In the longer run, when universities and departments see that students are specifically interested in good instructors, they may work to either (a) improve the quality of instruction, or (b) reduce hard limits on enrollment allowing more students to take classes with the instructor of their choice. It's unclear whether (a) is possible, and the merits of (b) are unclear.</li>\n</ul>\n<ol> </ol>\n<h3 id=\"The_analogy_with_satisficing_in_charity__and_why_optimizers_may_remain_a_minority__but_could_still_grow\">The analogy with satisficing in charity, and why optimizers may remain a minority, but could still grow</h3>\n<p>Charity is one domain where many people tend to satisfice rather than optimize. People choose a charity to donate to, then check (using Charity Navigator or a similar service) whether the charity satisfies some minimal threshold (low overhead, no fraud, etc.). They then donate to the charity. Status quo bias, personal relationships, and many other factors play important roles. The <a href=\"http://en.wikipedia.org/wiki/Effective_altruism\">effective altruism</a> movement aims to change the norms surrounding charitable giving from satisficing to optimizing. For instance, Giving What We Can <a href=\"http://givingwhatwecan.org/where-to-give/recommended-charities\">claims</a> that some charities can be 1000 times as efficient as others. GiveWell puts emphasis on <a href=\"http://www.givewell.org/giving101/Funding-the-Right-Program\">funding the right program</a> and spends hundreds of hours doing the background research for its <a href=\"http://www.givewell.org/charities/top-charities\">top charity recommendations</a>. Proponents of effective altruism are nonetheless quite sanguine about the prospects of converting a majority of people to the mindset of optimizing in charitable giving. They do, however, think that the minority that does care about optimization can be better served, and can grow to include others who <em>would</em> care about optimization if they are made to consider the issue. For instance, when <a href=\"http://blog.givewell.org/2010/07/20/the-money-for-good-study/\">responding to the Money for Good study</a>, GiveWell wrote:</p>\n<blockquote>\n<p><strong>Our goal isn\u2019t to create a product that the majority of people like; it\u2019s to create a product that some minority market loves.</strong> From what we\u2019re seeing now, it\u2019s still possible that the minority of donors interested in impact-focused research is quite large.</p>\n</blockquote>\n<p>In the same way, I think there is a minority market that is interested, and a somewhat larger market that potentially could be, interested in optimization with regard to facets of educational experience such as instructor selection. The advice I gave in the preceding post is geared for that minority market.</p>\n<h3 id=\"Looking_for_thoughts\">Looking for thoughts</h3>\n<p>I'm most interested in people's thoughts about the numbered questions 1, 2, and 3. However, I'd welcome thoughts on any of the other assertions I made as well. Thanks for reading!</p>\n<ol> </ol>", "sections": [{"title": "Satisficing versus optimizing: the descriptive, the possible, and the prescriptive", "anchor": "Satisficing_versus_optimizing__the_descriptive__the_possible__and_the_prescriptive", "level": 1}, {"title": "Is optimizing for instructor inherently zero-sum?", "anchor": "Is_optimizing_for_instructor_inherently_zero_sum_", "level": 1}, {"title": "The analogy with satisficing in charity, and why optimizers may remain a minority, but could still grow", "anchor": "The_analogy_with_satisficing_in_charity__and_why_optimizers_may_remain_a_minority__but_could_still_grow", "level": 1}, {"title": "Looking for thoughts", "anchor": "Looking_for_thoughts", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["J7RY9t2ph6Du8JHgD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-09T21:26:42.463Z", "modifiedAt": null, "url": null, "title": "The State of the Art of Scientific Research on Polyamoury", "slug": "the-state-of-the-art-of-scientific-research-on-polyamoury", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:23.811Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ritalin", "createdAt": "2012-05-01T18:00:25.863Z", "isAdmin": false, "displayName": "Ritalin"}, "userId": "ACGKS9W7iQQywxrWW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oDJ9vMsqmt7dFNmEx/the-state-of-the-art-of-scientific-research-on-polyamoury", "pageUrlRelative": "/posts/oDJ9vMsqmt7dFNmEx/the-state-of-the-art-of-scientific-research-on-polyamoury", "linkUrl": "https://www.lesswrong.com/posts/oDJ9vMsqmt7dFNmEx/the-state-of-the-art-of-scientific-research-on-polyamoury", "postedAtFormatted": "Monday, September 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20State%20of%20the%20Art%20of%20Scientific%20Research%20on%20Polyamoury&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20State%20of%20the%20Art%20of%20Scientific%20Research%20on%20Polyamoury%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoDJ9vMsqmt7dFNmEx%2Fthe-state-of-the-art-of-scientific-research-on-polyamoury%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20State%20of%20the%20Art%20of%20Scientific%20Research%20on%20Polyamoury%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoDJ9vMsqmt7dFNmEx%2Fthe-state-of-the-art-of-scientific-research-on-polyamoury", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoDJ9vMsqmt7dFNmEx%2Fthe-state-of-the-art-of-scientific-research-on-polyamoury", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 165, "htmlBody": "<p>The idea of polyamoury is one that interests me. However, while such books as The Ethical Slut have done a good job of providing me with tools to understand and possibly handle the challenges and rewards involved, I found them unsatisfying in that they were largely based on anecdotal evidence, with a very strong selection bias. Before making the jump of attempting to live that way, one would need to know precisely the state of the art of scientific, rigourous, credible research on the topic; it is a tedious job to seek out and compile everything, but I believe it is a job worth doing.&nbsp;</p>\n<p>I'll be initiating an ongoing process of data compilation, and will publish my findings on this thread as I discover and summarize them. Any help is greatly appreciated, as this promises to be long and tedious. I might especially need help extracting meaningful information from the masses of data; I am not a good statistician yet, far from it.</p>\n<p>To Be Expanded...</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oDJ9vMsqmt7dFNmEx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -10, "extendedScore": null, "score": -1.9e-05, "legacy": true, "legacyId": "24088", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-09T22:38:25.435Z", "modifiedAt": null, "url": null, "title": "rational dating - can we escape the rat race be setting smarter goals?", "slug": "rational-dating-can-we-escape-the-rat-race-be-setting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:07.748Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertChange", "createdAt": "2013-01-17T09:34:41.865Z", "isAdmin": false, "displayName": "RobertChange"}, "userId": "c7jX3DDBqSMM9FwvK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xWErKTwJ4QrFCHCYb/rational-dating-can-we-escape-the-rat-race-be-setting", "pageUrlRelative": "/posts/xWErKTwJ4QrFCHCYb/rational-dating-can-we-escape-the-rat-race-be-setting", "linkUrl": "https://www.lesswrong.com/posts/xWErKTwJ4QrFCHCYb/rational-dating-can-we-escape-the-rat-race-be-setting", "postedAtFormatted": "Monday, September 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20rational%20dating%20-%20can%20we%20escape%20the%20rat%20race%20be%20setting%20smarter%20goals%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Arational%20dating%20-%20can%20we%20escape%20the%20rat%20race%20be%20setting%20smarter%20goals%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWErKTwJ4QrFCHCYb%2Frational-dating-can-we-escape-the-rat-race-be-setting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=rational%20dating%20-%20can%20we%20escape%20the%20rat%20race%20be%20setting%20smarter%20goals%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWErKTwJ4QrFCHCYb%2Frational-dating-can-we-escape-the-rat-race-be-setting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWErKTwJ4QrFCHCYb%2Frational-dating-can-we-escape-the-rat-race-be-setting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 633, "htmlBody": "<p>According to evolutionary psychologists as well as the cultural main stream, men are going for sensually attractive women, while women are going for men who can provide them safety (plus other things we don't need to get into here). Whereas we might think that modern, cultured people are somewhat above those basic instincts and actually look for mates which fit their individual character, values, preferences, and interests, the world of online dating seems to throw us back a couple of centuries of progress. Many dating sites have been designed to increase usage and interaction with the site (so that more ads can be shown) and do this by relying a lot on pictures of their users to incite other users' interest. On the other hand, our individual character, values, preferences, and interests (or, ICVPI, for short) are to a large extent represented in text form as either essays or predefined answers to predefined questions. Now, while the text contains a lot of the relevant information, it is severely disadvantaged by this site design. Not only do the pictures have a much higher salience, their presence also directly speaks to our impulses and emotions (or system one, for those who have read Kahneman's \"Thinking fast and slow\") which hinders the already somewhat harder processing of text by system two.</p>\n<p>The result of this click-optimizing is that user's choices whom to contact are much more driven by pictures (and therefore, looks) than by other criteria. This leads to the destructive effect that visually attractive women are swamped in messages among which it is hard and tedious to chose ones to reply to, while the less attractive ones do not have enough choice to find men that match their individual preferences. In other words, the mutual matching between people who might fit each other is more sabotaged than helped by those picture-driven dating sites.&nbsp;</p>\n<p>Now, as a basic rationalist I will of course question my motivations and ask myself if the run for beauty is really for my own best or if it is a learned behavior stemming from our (in this case arguably superficial) culture. If we leave aside the antiquated Freudian principle of \"drive\" and read psychologists like Rogers and Fromm, we might realize that hunting for beautiful mates is just one way among others for a man to boost his self-esteem, not necessarily a motivator in and of itself. Indeed, modern studies have shown that lasting self-esteem and deep happiness can be created best by exercising our strengths in a meaningful way. (For details and the studies see Seligman's \"Authentic Happiness\".) Following this argument, if I get my self-esteem and social recognition from (for example) writing awesome blog articles and helping lots of people at work, then this positive emotion will \"buffer\" (in Seligman's terms) against any judgmental looks and statements that I will be facing when going out with my awesome, but ugly, new girl friend. (Can you hear them saying \"what? are you dating <em>her?</em>\"&nbsp;in a raising voice that bounces back from the ceiling?)</p>\n<p>To sum up, wouldn't it be the most rational thing to simply switch off pictures on the dating site (Firefox currently has AddOns for this, Chrome can do it natively, just type \"block\" in the search box on the settings page), thus keeping my impulsive system one at calm, write more thoughtful messages, and get more and better responses, both because I am writing to less message-swamped women and because my own messages are better. Then dating will be less like a meaningless competition, the resulting relationships are more profound, and I will find out that my friends are not actually prejudiced against ugly people and will not look down at me for making that choice. Just let go of my own prejudices and false beliefs and I win. Right?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xWErKTwJ4QrFCHCYb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": -14, "extendedScore": null, "score": 1.3348460542666637e-06, "legacy": true, "legacyId": "24090", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-09T23:48:09.525Z", "modifiedAt": null, "url": null, "title": "Military Rationalities and Irrationalities", "slug": "military-rationalities-and-irrationalities", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:33.420Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pscheyer", "createdAt": "2010-02-26T06:22:11.935Z", "isAdmin": false, "displayName": "pscheyer"}, "userId": "a9HpYyEsAMwMEj72e", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c7YzTturH2yQGjALZ/military-rationalities-and-irrationalities", "pageUrlRelative": "/posts/c7YzTturH2yQGjALZ/military-rationalities-and-irrationalities", "linkUrl": "https://www.lesswrong.com/posts/c7YzTturH2yQGjALZ/military-rationalities-and-irrationalities", "postedAtFormatted": "Monday, September 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Military%20Rationalities%20and%20Irrationalities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMilitary%20Rationalities%20and%20Irrationalities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc7YzTturH2yQGjALZ%2Fmilitary-rationalities-and-irrationalities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Military%20Rationalities%20and%20Irrationalities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc7YzTturH2yQGjALZ%2Fmilitary-rationalities-and-irrationalities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc7YzTturH2yQGjALZ%2Fmilitary-rationalities-and-irrationalities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 228, "htmlBody": "<p>In response to the question</p>\r\n<blockquote>\r\n<p>\"Does anyone happen to know of reliable ways for increasing one's supply of executive function, by the way? I seem to run out of it very quickly in general.\"</p>\r\n<p>(Kaj_Solata)</p>\r\n</blockquote>\r\n<p>I posted that my military experience seems effectively designed to increase executive function. Some examples of this from myself and <a title=\"metastable user profile\" href=\"/user/metastable/overview/\">metastable</a>&nbsp;are</p>\r\n<p>Uniforms- not having to think about your wardrobe, ever, saves a lot of time, mental effort, and money. <a href=\"http://www.forbes.com/sites/jacquelynsmith/2012/10/05/steve-jobs-always-dressed-exactly-the-same-heres-who-else-does/\">Steve Jobs and President Obama are known for also using uniforms specifically for this purpose.</a></p>\r\n<p>PT- Daily, routinized exercise. Done in a way that very few people are deciding what comes next.</p>\r\n<p>-Maximum use of daylight hours</p>\r\n<p>Med Group and Force Support-Minimized high-risk projects outside of workplace (paternalistic health care, insurance, and in many cases, housing and continuing education.)</p>\r\n<p>&nbsp;</p>\r\n<p>After a moment's thought it occurred to me that there are some double-edged swords in Military Rationality as well, some of which lead to classic jokes like 'Military Intelligence is an oxymoron.'</p>\r\n<p>&nbsp;</p>\r\n<p>Regulations- A select few 'experts' create policies which everyone else is required to follow at all times. Unfortunately these experts are never (never ever) encouraged to consider knock-on effects. Ugh.</p>\r\n<p>&nbsp;</p>\r\n<p>Anybody else have insights on the military they want to share here? I feel a couple of good posts on increasing executive function might come out of a discussion on&nbsp;the rationalities and irrationalities of the armed forces.</p>\r\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"txkDg4aLmiRq8wsSu": 1, "xXX3n22DQZuKqXEdT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c7YzTturH2yQGjALZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 35, "extendedScore": null, "score": 1.3349071191359126e-06, "legacy": true, "legacyId": "24091", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-10T02:03:25.897Z", "modifiedAt": null, "url": null, "title": "The Ultimate Newcomb's Problem", "slug": "the-ultimate-newcomb-s-problem", "viewCount": null, "lastCommentedAt": "2020-12-12T14:25:42.983Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RAh4fekdiRhZxb2Kw/the-ultimate-newcomb-s-problem", "pageUrlRelative": "/posts/RAh4fekdiRhZxb2Kw/the-ultimate-newcomb-s-problem", "linkUrl": "https://www.lesswrong.com/posts/RAh4fekdiRhZxb2Kw/the-ultimate-newcomb-s-problem", "postedAtFormatted": "Tuesday, September 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Ultimate%20Newcomb's%20Problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Ultimate%20Newcomb's%20Problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRAh4fekdiRhZxb2Kw%2Fthe-ultimate-newcomb-s-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Ultimate%20Newcomb's%20Problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRAh4fekdiRhZxb2Kw%2Fthe-ultimate-newcomb-s-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRAh4fekdiRhZxb2Kw%2Fthe-ultimate-newcomb-s-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 305, "htmlBody": "<p>You see two boxes and you can either take both boxes, or take only box B. Box A is transparent and contains $1000. Box B contains a visible number, say 1033. &nbsp;The Bank of Omega, which operates by very clear and transparent mechanisms, will pay you $1M if this number is prime, and $0 if it is composite. Omega is known to select prime numbers for Box B whenever Omega predicts that you will take only Box B; and conversely select composite numbers if Omega predicts that you will take both boxes. Omega has previously predicted correctly in 99.9% of cases.</p>\n<p>Separately, the Numerical Lottery has randomly selected 1033 and is displaying this number on a screen nearby. The Lottery Bank, likewise operating by a clear known mechanism, will pay you $2 million if it has selected a composite number, and otherwise pay you $0. &nbsp;(This event will take place regardless of whether you take only B or both boxes, and both the Bank of Omega and the Lottery Bank <em>will</em>&nbsp;carry out their payment processes - you don't have to choose one game or the other.)</p>\n<p>You previously played the game with Omega and the Numerical Lottery a few thousand times before you ran across this case where Omega's number and the Lottery number were the same, so this event is not suspicious.</p>\n<p>Omega also knew the Lottery number before you saw it, and while making its prediction, and Omega likewise predicts correctly in 99.9% of the cases where the Lottery number happens to match Omega's number. &nbsp;(Omega's number is chosen independently of the lottery number, however.)</p>\n<p>You have two minutes to make a decision, you don't have a calculator, and if you try to factor the number you will be run over by the trolley from the <a href=\"http://www.mindspring.com/~mfpatton/Tissues.htm\">Ultimate Trolley Problem</a>.</p>\n<p>Do you take only box B, or both boxes?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fihKHQuS5WZBJgkRm": 4, "JHYaBGQuuKHdwnrAK": 1, "dPPATLhRmhdJtJM2t": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RAh4fekdiRhZxb2Kw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 41, "extendedScore": null, "score": 0.000101, "legacy": true, "legacyId": "24094", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 116, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-09-10T02:03:25.897Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-10T18:24:21.981Z", "modifiedAt": null, "url": null, "title": "Three ways CFAR has changed my view of rationality", "slug": "three-ways-cfar-has-changed-my-view-of-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:37.746Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Julia_Galef", "createdAt": "2009-12-20T01:44:38.850Z", "isAdmin": false, "displayName": "Julia_Galef"}, "userId": "qkDSxJnyKhPCJyKdD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/68dHanLWsS6SEyZp9/three-ways-cfar-has-changed-my-view-of-rationality", "pageUrlRelative": "/posts/68dHanLWsS6SEyZp9/three-ways-cfar-has-changed-my-view-of-rationality", "linkUrl": "https://www.lesswrong.com/posts/68dHanLWsS6SEyZp9/three-ways-cfar-has-changed-my-view-of-rationality", "postedAtFormatted": "Tuesday, September 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Three%20ways%20CFAR%20has%20changed%20my%20view%20of%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThree%20ways%20CFAR%20has%20changed%20my%20view%20of%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F68dHanLWsS6SEyZp9%2Fthree-ways-cfar-has-changed-my-view-of-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Three%20ways%20CFAR%20has%20changed%20my%20view%20of%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F68dHanLWsS6SEyZp9%2Fthree-ways-cfar-has-changed-my-view-of-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F68dHanLWsS6SEyZp9%2Fthree-ways-cfar-has-changed-my-view-of-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1576, "htmlBody": "<p>The <a href=\"http://rationality.org\">Center for Applied Rationality's</a> perspective on rationality is quite similar to Less Wrong's. In particular, we share many of Less Wrong's differences from what's sometimes called <a href=\"http://wiki.lesswrong.com/wiki/Traditional_rationality\">\"traditional\" rationality</a>,&nbsp;such as Less Wrong's inclusion of Bayesian probability theory and the science on heuristics and biases.</p>\n<p>But after spending the last year and a half with CFAR as we've developed, tested, and attempted to teach hundreds of different versions of rationality techniques, I've noticed that my picture of what rationality looks like has shifted somewhat from what I perceive to be the most common picture of rationality on Less Wrong. Here are three ways I think CFAR has come to see the landscape of rationality differently than Less Wrong typically does &ndash; not disagreements per se, but differences in focus or approach. (Disclaimer: I'm not speaking for the rest of CFAR here; these are my own impressions.)</p>\n<p><strong>&nbsp;</strong></p>\n<p><strong>1. We think less in terms of epistemic versus instrumental rationality.</strong></p>\n<p>Formally, the methods of normative epistemic versus instrumental rationality are distinct: Bayesian inference and expected utility maximization. But methods like \"use Bayes' Theorem\" or \"maximize expected utility\" are usually too abstract and high-level to be helpful for a human being trying to take manageable steps towards improving her rationality. And when you zoom in from that high-level description of rationality down to the more concrete level of \"What <a href=\"/lw/5kz/the_5second_level/\">five-second mental habits</a> should I be training?\" the distinction between epistemic and instrumental rationality becomes less helpful.</p>\n<p>Here's an analogy: epistemic rationality is like physics, where the goal is to figure out what's true about the world, and instrumental rationality is like engineering, where the goal is to accomplish something you want as efficiently and effectively as possible. You need physics to do engineering; or I suppose you could say that doing engineering <em>is</em> doing physics, but with a practical goal. However, there's plenty of physics that's done for its own sake, and doesn't have obvious practical applications, at least not yet. (String theory, for example.) Similarly, you need a fair amount of epistemic rationality in order to be instrumentally rational, though there are parts of epistemic rationality that many of us practice for their own sake, and not as a means to an end. (For example, I appreciate clarifying my thinking about <a href=\"http://wiki.lesswrong.com/wiki/Free_will_(solution)\">free will</a> even though I don't expect it to change any of my behavior.)</p>\n<p>In this analogy, many skills we focus on at CFAR are akin to essential math, like linear algebra or differential equations, which compose the fabric of both physics and engineering. It would be foolish to expect someone who wasn't comfortable with math to successfully calculate a planet's trajectory or design a bridge. And it would be similarly foolish to expect you to successfully update like a Bayesian or maximize your utility if you lacked certain underlying skills. Like, for instance: Noticing your emotional reactions, and being able to shift them if it would be useful. Doing thought experiments. Noticing and overcoming learned helplessness. Visualizing in concrete detail. Preventing yourself from flinching away from a thought. Rewarding yourself for mental habits you want to reinforce.&nbsp;</p>\n<p>These and other building blocks of rationality are essential <em>both</em> for reaching truer beliefs, <em>and</em> for getting what you value; they don't fall cleanly into either an \"epistemic\" or an \"instrumental\" category. Which is why, when I consider what pieces of rationality CFAR should be developing, I've been thinking less in terms of \"How can we be more epistemically rational?\" or \"How can we be more instrumentally rational?\" and instead using queries like, \"How can we be more metacognitive?\"</p>\n<p><strong>&nbsp;</strong></p>\n<p><strong>2. We think more in terms of a modular mind.</strong></p>\n<p>The human mind isn't one coordinated, unified agent, but rather a collection of different processes that often aren't working in sync, or even aware of what each other is up to. Less Wrong certainly knows this; see, for example, discussions of <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">anticipations versus professions</a>, <a href=\"http://wiki.lesswrong.com/wiki/Alief\">aliefs</a>, and <a href=\"/lw/fv/wanting_to_want/\">metawanting</a>. But in general we gloss over that fact, because it's so much simpler and more natural to talk about \"what I believe\" or \"what I want,\" even if technically there is no single \"I\" doing the believing or wanting. And for many purposes that kind of approximation is fine.&nbsp;</p>\n<p>But a rationality-for-humans usually can't rely on that shorthand. Any attempt to change what \"I\" believe, or optimize for what \"I\" want, forces a confrontation of the fact that there are multiple, contradictory things that could reasonably be called \"beliefs,\" or \"wants,\" coexisting in the same mind. So a large part of applied rationality turns out to be about noticing those contradictions and trying to achieve coherence, in some fashion, before you can even begin to update on evidence or plan an action.</p>\n<p>Many of the techniques we're developing at CFAR fall roughly into the template of coordinating between your <a href=\"http://en.wikipedia.org/wiki/Dual_process_theory#Dual-process_accounts_of_reasoning\">two systems of cognition</a>: implicit-reasoning System 1 and explicit-reasoning System 2. For example, knowing when each system is more likely to be reliable. Or knowing how to get System 2 to convince System 1 of something (\"We're not going to die if we go talk to that stranger\"). Or knowing what kinds of questions System 2 should ask of System 1 to find out why it's uneasy about the conclusion at which System 2 has arrived.</p>\n<p>This is all, of course, with the disclaimer that the anthropomorphizing of the systems of cognition, and imagining them talking to each other, is merely a useful metaphor. Even the classification of human cognition into Systems 1 and 2 is probably not strictly true, but it's true enough to be useful. And other metaphors prove useful as well &ndash; for example, some difficulties with what feels like akrasia become more tractable when you model your future selves as different entities, as we do in the current version of our \"Delegating to yourself\" class.</p>\n<p><strong>&nbsp;</strong></p>\n<p><strong>3. We're more focused on emotions.</strong></p>\n<p>There's relatively little discussion of emotions on Less Wrong, but they occupy a central place in CFAR's curriculum and organizational culture.</p>\n<p>It used to frustrate me when people would say something that revealed they held a <a href=\"/lw/8ko/communicating_rationality_to_the_public_julia/\">Straw Vulcan-esque</a> belief that \"rationalist = emotionless robot\". But now when I encounter that misconception, it just makes me want to smile, because I'm thinking to myself: \"If you had <em>any</em> idea how much time we spend at CFAR talking about our feelings&hellip;\"</p>\n<p>Being able to put yourself into particular emotional states seems to make a lot of pieces of rationality easier. For example, for most of us, it's instrumentally rational to explore a wider set of possible actions &ndash; different ways of studying, holding conversations, trying to be happy, and so on &ndash; beyond whatever our defaults happen to be. And for most of us, inertia and aversions get in the way of that exploration. But getting yourself into \"playful\" mode (one of the hypothesized <a href=\"http://www.amazon.com/Affective-Neuroscience-Foundations-Emotions-Science/dp/019517805X\">primary emotional circuits</a> common across mammals) can make it easier to branch out into a wider swath of Possible-Action Space. Similarly, being able to call up a feeling of curiosity or of \"seeking\" (another candidate for a primary emotional circuit) can help you conquer <a href=\"http://wiki.lesswrong.com/wiki/Motivated_cognition\">motivated cognition</a> and <a href=\"/lw/5a9/learned_blankness/\">learned blankness</a>. &nbsp;</p>\n<p>And simply being able to notice your emotional state is rarer and more valuable than most people realize. For example, if you're in fight-or-flight mode, you're going to feel more compelled to reject arguments that feel like a challenge to your identity. Being attuned to the signs of sympathetic nervous system activation &ndash; that you're tensing up, or that your heart rate is increasing &ndash; means you get cues to double-check your reasoning, or to coax yourself into another emotional state.</p>\n<p>We also use emotions as sources of data. You can learn to tap into feelings of surprise or confusion to get a sense of how probable you implicitly expect some event to be. Or practice simulating hypotheticals (\"What if I knew that my novel would never sell well?\") and observing your resultant emotions, to get a clearer picture of your utility function.&nbsp;</p>\n<p>And emotions-as-data can be a valuable check on your System 2's conclusions. One of our standard classes is \"Goal Factoring,\" which entails finding some alternate set of actions through which you can purchase the goods you want more cheaply. So you might reason, \"I'm doing martial arts for the exercise and self-defense benefits... but I could purchase both of those things for less time investment by jogging to work and carrying Mace.\" If you listened to your emotional reaction to that proposal, however, you might notice you still feel sad about giving up martial arts even if you were getting the same amount of exercise and self-defense benefits somehow else.</p>\n<p>Which probably means you've got other reasons for doing martial arts that you haven't yet explicitly acknowledged -- for example, maybe you just think it's cool. If so, that's important, and deserves a place in your decisionmaking. Listening for those emotional cues that your explicit reasoning has missed something is a crucial step, and to the extent that aspiring rationalists sometimes forget it, I suppose that's a&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Steel_man\">Steel-Manned</a>&nbsp;Straw Vulcan (Steel Vulcan?) that actually is worth worrying about.</p>\n<p><strong>Conclusion</strong></p>\n<p>I'll name one more trait that unites, rather than divides, CFAR and Less Wrong. We both diverge from \"traditional\" rationality in that we're concerned with determining which general methods <a href=\"http://wiki.lesswrong.com/wiki/Rationality_is_systematized_winning\">systematically perform well</a>, rather than defending some set of methods as \"rational\" on a priori criteria alone. So CFAR's picture of what rationality looks like, and how to become more rational, will and should change over the coming years as we learn more about the effects of our rationality training efforts.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"X7v7Fyp9cgBYaMe2e": 13, "DLskYNGdAGDFpxBF8": 2, "3ee9k6NJfcGzL6kMS": 4, "Ng8Gice9KNkncxqcj": 2, "3RnEKrsNgNEDxuNnw": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "68dHanLWsS6SEyZp9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 120, "baseScore": 164, "extendedScore": null, "score": 0.000401, "legacy": true, "legacyId": "24087", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 165, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The <a href=\"http://rationality.org\">Center for Applied Rationality's</a> perspective on rationality is quite similar to Less Wrong's. In particular, we share many of Less Wrong's differences from what's sometimes called <a href=\"http://wiki.lesswrong.com/wiki/Traditional_rationality\">\"traditional\" rationality</a>,&nbsp;such as Less Wrong's inclusion of Bayesian probability theory and the science on heuristics and biases.</p>\n<p>But after spending the last year and a half with CFAR as we've developed, tested, and attempted to teach hundreds of different versions of rationality techniques, I've noticed that my picture of what rationality looks like has shifted somewhat from what I perceive to be the most common picture of rationality on Less Wrong. Here are three ways I think CFAR has come to see the landscape of rationality differently than Less Wrong typically does \u2013 not disagreements per se, but differences in focus or approach. (Disclaimer: I'm not speaking for the rest of CFAR here; these are my own impressions.)</p>\n<p><strong>&nbsp;</strong></p>\n<p><strong id=\"1__We_think_less_in_terms_of_epistemic_versus_instrumental_rationality_\">1. We think less in terms of epistemic versus instrumental rationality.</strong></p>\n<p>Formally, the methods of normative epistemic versus instrumental rationality are distinct: Bayesian inference and expected utility maximization. But methods like \"use Bayes' Theorem\" or \"maximize expected utility\" are usually too abstract and high-level to be helpful for a human being trying to take manageable steps towards improving her rationality. And when you zoom in from that high-level description of rationality down to the more concrete level of \"What <a href=\"/lw/5kz/the_5second_level/\">five-second mental habits</a> should I be training?\" the distinction between epistemic and instrumental rationality becomes less helpful.</p>\n<p>Here's an analogy: epistemic rationality is like physics, where the goal is to figure out what's true about the world, and instrumental rationality is like engineering, where the goal is to accomplish something you want as efficiently and effectively as possible. You need physics to do engineering; or I suppose you could say that doing engineering <em>is</em> doing physics, but with a practical goal. However, there's plenty of physics that's done for its own sake, and doesn't have obvious practical applications, at least not yet. (String theory, for example.) Similarly, you need a fair amount of epistemic rationality in order to be instrumentally rational, though there are parts of epistemic rationality that many of us practice for their own sake, and not as a means to an end. (For example, I appreciate clarifying my thinking about <a href=\"http://wiki.lesswrong.com/wiki/Free_will_(solution)\">free will</a> even though I don't expect it to change any of my behavior.)</p>\n<p>In this analogy, many skills we focus on at CFAR are akin to essential math, like linear algebra or differential equations, which compose the fabric of both physics and engineering. It would be foolish to expect someone who wasn't comfortable with math to successfully calculate a planet's trajectory or design a bridge. And it would be similarly foolish to expect you to successfully update like a Bayesian or maximize your utility if you lacked certain underlying skills. Like, for instance: Noticing your emotional reactions, and being able to shift them if it would be useful. Doing thought experiments. Noticing and overcoming learned helplessness. Visualizing in concrete detail. Preventing yourself from flinching away from a thought. Rewarding yourself for mental habits you want to reinforce.&nbsp;</p>\n<p>These and other building blocks of rationality are essential <em>both</em> for reaching truer beliefs, <em>and</em> for getting what you value; they don't fall cleanly into either an \"epistemic\" or an \"instrumental\" category. Which is why, when I consider what pieces of rationality CFAR should be developing, I've been thinking less in terms of \"How can we be more epistemically rational?\" or \"How can we be more instrumentally rational?\" and instead using queries like, \"How can we be more metacognitive?\"</p>\n<p><strong>&nbsp;</strong></p>\n<p><strong id=\"2__We_think_more_in_terms_of_a_modular_mind_\">2. We think more in terms of a modular mind.</strong></p>\n<p>The human mind isn't one coordinated, unified agent, but rather a collection of different processes that often aren't working in sync, or even aware of what each other is up to. Less Wrong certainly knows this; see, for example, discussions of <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">anticipations versus professions</a>, <a href=\"http://wiki.lesswrong.com/wiki/Alief\">aliefs</a>, and <a href=\"/lw/fv/wanting_to_want/\">metawanting</a>. But in general we gloss over that fact, because it's so much simpler and more natural to talk about \"what I believe\" or \"what I want,\" even if technically there is no single \"I\" doing the believing or wanting. And for many purposes that kind of approximation is fine.&nbsp;</p>\n<p>But a rationality-for-humans usually can't rely on that shorthand. Any attempt to change what \"I\" believe, or optimize for what \"I\" want, forces a confrontation of the fact that there are multiple, contradictory things that could reasonably be called \"beliefs,\" or \"wants,\" coexisting in the same mind. So a large part of applied rationality turns out to be about noticing those contradictions and trying to achieve coherence, in some fashion, before you can even begin to update on evidence or plan an action.</p>\n<p>Many of the techniques we're developing at CFAR fall roughly into the template of coordinating between your <a href=\"http://en.wikipedia.org/wiki/Dual_process_theory#Dual-process_accounts_of_reasoning\">two systems of cognition</a>: implicit-reasoning System 1 and explicit-reasoning System 2. For example, knowing when each system is more likely to be reliable. Or knowing how to get System 2 to convince System 1 of something (\"We're not going to die if we go talk to that stranger\"). Or knowing what kinds of questions System 2 should ask of System 1 to find out why it's uneasy about the conclusion at which System 2 has arrived.</p>\n<p>This is all, of course, with the disclaimer that the anthropomorphizing of the systems of cognition, and imagining them talking to each other, is merely a useful metaphor. Even the classification of human cognition into Systems 1 and 2 is probably not strictly true, but it's true enough to be useful. And other metaphors prove useful as well \u2013 for example, some difficulties with what feels like akrasia become more tractable when you model your future selves as different entities, as we do in the current version of our \"Delegating to yourself\" class.</p>\n<p><strong>&nbsp;</strong></p>\n<p><strong id=\"3__We_re_more_focused_on_emotions_\">3. We're more focused on emotions.</strong></p>\n<p>There's relatively little discussion of emotions on Less Wrong, but they occupy a central place in CFAR's curriculum and organizational culture.</p>\n<p>It used to frustrate me when people would say something that revealed they held a <a href=\"/lw/8ko/communicating_rationality_to_the_public_julia/\">Straw Vulcan-esque</a> belief that \"rationalist = emotionless robot\". But now when I encounter that misconception, it just makes me want to smile, because I'm thinking to myself: \"If you had <em>any</em> idea how much time we spend at CFAR talking about our feelings\u2026\"</p>\n<p>Being able to put yourself into particular emotional states seems to make a lot of pieces of rationality easier. For example, for most of us, it's instrumentally rational to explore a wider set of possible actions \u2013 different ways of studying, holding conversations, trying to be happy, and so on \u2013 beyond whatever our defaults happen to be. And for most of us, inertia and aversions get in the way of that exploration. But getting yourself into \"playful\" mode (one of the hypothesized <a href=\"http://www.amazon.com/Affective-Neuroscience-Foundations-Emotions-Science/dp/019517805X\">primary emotional circuits</a> common across mammals) can make it easier to branch out into a wider swath of Possible-Action Space. Similarly, being able to call up a feeling of curiosity or of \"seeking\" (another candidate for a primary emotional circuit) can help you conquer <a href=\"http://wiki.lesswrong.com/wiki/Motivated_cognition\">motivated cognition</a> and <a href=\"/lw/5a9/learned_blankness/\">learned blankness</a>. &nbsp;</p>\n<p>And simply being able to notice your emotional state is rarer and more valuable than most people realize. For example, if you're in fight-or-flight mode, you're going to feel more compelled to reject arguments that feel like a challenge to your identity. Being attuned to the signs of sympathetic nervous system activation \u2013 that you're tensing up, or that your heart rate is increasing \u2013 means you get cues to double-check your reasoning, or to coax yourself into another emotional state.</p>\n<p>We also use emotions as sources of data. You can learn to tap into feelings of surprise or confusion to get a sense of how probable you implicitly expect some event to be. Or practice simulating hypotheticals (\"What if I knew that my novel would never sell well?\") and observing your resultant emotions, to get a clearer picture of your utility function.&nbsp;</p>\n<p>And emotions-as-data can be a valuable check on your System 2's conclusions. One of our standard classes is \"Goal Factoring,\" which entails finding some alternate set of actions through which you can purchase the goods you want more cheaply. So you might reason, \"I'm doing martial arts for the exercise and self-defense benefits... but I could purchase both of those things for less time investment by jogging to work and carrying Mace.\" If you listened to your emotional reaction to that proposal, however, you might notice you still feel sad about giving up martial arts even if you were getting the same amount of exercise and self-defense benefits somehow else.</p>\n<p>Which probably means you've got other reasons for doing martial arts that you haven't yet explicitly acknowledged -- for example, maybe you just think it's cool. If so, that's important, and deserves a place in your decisionmaking. Listening for those emotional cues that your explicit reasoning has missed something is a crucial step, and to the extent that aspiring rationalists sometimes forget it, I suppose that's a&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Steel_man\">Steel-Manned</a>&nbsp;Straw Vulcan (Steel Vulcan?) that actually is worth worrying about.</p>\n<p><strong id=\"Conclusion\">Conclusion</strong></p>\n<p>I'll name one more trait that unites, rather than divides, CFAR and Less Wrong. We both diverge from \"traditional\" rationality in that we're concerned with determining which general methods <a href=\"http://wiki.lesswrong.com/wiki/Rationality_is_systematized_winning\">systematically perform well</a>, rather than defending some set of methods as \"rational\" on a priori criteria alone. So CFAR's picture of what rationality looks like, and how to become more rational, will and should change over the coming years as we learn more about the effects of our rationality training efforts.&nbsp;</p>", "sections": [{"title": "1. We think less in terms of epistemic versus instrumental rationality.", "anchor": "1__We_think_less_in_terms_of_epistemic_versus_instrumental_rationality_", "level": 1}, {"title": "2. We think more in terms of a modular mind.", "anchor": "2__We_think_more_in_terms_of_a_modular_mind_", "level": 1}, {"title": "3. We're more focused on emotions.", "anchor": "3__We_re_more_focused_on_emotions_", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "59 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JcpzFpPBSmzuksmWM", "a7n8GdKiAZRX86T5A", "azdqDRbcw3EkrnHNw", "zuJmtSqt3TsnBTYyu", "puhPJimawPuNZ5wAR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-10T20:29:53.519Z", "modifiedAt": null, "url": null, "title": "Two special CFAR classes in Sept: Installing Habits and Rationality for Programmers", "slug": "two-special-cfar-classes-in-sept-installing-habits-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:30.275Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "palladias", "createdAt": "2012-04-03T13:45:53.766Z", "isAdmin": false, "displayName": "palladias"}, "userId": "Bv2LXWzZf96WGpqJ5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HQEhtemyBhucEvrgr/two-special-cfar-classes-in-sept-installing-habits-and", "pageUrlRelative": "/posts/HQEhtemyBhucEvrgr/two-special-cfar-classes-in-sept-installing-habits-and", "linkUrl": "https://www.lesswrong.com/posts/HQEhtemyBhucEvrgr/two-special-cfar-classes-in-sept-installing-habits-and", "postedAtFormatted": "Tuesday, September 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Two%20special%20CFAR%20classes%20in%20Sept%3A%20Installing%20Habits%20and%20Rationality%20for%20Programmers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATwo%20special%20CFAR%20classes%20in%20Sept%3A%20Installing%20Habits%20and%20Rationality%20for%20Programmers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHQEhtemyBhucEvrgr%2Ftwo-special-cfar-classes-in-sept-installing-habits-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Two%20special%20CFAR%20classes%20in%20Sept%3A%20Installing%20Habits%20and%20Rationality%20for%20Programmers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHQEhtemyBhucEvrgr%2Ftwo-special-cfar-classes-in-sept-installing-habits-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHQEhtemyBhucEvrgr%2Ftwo-special-cfar-classes-in-sept-installing-habits-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 304, "htmlBody": "<p>The next four-day CFAR workshops are in October (Bay Area) and November (NYC), but there are two upcoming one-day classes that may be of interest to LessWrongers.</p>\n<p>&nbsp;</p>\n<p><strong><a href=\"http://rationality.org/2013/09/06/make-habits-that-stick-on-sept-15/\" target=\"_blank\">Making Habits that Stick</a></strong>&nbsp;- Sept 15, 3hrs, $90</p>\n<p>Learn how to make new, useful habits and replace pernicious old ones. &nbsp;Andrew Critch, one of CFAR&rsquo;s instructors will be teaching you how the brain holds on to patterns and cached responses. And then you&rsquo;ll make use of your brain&rsquo;s circuitry to set new habits deliberately.</p>\n<p>Alumni of our four-day workshops have used this material to do anything from remembering to check a to do list, right when you walk through the door at home, to making sure that a feeling of confusion triggers you to speak up and ask for an example.</p>\n<p><br /><strong><a href=\"http://rationality.org/programmers-beta/\" target=\"_blank\">One-Day Workshop for Programmers</a></strong>&nbsp;- Sept 29, full day, $330</p>\n<p>Take a selection of classes from our four-day workshop that have been tailored for relevance to people in programming careers. &nbsp;Why? Programmers continually manage huge amounts of their own time on large projects, and they know that anything you do&nbsp;<em>continually</em>&nbsp;is worth setting aside time to optimize.</p>\n<p>After spending a day with CFAR, you can expect to catch yourself making a lot more mistakes. They won&rsquo;t be new; you&rsquo;ll just be more attuned to how your brain&rsquo;s innate heuristics can lead you astray, so you&rsquo;ll catch on to slip-ups faster. And instead of feeling bad about these failure modes, you&rsquo;ll have the tools you need to jump out of them quickly. In fact, it won&rsquo;t feel as much like making mistakes as discovering bugs &mdash; you&rsquo;ll be curious and confident about figuring out a solution, and proud of yourself for catching the opportunity.</p>\n<p>&nbsp;</p>\n<p>If you're interested in our curriculum, but would like to do something smaller than a workshop because of time or money constraints, these two September offerings may be of interest. &nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HQEhtemyBhucEvrgr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 1.3359952985560552e-06, "legacy": true, "legacyId": "24105", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-10T23:04:54.042Z", "modifiedAt": null, "url": null, "title": "The Interrupted Ultimate Newcomb's Problem", "slug": "the-interrupted-ultimate-newcomb-s-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:27.368Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "linkhyrule5", "createdAt": "2011-05-11T06:03:56.654Z", "isAdmin": false, "displayName": "linkhyrule5"}, "userId": "dfDgYH8CfLSYPCaRj", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AD45vooiaDacXwpoH/the-interrupted-ultimate-newcomb-s-problem", "pageUrlRelative": "/posts/AD45vooiaDacXwpoH/the-interrupted-ultimate-newcomb-s-problem", "linkUrl": "https://www.lesswrong.com/posts/AD45vooiaDacXwpoH/the-interrupted-ultimate-newcomb-s-problem", "postedAtFormatted": "Tuesday, September 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Interrupted%20Ultimate%20Newcomb's%20Problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Interrupted%20Ultimate%20Newcomb's%20Problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAD45vooiaDacXwpoH%2Fthe-interrupted-ultimate-newcomb-s-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Interrupted%20Ultimate%20Newcomb's%20Problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAD45vooiaDacXwpoH%2Fthe-interrupted-ultimate-newcomb-s-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAD45vooiaDacXwpoH%2Fthe-interrupted-ultimate-newcomb-s-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 302, "htmlBody": "<p>While figuring out my error in my solution to the <a href=\"/r/discussion/lw/ila/the_ultimate_newcombs_problem/\">Ultimate Newcomb's Problem,</a> I ran across this (distinct) reformulation that helped me distinguish between what I was doing and what the problem was actually asking.</p>\n<p>... but that being said, I'm not sure if my answer to the reformulation is correct either.</p>\n<p>&nbsp;</p>\n<p>The question, cleaned for Discussion, looks like this:</p>\n<p>You approach the boxes and lottery, which are exactly as in the UNP. Before reaching it, you come to sign with a flashing red light. The sign reads: \"INDEPENDENT SCENARIO BEGIN.\"</p>\n<p>Omega, who has predicted that you will be confused, shows up to explain: \"This is considered an artificially independent experiment. Your algorithm for solving this problem will not be used in my simulations of your algorithm for my various other problems. In other words, you are allowed to two-box here but one-box Newcomb's problem, or vice versa.\"</p>\n<p>This is motivated by the realization that I've been making the same mistake as in the original Newcomb's Problem, though this justification does not (I believe) apply to the original.&nbsp; The mistake is simply this: that I assumed that I simply appear <em>in medias res</em>. When solving the UNP, it is (seems to be) important to remember that you may be in some very rare edge case of the main problem, and that you are choosing your algorithm for the problem as a whole.</p>\n<p>But if that's <em>not</em> true - if you're allowed to appear in the middle of the problem, and no counterfactual-yous are at risk - it sure seems like two-boxing is justified - as <a href=\"/r/discussion/lw/ila/the_ultimate_newcombs_problem/9q9v\">khafra put it</a>, \"trying to ambiently control basic arithmetic\".</p>\n<p>&nbsp;</p>\n<p>(Speaking of which, is there a write up of ambient decision theory anywhere? For that matter, is there any compilation of decision theories?)</p>\n<p>&nbsp;</p>\n<p>EDIT: (Yes to the first, though not under that name: <a href=\"/lw/2os/controlling_constant_programs/\">Controlling Constant Programs</a>.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AD45vooiaDacXwpoH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 1.3361312489923962e-06, "legacy": true, "legacyId": "24106", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RAh4fekdiRhZxb2Kw", "gZbHSWcLvj7ZopSas"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-10T23:26:48.518Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham NC/Triangle Area: Cognitive Biases, Continued", "slug": "meetup-durham-nc-triangle-area-cognitive-biases-continued", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:26.028Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Wq5e2gCQzYfbk3KZB/meetup-durham-nc-triangle-area-cognitive-biases-continued", "pageUrlRelative": "/posts/Wq5e2gCQzYfbk3KZB/meetup-durham-nc-triangle-area-cognitive-biases-continued", "linkUrl": "https://www.lesswrong.com/posts/Wq5e2gCQzYfbk3KZB/meetup-durham-nc-triangle-area-cognitive-biases-continued", "postedAtFormatted": "Tuesday, September 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Cognitive%20Biases%2C%20Continued&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Cognitive%20Biases%2C%20Continued%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWq5e2gCQzYfbk3KZB%2Fmeetup-durham-nc-triangle-area-cognitive-biases-continued%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Cognitive%20Biases%2C%20Continued%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWq5e2gCQzYfbk3KZB%2Fmeetup-durham-nc-triangle-area-cognitive-biases-continued", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWq5e2gCQzYfbk3KZB%2Fmeetup-durham-nc-triangle-area-cognitive-biases-continued", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qq'>Durham NC/Triangle Area: Cognitive Biases, Continued</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 September 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">420 West Geer St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Follow up discussion to the prior meetup's cognitive biases survey!  We will focus on higher-ranked biases from that meetup.</p>\n\n<p>7:00 obtain coffees <br />\n7:30 discussion <br />\n9:30ish adjourn to Fullsteam</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qq'>Durham NC/Triangle Area: Cognitive Biases, Continued</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Wq5e2gCQzYfbk3KZB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "24107", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Cognitive_Biases__Continued\">Discussion article for the meetup : <a href=\"/meetups/qq\">Durham NC/Triangle Area: Cognitive Biases, Continued</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 September 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">420 West Geer St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Follow up discussion to the prior meetup's cognitive biases survey!  We will focus on higher-ranked biases from that meetup.</p>\n\n<p>7:00 obtain coffees <br>\n7:30 discussion <br>\n9:30ish adjourn to Fullsteam</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Cognitive_Biases__Continued1\">Discussion article for the meetup : <a href=\"/meetups/qq\">Durham NC/Triangle Area: Cognitive Biases, Continued</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham NC/Triangle Area: Cognitive Biases, Continued", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Cognitive_Biases__Continued", "level": 1}, {"title": "Discussion article for the meetup : Durham NC/Triangle Area: Cognitive Biases, Continued", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Cognitive_Biases__Continued1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-11T14:55:58.368Z", "modifiedAt": null, "url": null, "title": "Definition of AI Friendliness", "slug": "definition-of-ai-friendliness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:27.370Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "djm", "createdAt": "2013-07-25T02:09:05.823Z", "isAdmin": false, "displayName": "djm"}, "userId": "n3CQbiPiYEKJhXjas", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yuX4kuQnDfjd3rSsc/definition-of-ai-friendliness", "pageUrlRelative": "/posts/yuX4kuQnDfjd3rSsc/definition-of-ai-friendliness", "linkUrl": "https://www.lesswrong.com/posts/yuX4kuQnDfjd3rSsc/definition-of-ai-friendliness", "postedAtFormatted": "Wednesday, September 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Definition%20of%20AI%20Friendliness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADefinition%20of%20AI%20Friendliness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyuX4kuQnDfjd3rSsc%2Fdefinition-of-ai-friendliness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Definition%20of%20AI%20Friendliness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyuX4kuQnDfjd3rSsc%2Fdefinition-of-ai-friendliness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyuX4kuQnDfjd3rSsc%2Fdefinition-of-ai-friendliness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 177, "htmlBody": "<h3 style=\"line-height: 1.15; margin-top: 8pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"background-color: transparent; font-family: Arial; font-weight: normal; white-space: pre-wrap; line-height: 1.15;\">How will we know if future AI&rsquo;s (or even existing planners) are making decisions that are bad for humans unless we spell out what we think is unfriendly?</span></h3>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">At a machine level the AI would be recursively minimising cost functions to produce the most effective plan of action to achieve the goal, but how will we know if its decision is going to cause harm?</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><strong id=\"docs-internal-guid-700e7664-0d84-6f77-6b53-fc2fef06972f\" style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Is there a model or dataset which describes what is friendly to humans? e.g.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><strong>Context</strong></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0 - running a simulation in a VM</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">2 - physical robot with vacuum attachment</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">9 - full control of a plane</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><strong>Actions</strong></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0 - selecting a song to play</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">5 - deciding which section of floor to vacuum</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">99 - deciding who is an &lsquo;enemy&rsquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">9999 - aiming a gun at an &lsquo;enemy&rsquo;</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><strong>Impact</strong></span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">1 - poor song selected to play, human mildly annoyed</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">2 - ineffective use of resources (vacuuming the same floor section twice)</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">99 - killing a human</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">99999 - killing all humans</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This may not be possible to get agreement from all countries/cultures/beliefs, but it is something we should discuss and attempt to get some agreement.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">. </span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yuX4kuQnDfjd3rSsc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -9, "extendedScore": null, "score": 1.3369659229615022e-06, "legacy": true, "legacyId": "24113", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-11T17:21:42.973Z", "modifiedAt": null, "url": null, "title": "Meetup : Helsinki Meetup", "slug": "meetup-helsinki-meetup-8", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:31.428Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "morrel", "createdAt": "2013-07-04T17:36:44.083Z", "isAdmin": false, "displayName": "morrel"}, "userId": "Xdtoje5pFmqC2CY6y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/k7x8xea2ce2E7ZMoz/meetup-helsinki-meetup-8", "pageUrlRelative": "/posts/k7x8xea2ce2E7ZMoz/meetup-helsinki-meetup-8", "linkUrl": "https://www.lesswrong.com/posts/k7x8xea2ce2E7ZMoz/meetup-helsinki-meetup-8", "postedAtFormatted": "Wednesday, September 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Helsinki%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Helsinki%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk7x8xea2ce2E7ZMoz%2Fmeetup-helsinki-meetup-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Helsinki%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk7x8xea2ce2E7ZMoz%2Fmeetup-helsinki-meetup-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk7x8xea2ce2E7ZMoz%2Fmeetup-helsinki-meetup-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qr'>Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 September 2013 02:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Helsinki Music Centre, 00100 Helsinki, Finland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The theme of this meetup will be <a href=\"http://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic/\">strategic</a> <a href=\"http://lesswrong.com/r/discussion/lw/iga/open_thread_how_much_strategic_thinking_have_you/\">thinking</a>, with Kaj Sotala leading the discussion.</p>\n\n<p>We'll meet in the cafe at <a href=\"http://www.musiikkitalo.fi\" rel=\"nofollow\">Helsinki Music Centre</a>. Afterwards we'll probably move to <a href=\"http://www.oluthuone.fi/oluthuoneet/kaisla/\" rel=\"nofollow\">Kaisla</a> for socializing.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qr'>Helsinki Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "k7x8xea2ce2E7ZMoz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3370939108996729e-06, "legacy": true, "legacyId": "24114", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup\">Discussion article for the meetup : <a href=\"/meetups/qr\">Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 September 2013 02:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Helsinki Music Centre, 00100 Helsinki, Finland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The theme of this meetup will be <a href=\"http://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic/\">strategic</a> <a href=\"http://lesswrong.com/r/discussion/lw/iga/open_thread_how_much_strategic_thinking_have_you/\">thinking</a>, with Kaj Sotala leading the discussion.</p>\n\n<p>We'll meet in the cafe at <a href=\"http://www.musiikkitalo.fi\" rel=\"nofollow\">Helsinki Music Centre</a>. Afterwards we'll probably move to <a href=\"http://www.oluthuone.fi/oluthuoneet/kaisla/\" rel=\"nofollow\">Kaisla</a> for socializing.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/qr\">Helsinki Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PBRWb2Em5SNeWYwwB", "56QS5H4puJHDQbusx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-11T18:52:06.196Z", "modifiedAt": null, "url": null, "title": "What's Your Hourly Rate?", "slug": "what-s-your-hourly-rate", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:28.850Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EZvpMgKjRWdqSYGJK/what-s-your-hourly-rate", "pageUrlRelative": "/posts/EZvpMgKjRWdqSYGJK/what-s-your-hourly-rate", "linkUrl": "https://www.lesswrong.com/posts/EZvpMgKjRWdqSYGJK/what-s-your-hourly-rate", "postedAtFormatted": "Wednesday, September 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What's%20Your%20Hourly%20Rate%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat's%20Your%20Hourly%20Rate%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEZvpMgKjRWdqSYGJK%2Fwhat-s-your-hourly-rate%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What's%20Your%20Hourly%20Rate%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEZvpMgKjRWdqSYGJK%2Fwhat-s-your-hourly-rate", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEZvpMgKjRWdqSYGJK%2Fwhat-s-your-hourly-rate", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 25, "htmlBody": "<p>Here's an <a href=\"http://techhouse.org/~lincoln/blosxom.cgi/rationality/hourly-rates.html\">interesting post</a> about calculating the value of your free time and why it might not be as simple as some tend to think.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EZvpMgKjRWdqSYGJK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 1.3371732972585086e-06, "legacy": true, "legacyId": "24115", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-12T02:38:45.911Z", "modifiedAt": null, "url": null, "title": "A concise version of \u201cTwelve Virtues of Rationality\u201d, with Anki deck", "slug": "a-concise-version-of-twelve-virtues-of-rationality-with-anki", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:03.193Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "DAomdEZSGbrYYpcpX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TNyEe3bzonreRW9nB/a-concise-version-of-twelve-virtues-of-rationality-with-anki", "pageUrlRelative": "/posts/TNyEe3bzonreRW9nB/a-concise-version-of-twelve-virtues-of-rationality-with-anki", "linkUrl": "https://www.lesswrong.com/posts/TNyEe3bzonreRW9nB/a-concise-version-of-twelve-virtues-of-rationality-with-anki", "postedAtFormatted": "Thursday, September 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20concise%20version%20of%20%E2%80%9CTwelve%20Virtues%20of%20Rationality%E2%80%9D%2C%20with%20Anki%20deck&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20concise%20version%20of%20%E2%80%9CTwelve%20Virtues%20of%20Rationality%E2%80%9D%2C%20with%20Anki%20deck%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTNyEe3bzonreRW9nB%2Fa-concise-version-of-twelve-virtues-of-rationality-with-anki%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20concise%20version%20of%20%E2%80%9CTwelve%20Virtues%20of%20Rationality%E2%80%9D%2C%20with%20Anki%20deck%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTNyEe3bzonreRW9nB%2Fa-concise-version-of-twelve-virtues-of-rationality-with-anki", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTNyEe3bzonreRW9nB%2Fa-concise-version-of-twelve-virtues-of-rationality-with-anki", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 797, "htmlBody": "<p>In an effort to internalise the <a href=\"http://yudkowsky.net/rational/virtues\">Twelve Virtues of Rationality</a>, I created an Anki deck. <a href=\"http://alexvermeer.com/anki-decks/\">It's already been done</a>, so the reason I'm posting is to share a condensed version of the article (created as a side effect of my making the deck).</p>\n<p>Hopefully it will make it easier to quickly refresh the concepts if you've already read the article.</p>\n<p>If you're not using <a href=\"http://wiki.lesswrong.com/wiki/Spaced_repetition\">spaced repetition</a>, you should. Don't believe me? Try reading <a href=\"http://www.gwern.net/Spaced%20repetition\">Gwern's thorough review of the topic</a>.</p>\n<p>Then <a href=\"https://www.dropbox.com/s/lbb66leouejvv1i/Twelve%20Virtues.apkg\">download the &ldquo;Twelve Virtues of Rationality&rdquo; deck</a>.</p>\n<hr />\n<div><br /></div>\n<div><strong>The twelve virtues of rationality</strong>: Curiosity, relinquishment, lightness, evenness, argument, empiricism, simplicity, humility, perfectionism, precision, scholarship, and the void.</div>\n<div><br /></div>\n<h2>The first virtue is curiosity.</h2>\n<div>Curiosity seeks to annihilate itself; there is no curiosity that does not want an answer.</div>\n<div><br /></div>\n<div>A burning itch to know is higher than a solemn vow to pursue truth.</div>\n<div><br /></div>\n<div>To feel the burning itch of curiosity requires both that you be ignorant, and that you desire to relinquish your ignorance.</div>\n<div><br /></div>\n<h2>The second virtue is relinquishment.</h2>\n<div>P. C. Hodgell said: &ldquo;That which can be destroyed by the truth should be.&rdquo;</div>\n<div><br /></div>\n<div>If the iron approaches your face, and you believe it is hot, and it is cool, the Way opposes your fear.</div>\n<div><br /></div>\n<div>Evaluate your beliefs first and then arrive at your emotions.</div>\n<div><br /></div>\n<h2>The third virtue is lightness.</h2>\n<div>Surrender to the truth as quickly as you can.</div>\n<div><br /></div>\n<div>If you regard evidence as a constraint and seek to free yourself, you sell yourself into the chains of your whims.</div>\n<div><br /></div>\n<h2>The fourth virtue is evenness.</h2>\n<div>Beware lest you place huge burdens of proof only on propositions you dislike, and then defend yourself by saying: &ldquo;But it is good to be skeptical.&rdquo; &nbsp;</div>\n<div><br /></div>\n<div>Do not seek to argue for one side or another, for if you knew your destination, you would already be there.</div>\n<div><br /></div>\n<div>To be clever in argument is not rationality but rationalization.</div>\n<div><br /></div>\n<h2>The fifth virtue is argument.&nbsp;</h2>\n<div>Those who smile wisely and say: &ldquo;I will not argue&rdquo; remove themselves from help, and withdraw from the communal effort.</div>\n<div><br /></div>\n<div>The part of yourself that distorts what you say to others also distorts your own thoughts.</div>\n<div><br /></div>\n<div>Seek a test that lets reality judge between you.</div>\n<div><br /></div>\n<h2>The sixth virtue is empiricism.</h2>\n<div>The roots of knowledge are in observation and its fruit is prediction.</div>\n<div><br /></div>\n<div>Do not ask which beliefs to profess, but which experiences to anticipate.</div>\n<div><br /></div>\n<h2>The seventh virtue is simplicity.&nbsp;</h2>\n<div>When you profess a huge belief with many details, each additional detail is another chance for the belief to be wrong.</div>\n<div><br /></div>\n<div>In mathematics a mountain of good deeds cannot atone for a single sin. Therefore, be careful on every step.</div>\n<div><br /></div>\n<h2>The eighth virtue is humility.</h2>\n<div>To be humble is to take specific actions in anticipation of your own errors.</div>\n<div><br /></div>\n<div>It is useless to be superior: Life is not graded on a curve.&nbsp;</div>\n<div><br /></div>\n<div>The best physicist in ancient Greece could not calculate the path of a falling apple.</div>\n<div><br /></div>\n<div><br /></div>\n<h2>The ninth virtue is perfectionism.</h2>\n<div>The more errors you correct in yourself, the more you notice.</div>\n<div><br /></div>\n<div>If you tolerate the error rather than correcting it, you will not advance to the next level and you will not gain the skill to notice new errors.</div>\n<div><br /></div>\n<div>Do not be content with the answer that is almost right; seek one that is exactly right.</div>\n<div><br /></div>\n<h2>The tenth virtue is precision.</h2>\n<div>What is true of one apple may not be true of another apple; thus more can be said about a single apple than about all the apples in the world.</div>\n<div><br /></div>\n<div>The narrowest statements slice deepest.</div>\n<div><br /></div>\n<div>Do not walk to the truth, but dance. On each and every step of that dance your foot comes down in exactly the right spot.</div>\n<div><br /></div>\n<h2>The eleventh virtue is scholarship.</h2>\n<div>Study many sciences and absorb their power as your own.</div>\n<div><br /></div>\n<div>If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole.&nbsp;</div>\n<div><br /></div>\n<div>The Art must have a purpose other than itself, or it collapses into infinite recursion.</div>\n<div><br /></div>\n<h2>Before these eleven virtues is a virtue which is nameless.</h2>\n<div>Miyamoto Musashi wrote, in The Book of Five Rings: &nbsp;&ldquo;The primary thing when you take a sword in your hands is your intention to cut the enemy.&rdquo;</div>\n<div><br /></div>\n<div>Every step of your reasoning must cut through to the correct answer in the same movement.</div>\n<div><br /></div>\n<div>Do not ask whether it is &ldquo;the Way&rdquo; to do this or that. Ask whether the sky is blue or green.</div>\n<div><br /></div>\n<div>All techniques are one technique.</div>\n<div><br /></div>\n<div><br /></div>\n<div><strong>The twelve virtues of rationality</strong>: Curiosity, relinquishment, lightness, evenness, argument, empiricism, simplicity, humility, perfectionism, precision, scholarship, and the void.</div>\n<div><br /></div>\n<div>\n<hr />\n</div>\n<div><br /></div>\n<div><br /></div>\n<div>If I've made any mistakes or omissions, <strong>please speak up</strong>!</div>\n<div><br /></div>\n<div><br /></div>\n<div><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TNyEe3bzonreRW9nB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 3, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "24118", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>In an effort to internalise the <a href=\"http://yudkowsky.net/rational/virtues\">Twelve Virtues of Rationality</a>, I created an Anki deck. <a href=\"http://alexvermeer.com/anki-decks/\">It's already been done</a>, so the reason I'm posting is to share a condensed version of the article (created as a side effect of my making the deck).</p>\n<p>Hopefully it will make it easier to quickly refresh the concepts if you've already read the article.</p>\n<p>If you're not using <a href=\"http://wiki.lesswrong.com/wiki/Spaced_repetition\">spaced repetition</a>, you should. Don't believe me? Try reading <a href=\"http://www.gwern.net/Spaced%20repetition\">Gwern's thorough review of the topic</a>.</p>\n<p>Then <a href=\"https://www.dropbox.com/s/lbb66leouejvv1i/Twelve%20Virtues.apkg\">download the \u201cTwelve Virtues of Rationality\u201d deck</a>.</p>\n<hr>\n<div><br></div>\n<div><strong>The twelve virtues of rationality</strong>: Curiosity, relinquishment, lightness, evenness, argument, empiricism, simplicity, humility, perfectionism, precision, scholarship, and the void.</div>\n<div><br></div>\n<h2 id=\"The_first_virtue_is_curiosity_\">The first virtue is curiosity.</h2>\n<div>Curiosity seeks to annihilate itself; there is no curiosity that does not want an answer.</div>\n<div><br></div>\n<div>A burning itch to know is higher than a solemn vow to pursue truth.</div>\n<div><br></div>\n<div>To feel the burning itch of curiosity requires both that you be ignorant, and that you desire to relinquish your ignorance.</div>\n<div><br></div>\n<h2 id=\"The_second_virtue_is_relinquishment_\">The second virtue is relinquishment.</h2>\n<div>P. C. Hodgell said: \u201cThat which can be destroyed by the truth should be.\u201d</div>\n<div><br></div>\n<div>If the iron approaches your face, and you believe it is hot, and it is cool, the Way opposes your fear.</div>\n<div><br></div>\n<div>Evaluate your beliefs first and then arrive at your emotions.</div>\n<div><br></div>\n<h2 id=\"The_third_virtue_is_lightness_\">The third virtue is lightness.</h2>\n<div>Surrender to the truth as quickly as you can.</div>\n<div><br></div>\n<div>If you regard evidence as a constraint and seek to free yourself, you sell yourself into the chains of your whims.</div>\n<div><br></div>\n<h2 id=\"The_fourth_virtue_is_evenness_\">The fourth virtue is evenness.</h2>\n<div>Beware lest you place huge burdens of proof only on propositions you dislike, and then defend yourself by saying: \u201cBut it is good to be skeptical.\u201d &nbsp;</div>\n<div><br></div>\n<div>Do not seek to argue for one side or another, for if you knew your destination, you would already be there.</div>\n<div><br></div>\n<div>To be clever in argument is not rationality but rationalization.</div>\n<div><br></div>\n<h2 id=\"The_fifth_virtue_is_argument__\">The fifth virtue is argument.&nbsp;</h2>\n<div>Those who smile wisely and say: \u201cI will not argue\u201d remove themselves from help, and withdraw from the communal effort.</div>\n<div><br></div>\n<div>The part of yourself that distorts what you say to others also distorts your own thoughts.</div>\n<div><br></div>\n<div>Seek a test that lets reality judge between you.</div>\n<div><br></div>\n<h2 id=\"The_sixth_virtue_is_empiricism_\">The sixth virtue is empiricism.</h2>\n<div>The roots of knowledge are in observation and its fruit is prediction.</div>\n<div><br></div>\n<div>Do not ask which beliefs to profess, but which experiences to anticipate.</div>\n<div><br></div>\n<h2 id=\"The_seventh_virtue_is_simplicity__\">The seventh virtue is simplicity.&nbsp;</h2>\n<div>When you profess a huge belief with many details, each additional detail is another chance for the belief to be wrong.</div>\n<div><br></div>\n<div>In mathematics a mountain of good deeds cannot atone for a single sin. Therefore, be careful on every step.</div>\n<div><br></div>\n<h2 id=\"The_eighth_virtue_is_humility_\">The eighth virtue is humility.</h2>\n<div>To be humble is to take specific actions in anticipation of your own errors.</div>\n<div><br></div>\n<div>It is useless to be superior: Life is not graded on a curve.&nbsp;</div>\n<div><br></div>\n<div>The best physicist in ancient Greece could not calculate the path of a falling apple.</div>\n<div><br></div>\n<div><br></div>\n<h2 id=\"The_ninth_virtue_is_perfectionism_\">The ninth virtue is perfectionism.</h2>\n<div>The more errors you correct in yourself, the more you notice.</div>\n<div><br></div>\n<div>If you tolerate the error rather than correcting it, you will not advance to the next level and you will not gain the skill to notice new errors.</div>\n<div><br></div>\n<div>Do not be content with the answer that is almost right; seek one that is exactly right.</div>\n<div><br></div>\n<h2 id=\"The_tenth_virtue_is_precision_\">The tenth virtue is precision.</h2>\n<div>What is true of one apple may not be true of another apple; thus more can be said about a single apple than about all the apples in the world.</div>\n<div><br></div>\n<div>The narrowest statements slice deepest.</div>\n<div><br></div>\n<div>Do not walk to the truth, but dance. On each and every step of that dance your foot comes down in exactly the right spot.</div>\n<div><br></div>\n<h2 id=\"The_eleventh_virtue_is_scholarship_\">The eleventh virtue is scholarship.</h2>\n<div>Study many sciences and absorb their power as your own.</div>\n<div><br></div>\n<div>If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole.&nbsp;</div>\n<div><br></div>\n<div>The Art must have a purpose other than itself, or it collapses into infinite recursion.</div>\n<div><br></div>\n<h2 id=\"Before_these_eleven_virtues_is_a_virtue_which_is_nameless_\">Before these eleven virtues is a virtue which is nameless.</h2>\n<div>Miyamoto Musashi wrote, in The Book of Five Rings: &nbsp;\u201cThe primary thing when you take a sword in your hands is your intention to cut the enemy.\u201d</div>\n<div><br></div>\n<div>Every step of your reasoning must cut through to the correct answer in the same movement.</div>\n<div><br></div>\n<div>Do not ask whether it is \u201cthe Way\u201d to do this or that. Ask whether the sky is blue or green.</div>\n<div><br></div>\n<div>All techniques are one technique.</div>\n<div><br></div>\n<div><br></div>\n<div><strong>The twelve virtues of rationality</strong>: Curiosity, relinquishment, lightness, evenness, argument, empiricism, simplicity, humility, perfectionism, precision, scholarship, and the void.</div>\n<div><br></div>\n<div>\n<hr>\n</div>\n<div><br></div>\n<div><br></div>\n<div>If I've made any mistakes or omissions, <strong>please speak up</strong>!</div>\n<div><br></div>\n<div><br></div>\n<div><br></div>", "sections": [{"title": "The first virtue is curiosity.", "anchor": "The_first_virtue_is_curiosity_", "level": 1}, {"title": "The second virtue is relinquishment.", "anchor": "The_second_virtue_is_relinquishment_", "level": 1}, {"title": "The third virtue is lightness.", "anchor": "The_third_virtue_is_lightness_", "level": 1}, {"title": "The fourth virtue is evenness.", "anchor": "The_fourth_virtue_is_evenness_", "level": 1}, {"title": "The fifth virtue is argument.\u00a0", "anchor": "The_fifth_virtue_is_argument__", "level": 1}, {"title": "The sixth virtue is empiricism.", "anchor": "The_sixth_virtue_is_empiricism_", "level": 1}, {"title": "The seventh virtue is simplicity.\u00a0", "anchor": "The_seventh_virtue_is_simplicity__", "level": 1}, {"title": "The eighth virtue is humility.", "anchor": "The_eighth_virtue_is_humility_", "level": 1}, {"title": "The ninth virtue is perfectionism.", "anchor": "The_ninth_virtue_is_perfectionism_", "level": 1}, {"title": "The tenth virtue is precision.", "anchor": "The_tenth_virtue_is_precision_", "level": 1}, {"title": "The eleventh virtue is scholarship.", "anchor": "The_eleventh_virtue_is_scholarship_", "level": 1}, {"title": "Before these eleven virtues is a virtue which is nameless.", "anchor": "Before_these_eleven_virtues_is_a_virtue_which_is_nameless_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "17 comments"}], "headingsCount": 14}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-12T07:21:30.255Z", "modifiedAt": null, "url": null, "title": "How well will policy-makers handle AGI? (initial findings)", "slug": "how-well-will-policy-makers-handle-agi-initial-findings", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:07.755Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4JGQtc4ZgmdsrtBhB/how-well-will-policy-makers-handle-agi-initial-findings", "pageUrlRelative": "/posts/4JGQtc4ZgmdsrtBhB/how-well-will-policy-makers-handle-agi-initial-findings", "linkUrl": "https://www.lesswrong.com/posts/4JGQtc4ZgmdsrtBhB/how-well-will-policy-makers-handle-agi-initial-findings", "postedAtFormatted": "Thursday, September 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20well%20will%20policy-makers%20handle%20AGI%3F%20(initial%20findings)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20well%20will%20policy-makers%20handle%20AGI%3F%20(initial%20findings)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4JGQtc4ZgmdsrtBhB%2Fhow-well-will-policy-makers-handle-agi-initial-findings%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20well%20will%20policy-makers%20handle%20AGI%3F%20(initial%20findings)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4JGQtc4ZgmdsrtBhB%2Fhow-well-will-policy-makers-handle-agi-initial-findings", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4JGQtc4ZgmdsrtBhB%2Fhow-well-will-policy-makers-handle-agi-initial-findings", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2445, "htmlBody": "<p><small>Cross-posted from <a href=\"http://intelligence.org/2013/09/12/how-well-will-policy-makers-handle-agi-initial-findings/\">MIRI's blog</a>.</small></p>\n<p>MIRI's <a href=\"http://intelligence.org/about/\">mission</a> is \"to ensure that the creation of smarter-than-human intelligence has a positive impact.\"  One policy-relevant question is: <strong>How well should we expect policy makers to handle the invention of AGI, and what does this imply about how much effort to put into AGI risk mitigation vs. other concerns?</strong></p>\n<p><strong></strong>To investigate these questions, we asked&nbsp;<a href=\"http://mathisbeauty.org/aboutme.html\">Jonah Sinick</a>&nbsp;to examine how well policy-makers handled past events analogous in some ways to the future invention of AGI, and summarize his findings. We pre-committed to publishing our entire email exchange on the topic (with minor editing), just as with our project on <a href=\"http://intelligence.org/2013/09/02/how-effectively-can-we-plan-for-future-decades\">how well we can plan for future decades</a>. The post below is a summary of findings from&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2013/09/Elites-and-AI.docx\">our full email exchange (.docx)</a>&nbsp;so far.</p>\n<p>As with our investigation of how well we can plan for future decades,<strong>&nbsp;we decided to publish our initial findings after investigating only a few historical cases</strong>. This allows us to gain feedback on the value of the project, as well as suggestions for improvement, before continuing. It also means that&nbsp;<strong>we aren't yet able to draw any confident conclusions about our core questions</strong>.</p>\n<p>The most significant results from this project so far are:</p>\n<ol>\n<li>We came up with a preliminary list of 6 seemingly-important ways in which a historical case could be analogous to the future invention of AGI, and evaluated several historical cases on these criteria.</li>\n<li>Climate change risk seems sufficiently disanalogous to AI risk that studying climate change mitigation efforts probably gives limited insight into how well policy-makers will deal with AGI risk: the expected damage of climate change appears to be very small relative to the the expected damage due to AI risk, especially when one looks at expected damage to policy makers.</li>\n<li>The 2008 financial crisis appears, after a shallow investigation, to be sufficiently analogous to AGI risk that it should give us some small reason to be concerned that policy-makers will not manage the invention of AGI wisely.</li>\n<li>The risks to critical infrastructure from geomagnetic storms are far too small to be in the same reference class with risks from AGI.</li>\n<li>The eradication of smallpox is only somewhat analogous to the invention of AGI.</li>\n<li>Jonah performed very shallow investigations of how policy-makers have handled risks from cyberwarfare, chlorofluorocarbons, and the Cuban missile crisis, but these cases need more study before even \"initial thoughts\" can be given.</li>\n<li>We identified additional historical cases that could be investigated in the future.</li>\n</ol>\n<p>Further details are given below. For sources and more, please see&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2013/09/Elites-and-AI.docx\">our full email exchange (.docx)</a>.</p>\n<!--more-->\n<h3><br /></h3>\n<h3>6 ways a historical case can be analogous to the invention of AGI</h3>\n<p>In conversation, Jonah and I identified six features of the future invention of AGI that, if largely shared by a historical case, seem likely to allow the historical case to shed light on how well policy-makers will deal with the invention of AGI:</p>\n<ol>\n<li>AGI may become a major threat in a somewhat unpredictable time.</li>\n<li>AGI may become a threat when the world has very limited experience with it.</li>\n<li>A good outcome with AGI may require solving a difficult global coordination problem.</li>\n<li>Preparing for the AGI threat adequately may require lots of careful work in advance.</li>\n<li>Policy-makers have strong personal incentives to solve the AGI problem.</li>\n<li>A bad outcome with AGI would be a global disaster, and a good outcome with AGI would have global humanitarian benefit.</li>\n</ol>\n<p>More details on these criteria and their use are given in the second email of our full email exchange.  &nbsp;</p>\n<h3><br /></h3>\n<h3>Risks from climate change</h3>\n<p>People began to see climate change as a potential problem in the early 1970s, but there was some ambiguity as to whether human activity was causing warming (because of carbon emissions) or cooling (because of smog particles). The first <a href=\"http://en.wikipedia.org/wiki/Intergovernmental_Panel_on_Climate_Change\">IPCC</a> report was issued in 1990, and stated that were was substantial anthropogenic global warming due to greenhouse gases. By 2001, there was a strong scientific consensus behind this claim.  While policy-makers' response to risks from climate change might seem likely to shed light on whether policy-makers will deal wisely with AGI, there are some important disanalogies:</p>\n<ul>\n<li>The harms of global warming are expected to fall disproportionately on disadvantaged people in poor countries, not on policy-makers. So policy-makers have much less personal incentive to solve the problem than is the case with AGI.</li>\n<li>In the median case, humanitarian losses from global warming <a href=\"/lw/hi1/potential_impacts_of_climate_change/\">seems to be</a> about 20% of GDP per year for the poorest people.&nbsp;In light of anticipated economic development and marginal diminishing utility, this is a <em>much</em> smaller negative humanitarian impact than AGI risk (even ignoring future generations). For example, economist Indur Goklany <a href=\"http://wattsupwiththat.com/2012/10/17/is-climate-change-the-number-one-threat-to-humanity/\">estimated</a> that \"through 2085, only 13% of [deaths] from hunger, malaria, and extreme weather events (including coastal flooding from sea level rise) should be from [global] warming.\"</li>\n<li>Thus, potential analogies to AGI risk come from climate change's <em>tail risk</em>. But there seem to be few credentialed scientists who have views compatible with a prediction that even a temperature increase in the 95th percentile of the probability distribution (by 2100) would do more than just begin to render some regions of Earth uninhabitable.</li>\n<li>According to the <a href=\"http://www.ipcc.ch/meetings/session31/inf3.pdf\">5th IPCC</a>, the risk of human extinction from climate change seems very low: \"Some thresholds that all would consider dangerous have no support in the literature as having a non-negligible chance of occurring. For instance, a 'runaway greenhouse effect'&mdash;analogous to Venus&mdash;appears to have virtually no chance of being induced by anthropogenic activities.\"</li>\n</ul>\n&nbsp;\n<h3>The 2008 financial crisis</h3>\n<p>Jonah did a shallow investigation of the 2008 financial crisis, but the preliminary findings are interesting enough for us to describe them in some detail. Jonah's impressions about the relevance of the 2008 financial crisis to the AGI situation are based on a reading of&nbsp;<em><a href=\"http://www.amazon.com/After-Music-Stopped-Financial-Response/dp/1594205302/\">After the Music Stopped</a></em> by Alan Blinder, who was the vice chairman of the federal reserve for 1.5 years during the Clinton administration. Naturally, many additional sources should be consulted before drawing firm conclusions about the relevance of policy-makers' handling of the financial crisis to their likelihood of handling AGI wisely.</p>\n<p>Blinder's seven main factors leading to the recession are (p. 27):</p>\n<ol>\n<li>Inflated asset prices, especially of houses (the housing bubble) but also of certain securities (the bond bubble);</li>\n<li>Excessive leverage (heavy borrowing) throughout the financial system and the economy;</li>\n<li>Lax financial regulation, both in terms of what the law left unregulated and how poorly the various regulators performed their duties;</li>\n<li>Disgraceful banking practices in subprime and other mortgage lending;</li>\n<li>The crazy-quilt of unregulated securities and derivatives that were built on these bad mortgages;</li>\n<li>The abysmal performance of the statistical rating agencies, which helped the crazy-quilt get stitched together; and</li>\n<li>The perverse compensation systems in many financial institutions that created powerful incentives to go for broke.</li>\n</ol>\n<p>With these factors in mind, let's look at the strength of the analogy between the 2008 financial crisis and the future invention of AGI:</p>\n<ol>\n<li>Almost tautologically, a financial crisis is unexpected, though we do know that financial crises happen with some regularity.</li>\n<li>The 2008 financial crisis was not unprecedented in kind, only in degree (in some ways).</li>\n<li>Avoiding the 2008 financial crisis would have required solving a difficult national coordination problem, rather than a global coordination problem. Still, this analogy seems fairly strong. As Jonah writes, \"While the 2008 financial crisis seems to have been largely US specific (while having broader ramifications), there's a sense in which preventing it would have required solving a difficult coordination problem. The causes of the crisis are diffuse, and responsibility falls on many distinct classes of actors.\"</li>\n<li>Jonah's analysis wasn't deep enough to discern whether the 2008 financial crisis is analogous to the future invention of AGI with regard to how much careful work would have been required in advance to avert the risk.</li>\n<li>In contrast with AI risk, the financial crisis wasn't a life or death matter for almost any of the actors involved. Many people in finance didn't have incentives to avert the financial crisis: indeed, some of the key figures involved were rewarded with large bonuses. But it's plausible that government decision makers had incentive to avert a financial crisis for reputational reasons, and many interest groups are adversely affected by financial crises.</li>\n<li>Once again, the scale of the financial crisis wasn't on a par with AI risk, but it was closer to that scale than the other risks Jonah looked at in this initial investigation.</li>\n</ol>\n<p>Jonah concluded that \"the conglomerate of poor decisions [leading up to] the 2008 financial crisis constitute a small but significant challenge to the view that [policy-makers] will successfully address AI risk.\" His reasons were:</p>\n<ol>\n<li>The magnitude of the financial crisis is nontrivial (even if small) compared with the magnitude of the AI risk problem (not counting future generations).</li>\n<li>The financial crisis adversely affected a very broad range of people, apparently including a large fraction of those people in positions of power (this seems truer here than in the case of climate change). A recession is bad for most businesses and for most workers. Yet these actors weren't able to recognize the problem, coordinate, and prevent it.</li>\n<li>The reasons that policy-makers weren't able to recognize the problem, coordinate, and prevent it seem related to reasons why people might not recognize AI risk as a problem, coordinate, and prevent it. First, several&nbsp;key actors involved seem to have exhibited conspicuous overconfidence and neglect of tail risk (e.g. Summers, etc. ignoring Brooksley Born's warnings about excessive leverage). If true, this shows that people in positions of power are notably susceptible to overconfidence and neglect of tail risk. Avoiding overconfidence and giving sufficient weight to tail risk may be crucial in mitigating AI risk.&nbsp;Second, one gets a sense that bystander effect and tragedy of the commons played a large role in the case of the financial crisis. There are risks that weren't adequately addressed because doing so didn't fall under the purview of any of the existing government agencies. This may have corresponded to a mentality of the type \"that's not my job &mdash; somebody else can take care of it.\" If people think that AI risk is large, then they might think \"if nobody's going to take care of it then I will, because otherwise I'm going to die.\" But if people think that AI risk is small, they might think \"This probably won't be really bad for me, and even though someone should take care of it, it's not going to be me.\"</li>\n</ol> &nbsp;\n<h3>Risks from geomagnetic storms</h3>\n<p>Large geomagnetic storms like the <a href=\"http://en.wikipedia.org/wiki/Solar_storm_of_1859\">1859 Carrington Event</a> are infrequent, but could cause serious damage to satellites and critical infrastructure. See <a href=\"http://www.oecd.org/gov/risk/46891645.pdf\">this OECD report</a> for an overview.</p>\n<p>Jonah's investigation revealed a wide range in expected losses from geomagnetic storms, from $30 million per year to $30 billion per year. But even this larger number amounts to $1.5 trillion in expected losses over the next 50 years. Compare this with the losses from the 2008 financial crisis (roughly a 1 in 50 years event), which are&nbsp;<a href=\"http://thinkprogress.org/economy/2012/09/13/846281/financial-crisis-lost-trillions/\">estimated</a> to be about $13 trillion for Americans alone.</p>\n<p>Though serious, the risks from geomagnetic storms appear to be small enough to be disanalogous to the future invention of AGI.  &nbsp;</p>\n<h3><br /></h3>\n<h3>The eradication of smallpox</h3>\n<p><a href=\"http://en.wikipedia.org/wiki/Smallpox\">Smallpox</a>, after killing more than 500 million people over the past several millennia, was eradicated in 1979 after a decades-long global eradication effort. Though a hallmark of successful global coordination, it doesn't seem especially relevant to whether policy-makers will handle the invention of AGI wisely.</p>\n<p>Here's how the eradication of smallpox does our doesn't fit our criteria for being analogous to the future invention of AGI:</p>\n<ol>\n<li>Smallpox didn't arrive at an unpredictable time; it arrived millennia before the eradication campaign.</li>\n<li>The world didn't have experience eradicating a disease before smallpox was eradicated, but a number of nations had eliminated smallpox.</li>\n<li>Smallpox eradication required solving a difficult global coordination problem, but in a way disanalogous to the invention of AGI safety (see the other points on this list).</li>\n<li>Preparing for smallpox eradication required effort in advance in some sense, but the effort had mostly already been exerted before the campaign was announced.</li>\n<li>Nations without smallpox had incentive to eradicate smallpox so that they didn't have to spend money to immunize citizens so that the virus would not be (re)-introduced to their countries. For example, in 1968, the United States spent about $100 million on routine smallpox vaccinations.</li>\n<li>Smallpox can be thought of as a global disaster: by 1966, about 2 million people died of smallpox each year.</li>\n</ol> &nbsp;\n<h3>Shallow investigations of&nbsp;risks from cyberwarfare, chlorofluorocarbons, and the Cuban missile crisis</h3>\n<p>Jonah's shallow investigation of risks from cyberwarfare revealed that experts disagree significantly about the nature and scope of these risks. It's likely that dozens of hours of research would be required to develop a well-informed model of these risks.</p>\n<p>To investigate how policy-makers handled the discovery that chlorofluorocarbons (CFCs) depleted the ozone layer, Jonah summarized the first 100 pages of&nbsp;<em><a href=\"http://www.amazon.com/Ozone-Crisis-Evolution-Emergency-Editions/dp/0471528234/\">Ozone Crisis: The 15-Year Evolution of a Sudden Global Emergency</a></em>&nbsp;(see our full email exchange for the summary).&nbsp;This historical case seems worth investigating further, and may be a case of policy-makers solving a global risk with surprising swiftness, though whether the response was appropriately prompt is debated.</p>\n<p>Jonah also did a shallow investigation of the <a href=\"http://en.wikipedia.org/wiki/Cuban_missile_crisis\">Cuban missile crisis</a>. It's difficult to assess how likely it was for the crisis to escalate into a global nuclear war, but it appears that policy-makers made many poor decisions leading up to and during the Cuban missile crisis (see our full email exchange for a list). Jonah concludes:</p>\n<blockquote>even if the probability of the Cuban missile crisis leading to an all out nuclear war was only 1% or so, the risk was still sufficiently great so that the way in which the actors handled the situation is evidence against elites handling the creation of AI well. (This contrasts with the situation with climate change, in that elites had strong personal incentives to avert an all-out nuclear war.)</blockquote>\n<p>However, this is only a guess based on a shallow investigation, and should not be taken too seriously before a more thorough investigation of the historical facts can be made.  &nbsp;</p>\n<h3><br /></h3>\n<h3>Additional historical cases that could be investigated</h3>\n<p>We also identified additional historical cases that could be investigated for potentially informative analogies to the future invention of AGI:</p>\n<ol>\n<li>The 2003 <a href=\"http://en.wikipedia.org/wiki/Iraq_War\">Iraq War</a></li>\n<li>The frequency with which dictators are deposed or assassinated due to \"unforced errors\" they made</li>\n<li><a href=\"http://en.wikipedia.org/wiki/Nuclear_proliferation\">Nuclear proliferation</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Recombinant_DNA\">Recombinant DNA</a></li>\n<li><a href=\"http://www.amazon.com/Radical-Abundance-Revolution-Nanotechnology-Civilization/dp/1610391136/\">Molecular nanotechnology</a></li>\n<li><a href=\"http://www.amazon.com/Near-Earth-Objects-Finding-Them-Before/dp/0691149291/\">Near Earth objects</a></li>\n<li>Pandemics and potential pandemics (e.g. <a href=\"http://en.wikipedia.org/wiki/HIV\">HIV</a>,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Severe_acute_respiratory_syndrome\">SARS</a>)</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4JGQtc4ZgmdsrtBhB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 25, "extendedScore": null, "score": 1.3378318123657663e-06, "legacy": true, "legacyId": "24125", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Cross-posted from <a href=\"http://intelligence.org/2013/09/12/how-well-will-policy-makers-handle-agi-initial-findings/\">MIRI's blog</a>.</small></p>\n<p>MIRI's <a href=\"http://intelligence.org/about/\">mission</a> is \"to ensure that the creation of smarter-than-human intelligence has a positive impact.\"  One policy-relevant question is: <strong>How well should we expect policy makers to handle the invention of AGI, and what does this imply about how much effort to put into AGI risk mitigation vs. other concerns?</strong></p>\n<p><strong></strong>To investigate these questions, we asked&nbsp;<a href=\"http://mathisbeauty.org/aboutme.html\">Jonah Sinick</a>&nbsp;to examine how well policy-makers handled past events analogous in some ways to the future invention of AGI, and summarize his findings. We pre-committed to publishing our entire email exchange on the topic (with minor editing), just as with our project on <a href=\"http://intelligence.org/2013/09/02/how-effectively-can-we-plan-for-future-decades\">how well we can plan for future decades</a>. The post below is a summary of findings from&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2013/09/Elites-and-AI.docx\">our full email exchange (.docx)</a>&nbsp;so far.</p>\n<p>As with our investigation of how well we can plan for future decades,<strong>&nbsp;we decided to publish our initial findings after investigating only a few historical cases</strong>. This allows us to gain feedback on the value of the project, as well as suggestions for improvement, before continuing. It also means that&nbsp;<strong>we aren't yet able to draw any confident conclusions about our core questions</strong>.</p>\n<p>The most significant results from this project so far are:</p>\n<ol>\n<li>We came up with a preliminary list of 6 seemingly-important ways in which a historical case could be analogous to the future invention of AGI, and evaluated several historical cases on these criteria.</li>\n<li>Climate change risk seems sufficiently disanalogous to AI risk that studying climate change mitigation efforts probably gives limited insight into how well policy-makers will deal with AGI risk: the expected damage of climate change appears to be very small relative to the the expected damage due to AI risk, especially when one looks at expected damage to policy makers.</li>\n<li>The 2008 financial crisis appears, after a shallow investigation, to be sufficiently analogous to AGI risk that it should give us some small reason to be concerned that policy-makers will not manage the invention of AGI wisely.</li>\n<li>The risks to critical infrastructure from geomagnetic storms are far too small to be in the same reference class with risks from AGI.</li>\n<li>The eradication of smallpox is only somewhat analogous to the invention of AGI.</li>\n<li>Jonah performed very shallow investigations of how policy-makers have handled risks from cyberwarfare, chlorofluorocarbons, and the Cuban missile crisis, but these cases need more study before even \"initial thoughts\" can be given.</li>\n<li>We identified additional historical cases that could be investigated in the future.</li>\n</ol>\n<p>Further details are given below. For sources and more, please see&nbsp;<a href=\"https://intelligence.org/wp-content/uploads/2013/09/Elites-and-AI.docx\">our full email exchange (.docx)</a>.</p>\n<!--more-->\n<h3><br></h3>\n<h3 id=\"6_ways_a_historical_case_can_be_analogous_to_the_invention_of_AGI\">6 ways a historical case can be analogous to the invention of AGI</h3>\n<p>In conversation, Jonah and I identified six features of the future invention of AGI that, if largely shared by a historical case, seem likely to allow the historical case to shed light on how well policy-makers will deal with the invention of AGI:</p>\n<ol>\n<li>AGI may become a major threat in a somewhat unpredictable time.</li>\n<li>AGI may become a threat when the world has very limited experience with it.</li>\n<li>A good outcome with AGI may require solving a difficult global coordination problem.</li>\n<li>Preparing for the AGI threat adequately may require lots of careful work in advance.</li>\n<li>Policy-makers have strong personal incentives to solve the AGI problem.</li>\n<li>A bad outcome with AGI would be a global disaster, and a good outcome with AGI would have global humanitarian benefit.</li>\n</ol>\n<p>More details on these criteria and their use are given in the second email of our full email exchange.  &nbsp;</p>\n<h3><br></h3>\n<h3 id=\"Risks_from_climate_change\">Risks from climate change</h3>\n<p>People began to see climate change as a potential problem in the early 1970s, but there was some ambiguity as to whether human activity was causing warming (because of carbon emissions) or cooling (because of smog particles). The first <a href=\"http://en.wikipedia.org/wiki/Intergovernmental_Panel_on_Climate_Change\">IPCC</a> report was issued in 1990, and stated that were was substantial anthropogenic global warming due to greenhouse gases. By 2001, there was a strong scientific consensus behind this claim.  While policy-makers' response to risks from climate change might seem likely to shed light on whether policy-makers will deal wisely with AGI, there are some important disanalogies:</p>\n<ul>\n<li>The harms of global warming are expected to fall disproportionately on disadvantaged people in poor countries, not on policy-makers. So policy-makers have much less personal incentive to solve the problem than is the case with AGI.</li>\n<li>In the median case, humanitarian losses from global warming <a href=\"/lw/hi1/potential_impacts_of_climate_change/\">seems to be</a> about 20% of GDP per year for the poorest people.&nbsp;In light of anticipated economic development and marginal diminishing utility, this is a <em>much</em> smaller negative humanitarian impact than AGI risk (even ignoring future generations). For example, economist Indur Goklany <a href=\"http://wattsupwiththat.com/2012/10/17/is-climate-change-the-number-one-threat-to-humanity/\">estimated</a> that \"through 2085, only 13% of [deaths] from hunger, malaria, and extreme weather events (including coastal flooding from sea level rise) should be from [global] warming.\"</li>\n<li>Thus, potential analogies to AGI risk come from climate change's <em>tail risk</em>. But there seem to be few credentialed scientists who have views compatible with a prediction that even a temperature increase in the 95th percentile of the probability distribution (by 2100) would do more than just begin to render some regions of Earth uninhabitable.</li>\n<li>According to the <a href=\"http://www.ipcc.ch/meetings/session31/inf3.pdf\">5th IPCC</a>, the risk of human extinction from climate change seems very low: \"Some thresholds that all would consider dangerous have no support in the literature as having a non-negligible chance of occurring. For instance, a 'runaway greenhouse effect'\u2014analogous to Venus\u2014appears to have virtually no chance of being induced by anthropogenic activities.\"</li>\n</ul>\n&nbsp;\n<h3 id=\"The_2008_financial_crisis\">The 2008 financial crisis</h3>\n<p>Jonah did a shallow investigation of the 2008 financial crisis, but the preliminary findings are interesting enough for us to describe them in some detail. Jonah's impressions about the relevance of the 2008 financial crisis to the AGI situation are based on a reading of&nbsp;<em><a href=\"http://www.amazon.com/After-Music-Stopped-Financial-Response/dp/1594205302/\">After the Music Stopped</a></em> by Alan Blinder, who was the vice chairman of the federal reserve for 1.5 years during the Clinton administration. Naturally, many additional sources should be consulted before drawing firm conclusions about the relevance of policy-makers' handling of the financial crisis to their likelihood of handling AGI wisely.</p>\n<p>Blinder's seven main factors leading to the recession are (p. 27):</p>\n<ol>\n<li>Inflated asset prices, especially of houses (the housing bubble) but also of certain securities (the bond bubble);</li>\n<li>Excessive leverage (heavy borrowing) throughout the financial system and the economy;</li>\n<li>Lax financial regulation, both in terms of what the law left unregulated and how poorly the various regulators performed their duties;</li>\n<li>Disgraceful banking practices in subprime and other mortgage lending;</li>\n<li>The crazy-quilt of unregulated securities and derivatives that were built on these bad mortgages;</li>\n<li>The abysmal performance of the statistical rating agencies, which helped the crazy-quilt get stitched together; and</li>\n<li>The perverse compensation systems in many financial institutions that created powerful incentives to go for broke.</li>\n</ol>\n<p>With these factors in mind, let's look at the strength of the analogy between the 2008 financial crisis and the future invention of AGI:</p>\n<ol>\n<li>Almost tautologically, a financial crisis is unexpected, though we do know that financial crises happen with some regularity.</li>\n<li>The 2008 financial crisis was not unprecedented in kind, only in degree (in some ways).</li>\n<li>Avoiding the 2008 financial crisis would have required solving a difficult national coordination problem, rather than a global coordination problem. Still, this analogy seems fairly strong. As Jonah writes, \"While the 2008 financial crisis seems to have been largely US specific (while having broader ramifications), there's a sense in which preventing it would have required solving a difficult coordination problem. The causes of the crisis are diffuse, and responsibility falls on many distinct classes of actors.\"</li>\n<li>Jonah's analysis wasn't deep enough to discern whether the 2008 financial crisis is analogous to the future invention of AGI with regard to how much careful work would have been required in advance to avert the risk.</li>\n<li>In contrast with AI risk, the financial crisis wasn't a life or death matter for almost any of the actors involved. Many people in finance didn't have incentives to avert the financial crisis: indeed, some of the key figures involved were rewarded with large bonuses. But it's plausible that government decision makers had incentive to avert a financial crisis for reputational reasons, and many interest groups are adversely affected by financial crises.</li>\n<li>Once again, the scale of the financial crisis wasn't on a par with AI risk, but it was closer to that scale than the other risks Jonah looked at in this initial investigation.</li>\n</ol>\n<p>Jonah concluded that \"the conglomerate of poor decisions [leading up to] the 2008 financial crisis constitute a small but significant challenge to the view that [policy-makers] will successfully address AI risk.\" His reasons were:</p>\n<ol>\n<li>The magnitude of the financial crisis is nontrivial (even if small) compared with the magnitude of the AI risk problem (not counting future generations).</li>\n<li>The financial crisis adversely affected a very broad range of people, apparently including a large fraction of those people in positions of power (this seems truer here than in the case of climate change). A recession is bad for most businesses and for most workers. Yet these actors weren't able to recognize the problem, coordinate, and prevent it.</li>\n<li>The reasons that policy-makers weren't able to recognize the problem, coordinate, and prevent it seem related to reasons why people might not recognize AI risk as a problem, coordinate, and prevent it. First, several&nbsp;key actors involved seem to have exhibited conspicuous overconfidence and neglect of tail risk (e.g. Summers, etc. ignoring Brooksley Born's warnings about excessive leverage). If true, this shows that people in positions of power are notably susceptible to overconfidence and neglect of tail risk. Avoiding overconfidence and giving sufficient weight to tail risk may be crucial in mitigating AI risk.&nbsp;Second, one gets a sense that bystander effect and tragedy of the commons played a large role in the case of the financial crisis. There are risks that weren't adequately addressed because doing so didn't fall under the purview of any of the existing government agencies. This may have corresponded to a mentality of the type \"that's not my job \u2014 somebody else can take care of it.\" If people think that AI risk is large, then they might think \"if nobody's going to take care of it then I will, because otherwise I'm going to die.\" But if people think that AI risk is small, they might think \"This probably won't be really bad for me, and even though someone should take care of it, it's not going to be me.\"</li>\n</ol> &nbsp;\n<h3 id=\"Risks_from_geomagnetic_storms\">Risks from geomagnetic storms</h3>\n<p>Large geomagnetic storms like the <a href=\"http://en.wikipedia.org/wiki/Solar_storm_of_1859\">1859 Carrington Event</a> are infrequent, but could cause serious damage to satellites and critical infrastructure. See <a href=\"http://www.oecd.org/gov/risk/46891645.pdf\">this OECD report</a> for an overview.</p>\n<p>Jonah's investigation revealed a wide range in expected losses from geomagnetic storms, from $30 million per year to $30 billion per year. But even this larger number amounts to $1.5 trillion in expected losses over the next 50 years. Compare this with the losses from the 2008 financial crisis (roughly a 1 in 50 years event), which are&nbsp;<a href=\"http://thinkprogress.org/economy/2012/09/13/846281/financial-crisis-lost-trillions/\">estimated</a> to be about $13 trillion for Americans alone.</p>\n<p>Though serious, the risks from geomagnetic storms appear to be small enough to be disanalogous to the future invention of AGI.  &nbsp;</p>\n<h3><br></h3>\n<h3 id=\"The_eradication_of_smallpox\">The eradication of smallpox</h3>\n<p><a href=\"http://en.wikipedia.org/wiki/Smallpox\">Smallpox</a>, after killing more than 500 million people over the past several millennia, was eradicated in 1979 after a decades-long global eradication effort. Though a hallmark of successful global coordination, it doesn't seem especially relevant to whether policy-makers will handle the invention of AGI wisely.</p>\n<p>Here's how the eradication of smallpox does our doesn't fit our criteria for being analogous to the future invention of AGI:</p>\n<ol>\n<li>Smallpox didn't arrive at an unpredictable time; it arrived millennia before the eradication campaign.</li>\n<li>The world didn't have experience eradicating a disease before smallpox was eradicated, but a number of nations had eliminated smallpox.</li>\n<li>Smallpox eradication required solving a difficult global coordination problem, but in a way disanalogous to the invention of AGI safety (see the other points on this list).</li>\n<li>Preparing for smallpox eradication required effort in advance in some sense, but the effort had mostly already been exerted before the campaign was announced.</li>\n<li>Nations without smallpox had incentive to eradicate smallpox so that they didn't have to spend money to immunize citizens so that the virus would not be (re)-introduced to their countries. For example, in 1968, the United States spent about $100 million on routine smallpox vaccinations.</li>\n<li>Smallpox can be thought of as a global disaster: by 1966, about 2 million people died of smallpox each year.</li>\n</ol> &nbsp;\n<h3 id=\"Shallow_investigations_of_risks_from_cyberwarfare__chlorofluorocarbons__and_the_Cuban_missile_crisis\">Shallow investigations of&nbsp;risks from cyberwarfare, chlorofluorocarbons, and the Cuban missile crisis</h3>\n<p>Jonah's shallow investigation of risks from cyberwarfare revealed that experts disagree significantly about the nature and scope of these risks. It's likely that dozens of hours of research would be required to develop a well-informed model of these risks.</p>\n<p>To investigate how policy-makers handled the discovery that chlorofluorocarbons (CFCs) depleted the ozone layer, Jonah summarized the first 100 pages of&nbsp;<em><a href=\"http://www.amazon.com/Ozone-Crisis-Evolution-Emergency-Editions/dp/0471528234/\">Ozone Crisis: The 15-Year Evolution of a Sudden Global Emergency</a></em>&nbsp;(see our full email exchange for the summary).&nbsp;This historical case seems worth investigating further, and may be a case of policy-makers solving a global risk with surprising swiftness, though whether the response was appropriately prompt is debated.</p>\n<p>Jonah also did a shallow investigation of the <a href=\"http://en.wikipedia.org/wiki/Cuban_missile_crisis\">Cuban missile crisis</a>. It's difficult to assess how likely it was for the crisis to escalate into a global nuclear war, but it appears that policy-makers made many poor decisions leading up to and during the Cuban missile crisis (see our full email exchange for a list). Jonah concludes:</p>\n<blockquote>even if the probability of the Cuban missile crisis leading to an all out nuclear war was only 1% or so, the risk was still sufficiently great so that the way in which the actors handled the situation is evidence against elites handling the creation of AI well. (This contrasts with the situation with climate change, in that elites had strong personal incentives to avert an all-out nuclear war.)</blockquote>\n<p>However, this is only a guess based on a shallow investigation, and should not be taken too seriously before a more thorough investigation of the historical facts can be made.  &nbsp;</p>\n<h3><br></h3>\n<h3 id=\"Additional_historical_cases_that_could_be_investigated\">Additional historical cases that could be investigated</h3>\n<p>We also identified additional historical cases that could be investigated for potentially informative analogies to the future invention of AGI:</p>\n<ol>\n<li>The 2003 <a href=\"http://en.wikipedia.org/wiki/Iraq_War\">Iraq War</a></li>\n<li>The frequency with which dictators are deposed or assassinated due to \"unforced errors\" they made</li>\n<li><a href=\"http://en.wikipedia.org/wiki/Nuclear_proliferation\">Nuclear proliferation</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Recombinant_DNA\">Recombinant DNA</a></li>\n<li><a href=\"http://www.amazon.com/Radical-Abundance-Revolution-Nanotechnology-Civilization/dp/1610391136/\">Molecular nanotechnology</a></li>\n<li><a href=\"http://www.amazon.com/Near-Earth-Objects-Finding-Them-Before/dp/0691149291/\">Near Earth objects</a></li>\n<li>Pandemics and potential pandemics (e.g. <a href=\"http://en.wikipedia.org/wiki/HIV\">HIV</a>,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Severe_acute_respiratory_syndrome\">SARS</a>)</li>\n</ol>", "sections": [{"title": "6 ways a historical case can be analogous to the invention of AGI", "anchor": "6_ways_a_historical_case_can_be_analogous_to_the_invention_of_AGI", "level": 1}, {"title": "Risks from climate change", "anchor": "Risks_from_climate_change", "level": 1}, {"title": "The 2008 financial crisis", "anchor": "The_2008_financial_crisis", "level": 1}, {"title": "Risks from geomagnetic storms", "anchor": "Risks_from_geomagnetic_storms", "level": 1}, {"title": "The eradication of smallpox", "anchor": "The_eradication_of_smallpox", "level": 1}, {"title": "Shallow investigations of\u00a0risks from cyberwarfare, chlorofluorocarbons, and the Cuban missile crisis", "anchor": "Shallow_investigations_of_risks_from_cyberwarfare__chlorofluorocarbons__and_the_Cuban_missile_crisis", "level": 1}, {"title": "Additional historical cases that could be investigated", "anchor": "Additional_historical_cases_that_could_be_investigated", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9qgQWEauLGuPwuYPh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-12T08:27:33.984Z", "modifiedAt": null, "url": null, "title": "Meetup : Berlin: Fermi paradox discussion", "slug": "meetup-berlin-fermi-paradox-discussion", "viewCount": null, "lastCommentedAt": "2013-09-12T23:10:35.774Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dimhpouwQGkaHCi2A/meetup-berlin-fermi-paradox-discussion", "pageUrlRelative": "/posts/dimhpouwQGkaHCi2A/meetup-berlin-fermi-paradox-discussion", "linkUrl": "https://www.lesswrong.com/posts/dimhpouwQGkaHCi2A/meetup-berlin-fermi-paradox-discussion", "postedAtFormatted": "Thursday, September 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berlin%3A%20Fermi%20paradox%20discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berlin%3A%20Fermi%20paradox%20discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdimhpouwQGkaHCi2A%2Fmeetup-berlin-fermi-paradox-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berlin%3A%20Fermi%20paradox%20discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdimhpouwQGkaHCi2A%2Fmeetup-berlin-fermi-paradox-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdimhpouwQGkaHCi2A%2Fmeetup-berlin-fermi-paradox-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qs'>Berlin: Fermi paradox discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 October 2013 07:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">S Wuhletal, 12621 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're meeting to discuss the Fermi paradox and the <a href=\"http://hanson.gmu.edu/greatfilter.html\" rel=\"nofollow\">great filter</a>. Everyone is welcome! There'll probably be 6-12 people.</p>\n\n<p>Please look at the <a href=\"http://groups.google.com/group/lw-berlin\" rel=\"nofollow\">mailing list</a> for details.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qs'>Berlin: Fermi paradox discussion</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dimhpouwQGkaHCi2A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.3378898900563128e-06, "legacy": true, "legacyId": "24130", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berlin__Fermi_paradox_discussion\">Discussion article for the meetup : <a href=\"/meetups/qs\">Berlin: Fermi paradox discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 October 2013 07:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">S Wuhletal, 12621 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're meeting to discuss the Fermi paradox and the <a href=\"http://hanson.gmu.edu/greatfilter.html\" rel=\"nofollow\">great filter</a>. Everyone is welcome! There'll probably be 6-12 people.</p>\n\n<p>Please look at the <a href=\"http://groups.google.com/group/lw-berlin\" rel=\"nofollow\">mailing list</a> for details.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berlin__Fermi_paradox_discussion1\">Discussion article for the meetup : <a href=\"/meetups/qs\">Berlin: Fermi paradox discussion</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berlin: Fermi paradox discussion", "anchor": "Discussion_article_for_the_meetup___Berlin__Fermi_paradox_discussion", "level": 1}, {"title": "Discussion article for the meetup : Berlin: Fermi paradox discussion", "anchor": "Discussion_article_for_the_meetup___Berlin__Fermi_paradox_discussion1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-09-12T08:27:33.984Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-12T08:53:22.133Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels monthly meetup", "slug": "meetup-brussels-monthly-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roxolan", "createdAt": "2011-10-23T19:06:17.298Z", "isAdmin": false, "displayName": "Roxolan"}, "userId": "jXG7tMhkQMNpCCXPN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FDiM4opgEjmJfYQs3/meetup-brussels-monthly-meetup", "pageUrlRelative": "/posts/FDiM4opgEjmJfYQs3/meetup-brussels-monthly-meetup", "linkUrl": "https://www.lesswrong.com/posts/FDiM4opgEjmJfYQs3/meetup-brussels-monthly-meetup", "postedAtFormatted": "Thursday, September 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20monthly%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20monthly%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFDiM4opgEjmJfYQs3%2Fmeetup-brussels-monthly-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20monthly%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFDiM4opgEjmJfYQs3%2Fmeetup-brussels-monthly-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFDiM4opgEjmJfYQs3%2Fmeetup-brussels-monthly-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 114, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/qt\">Brussels monthly meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">14 September 2013 01:00:00PM (+0200)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Brussels meetups now happen every month on the second Saturday of the month (even when we forget to announce them in time...).</p>\n<p>We are once again meeting at 'La fleur en papier dor&eacute;' close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n<p>If you are coming for the first time, please consider filling out <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform?pli=1\">this</a> one minute form, to share your contact information.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/qt\">Brussels monthly meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FDiM4opgEjmJfYQs3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3379125752116641e-06, "legacy": true, "legacyId": "24131", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_monthly_meetup\">Discussion article for the meetup : <a href=\"/meetups/qt\">Brussels monthly meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">14 September 2013 01:00:00PM (+0200)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Brussels meetups now happen every month on the second Saturday of the month (even when we forget to announce them in time...).</p>\n<p>We are once again meeting at 'La fleur en papier dor\u00e9' close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n<p>If you are coming for the first time, please consider filling out <a href=\"https://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform?pli=1\">this</a> one minute form, to share your contact information.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Brussels_monthly_meetup1\">Discussion article for the meetup : <a href=\"/meetups/qt\">Brussels monthly meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels monthly meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_monthly_meetup", "level": 1}, {"title": "Discussion article for the meetup : Brussels monthly meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_monthly_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-12T13:00:11.432Z", "modifiedAt": null, "url": null, "title": "Patternist friendly AI risk", "slug": "patternist-friendly-ai-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:01.777Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bokov", "createdAt": "2010-01-11T01:11:23.480Z", "isAdmin": false, "displayName": "bokov"}, "userId": "4sgsBYAsjDHNvB7Q6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cG4RiW5TSqWM5NdwN/patternist-friendly-ai-risk", "pageUrlRelative": "/posts/cG4RiW5TSqWM5NdwN/patternist-friendly-ai-risk", "linkUrl": "https://www.lesswrong.com/posts/cG4RiW5TSqWM5NdwN/patternist-friendly-ai-risk", "postedAtFormatted": "Thursday, September 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Patternist%20friendly%20AI%20risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APatternist%20friendly%20AI%20risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcG4RiW5TSqWM5NdwN%2Fpatternist-friendly-ai-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Patternist%20friendly%20AI%20risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcG4RiW5TSqWM5NdwN%2Fpatternist-friendly-ai-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcG4RiW5TSqWM5NdwN%2Fpatternist-friendly-ai-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 408, "htmlBody": "<p>It seems to me that most AI researchers on this site are patternists in the sense of believing that the anti-zombie principle necessarily implies:</p>\n<p>1. That it will ever become possible *in practice* to create uploads or sims that are close enough to our physical instantiations that their utility to us would be interchangeable with that of our physical instantiations.</p>\n<p>2. That we know (or will know) enough about the brain to know when this threshold is reached.</p>\n<p>&nbsp;</p>\n<p>But, like any rationalists extrapolating from unknown unknowns... or heck, extrapolating from anything... we must admit that one or both of the above statements could be wrong without also making friendly AI impossible. What would be the consequences of such error?</p>\n<p>I submit that one such consequence could be an FAI that is also wrong on these issues but not only do we fail to check for such a failure mode, it actually looks to us like what we would expect the right answer to look because we are making the same error.</p>\n<p>If simulation/uploading really does preserve what we value about our lives then the safest course of action is to encourage as many people to upload as possible. It would also imply that efforts to solve the problem of mortality by physical means will at best be given an even lower priority than they are now, or at worst cease altogether because they would seem to be a waste of resources.</p>\n<p>&nbsp;</p>\n<p>Result: people continue to die and nobody including the AI notices, except now they have no hope of reprieve because they think the problem is already solved.</p>\n<p>Pessimistic Result: uploads are so widespread that humanity quietly goes extinct, cheering themselves onward the whole time</p>\n<p>Really Pessimistic Result: what replaces humanity are zombies, not in the qualia sense but in the real sense that there is some relevant chemical/physical process that is not being simulated because we didn't realize it was relevant or hadn't noticed it in the first place.</p>\n<p>&nbsp;</p>\n<p>Possible Safeguards:</p>\n<p>&nbsp;</p>\n<p>* Insist on quantum level accuracy (yeah right)</p>\n<p>&nbsp;</p>\n<p>* Take seriously the general scenario of your FAI going wrong because you are wrong in the same way and fail to notice the problem.</p>\n<p>&nbsp;</p>\n<p>* Be as cautious about destructive uploads as you would be about, say, molecular nanotech.</p>\n<p>&nbsp;</p>\n<p>* Make sure you knowledge of neuroscience is at least as good as you knowledge of computer science and decision theory before you advocate digital immortality as anything more than an intriguing idea that might not turn out to be impossible.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cG4RiW5TSqWM5NdwN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 1, "extendedScore": null, "score": 1.3381296117560848e-06, "legacy": true, "legacyId": "24132", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-12T17:41:25.880Z", "modifiedAt": null, "url": null, "title": "Games for Rationalists", "slug": "games-for-rationalists", "viewCount": null, "lastCommentedAt": "2014-04-14T22:59:41.941Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gunnar_Zarncke", "createdAt": "2013-07-20T15:40:42.323Z", "isAdmin": false, "displayName": "Gunnar_Zarncke"}, "userId": "qmJFRN7jitjPsuF3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/toFESsZBQEZ4wq9od/games-for-rationalists", "pageUrlRelative": "/posts/toFESsZBQEZ4wq9od/games-for-rationalists", "linkUrl": "https://www.lesswrong.com/posts/toFESsZBQEZ4wq9od/games-for-rationalists", "postedAtFormatted": "Thursday, September 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Games%20for%20Rationalists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGames%20for%20Rationalists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtoFESsZBQEZ4wq9od%2Fgames-for-rationalists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Games%20for%20Rationalists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtoFESsZBQEZ4wq9od%2Fgames-for-rationalists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtoFESsZBQEZ4wq9od%2Fgames-for-rationalists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3403, "htmlBody": "<p style=\"margin-bottom: 0cm;\" lang=\"en-US\"><strong>Follow-up to:&nbsp;<a href=\"/lw/4fp/fun_and_games_with_cognitive_biases/\">Fun and Games with Cognitive Biases</a>&nbsp;</strong></p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\"><strong>Related to:&nbsp;<a href=\"/r/discussion/lw/dhn/rationality_games_apps_brainstorming/\">Rationality Games &amp; Apps Brainstorming</a>,&nbsp;</strong><strong><a href=\"/r/discussion/lw/7od/rationality_and_video_games/\">Rationality and Video Games</a></strong></p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\"><span lang=\"en-US\"><strong>Answering: <a href=\"/r/discussion/lw/4wp/designing_serious_games_a_request_for_help/\">Designing serious games - a request for help</a></strong></span></p>\n<h2><strong>Games on Lesswrong </strong></h2>\n<p style=\"margin-bottom: 0cm;\"><span lang=\"en-US\">Rational games or game ideas have been discussed or mentioned on lesswrong </span><span lang=\"en-US\">a few times: </span></p>\n<p style=\"margin-bottom: 0cm;\"><span lang=\"en-US\"><a href=\"/r/discussion/lw/4wp/designing_serious_games_a_request_for_help/\">Designing serious games - a request for help</a>&nbsp;</span>proposes to apply games for the rational cause.&nbsp;<a href=\"/lw/4fp/fun_and_games_with_cognitive_biases/\">Fun and Games with Cognitive Biases</a>&nbsp;plays (on a meta level) with concepts.&nbsp;<a href=\"/r/discussion/lw/dhn/rationality_games_apps_brainstorming/\">Rationality Games &amp; Apps Brainstorming</a>&nbsp;urges for such games. <a href=\"/r/discussion/lw/7od/rationality_and_video_games/\">Rationality and Video Games</a> provides an example.&nbsp;<span lang=\"en-US\">Taboo is explicitly mentioned in </span><a href=\"/lw/nu/taboo_your_words/\">http://lesswrong.com/lw/nu/taboo_your_words/</a>. Doing this with scientific concepts could make for a nice casual game at a LW meetup.&nbsp;<a href=\"/lw/3h/why_our_kind_cant_cooperate/\">http://lesswrong.com/lw/3h/why_our_kind_cant_cooperate/</a> generallly urges for community building mechanism &ndash; and (social) games address this. A game that has been <a href=\"/r/discussion/tag/diplomacy\">discussed extensively</a> on lesswrong is <a href=\"http://en.wikipedia.org/wiki/Diplomacy_(game)\">Diplomacy</a>&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<h2>Introduction</h2>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">I like to play games that are challenging and intellectually demanding. But I don't like to learn games that will gain nothing except the fun of learning and playing the game.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">Games for rationalists should not only be fun. They should also train or teach something beyond the game. Games that don't are parasitic memes.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">This post describe areas of rational thought addressed by games. By rationality I mean the <a href=\"/lw/31/what_do_we_mean_by_rationality/\">LW view</a>.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">I list games suitable for getting acquainted with rational thinking. I list areas underrepresented by games and I give an elaborate example of a game suitable to such an area. I propose trying to invent games for these areas and sketch possible strategies.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">To repeat I don't mean games that are just played frequently by rationalists or usually considered entertaining by rationalists (though they may). I mean games to train rationalists. <a href=\"http://en.wikipedia.org/wiki/Serious_game\">Serious Games</a>. To make it easy and fun to become a rationalist (or at least support such a cause).</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">Games supporting the acquisition of literacy and numeracy are out of scope here even though these are preconditions for more demanding concepts (and games). Also mostly out of scope are social and activity games (though these are be relevant to efficient group building) and games with a focus on motor ability (though such may make spatial or dynamical concepts more clear).</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">&nbsp;</p>\n<h3><strong>Outline</strong></h3>\n<ol>\n<li>I motivate the use of specifically designed games for (rational) education.</li>\n<li>I illustrate educational game theory with concrete examples.</li>\n<li>I argue for the applicability of games to some areas of rationality.</li>\n<li>I present a real game I invented and tested to address the overconfidence bias in particular.</li>\n</ol>\n<h2><strong>Using and Abusing Curiosity</strong></h2>\n<p style=\"margin-bottom: 0cm\"><span lang=\"en-US\">To use natural curiosity and to direct it to 'worthy' objectives has got the newfangled name <a href=\"http://en.wikipedia.org/wiki/Educational_entertainment#Games\">edutainment</a></span><span lang=\"en-US\">. Well known examples from TV are Sesame Street and <a href=\"http://en.wikipedia.org/wiki/Mammutland\">Mammutland</a></span>&nbsp;(which is based on <a href=\"http://en.wikipedia.org/wiki/The_Way_Things_Work\">The Way Things Work</a>. Examples of educational games are given below.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\"><span lang=\"en-US\">The opposite, where natural curiosity is diverted into cognitive dead ends and where attention is collected in trademarked <a href=\"http://de.wikipedia.org/wiki/Walled_Garden\">walled gardens</a> </span><span lang=\"en-US\">is called Marketing. An example is Pokemon where the cute animals engage the curiosity of children which learn lots of names and attributes &ndash; but none of these have any lasting cognitive use and all the accumulated knowledge can primarily be used for status and thus directing attention to the trademark owner in the end.</span></p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">Rational parents have to counter marketing when educating their children. Competing with media is hard because it is omnipresent and not in the best interest of education. Possibly we should lobby against maleducation but that will not help now. Instead we have to turn to games&nbsp;competetive with Pokemon.</p>\n<h3><span lang=\"en-US\"><strong>Educational Game Theory&nbsp;</strong></span></h3>\n<p style=\"margin-bottom: 0cm\"><span lang=\"en-US\"><a href=\"http://en.wikipedia.org/wiki/Educational_game#Theory\">Educational Game Theory</a></span><span lang=\"en-US\">&nbsp;</span>names three approaches to educational games:</p>\n<ul>\n<li>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">creation from scratch by educators</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">integrating off-the-shelf games</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">creating from scratch by the players (e.g. children)</p>\n</li>\n</ul>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">An example of the first is <a href=\"http://de.wikipedia.org/wiki/%C3%96kolopoly\">&Ouml;kolopoly</a>&nbsp;(English <a href=\"http://www.frederic-vester.de/eng/ecopolicy/\">Ecopolicy</a>) which has a strong focus on grand picture of complex systems. I'd really recommend it for children grade 6 and up (but use the card board version as the simple 'game mechanics' (connected lookup-tables) are inspectable there).</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">I will provide an example which I invented and tested myself later.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">Examples of the second kind are e.g. playing Taboo with math words or playing Liar's Dice and calculating expected values. Much more will be listed below with a focus on specific concepts.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">That children invent games all the time is no surprise. Because one natural aspect of games is that they support or stimulate learning most games invented by children are educational by nature.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">To illustrate this I'd like to describe a game my oldest son (9 years) invented:</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">He calls it 3D-computer and it consists of a 'user interface' made of paper and plastics, a number of 'input devices' and a large playing area made of taped together sheets of paper with a plan of a city. The player (usually one of his younger brothers) sits at the 'controls' and e.g. steers a car on a racing track thru the city. He has to press left and right (and say so loud) and my oldest son will act out the operation of the 3D-computer by moving the cars accordingly.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">Obviously this is modeled after real computer games but it requires significantly more abstraction and cooperation of the players. To build the 'game' he had to plan it and build corresponding pieces. He created a 'user interface' consisting of menues and sub menues (drawn on paper) for the possible games and options that can be played with the 3D-computer. It makes the logic of the game &ndash; its concepts &ndash; clear both in the planning and realization and in the acting too.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">&nbsp;</p>\n<h2><strong>Concepts in Games</strong></h2>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\"><strong>Concepts in Games</strong></p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Any normal game requires some rational thinking, but there are some areas of rational thought that are less covered by games than others.</p>\n<h4><span lang=\"en-US\">Game Theory </span></h4>\n<p><span lang=\"en-US\">This lends itself naturally to games with clear concepts. The simplified <a href=\"http://en.wikipedia.org/wiki/List_of_games_in_game_theory\">games analysed in game theory</a></span><span lang=\"en-US\">&nbsp;can be easily readapted to playable games or parts thereof</span>. Some of these are directly playable for school children. I played lots of these in a math course.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Game theoy obviously applies to most games, but the basic min-max principle is very clearly present in games that have a measurable of advantage; I identify the following kinds</p>\n<p>&nbsp;</p>\n<ol>\n<li>Distance of pieces from a finish on a game board (e.g. in the simple childrens games above</li>\n<li>Some in-game currency (examples: Monopoly)</li>\n<li>A winning criteria that includes a tally (example: Siedler von Catan)</li>\n</ol>\n<p>In these cases there may be strategic effects that outweigh the measurable but in the long run 'more is better' and a higher value predicts a win well.</p>\n<p>&nbsp;</p>\n<h4>Probability Theory</h4>\n<p>Classical games with dice or shuffled cards surely build some intuition for probability theory which is present in most games in so far as some most games need some controlled random variables (aka dice or shuffled cards) to support the game semantics.</p>\n<p>It is also present when simulating (or testing) games; the game state changes due to the game rules are implied stochastic processes.<br />Probability theory is relevant as games with numerous regular random events satisfy criteria of theorems of large number.</p>\n<p>Examples for dice games with simple rules and clear concepts are&nbsp;<a href=\"http://en.wikipedia.org/wiki/Cross_and_circle_game\">Cross And Circle</a>,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Snakes_and_Ladders\">Snake and Ladders</a>. These illustrate basic concepts and reflection about strategies using solid terminology can teach children a lot about probability.</p>\n<p>Intuition about different distributions resulting from superposition and their likelihood is fairly clearly present in&nbsp;<a href=\"http://en.wikipedia.org/wiki/Yatzee\">Yahtzee</a>.&nbsp;<br />Intuition about the law of large numbers (and updating due to new information) can be found in an entertaining way in&nbsp;<a href=\"http://en.wikipedia.org/wiki/Liar%27s_dice\">Liar's dice</a>.</p>\n<ul>\n</ul>\n<div>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">Anecdote: As a child I was a sore loser. Seems that at least one board flew through the room. Not that I'd lose that often, quite the contrary, but if...</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">I couldn't deal with chance playing tricks on me. Later I always tried to limit the effect of the dice. Both by hedging or by just changing the game rules beforehand. Once I printed an 'improved' list dice throws that were overly regular.</p>\n</div>\n<h4><span lang=\"en-US\">Decision theory </span></h4>\n<p style=\"margin-bottom: 0cm;\">Games that are not based on skill necessarily involve decisions by the players. These fall into three categories</p>\n<ul>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Decisions under certainty &ndash; games are seldom certain, but in some cases decisions can be modeled as if the game environment were fixed and then maximizing over multidimensional ratings of the game state (some measurables like win points)</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Decisions under risk &ndash; when some decisions involve probabilistic losses or gains</p>\n</li>\n<li>Decisions under uncertainty &ndash; if some decisions have uncertain consequences either due to unknown probabilities of game events or due to the other players.</li>\n</ul>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Clear decision strategies or concepts are not so easy come by. Most games imply lots of decisions but effective strategies are seldom obvious.</p>\n<ul>\n</ul>\n<p>Games where subjective valuations are rampant are trading cards (e.g. Pokemon or currently Star Wars) these are no simple trades but often involve a risk (gaining certain cards vs. losing some other cards due to a random or skill based process). What is unclear is whether there are any lessons learned from doing these games as the decisions are mostly individual and the conditions of the transactions vary from trade to trade so general insights are unlikely.</p>\n<h4><span lang=\"en-US\">Cognitive Science</span></h4>\n<p><a href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\">Cognitive biases</a> can be harvested for game aspects worth to address. I will demonstrate this with the Overconfidence Effect and with miscalibration of probabilities&nbsp;particular which is the main focus of the 'estimation game' presented below. I will cover other biases in more detail in a separate post.</p>\n<p>Humans are very good at <a href=\"http://www.linguistics.pomona.edu/thornton/lgcs11read/Fodor00.pdf\">detecting cheaters</a>&nbsp;this is well known since the work of <a href=\"http://www.cep.ucsb.edu/papers/instinctblindess.pdf\">Cosmides and Tooby</a>.This applies to real life as well as games. It would be interesting to develop games that try to move smoothly from obvious rules to abstract rules and try to train to perceive this as cheating.</p>\n<h4>Math and Logic</h4>\n<p>Math in general plays a role in most games. Correct logical inferences (even if done by intuition) train propositional logic (though a game which involves using modus ponens on unusual conditions would surely be a good idea).</p>\n<p>A very nice and pure example using Set Theory even in its name is&nbsp;<a href=\"http://en.wikipedia.org/wiki/Set_(game).\">Set</a>.&nbsp;</p>\n<p><span lang=\"en-US\"><a href=\"http://www.looneylabs.com/our-games\">Looneylabs</a> has a lot of games (or game material) suitable for logic. A very nice game building on this is <a href=\"http://www.koryheath.com/zendo/\">Zendo</a></span>.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">&nbsp;</p>\n<h2><strong>Estimation Game 'Who guesses best'</strong></h2>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">Now I'd like to present an example of a game which was created to explicitly addresses a cognitive bias &ndash; the overconfidence bias &ndash; which is hard to overcome even for scientifically schooled persons explicitly briefed in this bias.</p>\n<p style=\"margin-bottom: 0cm\"><span lang=\"en-US\">When I read chapter 21 by M. Alpertand H. Raiffa in Judgement under uncertainty - heuristics and biases 1982 by Kahneman et al (pages 294 &ndash; 305) see <a href=\"http://books.google.de/books?hl=de&amp;lr=&amp;id=_0H8gwj4a1MC&amp;oi=fnd&amp;pg=PR8&amp;dq=Judgement+under+uncertainty+kahneman+1982&amp;ots=YChhbTQ-_I&amp;sig=eKydDFt1r1puEWYijj8y1D3kLtI#v=onepage&amp;q=Judgement%20under%20uncertainty%20kahneman%201982&amp;f=false\">this Google Books link</a>&nbsp;and this <a href=\"/lw/721/judgment_under_uncertainty_summaries_part_1/\">less wrong review</a>.</span></p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">I couldn't believe that it could be that hard to calibrate well and I decided to test this on myself and on others in a comparable setting (kind of privately reproducing the study). This developed into a game that was since played about 5 times with a total of about 20 person and 30 questions (thus much less than in the cited study). The results were as expected &ndash; basically. A significant trend was visible in the test games and also in the main game rounds during a large birthday party. I had improved the 'game chart' and scoring rules and this made the effect clearer and the calibration of most players improved quickly. The game is simple enough to be played by a smart eight year old.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">The game chart can be downloaded <a href=\"http://www.frank-lounge.de/estimation_game.pdf\">here</a>&nbsp;(on page 2 in english).</p>\n<h3><strong>Estimation Game Rules</strong></h3>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">This is basically an estimation game &ndash; but with some extensions.</p>\n<h3><strong>The Questions</strong></h3>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">First a number of questions about quantities to guess are needed.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Each quantity should have an exact value that has a believable source but is somewhat unusual such that most players will not know the value.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">In a scholarly context the question should name a study or method and date of determination of the quantity. E.g. for the question &ldquo;How many residents live in Hamburg, Germany?&rdquo; the context could be: &ldquo;from wikipedia: official census of 2012 determined via population register&rdquo;. And for the trivial question &ldquo;How many lentils are in this jar?&rdquo; the context could be: &ldquo;This morning I filled the jar with green lentils, weight the content, counted and weighed a sample and calculated the total number.&rdquo;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Example questions:</p>\n<ul>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Number of steps to the next underground station.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Number of lentils in a jar.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Amount of rain per square meter per year in Jerusalem in 2001.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Number of files on my PC as reported by ls -R this morning.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">How long (days) was the first voyage of James cook into the south seas?</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Total egg production in th U.S. In 1965. (this is from the Alpert Study)</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Toll collections of the Panama Canal in 1967. (dito)</p>\n</li>\n</ul>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">The questions can be provided by the host (which has the disadvantage that host cannot take part in the game for most questions). The better idea is to have each player provide one to three questions.</p>\n<h3><strong>Guesstimation</strong></h3>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Each question is now dealt with as follows:</p>\n<ul>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">The question is read out loud. If needed, detail questions about the context of the question are answered.</p>\n</li>\n</ul>\n<ul>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">All players individually guesstimate the true value of the quantity in question. The number is written into the field in the middle below the green area. If the number is from one of the players he/she writes the correct number as his 'guess'.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Each player now considers what the other players might estimate. Values deemed to be definitely out of the range become the min/max values and written below the red/yellow border. Values deemed to be typical form the majority range and are written below the yellow/green border.</p>\n</li>\n</ul>\n<ul>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Points are awarded as follows:</p>\n<ul>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">For the best guess you get N points, for the next-best N/2, then N/4 (rounding down).... If multiple players are equally near, points are added and split. Numbers supplied by a players are excluded from scoring.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">For the green area you get N/2 points if it contains exactly N/2 numbers (including your own) for each more or less you get one point less (rounding up).</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">For the yellow area the player with the smallest range <strong>which still contains all guesses</strong> gets N+1 points, the next-best N/2+1 (and so on, rounding down).</p>\n</li>\n</ul>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Numbers falling on a boundary are scored to the players advantage.</p>\n</li>\n</ul>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Example:</p>\n<ul>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">For the number of lentils in the jar you estimate 1300. And you guess that half of the guesses fall within 500 and 2000. You also assume that nobody will believe less then 100 or more than 10000.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">It turns out that the other 6 players guessed 750, 1000, 1500, 2000, 2820, 4000.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">You write 750, 1000, 1500 and 2000 into your green area. You write 2800 and 4000 into the right yellow area. Congratulations: No numbers fell into a red area.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">The correct answer is revealed: 2800. You can circle it in the green area.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">The player guessing 2000 (distance 820) gets 7 points, the player guessing 4000 (distance 1180) gets 3 points floor(7/2), the player guessing 1500 (distance 1320) gets 1 point floor(7/4). For your 1300 you and the remaining players get nothing (7/8&lt;1).</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">You have got 4 numbers in your green area (you decide to include 2000). 4=ceil(7/2) is the optimum number of entries so you get 4 points. Had you excluded 2000 you'd got 3 points. If only your own number or if all 7 numbers were included you'd got 1 booby point.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">All the numbers are in your min-max-range, so you get at least 1 point. Lets assume that your range of 9900 is the second smallest of those containing all numbers. This nets you an additional floor(7/2)=3 points.</p>\n</li>\n</ul>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">May sound complicated. A summary of these rules is part of the game chart. The chart is mostly self-explanatory and even school children can fill-out the chart after one round of explanation.</p>\n<h3><strong>Gaming the Rules</strong></h3>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">It is possible to game the rules by e.g. using absurd guesses to bomb the max-range of the other players. The scoring is chosen to make this a losing strategy. Good guesses and good majority bounds score higher than the points reaped from being the only one with a valid max-range. And players can hedge against bombing nonetheless (in a way these are black swan events and thus interesting in their own right).</p>\n<h3><strong>Not quite Overconfidence</strong></h3>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">This game doesn't <em>directly </em>address the Overconfidence Bias. To do so the players would have to guess the range of <em>their own</em> guesses. Getting the range of ones own guesses via this kind of game takes much longer (times the number of players). The game works so well because it forces you to take the outside view. You have to consider what the <em>other</em> players might not know and then getting immediate feedback about ones own performance via</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">a) Distance from the truth of your own guess.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">b) In extreme cases exclamation from the other players whose ranges were shredded.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">It is critical to make the leap that your own guesses are as (over)confident as those of the others.</p>\n<h3><strong>Variants</strong></h3>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">It is quite possible to play a stricter version where you guess your own ranges with the same game chart. The following differences are necessary:</p>\n<ol>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Instead of the majority range you have to guess the range into which the true value will fall mostly. Mostly meaning on half of all questions this range should <em>actually</em> contain the value.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Instead of the maximum range you have to guess the range into which the true value will always fall.</p>\n</li>\n</ol><ol>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Don't write the numbers of the other players into the colored area.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Use a singe color strip for all of your own guesses. Just make a mark in the area where the actual value lay in the end.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Points are awarded only at the end of the game.</p>\n</li>\n</ol>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">This has the disadvantages of taking much longer and with the above simple rules it is too easy to game the rules (e.g. by using absurdly large or small ranges at the end of the game to ensure the required counts).</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">But this can be used after a few games to test whether you can make the leap to take the outside view on your own guesses. It can be done alone. Just think of a few arbitrary quantities and then research them later (on danger of choosing only quantities you can provide sensible estimates for).</p>\n<h3><strong>Anchoring</strong></h3>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Obviously your ranges are anchored to your guess. It is possible to add a small tweak to change the anchoring (disclaimer: I didn't try this): Allow the person knowing the correct answer to supply any example number and say it out load beforehand. This will provide another anchor (obviously anchored to the correct number somehow).</p>\n<h3><strong>Motivation</strong></h3>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">The test-games were fun. OK. The game may not be fun for everybody. My acquaintances are mostly smart and well-educated. But even my 8 year old son liked it (and scored in the middle range of 10 players). It is fun especially if people come with their own individual questions. The resulting discussions about individual (mis)reasoning is also often insightful and a nice ice-breaker.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">The performance of the players definitely improved.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Friends who are teachers asked me for the material and used it in highschool.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">I hope this is is step into the right direction.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">I'd like to close with an anecdote: I still try to win any game that I play and my friends know it. Sometimes, especially after I play unusual or when explaining tricks and then losing anyway I get remarks about it obviously not helping. To that my reply mostly is: I maximize my chances on infinitely many runs. So in the end I learned to lose.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "toFESsZBQEZ4wq9od", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 9, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "24116", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"margin-bottom: 0cm;\" lang=\"en-US\"><strong id=\"Follow_up_to__Fun_and_Games_with_Cognitive_Biases_\">Follow-up to:&nbsp;<a href=\"/lw/4fp/fun_and_games_with_cognitive_biases/\">Fun and Games with Cognitive Biases</a>&nbsp;</strong></p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\"><strong>Related to:&nbsp;<a href=\"/r/discussion/lw/dhn/rationality_games_apps_brainstorming/\">Rationality Games &amp; Apps Brainstorming</a>,&nbsp;</strong><strong><a href=\"/r/discussion/lw/7od/rationality_and_video_games/\">Rationality and Video Games</a></strong></p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\"><span lang=\"en-US\"><strong>Answering: <a href=\"/r/discussion/lw/4wp/designing_serious_games_a_request_for_help/\">Designing serious games - a request for help</a></strong></span></p>\n<h2 id=\"Games_on_Lesswrong_\"><strong>Games on Lesswrong </strong></h2>\n<p style=\"margin-bottom: 0cm;\"><span lang=\"en-US\">Rational games or game ideas have been discussed or mentioned on lesswrong </span><span lang=\"en-US\">a few times: </span></p>\n<p style=\"margin-bottom: 0cm;\"><span lang=\"en-US\"><a href=\"/r/discussion/lw/4wp/designing_serious_games_a_request_for_help/\">Designing serious games - a request for help</a>&nbsp;</span>proposes to apply games for the rational cause.&nbsp;<a href=\"/lw/4fp/fun_and_games_with_cognitive_biases/\">Fun and Games with Cognitive Biases</a>&nbsp;plays (on a meta level) with concepts.&nbsp;<a href=\"/r/discussion/lw/dhn/rationality_games_apps_brainstorming/\">Rationality Games &amp; Apps Brainstorming</a>&nbsp;urges for such games. <a href=\"/r/discussion/lw/7od/rationality_and_video_games/\">Rationality and Video Games</a> provides an example.&nbsp;<span lang=\"en-US\">Taboo is explicitly mentioned in </span><a href=\"/lw/nu/taboo_your_words/\">http://lesswrong.com/lw/nu/taboo_your_words/</a>. Doing this with scientific concepts could make for a nice casual game at a LW meetup.&nbsp;<a href=\"/lw/3h/why_our_kind_cant_cooperate/\">http://lesswrong.com/lw/3h/why_our_kind_cant_cooperate/</a> generallly urges for community building mechanism \u2013 and (social) games address this. A game that has been <a href=\"/r/discussion/tag/diplomacy\">discussed extensively</a> on lesswrong is <a href=\"http://en.wikipedia.org/wiki/Diplomacy_(game)\">Diplomacy</a>&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\">&nbsp;</p>\n<h2 id=\"Introduction\">Introduction</h2>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">I like to play games that are challenging and intellectually demanding. But I don't like to learn games that will gain nothing except the fun of learning and playing the game.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">Games for rationalists should not only be fun. They should also train or teach something beyond the game. Games that don't are parasitic memes.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">This post describe areas of rational thought addressed by games. By rationality I mean the <a href=\"/lw/31/what_do_we_mean_by_rationality/\">LW view</a>.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">I list games suitable for getting acquainted with rational thinking. I list areas underrepresented by games and I give an elaborate example of a game suitable to such an area. I propose trying to invent games for these areas and sketch possible strategies.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">To repeat I don't mean games that are just played frequently by rationalists or usually considered entertaining by rationalists (though they may). I mean games to train rationalists. <a href=\"http://en.wikipedia.org/wiki/Serious_game\">Serious Games</a>. To make it easy and fun to become a rationalist (or at least support such a cause).</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">Games supporting the acquisition of literacy and numeracy are out of scope here even though these are preconditions for more demanding concepts (and games). Also mostly out of scope are social and activity games (though these are be relevant to efficient group building) and games with a focus on motor ability (though such may make spatial or dynamical concepts more clear).</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">&nbsp;</p>\n<h3 id=\"Outline\"><strong>Outline</strong></h3>\n<ol>\n<li>I motivate the use of specifically designed games for (rational) education.</li>\n<li>I illustrate educational game theory with concrete examples.</li>\n<li>I argue for the applicability of games to some areas of rationality.</li>\n<li>I present a real game I invented and tested to address the overconfidence bias in particular.</li>\n</ol>\n<h2 id=\"Using_and_Abusing_Curiosity\"><strong>Using and Abusing Curiosity</strong></h2>\n<p style=\"margin-bottom: 0cm\"><span lang=\"en-US\">To use natural curiosity and to direct it to 'worthy' objectives has got the newfangled name <a href=\"http://en.wikipedia.org/wiki/Educational_entertainment#Games\">edutainment</a></span><span lang=\"en-US\">. Well known examples from TV are Sesame Street and <a href=\"http://en.wikipedia.org/wiki/Mammutland\">Mammutland</a></span>&nbsp;(which is based on <a href=\"http://en.wikipedia.org/wiki/The_Way_Things_Work\">The Way Things Work</a>. Examples of educational games are given below.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\"><span lang=\"en-US\">The opposite, where natural curiosity is diverted into cognitive dead ends and where attention is collected in trademarked <a href=\"http://de.wikipedia.org/wiki/Walled_Garden\">walled gardens</a> </span><span lang=\"en-US\">is called Marketing. An example is Pokemon where the cute animals engage the curiosity of children which learn lots of names and attributes \u2013 but none of these have any lasting cognitive use and all the accumulated knowledge can primarily be used for status and thus directing attention to the trademark owner in the end.</span></p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">Rational parents have to counter marketing when educating their children. Competing with media is hard because it is omnipresent and not in the best interest of education. Possibly we should lobby against maleducation but that will not help now. Instead we have to turn to games&nbsp;competetive with Pokemon.</p>\n<h3 id=\"Educational_Game_Theory_\"><span lang=\"en-US\"><strong>Educational Game Theory&nbsp;</strong></span></h3>\n<p style=\"margin-bottom: 0cm\"><span lang=\"en-US\"><a href=\"http://en.wikipedia.org/wiki/Educational_game#Theory\">Educational Game Theory</a></span><span lang=\"en-US\">&nbsp;</span>names three approaches to educational games:</p>\n<ul>\n<li>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">creation from scratch by educators</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">integrating off-the-shelf games</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">creating from scratch by the players (e.g. children)</p>\n</li>\n</ul>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">An example of the first is <a href=\"http://de.wikipedia.org/wiki/%C3%96kolopoly\">\u00d6kolopoly</a>&nbsp;(English <a href=\"http://www.frederic-vester.de/eng/ecopolicy/\">Ecopolicy</a>) which has a strong focus on grand picture of complex systems. I'd really recommend it for children grade 6 and up (but use the card board version as the simple 'game mechanics' (connected lookup-tables) are inspectable there).</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">I will provide an example which I invented and tested myself later.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">Examples of the second kind are e.g. playing Taboo with math words or playing Liar's Dice and calculating expected values. Much more will be listed below with a focus on specific concepts.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">That children invent games all the time is no surprise. Because one natural aspect of games is that they support or stimulate learning most games invented by children are educational by nature.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">To illustrate this I'd like to describe a game my oldest son (9 years) invented:</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">He calls it 3D-computer and it consists of a 'user interface' made of paper and plastics, a number of 'input devices' and a large playing area made of taped together sheets of paper with a plan of a city. The player (usually one of his younger brothers) sits at the 'controls' and e.g. steers a car on a racing track thru the city. He has to press left and right (and say so loud) and my oldest son will act out the operation of the 3D-computer by moving the cars accordingly.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">Obviously this is modeled after real computer games but it requires significantly more abstraction and cooperation of the players. To build the 'game' he had to plan it and build corresponding pieces. He created a 'user interface' consisting of menues and sub menues (drawn on paper) for the possible games and options that can be played with the 3D-computer. It makes the logic of the game \u2013 its concepts \u2013 clear both in the planning and realization and in the acting too.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">&nbsp;</p>\n<h2 id=\"Concepts_in_Games\"><strong>Concepts in Games</strong></h2>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\"><strong id=\"Concepts_in_Games1\">Concepts in Games</strong></p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Any normal game requires some rational thinking, but there are some areas of rational thought that are less covered by games than others.</p>\n<h4 id=\"Game_Theory_\"><span lang=\"en-US\">Game Theory </span></h4>\n<p><span lang=\"en-US\">This lends itself naturally to games with clear concepts. The simplified <a href=\"http://en.wikipedia.org/wiki/List_of_games_in_game_theory\">games analysed in game theory</a></span><span lang=\"en-US\">&nbsp;can be easily readapted to playable games or parts thereof</span>. Some of these are directly playable for school children. I played lots of these in a math course.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Game theoy obviously applies to most games, but the basic min-max principle is very clearly present in games that have a measurable of advantage; I identify the following kinds</p>\n<p>&nbsp;</p>\n<ol>\n<li>Distance of pieces from a finish on a game board (e.g. in the simple childrens games above</li>\n<li>Some in-game currency (examples: Monopoly)</li>\n<li>A winning criteria that includes a tally (example: Siedler von Catan)</li>\n</ol>\n<p>In these cases there may be strategic effects that outweigh the measurable but in the long run 'more is better' and a higher value predicts a win well.</p>\n<p>&nbsp;</p>\n<h4 id=\"Probability_Theory\">Probability Theory</h4>\n<p>Classical games with dice or shuffled cards surely build some intuition for probability theory which is present in most games in so far as some most games need some controlled random variables (aka dice or shuffled cards) to support the game semantics.</p>\n<p>It is also present when simulating (or testing) games; the game state changes due to the game rules are implied stochastic processes.<br>Probability theory is relevant as games with numerous regular random events satisfy criteria of theorems of large number.</p>\n<p>Examples for dice games with simple rules and clear concepts are&nbsp;<a href=\"http://en.wikipedia.org/wiki/Cross_and_circle_game\">Cross And Circle</a>,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Snakes_and_Ladders\">Snake and Ladders</a>. These illustrate basic concepts and reflection about strategies using solid terminology can teach children a lot about probability.</p>\n<p>Intuition about different distributions resulting from superposition and their likelihood is fairly clearly present in&nbsp;<a href=\"http://en.wikipedia.org/wiki/Yatzee\">Yahtzee</a>.&nbsp;<br>Intuition about the law of large numbers (and updating due to new information) can be found in an entertaining way in&nbsp;<a href=\"http://en.wikipedia.org/wiki/Liar%27s_dice\">Liar's dice</a>.</p>\n<ul>\n</ul>\n<div>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">Anecdote: As a child I was a sore loser. Seems that at least one board flew through the room. Not that I'd lose that often, quite the contrary, but if...</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">I couldn't deal with chance playing tricks on me. Later I always tried to limit the effect of the dice. Both by hedging or by just changing the game rules beforehand. Once I printed an 'improved' list dice throws that were overly regular.</p>\n</div>\n<h4 id=\"Decision_theory_\"><span lang=\"en-US\">Decision theory </span></h4>\n<p style=\"margin-bottom: 0cm;\">Games that are not based on skill necessarily involve decisions by the players. These fall into three categories</p>\n<ul>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Decisions under certainty \u2013 games are seldom certain, but in some cases decisions can be modeled as if the game environment were fixed and then maximizing over multidimensional ratings of the game state (some measurables like win points)</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Decisions under risk \u2013 when some decisions involve probabilistic losses or gains</p>\n</li>\n<li>Decisions under uncertainty \u2013 if some decisions have uncertain consequences either due to unknown probabilities of game events or due to the other players.</li>\n</ul>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Clear decision strategies or concepts are not so easy come by. Most games imply lots of decisions but effective strategies are seldom obvious.</p>\n<ul>\n</ul>\n<p>Games where subjective valuations are rampant are trading cards (e.g. Pokemon or currently Star Wars) these are no simple trades but often involve a risk (gaining certain cards vs. losing some other cards due to a random or skill based process). What is unclear is whether there are any lessons learned from doing these games as the decisions are mostly individual and the conditions of the transactions vary from trade to trade so general insights are unlikely.</p>\n<h4 id=\"Cognitive_Science\"><span lang=\"en-US\">Cognitive Science</span></h4>\n<p><a href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\">Cognitive biases</a> can be harvested for game aspects worth to address. I will demonstrate this with the Overconfidence Effect and with miscalibration of probabilities&nbsp;particular which is the main focus of the 'estimation game' presented below. I will cover other biases in more detail in a separate post.</p>\n<p>Humans are very good at <a href=\"http://www.linguistics.pomona.edu/thornton/lgcs11read/Fodor00.pdf\">detecting cheaters</a>&nbsp;this is well known since the work of <a href=\"http://www.cep.ucsb.edu/papers/instinctblindess.pdf\">Cosmides and Tooby</a>.This applies to real life as well as games. It would be interesting to develop games that try to move smoothly from obvious rules to abstract rules and try to train to perceive this as cheating.</p>\n<h4 id=\"Math_and_Logic\">Math and Logic</h4>\n<p>Math in general plays a role in most games. Correct logical inferences (even if done by intuition) train propositional logic (though a game which involves using modus ponens on unusual conditions would surely be a good idea).</p>\n<p>A very nice and pure example using Set Theory even in its name is&nbsp;<a href=\"http://en.wikipedia.org/wiki/Set_(game).\">Set</a>.&nbsp;</p>\n<p><span lang=\"en-US\"><a href=\"http://www.looneylabs.com/our-games\">Looneylabs</a> has a lot of games (or game material) suitable for logic. A very nice game building on this is <a href=\"http://www.koryheath.com/zendo/\">Zendo</a></span>.</p>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">&nbsp;</p>\n<h2 id=\"Estimation_Game__Who_guesses_best_\"><strong>Estimation Game 'Who guesses best'</strong></h2>\n<p style=\"margin-bottom: 0cm\" lang=\"en-US\">Now I'd like to present an example of a game which was created to explicitly addresses a cognitive bias \u2013 the overconfidence bias \u2013 which is hard to overcome even for scientifically schooled persons explicitly briefed in this bias.</p>\n<p style=\"margin-bottom: 0cm\"><span lang=\"en-US\">When I read chapter 21 by M. Alpertand H. Raiffa in Judgement under uncertainty - heuristics and biases 1982 by Kahneman et al (pages 294 \u2013 305) see <a href=\"http://books.google.de/books?hl=de&amp;lr=&amp;id=_0H8gwj4a1MC&amp;oi=fnd&amp;pg=PR8&amp;dq=Judgement+under+uncertainty+kahneman+1982&amp;ots=YChhbTQ-_I&amp;sig=eKydDFt1r1puEWYijj8y1D3kLtI#v=onepage&amp;q=Judgement%20under%20uncertainty%20kahneman%201982&amp;f=false\">this Google Books link</a>&nbsp;and this <a href=\"/lw/721/judgment_under_uncertainty_summaries_part_1/\">less wrong review</a>.</span></p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">I couldn't believe that it could be that hard to calibrate well and I decided to test this on myself and on others in a comparable setting (kind of privately reproducing the study). This developed into a game that was since played about 5 times with a total of about 20 person and 30 questions (thus much less than in the cited study). The results were as expected \u2013 basically. A significant trend was visible in the test games and also in the main game rounds during a large birthday party. I had improved the 'game chart' and scoring rules and this made the effect clearer and the calibration of most players improved quickly. The game is simple enough to be played by a smart eight year old.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">The game chart can be downloaded <a href=\"http://www.frank-lounge.de/estimation_game.pdf\">here</a>&nbsp;(on page 2 in english).</p>\n<h3 id=\"Estimation_Game_Rules\"><strong>Estimation Game Rules</strong></h3>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">This is basically an estimation game \u2013 but with some extensions.</p>\n<h3 id=\"The_Questions\"><strong>The Questions</strong></h3>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">First a number of questions about quantities to guess are needed.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Each quantity should have an exact value that has a believable source but is somewhat unusual such that most players will not know the value.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">In a scholarly context the question should name a study or method and date of determination of the quantity. E.g. for the question \u201cHow many residents live in Hamburg, Germany?\u201d the context could be: \u201cfrom wikipedia: official census of 2012 determined via population register\u201d. And for the trivial question \u201cHow many lentils are in this jar?\u201d the context could be: \u201cThis morning I filled the jar with green lentils, weight the content, counted and weighed a sample and calculated the total number.\u201d</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Example questions:</p>\n<ul>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Number of steps to the next underground station.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Number of lentils in a jar.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Amount of rain per square meter per year in Jerusalem in 2001.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Number of files on my PC as reported by ls -R this morning.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">How long (days) was the first voyage of James cook into the south seas?</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Total egg production in th U.S. In 1965. (this is from the Alpert Study)</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Toll collections of the Panama Canal in 1967. (dito)</p>\n</li>\n</ul>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">The questions can be provided by the host (which has the disadvantage that host cannot take part in the game for most questions). The better idea is to have each player provide one to three questions.</p>\n<h3 id=\"Guesstimation\"><strong>Guesstimation</strong></h3>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Each question is now dealt with as follows:</p>\n<ul>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">The question is read out loud. If needed, detail questions about the context of the question are answered.</p>\n</li>\n</ul>\n<ul>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">All players individually guesstimate the true value of the quantity in question. The number is written into the field in the middle below the green area. If the number is from one of the players he/she writes the correct number as his 'guess'.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Each player now considers what the other players might estimate. Values deemed to be definitely out of the range become the min/max values and written below the red/yellow border. Values deemed to be typical form the majority range and are written below the yellow/green border.</p>\n</li>\n</ul>\n<ul>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Points are awarded as follows:</p>\n<ul>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">For the best guess you get N points, for the next-best N/2, then N/4 (rounding down).... If multiple players are equally near, points are added and split. Numbers supplied by a players are excluded from scoring.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">For the green area you get N/2 points if it contains exactly N/2 numbers (including your own) for each more or less you get one point less (rounding up).</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">For the yellow area the player with the smallest range <strong>which still contains all guesses</strong> gets N+1 points, the next-best N/2+1 (and so on, rounding down).</p>\n</li>\n</ul>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Numbers falling on a boundary are scored to the players advantage.</p>\n</li>\n</ul>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Example:</p>\n<ul>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">For the number of lentils in the jar you estimate 1300. And you guess that half of the guesses fall within 500 and 2000. You also assume that nobody will believe less then 100 or more than 10000.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">It turns out that the other 6 players guessed 750, 1000, 1500, 2000, 2820, 4000.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">You write 750, 1000, 1500 and 2000 into your green area. You write 2800 and 4000 into the right yellow area. Congratulations: No numbers fell into a red area.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">The correct answer is revealed: 2800. You can circle it in the green area.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">The player guessing 2000 (distance 820) gets 7 points, the player guessing 4000 (distance 1180) gets 3 points floor(7/2), the player guessing 1500 (distance 1320) gets 1 point floor(7/4). For your 1300 you and the remaining players get nothing (7/8&lt;1).</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">You have got 4 numbers in your green area (you decide to include 2000). 4=ceil(7/2) is the optimum number of entries so you get 4 points. Had you excluded 2000 you'd got 3 points. If only your own number or if all 7 numbers were included you'd got 1 booby point.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">All the numbers are in your min-max-range, so you get at least 1 point. Lets assume that your range of 9900 is the second smallest of those containing all numbers. This nets you an additional floor(7/2)=3 points.</p>\n</li>\n</ul>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">&nbsp;</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">May sound complicated. A summary of these rules is part of the game chart. The chart is mostly self-explanatory and even school children can fill-out the chart after one round of explanation.</p>\n<h3 id=\"Gaming_the_Rules\"><strong>Gaming the Rules</strong></h3>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">It is possible to game the rules by e.g. using absurd guesses to bomb the max-range of the other players. The scoring is chosen to make this a losing strategy. Good guesses and good majority bounds score higher than the points reaped from being the only one with a valid max-range. And players can hedge against bombing nonetheless (in a way these are black swan events and thus interesting in their own right).</p>\n<h3 id=\"Not_quite_Overconfidence\"><strong>Not quite Overconfidence</strong></h3>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">This game doesn't <em>directly </em>address the Overconfidence Bias. To do so the players would have to guess the range of <em>their own</em> guesses. Getting the range of ones own guesses via this kind of game takes much longer (times the number of players). The game works so well because it forces you to take the outside view. You have to consider what the <em>other</em> players might not know and then getting immediate feedback about ones own performance via</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">a) Distance from the truth of your own guess.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">b) In extreme cases exclamation from the other players whose ranges were shredded.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">It is critical to make the leap that your own guesses are as (over)confident as those of the others.</p>\n<h3 id=\"Variants\"><strong>Variants</strong></h3>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">It is quite possible to play a stricter version where you guess your own ranges with the same game chart. The following differences are necessary:</p>\n<ol>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Instead of the majority range you have to guess the range into which the true value will fall mostly. Mostly meaning on half of all questions this range should <em>actually</em> contain the value.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Instead of the maximum range you have to guess the range into which the true value will always fall.</p>\n</li>\n</ol><ol>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Don't write the numbers of the other players into the colored area.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Use a singe color strip for all of your own guesses. Just make a mark in the area where the actual value lay in the end.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Points are awarded only at the end of the game.</p>\n</li>\n</ol>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">This has the disadvantages of taking much longer and with the above simple rules it is too easy to game the rules (e.g. by using absurdly large or small ranges at the end of the game to ensure the required counts).</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">But this can be used after a few games to test whether you can make the leap to take the outside view on your own guesses. It can be done alone. Just think of a few arbitrary quantities and then research them later (on danger of choosing only quantities you can provide sensible estimates for).</p>\n<h3 id=\"Anchoring\"><strong>Anchoring</strong></h3>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Obviously your ranges are anchored to your guess. It is possible to add a small tweak to change the anchoring (disclaimer: I didn't try this): Allow the person knowing the correct answer to supply any example number and say it out load beforehand. This will provide another anchor (obviously anchored to the correct number somehow).</p>\n<h3 id=\"Motivation\"><strong>Motivation</strong></h3>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">The test-games were fun. OK. The game may not be fun for everybody. My acquaintances are mostly smart and well-educated. But even my 8 year old son liked it (and scored in the middle range of 10 players). It is fun especially if people come with their own individual questions. The resulting discussions about individual (mis)reasoning is also often insightful and a nice ice-breaker.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">The performance of the players definitely improved.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">Friends who are teachers asked me for the material and used it in highschool.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">I hope this is is step into the right direction.</p>\n<p style=\"margin-bottom: 0cm;\" lang=\"en-US\">I'd like to close with an anecdote: I still try to win any game that I play and my friends know it. Sometimes, especially after I play unusual or when explaining tricks and then losing anyway I get remarks about it obviously not helping. To that my reply mostly is: I maximize my chances on infinitely many runs. So in the end I learned to lose.</p>", "sections": [{"title": "Follow-up to:\u00a0Fun and Games with Cognitive Biases\u00a0", "anchor": "Follow_up_to__Fun_and_Games_with_Cognitive_Biases_", "level": 4}, {"title": "Games on Lesswrong ", "anchor": "Games_on_Lesswrong_", "level": 1}, {"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Outline", "anchor": "Outline", "level": 2}, {"title": "Using and Abusing Curiosity", "anchor": "Using_and_Abusing_Curiosity", "level": 1}, {"title": "Educational Game Theory\u00a0", "anchor": "Educational_Game_Theory_", "level": 2}, {"title": "Concepts in Games", "anchor": "Concepts_in_Games", "level": 1}, {"title": "Concepts in Games", "anchor": "Concepts_in_Games1", "level": 4}, {"title": "Game Theory ", "anchor": "Game_Theory_", "level": 3}, {"title": "Probability Theory", "anchor": "Probability_Theory", "level": 3}, {"title": "Decision theory ", "anchor": "Decision_theory_", "level": 3}, {"title": "Cognitive Science", "anchor": "Cognitive_Science", "level": 3}, {"title": "Math and Logic", "anchor": "Math_and_Logic", "level": 3}, {"title": "Estimation Game 'Who guesses best'", "anchor": "Estimation_Game__Who_guesses_best_", "level": 1}, {"title": "Estimation Game Rules", "anchor": "Estimation_Game_Rules", "level": 2}, {"title": "The Questions", "anchor": "The_Questions", "level": 2}, {"title": "Guesstimation", "anchor": "Guesstimation", "level": 2}, {"title": "Gaming the Rules", "anchor": "Gaming_the_Rules", "level": 2}, {"title": "Not quite Overconfidence", "anchor": "Not_quite_Overconfidence", "level": 2}, {"title": "Variants", "anchor": "Variants", "level": 2}, {"title": "Anchoring", "anchor": "Anchoring", "level": 2}, {"title": "Motivation", "anchor": "Motivation", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "55 comments"}], "headingsCount": 24}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Ytr4dJT79sCBuuEjG", "CNZQesEh6Ts5G2r7b", "7rhfWs3Ttw2XM9YrL", "mAK7ryHLq5PJqoXLN", "WBdvyyHLdxZSAMmoz", "7FzD7pNm9X68Gp5ZC", "RcZCwxFiZzE6X7nsv", "ZfAqkomEFE9NGfwxG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-09-12T17:41:25.880Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-13T01:52:45.751Z", "modifiedAt": null, "url": null, "title": "Meetup : Applied Rationality Talks: Thinking in Bayes", "slug": "meetup-applied-rationality-talks-thinking-in-bayes", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmer963", "createdAt": "2010-09-28T01:54:53.120Z", "isAdmin": false, "displayName": "Swimmer963"}, "userId": "6Fx2vQtkYSZkaCvAg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WpSYrDpDFRKuDSig9/meetup-applied-rationality-talks-thinking-in-bayes", "pageUrlRelative": "/posts/WpSYrDpDFRKuDSig9/meetup-applied-rationality-talks-thinking-in-bayes", "linkUrl": "https://www.lesswrong.com/posts/WpSYrDpDFRKuDSig9/meetup-applied-rationality-talks-thinking-in-bayes", "postedAtFormatted": "Friday, September 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Applied%20Rationality%20Talks%3A%20Thinking%20in%20Bayes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Applied%20Rationality%20Talks%3A%20Thinking%20in%20Bayes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWpSYrDpDFRKuDSig9%2Fmeetup-applied-rationality-talks-thinking-in-bayes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Applied%20Rationality%20Talks%3A%20Thinking%20in%20Bayes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWpSYrDpDFRKuDSig9%2Fmeetup-applied-rationality-talks-thinking-in-bayes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWpSYrDpDFRKuDSig9%2Fmeetup-applied-rationality-talks-thinking-in-bayes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qu'>Applied Rationality Talks: Thinking in Bayes</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 September 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Royal Oak 161 Laurier Avenue East, Ottawa, ON</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is the second in a series of talks on CFAR and Less Wrong rationality topics offered to the Ottawa Skeptics meetup group. The format will be a fifteen-minute talk followed by drinks and structured discussion.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qu'>Applied Rationality Talks: Thinking in Bayes</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WpSYrDpDFRKuDSig9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.3388093567049957e-06, "legacy": true, "legacyId": "24134", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Applied_Rationality_Talks__Thinking_in_Bayes\">Discussion article for the meetup : <a href=\"/meetups/qu\">Applied Rationality Talks: Thinking in Bayes</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 September 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Royal Oak 161 Laurier Avenue East, Ottawa, ON</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is the second in a series of talks on CFAR and Less Wrong rationality topics offered to the Ottawa Skeptics meetup group. The format will be a fifteen-minute talk followed by drinks and structured discussion.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Applied_Rationality_Talks__Thinking_in_Bayes1\">Discussion article for the meetup : <a href=\"/meetups/qu\">Applied Rationality Talks: Thinking in Bayes</a></h2>", "sections": [{"title": "Discussion article for the meetup : Applied Rationality Talks: Thinking in Bayes", "anchor": "Discussion_article_for_the_meetup___Applied_Rationality_Talks__Thinking_in_Bayes", "level": 1}, {"title": "Discussion article for the meetup : Applied Rationality Talks: Thinking in Bayes", "anchor": "Discussion_article_for_the_meetup___Applied_Rationality_Talks__Thinking_in_Bayes1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-13T03:33:08.874Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign, Illinois Games/Discussion", "slug": "meetup-urbana-champaign-illinois-games-discussion", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EzMFKhY237stTwamF/meetup-urbana-champaign-illinois-games-discussion", "pageUrlRelative": "/posts/EzMFKhY237stTwamF/meetup-urbana-champaign-illinois-games-discussion", "linkUrl": "https://www.lesswrong.com/posts/EzMFKhY237stTwamF/meetup-urbana-champaign-illinois-games-discussion", "postedAtFormatted": "Friday, September 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%2C%20Illinois%20Games%2FDiscussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%2C%20Illinois%20Games%2FDiscussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEzMFKhY237stTwamF%2Fmeetup-urbana-champaign-illinois-games-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%2C%20Illinois%20Games%2FDiscussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEzMFKhY237stTwamF%2Fmeetup-urbana-champaign-illinois-games-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEzMFKhY237stTwamF%2Fmeetup-urbana-champaign-illinois-games-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 79, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qv'>Urbana-Champaign, Illinois Games/Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 September 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Illini Union North Lounge 1401 W Green St Urbana, IL 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Moving to the North Lounge because last time the South Lounge was unexpectedly scheduled for a private event, but the North Lounge is (as far as I know) always public. I will have Wits and Wagers, Zendo, and Pandemic. <a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/lY7mWBzhdqs\">Cross posted on the mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qv'>Urbana-Champaign, Illinois Games/Discussion</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EzMFKhY237stTwamF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.3388977252256946e-06, "legacy": true, "legacyId": "24137", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Illinois_Games_Discussion\">Discussion article for the meetup : <a href=\"/meetups/qv\">Urbana-Champaign, Illinois Games/Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 September 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Illini Union North Lounge 1401 W Green St Urbana, IL 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Moving to the North Lounge because last time the South Lounge was unexpectedly scheduled for a private event, but the North Lounge is (as far as I know) always public. I will have Wits and Wagers, Zendo, and Pandemic. <a href=\"https://groups.google.com/forum/#!topic/lesswrong-urbana-champaign/lY7mWBzhdqs\">Cross posted on the mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Illinois_Games_Discussion1\">Discussion article for the meetup : <a href=\"/meetups/qv\">Urbana-Champaign, Illinois Games/Discussion</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign, Illinois Games/Discussion", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Illinois_Games_Discussion", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign, Illinois Games/Discussion", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Illinois_Games_Discussion1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-13T05:54:20.462Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC fun and games meetup", "slug": "meetup-washington-dc-fun-and-games-meetup-7", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KPQhRNTw9xE2ZXRWA/meetup-washington-dc-fun-and-games-meetup-7", "pageUrlRelative": "/posts/KPQhRNTw9xE2ZXRWA/meetup-washington-dc-fun-and-games-meetup-7", "linkUrl": "https://www.lesswrong.com/posts/KPQhRNTw9xE2ZXRWA/meetup-washington-dc-fun-and-games-meetup-7", "postedAtFormatted": "Friday, September 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKPQhRNTw9xE2ZXRWA%2Fmeetup-washington-dc-fun-and-games-meetup-7%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKPQhRNTw9xE2ZXRWA%2Fmeetup-washington-dc-fun-and-games-meetup-7", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKPQhRNTw9xE2ZXRWA%2Fmeetup-washington-dc-fun-and-games-meetup-7", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 45, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qw'>Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 September 2013 03:00:00AM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qw'>Washington DC fun and games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KPQhRNTw9xE2ZXRWA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.339022034042205e-06, "legacy": true, "legacyId": "24138", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup\">Discussion article for the meetup : <a href=\"/meetups/qw\">Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 September 2013 03:00:00AM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/qw\">Washington DC fun and games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-13T06:07:14.438Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow: Yet Another Meetup", "slug": "meetup-moscow-yet-another-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PhftWe9TpNft8XZkf/meetup-moscow-yet-another-meetup", "pageUrlRelative": "/posts/PhftWe9TpNft8XZkf/meetup-moscow-yet-another-meetup", "linkUrl": "https://www.lesswrong.com/posts/PhftWe9TpNft8XZkf/meetup-moscow-yet-another-meetup", "postedAtFormatted": "Friday, September 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%3A%20Yet%20Another%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%3A%20Yet%20Another%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPhftWe9TpNft8XZkf%2Fmeetup-moscow-yet-another-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%3A%20Yet%20Another%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPhftWe9TpNft8XZkf%2Fmeetup-moscow-yet-another-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPhftWe9TpNft8XZkf%2Fmeetup-moscow-yet-another-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 123, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qx'>Moscow: Yet Another Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 September 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door near the swing or just look for group of geek-looking people. We will meet you at 16:00 inside.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li>Applied rationality exercise: fungibility.</li>\n<li>Discussion and short presentations.</li>\n<li>Game session.</li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qx'>Moscow: Yet Another Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PhftWe9TpNft8XZkf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.339033392061661e-06, "legacy": true, "legacyId": "24139", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Yet_Another_Meetup\">Discussion article for the meetup : <a href=\"/meetups/qx\">Moscow: Yet Another Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 September 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door near the swing or just look for group of geek-looking people. We will meet you at 16:00 inside.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li>Applied rationality exercise: fungibility.</li>\n<li>Discussion and short presentations.</li>\n<li>Game session.</li>\n</ul>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Yet_Another_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/qx\">Moscow: Yet Another Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow: Yet Another Meetup", "anchor": "Discussion_article_for_the_meetup___Moscow__Yet_Another_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Moscow: Yet Another Meetup", "anchor": "Discussion_article_for_the_meetup___Moscow__Yet_Another_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-13T10:20:56.601Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA\u2014Talk Like a Pirate Day", "slug": "meetup-west-la-talk-like-a-pirate-day", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FReyqhZHGGo6Lngm3/meetup-west-la-talk-like-a-pirate-day", "pageUrlRelative": "/posts/FReyqhZHGGo6Lngm3/meetup-west-la-talk-like-a-pirate-day", "linkUrl": "https://www.lesswrong.com/posts/FReyqhZHGGo6Lngm3/meetup-west-la-talk-like-a-pirate-day", "postedAtFormatted": "Friday, September 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%E2%80%94Talk%20Like%20a%20Pirate%20Day&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%E2%80%94Talk%20Like%20a%20Pirate%20Day%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFReyqhZHGGo6Lngm3%2Fmeetup-west-la-talk-like-a-pirate-day%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%E2%80%94Talk%20Like%20a%20Pirate%20Day%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFReyqhZHGGo6Lngm3%2Fmeetup-west-la-talk-like-a-pirate-day", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFReyqhZHGGo6Lngm3%2Fmeetup-west-la-talk-like-a-pirate-day", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 255, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qy'>West LA\u2014Talk Like a Pirate Day</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 September 2013 07:00:00AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (wee swashbucklers be welcome), in th' <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, by the movie theaters. The sign thar say: \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours, or longer if ye be a scurvy dog.</p>\n\n<p><strong>Discussion</strong>: Ahoy! This week be a special convening. In celerberation of the world-renowned <a href=\"http://en.wikipedia.org/wiki/International_Talk_Like_a_Pirate_Day\" rel=\"nofollow\">Talk Like a Pirate Day</a>, we'll be forgoin' the usual manner o'speech, and adoptin a manner more befittin a lawless captain or squire on the high seas of yestercentury. Arrr, thar be nothin ter tickle a man's fancy more'n pretendin toward adventure n romance n camaraderie and what have ye. That's why we be announcin this meetin', rather than keepin it below-decks, as 'twer, as we bin doin fer half th' meetups. (Yar.)</p>\n\n<p>Recommended readins:</p>\n\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=nzcv5TJkJBA\" rel=\"nofollow\">Yo Ho Ho, and a Bottle of Rum</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=qGyPuey-1Jw\" rel=\"nofollow\">What Shall We Do with a Drunken Sailor?</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=Aovi-kEpui4\" rel=\"nofollow\">An economic analysis of typical emotional transactions occurring in the moments before and during an impending death</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=BjqEr9zOBkY\" rel=\"nofollow\">All for Me Grog</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=ZxC4pz9pO7c\" rel=\"nofollow\">Against Ninjas 1</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=KZAzZeSHAgs\" rel=\"nofollow\">Against Ninjas 2</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=TQt8uVWQyyc\" rel=\"nofollow\">Pyrats</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=EiEzOgeyho0\" rel=\"nofollow\">MooBeard</a></li>\n<li><a href=\"http://books.google.com/books?id=_BsEAAAAMAAJ\" rel=\"nofollow\">Sailors&#39; Language: A Collection of Sea-Terms and their Definitions</a></li>\n</ul>\n\n<p>A priori exposure ter \"Less Wrong\" not even be recommended this time.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qy'>West LA\u2014Talk Like a Pirate Day</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FReyqhZHGGo6Lngm3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.339256810767385e-06, "legacy": true, "legacyId": "24145", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Talk_Like_a_Pirate_Day\">Discussion article for the meetup : <a href=\"/meetups/qy\">West LA\u2014Talk Like a Pirate Day</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 September 2013 07:00:00AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to get in</strong>: Go to the Westside Tavern in the upstairs Wine Bar (wee swashbucklers be welcome), in th' <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, by the movie theaters. The sign thar say: \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours, or longer if ye be a scurvy dog.</p>\n\n<p><strong>Discussion</strong>: Ahoy! This week be a special convening. In celerberation of the world-renowned <a href=\"http://en.wikipedia.org/wiki/International_Talk_Like_a_Pirate_Day\" rel=\"nofollow\">Talk Like a Pirate Day</a>, we'll be forgoin' the usual manner o'speech, and adoptin a manner more befittin a lawless captain or squire on the high seas of yestercentury. Arrr, thar be nothin ter tickle a man's fancy more'n pretendin toward adventure n romance n camaraderie and what have ye. That's why we be announcin this meetin', rather than keepin it below-decks, as 'twer, as we bin doin fer half th' meetups. (Yar.)</p>\n\n<p>Recommended readins:</p>\n\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=nzcv5TJkJBA\" rel=\"nofollow\">Yo Ho Ho, and a Bottle of Rum</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=qGyPuey-1Jw\" rel=\"nofollow\">What Shall We Do with a Drunken Sailor?</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=Aovi-kEpui4\" rel=\"nofollow\">An economic analysis of typical emotional transactions occurring in the moments before and during an impending death</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=BjqEr9zOBkY\" rel=\"nofollow\">All for Me Grog</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=ZxC4pz9pO7c\" rel=\"nofollow\">Against Ninjas 1</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=KZAzZeSHAgs\" rel=\"nofollow\">Against Ninjas 2</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=TQt8uVWQyyc\" rel=\"nofollow\">Pyrats</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=EiEzOgeyho0\" rel=\"nofollow\">MooBeard</a></li>\n<li><a href=\"http://books.google.com/books?id=_BsEAAAAMAAJ\" rel=\"nofollow\">Sailors' Language: A Collection of Sea-Terms and their Definitions</a></li>\n</ul>\n\n<p>A priori exposure ter \"Less Wrong\" not even be recommended this time.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Talk_Like_a_Pirate_Day1\">Discussion article for the meetup : <a href=\"/meetups/qy\">West LA\u2014Talk Like a Pirate Day</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA\u2014Talk Like a Pirate Day", "anchor": "Discussion_article_for_the_meetup___West_LA_Talk_Like_a_Pirate_Day", "level": 1}, {"title": "Discussion article for the meetup : West LA\u2014Talk Like a Pirate Day", "anchor": "Discussion_article_for_the_meetup___West_LA_Talk_Like_a_Pirate_Day1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-13T13:20:44.900Z", "modifiedAt": null, "url": null, "title": "[LINK] Behind the Shock Machine: book reexamining Milgram obedience experiments", "slug": "link-behind-the-shock-machine-book-reexamining-milgram", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:31.150Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanArmak", "createdAt": "2009-08-05T23:08:24.020Z", "isAdmin": false, "displayName": "DanArmak"}, "userId": "7KSbntzeQ2RNZq6Jw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dpGcbtSrAmkfa4tLY/link-behind-the-shock-machine-book-reexamining-milgram", "pageUrlRelative": "/posts/dpGcbtSrAmkfa4tLY/link-behind-the-shock-machine-book-reexamining-milgram", "linkUrl": "https://www.lesswrong.com/posts/dpGcbtSrAmkfa4tLY/link-behind-the-shock-machine-book-reexamining-milgram", "postedAtFormatted": "Friday, September 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Behind%20the%20Shock%20Machine%3A%20book%20reexamining%20Milgram%20obedience%20experiments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Behind%20the%20Shock%20Machine%3A%20book%20reexamining%20Milgram%20obedience%20experiments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdpGcbtSrAmkfa4tLY%2Flink-behind-the-shock-machine-book-reexamining-milgram%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Behind%20the%20Shock%20Machine%3A%20book%20reexamining%20Milgram%20obedience%20experiments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdpGcbtSrAmkfa4tLY%2Flink-behind-the-shock-machine-book-reexamining-milgram", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdpGcbtSrAmkfa4tLY%2Flink-behind-the-shock-machine-book-reexamining-milgram", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 290, "htmlBody": "<p>There's a book called&nbsp;<a href=\"/Behind the Shock Machine\">Behind the Shock Machine</a>&nbsp;by psychologist&nbsp;<a href=\"http://www.gina-perry.com/about-gina/\">Gina Perry</a>, published just a week ago, which investigates the original <a href=\"http://en.wikipedia.org/wiki/Milgram_experiment\">Milgram obedience experiments</a>. I haven't read it, but I've read a <a href=\"http://www.psmag.com/health/electric-schlock-65377/\">summary / editorial</a> published in the Pacific Standard.</p>\n<p>Of course, the editorial is in some measure designed to provoke outrage, generate click-throughs, and leave readers biased against Milgram. I don't trust the editorial to report unbiased truth. If anyone has read the book, what do you think about it?</p>\n<p>Key quote from the editorial:</p>\n<blockquote>\n<p>Perry also caught Milgram cooking his data. In his articles, Milgram stressed the uniformity of his procedures, hoping to appear as scientific as possible. By his account, each time a subject protested or expressed doubt about continuing, the experimenter would employ a set series of four counter-prompts. If, after the fourth prompt (&ldquo;You have no other choice, teacher; you must go on&rdquo;), the subject still refused to continue, the experiment would be called to a halt, and the subject counted as &ldquo;disobedient.&rdquo; But on the audiotapes in the Yale archives, Perry heard Milgram&rsquo;s experimenter improvising, roaming further and further off script, coaxing or, depending on your point of view, coercing participants into continuing. Inconsistency in the standards meant that the line between obedience and disobedience was shifting from subject to subject, and from variation to variation&mdash;and that the famous 65 percent compliance rate had less to do with human nature than with arbitrary semantic distinctions.</p>\n<p>The wrinkles in Milgram&rsquo;s research kept revealing themselves. Perhaps most damningly, after Perry tracked down one of Milgram&rsquo;s research analysts, she found reason to believe that most of his subjects had actually seen through the deception. They knew, in other words, that they were taking part in a low-stakes charade.</p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dpGcbtSrAmkfa4tLY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 14, "extendedScore": null, "score": 1.3394151928512976e-06, "legacy": true, "legacyId": "24146", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-13T13:41:53.584Z", "modifiedAt": null, "url": null, "title": "Please share your reading habits/techniques/strategies ", "slug": "please-share-your-reading-habits-techniques-strategies", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:28.375Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Torello", "createdAt": "2013-07-01T17:38:37.441Z", "isAdmin": false, "displayName": "Torello"}, "userId": "xoRpeFN7K5MgDRcvM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vfBGrPbHFcKK2pLMK/please-share-your-reading-habits-techniques-strategies", "pageUrlRelative": "/posts/vfBGrPbHFcKK2pLMK/please-share-your-reading-habits-techniques-strategies", "linkUrl": "https://www.lesswrong.com/posts/vfBGrPbHFcKK2pLMK/please-share-your-reading-habits-techniques-strategies", "postedAtFormatted": "Friday, September 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Please%20share%20your%20reading%20habits%2Ftechniques%2Fstrategies%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APlease%20share%20your%20reading%20habits%2Ftechniques%2Fstrategies%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvfBGrPbHFcKK2pLMK%2Fplease-share-your-reading-habits-techniques-strategies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Please%20share%20your%20reading%20habits%2Ftechniques%2Fstrategies%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvfBGrPbHFcKK2pLMK%2Fplease-share-your-reading-habits-techniques-strategies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvfBGrPbHFcKK2pLMK%2Fplease-share-your-reading-habits-techniques-strategies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 244, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> </w:WordDocument> </xml><![endif]--></p>\n<p><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" LatentStyleCount=\"156\"> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]>\n<style>\n /* Style Definitions */\n table.MsoNormalTable\n\t{mso-style-name:\"Table Normal\";\n\tmso-tstyle-rowband-size:0;\n\tmso-tstyle-colband-size:0;\n\tmso-style-noshow:yes;\n\tmso-style-parent:\"\";\n\tmso-padding-alt:0in 5.4pt 0in 5.4pt;\n\tmso-para-margin:0in;\n\tmso-para-margin-bottom:.0001pt;\n\tmso-pagination:widow-orphan;\n\tfont-size:10.0pt;\n\tfont-family:\"Times New Roman\";\n\tmso-ansi-language:#0400;\n\tmso-fareast-language:#0400;\n\tmso-bidi-language:#0400;}\n</style>\n<![endif]-->\n<p>I'm looking to build up a &ldquo;tool-box&rdquo; of strategies/techniques/habits for reading non-fiction effectively and efficiently.<span style=\"mso-spacerun: yes;\">&nbsp; </span>I'm looking for methods to help me retain concepts, locate main ideas, make connections, etc.</p>\n<p>If anyone has posted about this topic previously, please link to the post.<span style=\"mso-spacerun: yes;\">&nbsp; </span></p>\n<p>Please point to relevant resources that have worked for you; additionally please describe skills/systems that you've developed personally.<span style=\"mso-spacerun: yes;\">&nbsp; </span></p>\n<p><span style=\"mso-spacerun: yes;\"><br /></span></p>\n<p class=\"MsoNormal\">An example of a useful comment I got posting in an open thread, from <span class=\"author\"><a href=\"http://lesswrong.com/user/Jayson_Virissimo/\">Jayson_Virissimo</a></span></p>\n<p class=\"MsoNormal\">&nbsp;&ldquo;As I read textbooks, I summarize the most important concepts (along with doing the exercises, if there are any) and write them in a notebook and then later (less than a week) enter the notes into Anki as cloze-delete flashcards. I don't have an objective measure of retention, but I believe that it has vastly improved relative to when I would simply read the book.&rdquo;<span style=\"mso-spacerun: yes;\">&nbsp; </span></p>\nHere is an example of an existing resource that I found useful:\n<p class=\"MsoNormal\"><a href=\"http://violentmetaphors.com/2013/08/25/how-to-read-and-understand-a-scientific-paper-2/\">http://violentmetaphors.com/2013/08/25/how-to-read-and-understand-a-scientific-paper-2/</a></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">Here are some questions/prompts that may spur your thinking:</p>\n<p class=\"MsoNormal\" style=\"tab-stops: 168.0pt;\">Describe the setting where you read.</p>\n<p class=\"MsoNormal\" style=\"tab-stops: 168.0pt;\">Do you schedule reading time?<span style=\"mso-spacerun: yes;\">&nbsp; </span>How?<span style=\"mso-spacerun: yes;\">&nbsp; </span><span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 168.0pt;\">How do you decide what to read next?</p>\n<p class=\"MsoNormal\" style=\"tab-stops: 168.0pt;\">Do you write notes by hand, on a computer?</p>\n<p class=\"MsoNormal\" style=\"tab-stops: 168.0pt;\">Do you wear noise-canceling headphones?<span style=\"mso-spacerun: yes;\">&nbsp; </span></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 168.0pt;\">Do you skim texts?</p>\n<p class=\"MsoNormal\" style=\"tab-stops: 168.0pt;\">Do you reread texts?</p>\n<p class=\"MsoNormal\" style=\"tab-stops: 168.0pt;\">How often do you reread &ldquo;foundational&rdquo; texts, or texts that shifted your paradigm?<span style=\"mso-spacerun: yes;\">&nbsp; </span></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 168.0pt;\">How often do you decide not to finish a book?</p>\n<p class=\"MsoNormal\" style=\"tab-stops: 168.0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"tab-stops: 168.0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"tab-stops: 168.0pt;\">I may do a series of posts on this in discussion, and if other users find it interesting/useful I may eventually make it into a post for the main page.<span style=\"mso-spacerun: yes;\">&nbsp; </span><span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vfBGrPbHFcKK2pLMK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 1.3394338204560033e-06, "legacy": true, "legacyId": "24147", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-13T15:58:35.332Z", "modifiedAt": null, "url": null, "title": "New LW Meetup: Phoenix", "slug": "new-lw-meetup-phoenix", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tudnA3p35qp6fxzgu/new-lw-meetup-phoenix", "pageUrlRelative": "/posts/tudnA3p35qp6fxzgu/new-lw-meetup-phoenix", "linkUrl": "https://www.lesswrong.com/posts/tudnA3p35qp6fxzgu/new-lw-meetup-phoenix", "postedAtFormatted": "Friday, September 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20LW%20Meetup%3A%20Phoenix&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20LW%20Meetup%3A%20Phoenix%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtudnA3p35qp6fxzgu%2Fnew-lw-meetup-phoenix%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20LW%20Meetup%3A%20Phoenix%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtudnA3p35qp6fxzgu%2Fnew-lw-meetup-phoenix", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtudnA3p35qp6fxzgu%2Fnew-lw-meetup-phoenix", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 554, "htmlBody": "<p><strong>This summary was posted to LessWrong main on September 6th. The following week's summary is <a href=\"/lw/ims/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>\n<ul>\n<li><a href=\"/meetups/qb\"></a><a href=\"/meetups/qi\">Phoenix Tuesday Lunch Group:&nbsp;<span class=\"date\">10 September 2013 12:00PM</span></a></li>\n</ul>\n<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/qc\"></a><a href=\"/meetups/qn\">Atlanta Lesswrong September Meetup (1st of 2):&nbsp;<span class=\"date\">08 September 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/q4\">Helsinki Meetup:&nbsp;<span class=\"date\">08 September 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/pn\">LessWrong Israel September meetup:&nbsp;<span class=\"date\">12 September 2013 08:00PM</span></a></li>\n<li><a href=\"/meetups/qk\">Rutgers New Brunswick:&nbsp;<span class=\"date\">08 September 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/ql\">Saskatoon: Zendo!:&nbsp;<span class=\"date\">07 September 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/qh\">Urbana-Champaign, Illinois:&nbsp;<span class=\"date\">08 September 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/qj\">Zagreb Meetup:&nbsp;<span class=\"date\">07 September 2013 05:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/qe\">[Salt Lake City] Fall Equinox: Festival of Heroes:&nbsp;<span class=\"date\">21 September 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/px\">[Washington DC] Robin Hanson visits to talk about prediction markets:&nbsp;<span class=\"date\">08 September 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/qg\">Washington, DC: Goals:&nbsp;<span class=\"date\">31 August 2014 03:00PM</span></a></li>\n<li><a href=\"/meetups/qm\">West LA&mdash;What, Exactly, Is a Person?:&nbsp;<span class=\"date\">11 September 2013 07:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tudnA3p35qp6fxzgu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.3395542547284205e-06, "legacy": true, "legacyId": "24043", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QN7trqHoWWJzppKJ2", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-13T20:49:51.412Z", "modifiedAt": null, "url": null, "title": "Notes on Brainwashing & 'Cults'", "slug": "notes-on-brainwashing-and-cults", "viewCount": null, "lastCommentedAt": "2022-03-12T00:08:15.594Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TiG8cLkBRW4QgsfrR/notes-on-brainwashing-and-cults", "pageUrlRelative": "/posts/TiG8cLkBRW4QgsfrR/notes-on-brainwashing-and-cults", "linkUrl": "https://www.lesswrong.com/posts/TiG8cLkBRW4QgsfrR/notes-on-brainwashing-and-cults", "postedAtFormatted": "Friday, September 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Notes%20on%20Brainwashing%20%26%20'Cults'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANotes%20on%20Brainwashing%20%26%20'Cults'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTiG8cLkBRW4QgsfrR%2Fnotes-on-brainwashing-and-cults%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Notes%20on%20Brainwashing%20%26%20'Cults'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTiG8cLkBRW4QgsfrR%2Fnotes-on-brainwashing-and-cults", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTiG8cLkBRW4QgsfrR%2Fnotes-on-brainwashing-and-cults", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4724, "htmlBody": "<blockquote>\n<p>&ldquo;Brainwashing&rdquo;, as popularly understood, does not exist or is of almost zero effectiveness. The belief stems from American panic over Communism post-Korean War combined with fear of new religions and sensationalized incidents; in practice, &ldquo;cults&rdquo; have retention rates in the single percentage point range and ceased to be an issue decades ago. Typically, a conversion sticks because an organization provides value to its members.</p>\n</blockquote>\n<p><a id=\"more\"></a></p>\n<p>Some old SIAI work of mine. Researching this was very difficult because the relevant religious studies area, while apparently completely repudiating most public beliefs about the subject (eg. the effectiveness of brainwashing, how damaging cults are, how large they are, whether that&rsquo;s even a meaningful category which can be distinguished from mainstream religions rather than a hidden inference - a claim, I will note, which is much more plausible when you consider how abusive Scientology is to its members as compared to how abusive the Catholic Church has been etc), prefer to publish their research in book form, which makes it very hard to review any of it. Some of the key citation were papers - but the cult panic was so long ago that most of them are not online or have been digitized! I recently added some cites and realized I had not touched the draft in a year; so while this collection of notes is not really up to my preferred standards, I&rsquo;m simply posting it for what it&rsquo;s worth. (One lesson to take away from this is that controlling uploaded human brains will not be nearly as simple &amp; easy as applying classic &lsquo;brainwashing&rsquo; strategies - because those don&rsquo;t actually work.)</p>\n<p>Reading through the literature and especially the law review articles (courts flirted disconcertingly much with licensing kidnapping and abandoning free speech), I was reminded very heavily - and not in a good way - of the War on Terror.</p>\n<p>Old American POW studies:</p>\n<ul>\n<li>Clark et al 1981 <em>Destructive Cult Conversion: Theory, Research and Practice</em></li>\n<li>Lifton 1961 <em>Thought Reform and the Psychology of Totalism</em></li>\n<li>Ross &amp; Langone 1988 <em>Cults: What Parents Should Know</em></li>\n<li>Schein, Schneier &amp; Barker 1961 <em>Coercive Persuasion</em></li>\n<li>Singer 1978, 1979 &ldquo;Therapy with Ex-cult Members&rdquo; <em>Journal of the National Association of Private Psychiatric Hospitals</em>; &ldquo;Coming Out of the Cults&rdquo;, <em>Psychology Today</em></li>\n</ul>\n<p>Started the myth of effective brain-washing. But in practice, cult attrition rates are very high! (As makes sense: if cults did not have high attrition rates, they would long ago have dominated the world due to exponential growth.) This attrition claim is made all over the literature, with some example citations being:</p>\n<ul>\n<li>Barker 1984, 1987 <em>The Making of a Moonie: Choice of Brainwashing?</em>; &ldquo;Quo Vadis? The Unification Church&rdquo;, pg141-152, <em>The Future of New Religious Movements</em></li>\n<li>Beckford 1981 &ldquo;Conversion and Apostasy: Antithesis or Complementarity?&rdquo;</li>\n<li>Bird &amp; Reimer 1982 &ldquo;Participation rates in new religious movements and para-religious movements&rdquo;</li>\n<li>Robbins 1988 <em>Cults, Converts and Charisma</em></li>\n<li>Shupe &amp; Bromley 1980 <em>The New Vigilantes: Deprogrammers, Anticultists and the New Religions</em></li>\n<li>Wright &amp; Piper 1986 &ldquo;Families and Cults: Familial Factors Related to Youth Leaving or Remaining in Deviant Religious Groups&rdquo;</li>\n<li>Wright 1983, 1987, 1988 &ldquo;Defection from New Religious Movements: A Test of Some Theoretical Propositions&rdquo; pg106-121 <em>The Brainwashing/Deprogramming Controversy</em>; <em>Leaving Cults: The Dynamics of Defection</em>; &ldquo;Leaving New Religious Movements: Issues, Theory and Research&rdquo;, pg143-165 <em>Falling from the Faith: Causes and Consequences of Religious Apostasy</em></li>\n<li>Wikipedia cites <em>The Handbook of Cults and Sects in America</em>, Hadden, J and Bromley, D eds. (1993)</li>\n<li>\n<p>a back of the envelope estimate for Scientology by <a href=\"http://www.holysmoke.org/cos/retention.htm\">Steve Plakos</a> in 2000:</p>\n<blockquote>\n<p>In absolute numbers, that is from 8 million exposed to 150k active current, it means they&rsquo;ve lost 7,850,000 bodies in the shop. That equates to a Retention Rate of 1.875%. Now, to be fair, over the course of 50 years &ldquo;X&rdquo; number of scientologists have dropped their bodies and gone off to Mars, etc,. who might still be members today if they weren&rsquo;t dead We do not know what the mortality rate is for Scientologists. To significantly impact the RR, there would have to have been a 100% turn over in active membership due to generational shifting. There is no evidence that 150,000 active members of the CofS have died over the past 50 years. Beyond that, we would also need to apply the RR to deceased members to see what number would have continued beyond 15 years. Therefore, using the most favorable membership numbers and not discounting for lose of membership beyond the 15th year, we see a RR of 1.875%+&ldquo;X&rdquo;. If we assume that generational shifting accounts for a 10% turnover amongst current membership, that is, that the current membership would be 10% greater had members survived, X would equal 15,000 dead members, or, a total Retained Membership of 165,000. That would give the CofS a 50 year Retention Rate of 2.0625%.</p>\n</blockquote>\n</li>\n</ul>\n<p>Iannaccone 2003, <a href=\"http://www.religionomics.com/archives/file_download/30/Iannaccone+-+Market+for+Martyrs.pdf\">&ldquo;The Market for Martyrs&rdquo;</a> (quasi-review)</p>\n<blockquote>\n<p>From the late-1960s through the mid-1980s, sociologists devoted immense energy to the study of New Religious Movements. [For overviews of the literature, see Bromley (1987), Robbins (1988), and Stark (1985).] They did so in part because NRM growth directly contradicted their traditional theories of secularization, not to mention the sensational mid-sixties claims God was &ldquo;dead&rdquo; (Cox 1966; Murchland 1967). NRM&rsquo;s also were ideal subjects for case stud ies, on account of their small size, brief histories, distinctive practices, charismatic leaders, devoted members, and rapid evolution. But above all, the NRM&rsquo;s attracted attention because they <em>scared</em> people.</p>\n<p>&hellip;We have trouble recalling the fear provoked by groups like the Krishnas, Moonies, and Rajneeshees. Their years of explosive growth are long past, and many of their &ldquo;strange&rdquo; ideas have become staples of popular culture. [We see this influence not only in today&rsquo;s New Age and Neo-Pagan movements, but also in novels, music, movies, TV shows, video games, university courses, environmentalism, respect for &ldquo;cultural diversity,&rdquo; and the intellectual elite&rsquo;s broad critique of Christian culture.] But they looked far more threatening in the seventies and eighties, especially after November 18, 1978. On that day, the Reverend Jim Jones, founder of the People&rsquo;s Temple, ordered the murder of a U.S. Congressman followed by the mass murder/suicide of 913 members of his cult, including nearly 300 children.</p>\n<p>The &ldquo;cults&rdquo; aggressively proselytized and solicited on sidewalks, airports, and shopping centers all over America. They recruited young adults to the dismay of their parents. Their leaders promoted bizarre beliefs, dress, and diet. Their members often lived communally, devoted their time and money to the group, and adopted highly deviant lifestyles. Cults were accused of gaining converts via deception and coercion; funding themselves through illegal activities; preying upon people the young, alienated, or mentally unstable ; luring members into strange sexual liaisons; and using force, drugs, or threats to deter the exit of disillusioned members. The accusations were elaborated in books, magazine articles, newspaper accounts, and TV drama. By the late-1970s, public concern and media hype had given birth to anti-cult organizations, anti-cult legislation, and anti-cult judicial rulings. The public, the media, many psychologists, and the courts largely accepted the claim that cults could &ldquo;brainwash&rdquo; their members, thereby rendering them incapable of rational choice, including the choice to leave. [Parents hired private investigators to literally kidnap their adult children and subject them to days of highly-coercive &ldquo;deprogramming.&rdquo; Courts often agreed that these violations of normal constitutional rights were justified, given the victim&rsquo;s presumed inability to think and act rationally (Anthony 1990; Anthony and Robbins 1992; Bromley 1983; Richardson 1991; Robbins 1985).]</p>\n<p>We now know that nearly all the anti-cult claims were overblown, mistaken, or outright lies. Americans no longer obsess about Scientology, Transcendental Meditation, or the Children of God. But a large body of research remains. <em>It witnesses to the ease with which the public, media, policy-makers, and even academics accept irrationality as an explanation for behavior that is new, strange, and (apparently or actually) dangerous.</em></p>\n<p>&hellip;As the case stud ies piled up, it became apparent that both the media stereotypes (of sleep-deprived, sugar-hyped, brainwashed automatons) and academic theories (of alienated, authoritarian, neurotics) were far off mark. Most cult converts were children of privilege raised by educated parents in suburban homes. Young, healthy, intelligent, and college educated, they could look forward to solid careers and comfortable incomes. [Rodney Stark (2002) has recently shown that an analogous result holds for Medieval saints - arguably the most dedicated &ldquo;cult converts&rdquo; of their day.]</p>\n<p>Psychologists searched in vain for a prevalence of &ldquo;authoritarian personalities,&rdquo; neurotic fears, repressed anger, high anxiety, religious obsession, personality disorders, deviant needs, and other mental pathologies. The y likewise failed to find alienation, strained relationships, and poor social skills. In nearly all respects - economically, socially, psychologically - the typical cult converts tested out normal. Moreover, nearly all those who left cults after weeks, months, or even years of membership showed no sign of physical, mental, or social harm. Normal background and circumstances, normal personalities and relationships, and a normal subsequent life - this was the &ldquo;profile&rdquo; of the typical cultist.</p>\n<p>&hellip;Numerous studies of cult recruitment, conversion, and retention found no evidence of &ldquo;brainwashing.&rdquo; The Moonies and other new religious movements did indeed devote tremendous energy to outreach and persuasion, but they employed conventional methods and enjoyed very limited success. In the most comprehensive study to date, Eileen Barker (1984) could find no evidence that Moonie recruits were ever kidnapped, confined, or coerced (though it was true that some anti-cult &ldquo;deprogrammers&rdquo; kidnapped and restrained converts so as to &ldquo;rescue&rdquo; them from the movement). Seminar participants were not deprived of sleep; the food was &ldquo;no worse than that in most college residences;&rdquo; the lectures were &ldquo;no more trance-inducing than those given everyday&rdquo; at many colleges; and there was very little chanting, no drugs or alcohol, and little that could be termed &ldquo;frenzy&rdquo; or &ldquo;ecstatic&rdquo; experience (Barker 1984). People were free to leave, and leave they did - in droves.</p>\n<p>Barker&rsquo;s comprehensive enumeration showed that among the relatively modest number of recruits who went so far as to attend two-day retreats (claimed to be Moonies&rsquo; most effective means of &ldquo;brainwashing&rdquo;), fewer than 25% joined the group for more than a week, and only 5% remained full-time members 1 year later. Among the larger numbers who visited a Moonie centre, not 1 in 200 remained in the movement 2 years later. With failure rates exceeding 99.5%, it comes as no surprise that full-time Moonie membership in the U.S. never exceeded a few thousand. And this was one of the most successful cults of the era! Once researchers began checking, rather than simply repeating the numbers claimed by the groups, defectors, or journalists, they discovered dismal retention rates in nearly all groups. [For more on the prevalence and process of cult defection, see Wight (1987) and Bromley (1988).] By the mid-1980s, researchers had so thoroughly discredited &ldquo;brainwashing&rdquo; theories that both the Society for the Scientific Study of Religion and the American Sociological Association agreed to add their names to an amicus brief denouncing the theory in court (Richardson 1985).</p>\n<ul>\n<li>Anthony, Dick, and Thomas Robbins. 1992. <a href=\"https://dl.dropboxusercontent.com/u/182368464/1992-anthony.pdf\">&ldquo;Law, Social Science and the &lsquo;Brainwashing&rsquo; Exception to the First Amendment&rdquo;</a>. <em>Behavioral Sciences and the Law</em> 10:5-29.</li>\n<li>Anthony, Dick (Ed.). 1990. <em>Religious Movements and Brainwashing Litigation: Evaluating Key Testimony</em>. New Brunswick, NJ: Transaction Publishers.</li>\n<li>Bromley, David and Richardson, James T. 1983. <em>The Brainwashing/Deprogramming Controversy: Sociological, Psychological, Legal, and Historical Perspectives</em>. New York: The Edwin Mellen Press.</li>\n<li>Bromley, David G., and Phillip E Hammond. 1987. <em>The Future of new religious movements</em>. Macon, Ga. :: Mercer University Press.</li>\n<li>Bromley, David G. 1988. <em>Falling From the Faith; Causes and Consequences of Religious Apostasy</em>. London: Sage Publications.</li>\n<li>Cox, Harvey. 1966. <em>The Secular City: Secularization and Urbanization in Theological Perspective</em>. New York, NY: Macmillan.</li>\n<li>Barker, Eileen. 1984. <em>The Making of A Moonie: Choice or Brainwashing?</em> Oxford: Basil Blackwell.</li>\n<li>Murchland, Bernard (Ed.). 1967. <em>The Meaning of the Death of God: Protestant, Jewish and Catholic Scholars Explore Atheistic Theology</em>. New York: Random House.</li>\n<li>Richardson, James T. 1985. <a href=\"https://dl.dropboxusercontent.com/u/182368464/1985-richardson.pdf\">&ldquo;The active vs.&nbsp;passive convert: paradigm conflict in conversion/recruitment research&rdquo;</a>. <em>Journal for the Scientific of Religion</em> 24:163-179.</li>\n<li>Richardson, James T. 1991. <a href=\"http://jcs.oxfordjournals.org/content/33/1/55.full.pdf\">&ldquo;Cult/Brainwashing Cases and Freedom of Religion&rdquo;</a>. <em>Journal of Church and State</em> 33:55-74.</li>\n<li>Robbins, Thomas. 1985. <a href=\"https://dl.dropboxusercontent.com/u/182368464/1985-robbins.pdf\">&ldquo;New Religious Movements, Brainwashing, and Deprogramming - The View from the Law Journals: A Review Essay and Survey&rdquo;</a>. <em>Religious Studies Review</em> 11:361-370.</li>\n<li>Robbins, Thomas. 1988. Cults, <em>Converts and Charisma: The Sociology of New Religious Movements</em>. London: Sage.</li>\n<li>Stark, Rodney (Ed.). 1985. <em>Religious Movements: Genesis, Exodus, and Numbers</em>. New York: Paragon House Publishers.</li>\n<li>Rodney Stark 2002. <a href=\"http://asketikos.info/pdfarticles/stark.pdf\">&ldquo;Upper Class Asceticism: Social Origins of Ascetic Movements and Medieval Saints&rdquo;</a>. Working Paper.</li>\n<li>Wright, Stuart A. 1987. <em>Leaving Cults: The Dynamics of Defection</em>. Washington D.C.: Society for the Scientific Study of Religion.</li>\n</ul>\n</blockquote>\n<p>Singer in particular has been heavily criticized; <a href=\"http://dl.dropbox.com/u/85192141/1991-richardson.pdf\">&ldquo;Cult/Brainwashing Cases and Freedom of Religion&rdquo;</a>, Richardson 1991:</p>\n<blockquote>\n<p>Dr.&nbsp;Singer is a clinical psychologist in private practice who earns a considerable portion of her income from cult cases. She has been an adjunct professor at the University of California at Berkeley, but has never held a paid or tenured-track position there. See H. Newton Malony, &ldquo;Anticultism: The Ethics of Psychologists&rsquo; Reactions to New Religions,&rdquo; presented at annual meeting of the American Psychological Association (New York, 1987) and Anthony, &ldquo;Evaluating Key Testimony&rdquo; for more details on Singer&rsquo;s career.</p>\n<p>&hellip;The [amicus curiae] brief further claimed that Singer misrepresents the tradition of research out of which terms like &ldquo;thought reform&rdquo; and &ldquo;coercive persuasion&rdquo; come. She ignores the fact that these earlier studies focus on physical coercion and fear as motivators, and that even when using such tactics the earlier efforts were not very successful. With great facility, Singer moves quickly from situations of physical force to those where none is applied, claiming that these &ldquo;&lsquo;second generation&rsquo;&rdquo; thought reform techniques using affection are actually more effective than the use of force in brainwashing people to become members. Thus, Singer is criticized for claiming to stand squarely on the tradition of research developed by scholars such as Edgar Schein and Robert Lifton, while she shifts the entire focus to non-coercive situations quite unlike those encountered in Communist China or Korean prisoner of war camps. The brief points out, as well, that Singer ignores a vast amount of research supporting the conclusion that virtually all who participate in the new religions do so voluntarily, and for easily understandable reasons. No magical &ldquo;black box&rdquo; of brainwashing is needed to explain why significant numbers of young people chose, in the 1960s and 1970s, to abandon their place in society and experiment with alternative life styles and beliefs. Many youth were leaving lifestyles that they felt were hypocritical, and experimenting with other ways of life that they found to be more fulfilling, at least temporarily. Particularly noteworthy, but ignored by Singer, are the extremely high attrition rates of all the new religions. These groups are actually very small in numbers (the Hare Krishna and the Unification Church each have no more than two to three thousand members nationwide), which puts the lie to brainwashing claims. If &ldquo;brainwashing&rdquo; practiced by new religions is so powerful, why are the groups experiencing so much voluntary attrition, and why are they so small?</p>\n<p>&hellip;Considerable research reported in refereed scholarly journals and other sources supports the idea that the new religions may be serving an important ameliorative function for American society. The groups may be functioning as &ldquo;half-way houses&rdquo; for many youth who have withdrawn from society, but still need a place to be until they decide to &ldquo;return home.&rdquo; Participation in some new religions has been shown to have demonstrable positive effects on the psychological functioning of individuals, a finding that Singer refuses to acknowledge.</p>\n</blockquote>\n<p><a href=\"http://www.answers.org/cultsandreligions/mind_control.html\">&ldquo;Overcoming The Bondage Of Victimization: A Critical Evaluation of Cult Mind Control Theories&rdquo;</a>, Bob and Gretchen Passantino <em>Cornerstone Magazine</em> 1994:</p>\n<blockquote>\n<p>Neither brainwashing, mind control&rsquo;s supposed precursor, nor mind control itself, have any appreciable demonstrated effectiveness. Singer and other mind control model proponents are not always candid about this fact: The early brainwashing attempts were largely unsuccessful. Even though the Koreans and Chinese used extreme forms of physical coercion as well as persuasive coercion, very few individuals subjected to their techniques changed their basic world views or commitments. The CIA also experimented with brainwashing. Though not using Korean or Chinese techniques of torture, beatings, and group dynamics, the CIA did experiment with drugs (including LSD) and medical therapies such as electroshock in their research on mind control. Their experiments failed to produce even one potential Manchurian Candidate, and the program was finally abandoned.</p>\n<p>Although some mind control model advocates bring up studies that appear to provide objective data in support of their theories, such is not the case. These studies are generally flawed in several areas: (1) Frequently the respondents are not from a wide cross-section of ex-members but disproportionately are those who have been exit-counseled by mind control model advocates who tell them they were under mind control; (2) Frequently the sample group is so small its results cannot be fairly representative of cult membership in general; (3) It is almost impossible to gather data from the same individuals before cult affiliation, during cult affiliation, and after cult disaffection, so respondents are sometimes asked to answer as though they were not yet members, or as though they were still members, etc. Each of these flaws introduces unpredicatiblity and subjectivity that make such study results unreliable&hellip;The evidence against the effectiveness of mind control techniques is even more overwhelming. Studies show that the vast majority of young people approached by new religious movements (NRMs) never join despite heavy recruitment tactics. This low rate of recruitment provides ample evidence that whatever techniques of purported mind control are used as cult recruiting tools, they do not work on most people. Even of those interested enough to attend a recruitment seminar or weekend, the majority do not join the group. Eileen Barker documents [Barker, Eileen. <em>New Religious Movements: A Practical Introduction</em>. London: Her Majesty&rsquo;s Stationery Office, 1989.] that out of 1000 people persuaded by the Moonies to attend one of their overnight programs in 1979, 90% had no further involvement. Only 8% joined for more than one week, and less than 4% remained members in 1981, two years later:</p>\n<blockquote>\n<p>. . . and, with the passage of time, the number of continuing members who joined in 1979 has continued to fall. If the calculation were to start from those who, for one reason or another, had visited one of the movement&rsquo;s centres in 1979, at least 999 out of every 1,000 of those people had, by the mid-1980s, succeeded in resisting the persuasive techniques of the Unification Church.</p>\n</blockquote>\n<p>Of particular importance is that this extremely low rate of conversion is known even to Hassan, the best-known mind control model advocate whose book [Hassan, Steven. <em>Combatting Cult Mind Control</em>. Rochester, VT: Park Street Press, 1990?] is the standard text for introducing concerned parents to mind control/exit counseling. In his personal testimony of his own involvement with the Unification Church, he notes that he was the first convert to join at the center in Queens; that during the first three months of his membership he only recruited two more people; and that pressure to recruit new members was only to reach the goal of one new person per member per month, a surprisingly low figure if we are to accept the inevitable success of cult mind control techniques.</p>\n<p>Objection: High Attrition Rates Additionally, natural attrition (people leaving the group without specific intervention) was much higher than the self-claimed 65% deprogramming success figure! It is far more likely a new convert would leave the cult within the first year of his membership than it is that he would become a long term member.</p>\n</blockquote>\n<p>Gomes, <em>Unmasking the Cults</em> (Wikipedia quote):</p>\n<blockquote>\n<p>While advocates of the deprogramming position have claimed high rates of success, studies show that natural attrition rates actually are higher than the success rate achieved through deprogramming</p>\n</blockquote>\n<p><a href=\"http://www.csj.org/infoserv_bookreview/csjbkrev122spy.htm\">&ldquo;Psychological Manipulation and Society&rdquo;</a>, book review of <em>Spying in Guruland: Inside Britain&rsquo;s Cults</em>, Shaw 1994</p>\n<blockquote>\n<p>Eventually Shaw quit the Emin group. Two months later he checked in with some Emin members at the Healing Arts Festival, a psychic fair. He avoided many Emin phone invitations for him to attend another meeting. He discovered that most, if not all, of the people who joined with him had dropped out. This is consistent with what Shaw has noted about most cults and recruits: the dropout rate is high.</p>\n</blockquote>\n<p>Anthony &amp; Robbins 1992, <a href=\"https://dl.dropboxusercontent.com/u/182368464/1992-anthony.pdf\">&ldquo;Law, Social Science and the &lsquo;Brainwashing&rsquo; Exception to the First Amendment&rdquo;</a>:</p>\n<blockquote>\n<p>Lifton and Schein are also characterized in Molko (54) as attesting to the effectiveness of brainwashing, although Schein, an expert on Chinese coercive persuasion of Korean War POWs, actually thought, as do a number of scholars, that the Chinese program was relatively ineffective (Schein, 1959, p.&nbsp;332; see also Anthony, 1990a; Schefiin &amp; Opton, 1978)&hellip;Schein appears to actually have considered the communist Chinese program to be a relative &ldquo;failure&rdquo; at least, &ldquo;considering the effort devoted to it&rdquo; (Schein, 1959, p.&nbsp;332; Anthony, 1990a, p.&nbsp;302)&hellip;Various clinical and psychometric studies of devotees of well-known &ldquo;cults&rdquo; (Ross, 1983; Ungerleider &amp; Wellisch, 1979) have found little or no personality disorder or cognitive impairment.</p>\n<ul>\n<li>Ross 1983. &ldquo;Clinical profile of Hare Krishna devotees&rdquo;, <em>American Journal of Psychiatry</em></li>\n<li>Schein, E. (1959). <a href=\"https://dl.dropboxusercontent.com/u/182368464/1959-schein.pdf\">&ldquo;Brainwashing and totalitarianization in modern society&rdquo;</a>. <em>World Politics</em>, 2,430441.</li>\n<li>Ungerleider, T., &amp; Wellisch, D. K (1979). &ldquo;Coercive persuasion (brainwashing), religious cults, and deprogramming&rdquo;. <em>American Journal of Psychiatry</em> , 136,3,279-82.</li>\n</ul>\n</blockquote>\n<p><a href=\"http://linguafranca.mirror.theinfo.org/9812/allen.html\">&ldquo;Brainwashed! Scholars of cults accuse each other of bad faith&rdquo;</a>, by Charlotte Allen, <em>Lingua Franca</em> Dec/Jan 1998:</p>\n<blockquote>\n<p>Zablocki&rsquo;s conversion to brainwashing theory may sound like common sense to a public brought up on TV images of zombielike cultists committing fiendish crimes or on the Chinese mind control experiments dramatized in the 1962 film The Manchurian Candidate. But among social scientists, brainwashing has been a bitterly contested theory for some time. No one doubts that a person can be made to behave in particular ways when he is threatened with physical force (what wouldn&rsquo;t you do with a gun pressed to your head?), but in the absence of weapons or torture, can a person be manipulated against his will?</p>\n<p>Most sociologists and psychologists who study cults think not. For starters, brainwashing isn&rsquo;t, as Zablocki himself admits, &ldquo;a process that is directly observable.&rdquo; And even if brainwashing could be isolated and measured in a clinical trial, ethical objections make conducting such a test almost unthinkable. (What sort of waivers would you have to sign before allowing yourself to be brainwashed?) In the last decade, while brainwashing has enjoyed a high profile in the media-invoked to explain sensational cult disasters from the mass suicide of Heaven&rsquo;s Gate members to the twelve sarin deaths on the Tokyo subway attributed to the Aum Shinrikyo cult-social scientists have shunned the the term as a symptom of Cold War paranoia and anticult hysteria. Instead, they favor more benign explanations of cult membership. Alternatives include &ldquo;labeling&rdquo; theory, which argues there is simply nothing sinister about alternative religions, that the problem is one of prejudicial labeling on the part of a mainstream culture that sees cult members as brainwashed dupes, and &ldquo;preexisting condition&rdquo; theory, which posits that cult members are people who are mentally ill or otherwise maladjusted before they join. (A couple of scholars have even proposed malnutrition as a preexisting condition, arguing that calcium deficiency may make people prone to charismatic susceptibility.)</p>\n<p>Thus, when Zablocki published an indignant 2-part, 60-page defense of brainwashing theory in the October 1997 and April 1998 issues of Nova Religio, a scholarly journal devoted to alternative belief systems, he ignited a furor in the field. Pointing to the &ldquo;high exit costs&rdquo; that some cults exacted from those who tried to defect-shunning, forfeiture of parental rights and property, and veiled threats-Zablocki argued that these were indications of brainwashing, signs that some groups were using psychological coercion to maintain total control over their members. Although he admitted he could not prove brainwashing empirically, he argued that at the very least brainwashing should not be dismissed out of hand.</p>\n<p>&hellip;Zablocki&rsquo;s colleagues were unimpressed. In a response also published in Nova Religio, David Bromley, a sociologist at Virginia Commonwealth University who has studied the Reverend Sun Myung Moon&rsquo;s Unification Church, complained that in Zablocki&rsquo;s formulation brainwashing remained a vague, slippery, limiting, and ultimately untestable concept. Moreover, he pointed out, cults typically have low recruitment success, high turnover rates (recruits typically leave after a few months, and hardly anyone lasts longer than two years), and short life spans, all grounds for serious skepticism about the brainwashing hypothesis. Even if you overlook these facts, Bromley added, &ldquo;the extraordinarily varied cultural origins, patterns of organizational development, and leadership styles of such groups pose a problem in explaining how they seem to have discovered the same &lsquo;brainwashing&rsquo; psycho-technology at almost precisely the same historical moment.&rdquo; A quick survey of the field reveals that Bromley is far from being the only doubter. Eileen Barker, a sociologist at the London School of Economics who has also studied the Unification Church, says, &ldquo;People regularly leave the Moonies of their own free will. The cults are actually less efficient at retaining their members than other social groups. They put a lot of pressure on them to stay in-love-bombing, guilt trips-but it doesn&rsquo;t work. They&rsquo;d like to brainwash them, but they can&rsquo;t.&rdquo;</p>\n<p>&hellip;To further complicate matters, researchers often bring very different, even conflicting approaches to their work. Psychologists, for example, tend to emphasize how a repeated environmental stimulus can elicit a conditioned response-depriving subjects of their autonomy. Sociologists, by contrast, typically endorse a voluntarist conversion model for religion, which posits that people join cults for generally rational reasons connected to the group&rsquo;s ability to satisfy their needs: for a transcendent theology; for strong bonds of kinship and solidarity; for enough social support to enable them to quit drugs or otherwise turn their personal lives around. (For example, one study has shown that schizophrenics who joined cults functioned better than those who tried drugs or conventional psychotherapy.)</p>\n<p>&hellip;In 1980 the New York state legislature, over objections from the American Civil Liberties Union, passed a bill that would have legalized deprogramming (it was vetoed by Governor Hugh Carey). &ldquo;With deprogramming-with parents having their children abducted and held captive-the whole thing became intensely emotional,&rdquo; says Thomas Robbins. &ldquo;Who were the kidnappers: the parents, the cults, or the police? There were hard feelings on both sides.&rdquo; Among the most outraged were social scientists who had never believed that people could be brainwashed into joining cults and who, as good civil libertarians, were appalled by deprogramming. Ofshe and Singer&rsquo;s scholarly testimony (and fat fees) distressed a number of these scholars, whose credentials were equally respectable and whose own research had led them to conclude that coercive persuasion was impossible in the absence of some sort of physical coercion such as prison or torture.</p>\n<p>&hellip;Zablocki made another, potentially more damning charge, however-one that Robbins did not take up. A significant amount of cult money, he wrote, has gone to scholars-in support of research, publication, conference participation, and other services. Zablocki did not name names. But a number of professors freely admit that nontraditional religions (in most cases, the Unificationists and Scientologists) have cut them checks. The list includes some of the most prominent scholars in the discipline: Bromley, Barker, Rodney Stark of the University of Washington, Jeffrey Hadden of the University of Virginia, and James Richardson, a sociologist of religion at the University of Nevada at Reno. All five have attended cult-subsidized conferences, and Bromley, Hadden, and Richardson have occasionally testified in court on behalf of cults or offered their services as expert witnesses against brainwashing theory. &ldquo;This is an issue,&rdquo; Zablocki wrote sternly, &ldquo;of a whole different ethical magnitude from that of taking research funding from the Methodists to find out why the collection baskets are not coming back as heavy as they used to.&rdquo;</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PHPHovzkfjyap9FiK": 4, "gHCNhqxuJq2bZ2akb": 2, "3uE2pXvbcnS9nnZRE": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TiG8cLkBRW4QgsfrR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 66, "extendedScore": null, "score": 0.000162, "legacy": true, "legacyId": "24150", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 66, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 105, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-14T09:21:40.971Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta Lesswrong September Meetup (2nd of 2)", "slug": "meetup-atlanta-lesswrong-september-meetup-2nd-of-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bJPDfxFNEv5nLdJpa/meetup-atlanta-lesswrong-september-meetup-2nd-of-2", "pageUrlRelative": "/posts/bJPDfxFNEv5nLdJpa/meetup-atlanta-lesswrong-september-meetup-2nd-of-2", "linkUrl": "https://www.lesswrong.com/posts/bJPDfxFNEv5nLdJpa/meetup-atlanta-lesswrong-september-meetup-2nd-of-2", "postedAtFormatted": "Saturday, September 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20Lesswrong%20September%20Meetup%20(2nd%20of%202)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20Lesswrong%20September%20Meetup%20(2nd%20of%202)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbJPDfxFNEv5nLdJpa%2Fmeetup-atlanta-lesswrong-september-meetup-2nd-of-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20Lesswrong%20September%20Meetup%20(2nd%20of%202)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbJPDfxFNEv5nLdJpa%2Fmeetup-atlanta-lesswrong-september-meetup-2nd-of-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbJPDfxFNEv5nLdJpa%2Fmeetup-atlanta-lesswrong-september-meetup-2nd-of-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 115, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/qz'>Atlanta Lesswrong September Meetup (2nd of 2)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 September 2013 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Apt L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come join us for the second meetup for the month of September! We'll be doing our normal eclectic mix of self-improvement brainstorming, educational mini-presentations, structured discussion, unstructured discussion, and social fun and games times!</p>\n\n<p>Please contact me if you have cat allergies, as our meeting space has cats. Incredibly cute cats.</p>\n\n<p>And check out ATLesswrong's facebook group, if you haven't already: https://www.facebook.com/groups/100137206844878/\nwhere you can connect with Atlanta Lesswrongers and suggest a topics for discussion at this meetup!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/qz'>Atlanta Lesswrong September Meetup (2nd of 2)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bJPDfxFNEv5nLdJpa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.340473891999473e-06, "legacy": true, "legacyId": "24157", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Lesswrong_September_Meetup__2nd_of_2_\">Discussion article for the meetup : <a href=\"/meetups/qz\">Atlanta Lesswrong September Meetup (2nd of 2)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 September 2013 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2388 Lawrenceville Hwy. Apt L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come join us for the second meetup for the month of September! We'll be doing our normal eclectic mix of self-improvement brainstorming, educational mini-presentations, structured discussion, unstructured discussion, and social fun and games times!</p>\n\n<p>Please contact me if you have cat allergies, as our meeting space has cats. Incredibly cute cats.</p>\n\n<p>And check out ATLesswrong's facebook group, if you haven't already: https://www.facebook.com/groups/100137206844878/\nwhere you can connect with Atlanta Lesswrongers and suggest a topics for discussion at this meetup!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Lesswrong_September_Meetup__2nd_of_2_1\">Discussion article for the meetup : <a href=\"/meetups/qz\">Atlanta Lesswrong September Meetup (2nd of 2)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta Lesswrong September Meetup (2nd of 2)", "anchor": "Discussion_article_for_the_meetup___Atlanta_Lesswrong_September_Meetup__2nd_of_2_", "level": 1}, {"title": "Discussion article for the meetup : Atlanta Lesswrong September Meetup (2nd of 2)", "anchor": "Discussion_article_for_the_meetup___Atlanta_Lesswrong_September_Meetup__2nd_of_2_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-14T15:19:37.300Z", "modifiedAt": null, "url": null, "title": "[link] \"The Survival of Humanity\"", "slug": "link-the-survival-of-humanity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:28.773Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pablo_Stafforini", "createdAt": "2009-03-24T12:56:00.130Z", "isAdmin": false, "displayName": "Pablo"}, "userId": "W7ETRtvRMqYetyQE9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GqBQhEmopi8LbKPpM/link-the-survival-of-humanity", "pageUrlRelative": "/posts/GqBQhEmopi8LbKPpM/link-the-survival-of-humanity", "linkUrl": "https://www.lesswrong.com/posts/GqBQhEmopi8LbKPpM/link-the-survival-of-humanity", "postedAtFormatted": "Saturday, September 14th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20%22The%20Survival%20of%20Humanity%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20%22The%20Survival%20of%20Humanity%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGqBQhEmopi8LbKPpM%2Flink-the-survival-of-humanity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20%22The%20Survival%20of%20Humanity%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGqBQhEmopi8LbKPpM%2Flink-the-survival-of-humanity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGqBQhEmopi8LbKPpM%2Flink-the-survival-of-humanity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 726, "htmlBody": "<p><a href=\"http://blogs.scientificamerican.com/guest-blog/2013/09/13/the-survival-of-humanity/\">The Survival of Humanity</a>, by Lawrence Rifkin (September 13, 2013). Some excerpts:</p>\n<blockquote>\n<p>An existential catastrophe would obliterate or severely limit the existence of all future humanity.</p>\n<p>As defined by Nick Bostrom at Oxford University, an existential catastrophe is one which extinguishes Earth-originating intelligent life or permanently destroys a substantial part of its potential. &nbsp;As such it must be considered a harm of unfathomable magnitude, far beyond tragedy affecting those alive at the time. Because such risks jeopardize the entire future of humankind and conscious life, even relatively small probabilities, especially when seen statistically over a long period of time, may become significant in the extreme. It would follow that if such risks are non-trivial, the importance of existential catastrophes dramatically eclipse most of the social and political issues that commonly ignite our passions and tend to get our blood boiling today. [...]</p>\n<p>One would think that if we are mobilized to fight for issues that affect a relatively small number of people, we would have an even stronger moral and social emotional motivation to prevent potential catastrophes that could kill or incapacitate the entire human population. But there are significant psychological barriers to overcome. People who would be emotionally crushed just hearing about a tortured child or animal may not register even the slightest emotional response when contemplating the idea that all human life may one day become extinct. As Eliezer Yudkowsky wrote, &ldquo;The challenge of existential risks to rationality is that, the catastrophes being so huge, people snap into a different mode of thinking.&rdquo; [...]</p>\n<p>Here is a partial list of suggestion worthy of consideration. The idea here is not to advocate for some extreme survivalist or &ldquo;Chicken Little&rdquo; mentality, but rather to use reason, foresight, and judgment about how best to protect our future.</p>\n<p>\n<ul>\n<li>Create a larger worldwide stockpile of grains and other food reserves.</li>\n<li>Support and prioritize global measures to detect, prevent, and halt emerging pandemic infectious diseases, such as the WHO&rsquo;s The Global Outbreak Alert and Response Network.</li>\n<li>Invest in technologies to discover and deflect large asteroids and comets on a deadly collision course with our planet.</li>\n<li>Consider banning the synthesis and public publication of the genome sequences of deadly microorganisms such as smallpox and the 1918 influenza virus, thereby reducing the risks of bioterrorism or accidental release.</li>\n<li>Maintain stores in multiple locations of wild plant species, seed banks, and gene banks to safeguard genetic diversity.</li>\n<li>Invest in space station research. Because of the Sun&rsquo;s ultimate expansion heating up the planet, Earth will become uninhabitable for humans in about 1-1.5 billion years (it will become uninhabitable for all life on Earth several billion years after that). This is, understandably, almost too long from now to contemplate. Nonetheless, our best (and possibly only) chance for survival in the very distant future may be to live in space or to colonize other planets or moons.</li>\n<li>Create strains of agricultural species better able to withstand major environmental change and threats.</li>\n<li>Continue to strive towards scientific accuracy in predicting climate change effects, and work towards renewable energy sources, sustainable use, technological solutions, and other measures to prevent potential climate catastrophes. Human-caused environmental changes that increase the risk of global pandemics deserve particular attention.</li>\n<li>Develop appropriate oversight of new molecular manufacturing technologies.</li>\n<li>Prioritize international cooperation to reduce nuclear proliferation, secure existing nuclear weapons, develop systems to minimize technological mishaps, and decrease the world&rsquo;s nuclear armamentarium.</li>\n<li>Maintain a well-chosen small number of people in a deep, well protected refugee sanctuary, with adequate supplies to last for years to buffer against human extinction from a range of causes. Genetically diverse international volunteers who live in such a bunker could be rotated, say, every two months. A similar Noah&rsquo;s ark refuge could be established on a space station.</li>\n<li>Work towards changing the social conditions that foster ideological absolutism.</li>\n<li>Promote evidence-based thinking and reason at all levels of society.</li>\n<li>Plan in detail to quickly produce and administer vaccines and other medical interventions during a pandemic.</li>\n</ul>\nThe idea is not that we should do all these, but that the issue deserves our very highest consideration.&nbsp;</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GqBQhEmopi8LbKPpM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 3, "extendedScore": null, "score": 1.3407897240194476e-06, "legacy": true, "legacyId": "24158", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-15T00:45:34.682Z", "modifiedAt": null, "url": null, "title": "College courses versus LessWrong", "slug": "college-courses-versus-lesswrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:31.032Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VipulNaik", "createdAt": "2013-09-02T18:51:08.862Z", "isAdmin": false, "displayName": "VipulNaik"}, "userId": "t3pZcNZXqhaM5avBE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MMbzc7MDb64Fekk7s/college-courses-versus-lesswrong", "pageUrlRelative": "/posts/MMbzc7MDb64Fekk7s/college-courses-versus-lesswrong", "linkUrl": "https://www.lesswrong.com/posts/MMbzc7MDb64Fekk7s/college-courses-versus-lesswrong", "postedAtFormatted": "Sunday, September 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20College%20courses%20versus%20LessWrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACollege%20courses%20versus%20LessWrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMMbzc7MDb64Fekk7s%2Fcollege-courses-versus-lesswrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=College%20courses%20versus%20LessWrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMMbzc7MDb64Fekk7s%2Fcollege-courses-versus-lesswrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMMbzc7MDb64Fekk7s%2Fcollege-courses-versus-lesswrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 312, "htmlBody": "<p>Compared to many of the people reading this, I've not participated extensively on LessWrong. In fact, I created my account only about a week ago. That said, I <em>have</em> read many LessWrong articles by contributors such as <a href=\"/user/Eliezer_Yudkowsky\">Eliezer</a>, <a href=\"/user/JonahSinick/\">Jonah</a>, <a href=\"/user/Yvain/\">Yvain</a>, <a href=\"/user/Gwern/\">Gwern</a>, and many others (if I missed you, my apologies). I wouldn't say it was a huge transformative experience. But I have probably learned a bit more from LessWrong than I learned sitting in on a class by Nobel Prize-winning economist <a href=\"http://en.wikipedia.org/wiki/Gary_Becker\">Gary Becker</a> on <a href=\"http://en.wikipedia.org/wiki/Human_capital\">human capital</a> (without formally registering for the class or doing the coursework). I've learned more of value from LessWrong than all the <a href=\"http://ocw.mit.edu\">MIT OpenCourseWare</a> lectures I've consumed. There are a few online experiences, such as <a href=\"http://econlog.econlib.org\">reading EconLog</a>, that have been more educational for me than LessWrong, but I can count these on the fingers of one hand.</p>\n<p>Some of my friends have claimed that reading LessWrong systematically (and perhaps participating in the comments and attempting to write posts) would generate more value for an undergraduate than a typical core college class (with the possible exception of technical classes specific to the person's major or area of specialization). I'm curious about whether readers agree with this assessment. Do you feel, for instance, that LessWrong provided you with more valuable human capital than your introductory general chemistry sequence? What about comparing LessWrong with an undergraduate \"intro to philosophy\" class? Or an undergraduate intro class on the history of economic thought? At what percentile would you rank LessWrong relative to your college classes?</p>\n<p>A second related question is whether there's a possibility of building a college course -- or college-like course, perhaps a <a href=\"http://en.wikipedia.org/wiki/Massive_open_online_course\">MOOC</a> -- specifically revolving around mastery of the content in LessWrong (perhaps starting with the <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">Sequences</a>). Would such a college course be possible to design in principle? How would such a college course compare with core requirements for undergraduates today?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MMbzc7MDb64Fekk7s", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 1.341289371811968e-06, "legacy": true, "legacyId": "24159", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-15T03:13:36.553Z", "modifiedAt": null, "url": null, "title": "Help MIRI run its Oxford UK workshop in November", "slug": "help-miri-run-its-oxford-uk-workshop-in-november", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F6wRgFH9kz63LzTAB/help-miri-run-its-oxford-uk-workshop-in-november", "pageUrlRelative": "/posts/F6wRgFH9kz63LzTAB/help-miri-run-its-oxford-uk-workshop-in-november", "linkUrl": "https://www.lesswrong.com/posts/F6wRgFH9kz63LzTAB/help-miri-run-its-oxford-uk-workshop-in-november", "postedAtFormatted": "Sunday, September 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20MIRI%20run%20its%20Oxford%20UK%20workshop%20in%20November&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20MIRI%20run%20its%20Oxford%20UK%20workshop%20in%20November%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF6wRgFH9kz63LzTAB%2Fhelp-miri-run-its-oxford-uk-workshop-in-november%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20MIRI%20run%20its%20Oxford%20UK%20workshop%20in%20November%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF6wRgFH9kz63LzTAB%2Fhelp-miri-run-its-oxford-uk-workshop-in-november", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF6wRgFH9kz63LzTAB%2Fhelp-miri-run-its-oxford-uk-workshop-in-november", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<p>This <a href=\"http://intelligence.org/2013/08/30/miris-november-2013-workshop-in-oxford/\">November 23-29</a>, MIRI is running its first European research workshop, at Oxford University.</p>\n<p>We need somebody familiar with Oxford UK to (1) help us locate and secure lodging for the workshop participants ahead of time, (2) order food for delivery during the workshop, and (3) generally handle on-the-ground logistics.</p>\n<p><a href=\"https://docs.google.com/forms/d/1OPrgtD-ME6_3lcd7-6wQyWJCK1SGnZBq3lg6Uo0KoEA/viewform\">Apply here</a> for the chance to:</p>\n<p><ol>\n<li>Work with, and hang out with, MIRI staff.</li>\n<li>Spend some time (during breaks) with the workshop participants.</li>\n<li>Help MIRI work towards its goals.</li>\n</ol></p>\n<p>You can either volunteer to help us for free, or indicate how much you'd need to be paid per hour to take the job.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F6wRgFH9kz63LzTAB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 1.3414201136984898e-06, "legacy": true, "legacyId": "24160", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-15T03:38:08.674Z", "modifiedAt": null, "url": null, "title": "Large introductory science classes", "slug": "large-introductory-science-classes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:30.035Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VipulNaik", "createdAt": "2013-09-02T18:51:08.862Z", "isAdmin": false, "displayName": "VipulNaik"}, "userId": "t3pZcNZXqhaM5avBE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iXnhvEpZPg697R8bJ/large-introductory-science-classes", "pageUrlRelative": "/posts/iXnhvEpZPg697R8bJ/large-introductory-science-classes", "linkUrl": "https://www.lesswrong.com/posts/iXnhvEpZPg697R8bJ/large-introductory-science-classes", "postedAtFormatted": "Sunday, September 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Large%20introductory%20science%20classes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALarge%20introductory%20science%20classes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiXnhvEpZPg697R8bJ%2Flarge-introductory-science-classes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Large%20introductory%20science%20classes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiXnhvEpZPg697R8bJ%2Flarge-introductory-science-classes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiXnhvEpZPg697R8bJ%2Flarge-introductory-science-classes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 599, "htmlBody": "<p>The undergraduate course experience at a number of colleges and universities across the world includes introductory science classes. Some colleges require only science-based majors (including sciences and engineering) to take the introductory science classes, whereas others require students majoring in all subjects to take at least some introductory science classes. All my examples are drawn from the United States, but the questions that I raise are of general interest, and I'm interesting in hearing about people's thoughts based on their experiences studying or teaching at the undergraduate level, regardless of geographic location.</p>\n<p>I want to focus on one particular type of introductory science class: the <em>large introductory science class</em>. I'll identify the three key features of the courses I'm talking about:</p>\n<ul>\n<li>The class size is <em>large</em>: Classes with more than 75 students would qualify as \"large\" in my view, whereas classes with fewer than 50 students. The 50-75 range is more ambiguous, and may depend a lot on how the course is run.</li>\n<li>The class is an <em>introductory general requirement class</em>: I'm thinking of classes that first-year college students (freshmen) are generally prepared to take and would take if they have adequate prior preparation and do not intend to major in the subject. I'm excluding Honors classes here (though Honors classes are anyway rarely that large).</li>\n<li>The class is a <em>science or math class</em>: I'm including physics, chemistry, biology, mathematics, computer science, geology, and astronomy here.</li>\n</ul>\n<p>I'm interested in the following questions:</p>\n<ul>\n<li>What has been your overall experience with large introductory science classes as a student, a TA, or a professor? My general impression is that evaluations of these classes generally run more negative compared to other classes, but I hardly have access to a representative sample.</li>\n<li>To what extent did your experience (as a student) with large introductory science classes influence your decision regarding career, major, minor, and future elective classes? For instance, did a bad large intro calc class turn you off math? Did a great intro chem class make you decide to minor in chemistry?</li>\n<li>Did your large introductory science class make use of modern technological aids such as <a href=\"http://www.iclicker.com/\">clickers</a> or online discussion forums? If so, how did these affect your course experience?</li>\n</ul>\n<p>PS1: Here are a few links I get by Googling for large introductory science classes, and some of them may be of interest: <a href=\"http://www.ics.uci.edu/~kay/pubs/managing-large-courses.html\">Large Introductory Computer Science Classes: Strategies for Effective Management</a>, <a href=\"http://www.facultyfocus.com/articles/teaching-professor-blog/teaching-large-introductory-survey-courses/\">Teaching Large Introductory Survey Courses</a>, and <a href=\"http://ww.lifescied.org/content/4/2/143.short\">Introductory Biology Courses: A Framework To Support Active Learning in Large Enrollment Introductory Science Courses</a>. There is also some research indicating or suggesting that introductory science classes <a href=\"http://www.thedailyriff.com/articles/why-science-is-just-so-darn-hard-854.php\">play an important role in encouraging or discouraging the choice of STEM careers</a>, and \"fixing\" such classes has been hailed by many people as a way of getting more people into STEM careers and also potentially reducing gender or ethnic disparities in the number of people in such careers.</p>\n<p>PS2: Clicker technology has been used in a variety of large classes. The simplest implementation is to use clickers to get answers to simple multiple choice questions (correct response rates varying between 30% and 70% in general). For instance, this <a href=\"http://ocw.mit.edu/courses/chemistry/5-111-principles-of-chemical-science-fall-2008/video-lectures/lecture-7/\">chemistry lecture at MIT</a> uses clickers in the standard fashion (here's an <a href=\"http://ocw.mit.edu/courses/chemistry/5-112-principles-of-chemical-science-fall-2005/video-lectures/lecture-8-p-orbitals/\">equivalent lecture that does not make use of clickers</a>). Eric Mazur of Harvard, a <a href=\"http://www.youtube.com/watch?v=WwslBPj8GgI\">proponent of Interactive Engagement</a>, uses clickers in a <a href=\"http://clickers.commons.yale.edu/2012/01/15/peer-instruction-and-clickers-the-eric-mazur-method/\">more fancy way</a>: he first asks students to attempt the question on their own and convey their answer using a clicker, then he gives people some time to discuss and give their updated answer using a clicker.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iXnhvEpZPg697R8bJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 3, "extendedScore": null, "score": 1.341441785596837e-06, "legacy": true, "legacyId": "24163", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-15T06:33:49.203Z", "modifiedAt": null, "url": null, "title": "Meetup : Columbus, OH MEGA-MEETUP, Oct 11-13", "slug": "meetup-columbus-oh-mega-meetup-oct-11-13", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:04.475Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uxg3mLAJx8oZ8Ff6w/meetup-columbus-oh-mega-meetup-oct-11-13", "pageUrlRelative": "/posts/uxg3mLAJx8oZ8Ff6w/meetup-columbus-oh-mega-meetup-oct-11-13", "linkUrl": "https://www.lesswrong.com/posts/uxg3mLAJx8oZ8Ff6w/meetup-columbus-oh-mega-meetup-oct-11-13", "postedAtFormatted": "Sunday, September 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Columbus%2C%20OH%20MEGA-MEETUP%2C%20Oct%2011-13&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Columbus%2C%20OH%20MEGA-MEETUP%2C%20Oct%2011-13%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuxg3mLAJx8oZ8Ff6w%2Fmeetup-columbus-oh-mega-meetup-oct-11-13%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Columbus%2C%20OH%20MEGA-MEETUP%2C%20Oct%2011-13%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuxg3mLAJx8oZ8Ff6w%2Fmeetup-columbus-oh-mega-meetup-oct-11-13", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuxg3mLAJx8oZ8Ff6w%2Fmeetup-columbus-oh-mega-meetup-oct-11-13", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 613, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/r0\">Columbus, OH MEGA-MEETUP, Oct 11-14</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">12 October 2013 02:33:00AM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Columbus, OH</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p><strong>PRE-REGISTRATION REQUIRED!</strong></p>\n<p><strong>Pre-Register&nbsp;<a rel=\"nofollow\" href=\"https://docs.google.com/forms/d/1RI-ATl67yy9k3DJeNp8ohtIEwJy2LOGqPBWX3FCFonI/viewform\">HERE!</a>&nbsp; </strong>(Google Forms are occasionally buggy. If you don't get an email from me within a day or two, then then I didn't get your pre-reg.)</p>\n<p>Space is limited! The conference room can hold 50 people, 60 max, and our average meetup draws about half of that. Therefore, pre-registration is REQUIRED.</p>\n<p>Pre-registration CLOSES on OCTOBER 4. This is the last day to either pre-register, or to cancel your pre-registration. If you discover you can not come, please cancel your pre-registration so that someone else may attend.</p>\n<p>Housing is provided to out-of-towners, on a limited, first-registered, first-serve basis.</p>\n<p>If you are interested in giving a talk, leading a discussion, or workshop, please describe what you want to do on the pre-reg page.</p>\n<p><br /><strong>Pre-Register </strong><a style=\"font-weight: bold;\" rel=\"nofollow\" href=\"https://docs.google.com/forms/d/1RI-ATl67yy9k3DJeNp8ohtIEwJy2LOGqPBWX3FCFonI/viewform\">HERE!</a></p>\n<p>&nbsp;</p>\n<p><strong>SCHEDULE</strong></p>\n<p><strong>FRIDAY</strong></p>\n<p>Socializing begins at 7:30p. There may be a roundtable discussion on community building, depending on interest. (If you come early, there will be TEDxColumbus, which is over-priced, IMO.)</p>\n<p><strong>SATURDAY</strong> <br /> Official workshop begins at 3p</p>\n<p><em>Introduction- Erica Edelman (me)</em> <br /> Who We Are and What We Do.</p>\n<p><em>Down the Rabbit Hole: Magic as Psychic Entertainment -Jack Strauss</em> <br /> Magician/Mentalist Jack Strauss will present a stage act as a psychic entertainer. Afterwards, there will be a sit-down talkback with the audience. Topics will be determined by audience questions and may include: ethics of performing on stage with a psychic persona, psychology of deception, techniques of cold reading, etc. The only topic off the table will be the specifics of how the act you just saw is performed.</p>\n<p><em>Defense Against the Dark Arts: The Ethics and Psychology of Persuasion- Jesse Galef</em> <br /> How do we convince other people of what's true? What tactics work and don't work? What rhetorical or psychological strategies can we practice to make ourselves better at it? How can we make ourselves less likely to be seduced by the Dark Arts of others? And of course, what's the ethics of all of this? Are all forms of persuasion ethical? How can we make those distinctions?</p>\n<p><em>Rita Messer- Cognitive Behavioral Therapy</em> <br /> Effective Self-Help: Cognitive tricks for understanding and making your emotions work for you.</p>\n<p><em>Rationality &ndash; a synergy of Western and Eastern approaches- Don Sutterfield</em> <br /> In the western tradition (European / American), rationality is understood primarily from the perspective of the individual. In the eastern tradition, (China, Japan etc.) the fundamental framing of goals is less individualistic, given the more communitarian sense of identity that is a key component in &ldquo;eastern&rdquo; thought. In the eastern tradition, while not labeled as such, &ldquo;rationality&rdquo; takes as its subjects 1- the deconstruction of the concept and constant experience of individuality &ldquo;the illusion of self&rdquo;, and 2- the improvement of one&rsquo;s systems of response to experiences be they negative, or positive. (Mindfulness).</p>\n<p><em>Effective Altruism- Elissa Caffery Fleming<br /></em>Ever wonder how you can do the most good with your life? Is it rational to give to charity, and which ones? Lots of smart people have spent a lot of time working on those problems, and you might be surprised at what they've come up with!</p>\n<p><em>Applications of Models in Everyday Life- Eric Huff</em><br />What does it mean to think about the world using models? This is a style of thinking that's especially developed and formalized in science, but is really useful for everyday reasoning as well. I'll explain what I mean by a model, talk about the differences &nbsp;between commonly-understood Bayesian reasoning and model-checking, and give some examples pulled from my research and from everyday experience.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>SUNDAY</strong></p>\n<p><a rel=\"nofollow\" href=\"http://columbusminimakerfaire.com/\">Mini Maker's Faire</a> (free) at <a rel=\"nofollow\" href=\"http://cosi.org/\">COSI</a></p>\n<p>(Note: Friday and Sunday activities are <em>unofficial</em>, and not sponsored by our hosting organization)</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/r0\">Columbus, OH MEGA-MEETUP, Oct 11-14</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uxg3mLAJx8oZ8Ff6w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "24164", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Columbus__OH_MEGA_MEETUP__Oct_11_14\">Discussion article for the meetup : <a href=\"/meetups/r0\">Columbus, OH MEGA-MEETUP, Oct 11-14</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">12 October 2013 02:33:00AM (-0400)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Columbus, OH</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p><strong id=\"PRE_REGISTRATION_REQUIRED_\">PRE-REGISTRATION REQUIRED!</strong></p>\n<p><strong>Pre-Register&nbsp;<a rel=\"nofollow\" href=\"https://docs.google.com/forms/d/1RI-ATl67yy9k3DJeNp8ohtIEwJy2LOGqPBWX3FCFonI/viewform\">HERE!</a>&nbsp; </strong>(Google Forms are occasionally buggy. If you don't get an email from me within a day or two, then then I didn't get your pre-reg.)</p>\n<p>Space is limited! The conference room can hold 50 people, 60 max, and our average meetup draws about half of that. Therefore, pre-registration is REQUIRED.</p>\n<p>Pre-registration CLOSES on OCTOBER 4. This is the last day to either pre-register, or to cancel your pre-registration. If you discover you can not come, please cancel your pre-registration so that someone else may attend.</p>\n<p>Housing is provided to out-of-towners, on a limited, first-registered, first-serve basis.</p>\n<p>If you are interested in giving a talk, leading a discussion, or workshop, please describe what you want to do on the pre-reg page.</p>\n<p><br><strong>Pre-Register </strong><a style=\"font-weight: bold;\" rel=\"nofollow\" href=\"https://docs.google.com/forms/d/1RI-ATl67yy9k3DJeNp8ohtIEwJy2LOGqPBWX3FCFonI/viewform\">HERE!</a></p>\n<p>&nbsp;</p>\n<p><strong id=\"SCHEDULE\">SCHEDULE</strong></p>\n<p><strong id=\"FRIDAY\">FRIDAY</strong></p>\n<p>Socializing begins at 7:30p. There may be a roundtable discussion on community building, depending on interest. (If you come early, there will be TEDxColumbus, which is over-priced, IMO.)</p>\n<p><strong>SATURDAY</strong> <br> Official workshop begins at 3p</p>\n<p><em>Introduction- Erica Edelman (me)</em> <br> Who We Are and What We Do.</p>\n<p><em>Down the Rabbit Hole: Magic as Psychic Entertainment -Jack Strauss</em> <br> Magician/Mentalist Jack Strauss will present a stage act as a psychic entertainer. Afterwards, there will be a sit-down talkback with the audience. Topics will be determined by audience questions and may include: ethics of performing on stage with a psychic persona, psychology of deception, techniques of cold reading, etc. The only topic off the table will be the specifics of how the act you just saw is performed.</p>\n<p><em>Defense Against the Dark Arts: The Ethics and Psychology of Persuasion- Jesse Galef</em> <br> How do we convince other people of what's true? What tactics work and don't work? What rhetorical or psychological strategies can we practice to make ourselves better at it? How can we make ourselves less likely to be seduced by the Dark Arts of others? And of course, what's the ethics of all of this? Are all forms of persuasion ethical? How can we make those distinctions?</p>\n<p><em>Rita Messer- Cognitive Behavioral Therapy</em> <br> Effective Self-Help: Cognitive tricks for understanding and making your emotions work for you.</p>\n<p><em>Rationality \u2013 a synergy of Western and Eastern approaches- Don Sutterfield</em> <br> In the western tradition (European / American), rationality is understood primarily from the perspective of the individual. In the eastern tradition, (China, Japan etc.) the fundamental framing of goals is less individualistic, given the more communitarian sense of identity that is a key component in \u201ceastern\u201d thought. In the eastern tradition, while not labeled as such, \u201crationality\u201d takes as its subjects 1- the deconstruction of the concept and constant experience of individuality \u201cthe illusion of self\u201d, and 2- the improvement of one\u2019s systems of response to experiences be they negative, or positive. (Mindfulness).</p>\n<p><em>Effective Altruism- Elissa Caffery Fleming<br></em>Ever wonder how you can do the most good with your life? Is it rational to give to charity, and which ones? Lots of smart people have spent a lot of time working on those problems, and you might be surprised at what they've come up with!</p>\n<p><em>Applications of Models in Everyday Life- Eric Huff</em><br>What does it mean to think about the world using models? This is a style of thinking that's especially developed and formalized in science, but is really useful for everyday reasoning as well. I'll explain what I mean by a model, talk about the differences &nbsp;between commonly-understood Bayesian reasoning and model-checking, and give some examples pulled from my research and from everyday experience.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"SUNDAY\">SUNDAY</strong></p>\n<p><a rel=\"nofollow\" href=\"http://columbusminimakerfaire.com/\">Mini Maker's Faire</a> (free) at <a rel=\"nofollow\" href=\"http://cosi.org/\">COSI</a></p>\n<p>(Note: Friday and Sunday activities are <em>unofficial</em>, and not sponsored by our hosting organization)</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Columbus__OH_MEGA_MEETUP__Oct_11_141\">Discussion article for the meetup : <a href=\"/meetups/r0\">Columbus, OH MEGA-MEETUP, Oct 11-14</a></h2>", "sections": [{"title": "Discussion article for the meetup : Columbus, OH MEGA-MEETUP, Oct 11-14", "anchor": "Discussion_article_for_the_meetup___Columbus__OH_MEGA_MEETUP__Oct_11_14", "level": 1}, {"title": "PRE-REGISTRATION REQUIRED!", "anchor": "PRE_REGISTRATION_REQUIRED_", "level": 2}, {"title": "SCHEDULE", "anchor": "SCHEDULE", "level": 2}, {"title": "FRIDAY", "anchor": "FRIDAY", "level": 2}, {"title": "SUNDAY", "anchor": "SUNDAY", "level": 2}, {"title": "Discussion article for the meetup : Columbus, OH MEGA-MEETUP, Oct 11-14", "anchor": "Discussion_article_for_the_meetup___Columbus__OH_MEGA_MEETUP__Oct_11_141", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-15T10:26:27.715Z", "modifiedAt": null, "url": null, "title": "Winter Solstice 2013 Kickstarter - The Big One. ", "slug": "winter-solstice-2013-kickstarter-the-big-one", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:58.107Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bJSuBC35NHNZXrB2x/winter-solstice-2013-kickstarter-the-big-one", "pageUrlRelative": "/posts/bJSuBC35NHNZXrB2x/winter-solstice-2013-kickstarter-the-big-one", "linkUrl": "https://www.lesswrong.com/posts/bJSuBC35NHNZXrB2x/winter-solstice-2013-kickstarter-the-big-one", "postedAtFormatted": "Sunday, September 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Winter%20Solstice%202013%20Kickstarter%20-%20The%20Big%20One.%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWinter%20Solstice%202013%20Kickstarter%20-%20The%20Big%20One.%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbJSuBC35NHNZXrB2x%2Fwinter-solstice-2013-kickstarter-the-big-one%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Winter%20Solstice%202013%20Kickstarter%20-%20The%20Big%20One.%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbJSuBC35NHNZXrB2x%2Fwinter-solstice-2013-kickstarter-the-big-one", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbJSuBC35NHNZXrB2x%2Fwinter-solstice-2013-kickstarter-the-big-one", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 835, "htmlBody": "<blockquote class=\"gmail_quote\" style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: #cccccc; padding-left: 1ex;\"><em><span style=\"font-family: Times;\"><span style=\"font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Like most things, winter was once a mystery.</span></span>&nbsp;</em></blockquote>\n<blockquote class=\"gmail_quote\" style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: #cccccc; padding-left: 1ex;\"><em><br /><span style=\"font-family: Arial; color: #333333; vertical-align: baseline; white-space: pre-wrap;\">The world got cold, and dark. Life became fragile. People died. And they didn't know what was happening or understand why. They desperately threw festivals in honor of sun gods with all-too-human motivations, and prayed for the light's return.</span>&nbsp;</em></blockquote>\n<blockquote class=\"gmail_quote\" style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: #cccccc; padding-left: 1ex;\"><em><br /><span style=\"font-family: Arial; color: #333333; vertical-align: baseline; white-space: pre-wrap;\">It didn't help. Though we did discover that throwing parties in the middle of winter is an excellent idea.</span>&nbsp;</em></blockquote>\n<blockquote class=\"gmail_quote\" style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: #cccccc; padding-left: 1ex;\"><em><br /><span style=\"font-family: Arial; color: #333333; vertical-align: baseline; white-space: pre-wrap;\">But then something incredible and beautiful happened. We studied the sky. We invented astronomy, and other sciences. We began a long journey towards truly understanding our place in the universe. And we used that knowledge to plan for the future, and make our world better. Five thousand years later, the winter isn't so scary. But the symbol of the solstice - the departure and return of the sun - is still powerful. The work we have done to transform winter from a terrifying season of darkness into a modern festival of light deserves a reverence with all the weight of an ancient cultural cornerstone.</span></em></blockquote>\n<blockquote class=\"gmail_quote\" style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: #cccccc; padding-left: 1ex;\"><br /></blockquote>\n<blockquote class=\"gmail_quote\" style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: #cccccc; padding-left: 1ex;\"><em><span style=\"font-family: Arial; color: #333333; vertical-align: baseline; white-space: pre-wrap;\">And the work we have yet to do, to fully explore humanity's potential, is even more inspiring.</span></em></blockquote>\n<blockquote class=\"gmail_quote\" style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: #cccccc; padding-left: 1ex;\"><br /></blockquote>\n<blockquote class=\"gmail_quote\" style=\"font-size: 13px; margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: #cccccc; padding-left: 1ex;\"><span style=\"font-family: Arial; color: #333333;\"><span style=\"white-space: pre-wrap;\"><em> - <a href=\"http://www.humanistculture.com\">humanistculture.com</a></em></span></span></blockquote>\n<p>Almost three years ago, the NY Rationality community celebrated their first Winter Solstice. This year, I'm ready to share this with as many people as possible. I'm aiming to host a large, concert event, with a goal of filling an 800-seat auditorium. I'm hoping to reach several different communities, connect people, and teach a broader audience about some important rationality concepts.</p>\n<p><a href=\"http://www.kickstarter.com/projects/244974495/brighter-than-today-a-secular-solstice\">To make this happen, we're running a kickstarter, beginning today, that'll determine the scope of the event.</a></p>\n<p><strong>A Brief History:</strong></p>\n<p>We didn't intend it to be a big thing - it was just twenty of us gathered in a room, celebrating something we thought was important. I put together a series of songs and stories about the oddly specific things that we valued.&nbsp;</p>\n<p>I meant it as a small, personal holiday. But I took its construction seriously. I put a lot of thought into why ritual works, why it's so hard to build from scratch, but how you might build it from scratch anyway.</p>\n<p>And then I wrote some blog posts about it, and... it turned out to really really resonate with people here. And when I shared with non-Less Wrong folk, even people who had little interest in rationality, they still by and large found it interesting and powerful. I had worried it might look weird - to the contrary, it almost seemed reassuringly normal... and yet also something distinct and new that seemed novel and compelling.</p>\n<p>The first year, there were 20 attendees from NY. The second year, we had 50 people coming from Boston, San Francisco and other places, and a group in Ohio who took as inspiration to create their own event. The event was far more successful from an emotional standpoint - many people walked away feeling inspired, connected and awed.</p>\n<p>A few months ago, the Melbourne rationalists, put on their own Winter Solstice as well. And this December, we have people in Boston, San Francisco, Ohio, Germany, and Washington DC planning their own events.</p>\n<p><strong>What Comes Next:</strong></p>\n<p>In NYC this year, my goal is to make a serious stab at creating mainstream culture. I should note that this is somewhat different from \"Less Wrong Culture\" - there are somethings that LW offers that I think are genuinely valuable to everyone, and there are things I think we focus on because of the people LW attracts. I'm branding this as distinct from Less Wrong (the website I'm running this from is 'humanistculture.com', and is intended to be a general hub for skeptical/rational/humanist artwork and culture.</p>\n<p>But we have important messages that the rest of the world should hear. One of my primary goals with the event is to make the ideas behind Effective Altruism not just intellectually but emotionally salient, and to hightlight Existential Risk, in particular, as a concept that people should seriously evaluate. Dovetailing with this will be an attempt to reach people who have the potential to become valuable agents of positive change, and giving them some activation energy.</p>\n<p>One event isn't enough to radically change anyone's life, unless they were already hovering on the cusp of agent-hood. So it is also my hope to host a large reception afterwards, where people can connect over what they just experienced, and find their way to a community that will meet their needs, where they can become the people they want to be, over time. For some people, this will be Less Wrong or similar groups. For others it may be Effective Altruism-focused groups, or more mainstream secular communities.<br /><br />Although this is growing beyond the LW community at this point, it has firm roots in some of our most important ideas. I think it will continue to have value to the community here.&nbsp;<br /><br /><a href=\"http://www.kickstarter.com/projects/244974495/brighter-than-today-a-secular-solstice\">The kickstarter page is here, for those who wish to come, or to support the event.</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vtozKm5BZ8gf6zd45": 1, "w2CXH4hsQtihvwT4v": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bJSuBC35NHNZXrB2x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 42, "extendedScore": null, "score": 0.000127, "legacy": true, "legacyId": "24165", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote class=\"gmail_quote\" style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: #cccccc; padding-left: 1ex;\"><em><span style=\"font-family: Times;\"><span style=\"font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Like most things, winter was once a mystery.</span></span>&nbsp;</em></blockquote>\n<blockquote class=\"gmail_quote\" style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: #cccccc; padding-left: 1ex;\"><em><br><span style=\"font-family: Arial; color: #333333; vertical-align: baseline; white-space: pre-wrap;\">The world got cold, and dark. Life became fragile. People died. And they didn't know what was happening or understand why. They desperately threw festivals in honor of sun gods with all-too-human motivations, and prayed for the light's return.</span>&nbsp;</em></blockquote>\n<blockquote class=\"gmail_quote\" style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: #cccccc; padding-left: 1ex;\"><em><br><span style=\"font-family: Arial; color: #333333; vertical-align: baseline; white-space: pre-wrap;\">It didn't help. Though we did discover that throwing parties in the middle of winter is an excellent idea.</span>&nbsp;</em></blockquote>\n<blockquote class=\"gmail_quote\" style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: #cccccc; padding-left: 1ex;\"><em><br><span style=\"font-family: Arial; color: #333333; vertical-align: baseline; white-space: pre-wrap;\">But then something incredible and beautiful happened. We studied the sky. We invented astronomy, and other sciences. We began a long journey towards truly understanding our place in the universe. And we used that knowledge to plan for the future, and make our world better. Five thousand years later, the winter isn't so scary. But the symbol of the solstice - the departure and return of the sun - is still powerful. The work we have done to transform winter from a terrifying season of darkness into a modern festival of light deserves a reverence with all the weight of an ancient cultural cornerstone.</span></em></blockquote>\n<blockquote class=\"gmail_quote\" style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: #cccccc; padding-left: 1ex;\"><br></blockquote>\n<blockquote class=\"gmail_quote\" style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: #cccccc; padding-left: 1ex;\"><em><span style=\"font-family: Arial; color: #333333; vertical-align: baseline; white-space: pre-wrap;\">And the work we have yet to do, to fully explore humanity's potential, is even more inspiring.</span></em></blockquote>\n<blockquote class=\"gmail_quote\" style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: #cccccc; padding-left: 1ex;\"><br></blockquote>\n<blockquote class=\"gmail_quote\" style=\"font-size: 13px; margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: #cccccc; padding-left: 1ex;\"><span style=\"font-family: Arial; color: #333333;\"><span style=\"white-space: pre-wrap;\"><em> - <a href=\"http://www.humanistculture.com\">humanistculture.com</a></em></span></span></blockquote>\n<p>Almost three years ago, the NY Rationality community celebrated their first Winter Solstice. This year, I'm ready to share this with as many people as possible. I'm aiming to host a large, concert event, with a goal of filling an 800-seat auditorium. I'm hoping to reach several different communities, connect people, and teach a broader audience about some important rationality concepts.</p>\n<p><a href=\"http://www.kickstarter.com/projects/244974495/brighter-than-today-a-secular-solstice\">To make this happen, we're running a kickstarter, beginning today, that'll determine the scope of the event.</a></p>\n<p><strong id=\"A_Brief_History_\">A Brief History:</strong></p>\n<p>We didn't intend it to be a big thing - it was just twenty of us gathered in a room, celebrating something we thought was important. I put together a series of songs and stories about the oddly specific things that we valued.&nbsp;</p>\n<p>I meant it as a small, personal holiday. But I took its construction seriously. I put a lot of thought into why ritual works, why it's so hard to build from scratch, but how you might build it from scratch anyway.</p>\n<p>And then I wrote some blog posts about it, and... it turned out to really really resonate with people here. And when I shared with non-Less Wrong folk, even people who had little interest in rationality, they still by and large found it interesting and powerful. I had worried it might look weird - to the contrary, it almost seemed reassuringly normal... and yet also something distinct and new that seemed novel and compelling.</p>\n<p>The first year, there were 20 attendees from NY. The second year, we had 50 people coming from Boston, San Francisco and other places, and a group in Ohio who took as inspiration to create their own event. The event was far more successful from an emotional standpoint - many people walked away feeling inspired, connected and awed.</p>\n<p>A few months ago, the Melbourne rationalists, put on their own Winter Solstice as well. And this December, we have people in Boston, San Francisco, Ohio, Germany, and Washington DC planning their own events.</p>\n<p><strong id=\"What_Comes_Next_\">What Comes Next:</strong></p>\n<p>In NYC this year, my goal is to make a serious stab at creating mainstream culture. I should note that this is somewhat different from \"Less Wrong Culture\" - there are somethings that LW offers that I think are genuinely valuable to everyone, and there are things I think we focus on because of the people LW attracts. I'm branding this as distinct from Less Wrong (the website I'm running this from is 'humanistculture.com', and is intended to be a general hub for skeptical/rational/humanist artwork and culture.</p>\n<p>But we have important messages that the rest of the world should hear. One of my primary goals with the event is to make the ideas behind Effective Altruism not just intellectually but emotionally salient, and to hightlight Existential Risk, in particular, as a concept that people should seriously evaluate. Dovetailing with this will be an attempt to reach people who have the potential to become valuable agents of positive change, and giving them some activation energy.</p>\n<p>One event isn't enough to radically change anyone's life, unless they were already hovering on the cusp of agent-hood. So it is also my hope to host a large reception afterwards, where people can connect over what they just experienced, and find their way to a community that will meet their needs, where they can become the people they want to be, over time. For some people, this will be Less Wrong or similar groups. For others it may be Effective Altruism-focused groups, or more mainstream secular communities.<br><br>Although this is growing beyond the LW community at this point, it has firm roots in some of our most important ideas. I think it will continue to have value to the community here.&nbsp;<br><br><a href=\"http://www.kickstarter.com/projects/244974495/brighter-than-today-a-secular-solstice\">The kickstarter page is here, for those who wish to come, or to support the event.</a></p>", "sections": [{"title": "A Brief History:", "anchor": "A_Brief_History_", "level": 1}, {"title": "What Comes Next:", "anchor": "What_Comes_Next_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "21 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-15T11:27:28.273Z", "modifiedAt": null, "url": null, "title": "Meetup : Frankfurt", "slug": "meetup-frankfurt", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kendra", "createdAt": "2012-02-29T23:10:44.583Z", "isAdmin": false, "displayName": "Kendra"}, "userId": "BPB6kHkfZwFLrhcbG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fiKCW4CHez8iyHFmn/meetup-frankfurt", "pageUrlRelative": "/posts/fiKCW4CHez8iyHFmn/meetup-frankfurt", "linkUrl": "https://www.lesswrong.com/posts/fiKCW4CHez8iyHFmn/meetup-frankfurt", "postedAtFormatted": "Sunday, September 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Frankfurt&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Frankfurt%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfiKCW4CHez8iyHFmn%2Fmeetup-frankfurt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Frankfurt%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfiKCW4CHez8iyHFmn%2Fmeetup-frankfurt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfiKCW4CHez8iyHFmn%2Fmeetup-frankfurt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/r1'>Frankfurt</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 September 2013 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Frankfurt, Ginnheimer Landstra\u00dfe </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Join us! The location is a private flat. Stra\u00dfenbahnhaltestelle Frauenfriedenskirche, Linie 16. Please contact me for further details of the location (it's the same as last time.) Phone number is 0176 34 095 760, text message only, please. We have a mailing list, you can also write there:\nless-wrong-frankfurt@googlegroups.com\nThere will probably be a PredictionBook-like game (and chocolate muffins).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/r1'>Frankfurt</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fiKCW4CHez8iyHFmn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.341856458216354e-06, "legacy": true, "legacyId": "24166", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Frankfurt\">Discussion article for the meetup : <a href=\"/meetups/r1\">Frankfurt</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 September 2013 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Frankfurt, Ginnheimer Landstra\u00dfe </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Join us! The location is a private flat. Stra\u00dfenbahnhaltestelle Frauenfriedenskirche, Linie 16. Please contact me for further details of the location (it's the same as last time.) Phone number is 0176 34 095 760, text message only, please. We have a mailing list, you can also write there:\nless-wrong-frankfurt@googlegroups.com\nThere will probably be a PredictionBook-like game (and chocolate muffins).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Frankfurt1\">Discussion article for the meetup : <a href=\"/meetups/r1\">Frankfurt</a></h2>", "sections": [{"title": "Discussion article for the meetup : Frankfurt", "anchor": "Discussion_article_for_the_meetup___Frankfurt", "level": 1}, {"title": "Discussion article for the meetup : Frankfurt", "anchor": "Discussion_article_for_the_meetup___Frankfurt1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-15T22:43:35.864Z", "modifiedAt": null, "url": null, "title": "Notes on logical priors from the MIRI workshop", "slug": "notes-on-logical-priors-from-the-miri-workshop", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:30.774Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wXxPmc9W6kPb6i7vj/notes-on-logical-priors-from-the-miri-workshop", "pageUrlRelative": "/posts/wXxPmc9W6kPb6i7vj/notes-on-logical-priors-from-the-miri-workshop", "linkUrl": "https://www.lesswrong.com/posts/wXxPmc9W6kPb6i7vj/notes-on-logical-priors-from-the-miri-workshop", "postedAtFormatted": "Sunday, September 15th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Notes%20on%20logical%20priors%20from%20the%20MIRI%20workshop&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANotes%20on%20logical%20priors%20from%20the%20MIRI%20workshop%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwXxPmc9W6kPb6i7vj%2Fnotes-on-logical-priors-from-the-miri-workshop%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Notes%20on%20logical%20priors%20from%20the%20MIRI%20workshop%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwXxPmc9W6kPb6i7vj%2Fnotes-on-logical-priors-from-the-miri-workshop", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwXxPmc9W6kPb6i7vj%2Fnotes-on-logical-priors-from-the-miri-workshop", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1957, "htmlBody": "<p>(This post contains a lot of math, and assumes some familiarity with the earlier LW posts on decision theory. Most of the ideas are by me and Paul Christiano, building on earlier ideas of Wei Dai.)</p>\n<p>The September MIRI workshop has just finished. There were discussions of many topics, which will probably be written up by different people. In this post I'd like to describe a certain problem I brought up there and a sequence of ideas that followed. Eventually we found an idea that seems to work, and also it's interesting to tell the story of how each attempted idea led to the next one.</p>\n<p>The problem is a variant of Wei Dai's <a href=\"/lw/b0y/a_problem_about_bargaining_and_logical_uncertainty/\">A Problem About Bargaining and Logical Uncertainty</a>. My particular variant is described in <a href=\"/lw/b0y/a_problem_about_bargaining_and_logical_uncertainty/64ef\">this comment</a>, and was also discussed in <a href=\"/lw/b91/should_logical_probabilities_be_updateless_too/\">this post</a>.&nbsp;Here's a short summary:</p>\n<blockquote>\n<p>In <a href=\"/lw/3l/counterfactual_mugging/\">Counterfactual Mugging</a> with a logical coin, a \"stupid\" agent that can't compute the outcome of the coinflip should agree to pay, and a \"smart\" agent that considers the coinflip as obvious as 1=1 should refuse to pay. But if a stupid agent is asked to write a smart agent, it will want to write an agent that will agree to pay. Therefore the smart agent who refuses to pay is reflectively inconsistent in some sense. What's the right thing to do in this case?</p>\n</blockquote>\n<p>Thinking about that problem, it seems like the right decision theory should refuse to calculate certain things, and instead behave updatelessly with regard to some sort of \"logical prior\" inherited from its creators, who didn't have enough power to calculate these things. In particular, it should agree to pay in a Counterfactual Mugging with a digit of pi, but still go ahead and calculate that digit of pi if offered a straight-up bet instead of a counterfactual one.</p>\n<p>What could such a decision theory look like? What kind of mathematical object is a \"logical prior\"? Perhaps it's a probability distribution over inconsistent theories that the creators didn't yet know to be inconsistent. How do we build such an object, what constraints should it satisfy, and how do we use it in a decision algorithm?<a id=\"more\"></a></p>\n<h4>Attempt 1</h4>\n<p>Sometime ago, Benja Fallenstein and Paul Christiano came up with a way to build a probability distribution over all consistent theories. We start with a single empty theory with weight 1, and then refine it by successively adding all possible axioms. Each new axiom A is either already settled by the current theory T (i.e. either A or&nbsp;&not;A is provable in T), in which case we do nothing; or A is independent of T, in which case we choose randomly between T+A and T+&not;A. This process gives nonzero probabilities to all finitely axiomatized consistent theories, and leads to well-behaved conditional probabilities like P(PA+Con(PA)|PA). (I gloss over the distinction between finitely and recursively axiomatizable theories. Building a variant of this construction that works for recursively axiomatizable theories is left as an exercise to the reader.)</p>\n<p>At the workshop we tried to use a similar construction, but with local consistency instead of global consistency. (Let's say that a \"locally consistent theory\" is a set of statements that has no short proof of inconsistency, for some reasonable meaning of \"short\".) We keep adding axioms as before, but only check that the theory so far is locally consistent. Hopefully this way we might end up with a nice distribution over all theories, including inconsistent ones.</p>\n<p>The trouble with this approach is that for some axiom A and some locally consistent theory T, both T+A and T+&not;A might be locally inconsistent, so we won't be able to continue. We can try to salvage the idea by somehow adjusting the probabilities retroactively, but then it seems hard to guarantee that the probability of each statement goes to a well-defined limit, it might end up oscillating. So we abandoned this idea.</p>\n<h4>Attempt 2</h4>\n<p>Instead of trying to build up a prior over possibly inconsistent theories, we can try assigning each theory a \"quality\" between 0 and 1. For example, we can take a probability distribution over all local consistency constraints that a theory must satisfy, and define the \"quality\" of a theory as the probability that it satisfies a randomly chosen constraint. For purposes of that definition, the theories have to be complete, but possibly inconsistent. Theories with quality 1 will be the consistent ones, because they satisfy all the constraints. Then we can define the quality of an individual statement as the maximum quality of any theory that includes that statement.</p>\n<p>This all seems fine so far, but at the next step we run into trouble. In Counterfactual Mugging with a logical coin, Omega needs to evaluate a counterfactual statement like \"if a digit of pi was different, the agent would do such-and-such thing\". We can try to straightforwardly define \"A counterfactually implies B\" as true if the quality of \"A and B\" is equal to the quality of A. Intuitively that means that the most consistent theory that includes A also includes B.</p>\n<p>Unfortunately, that definition does not have very good properties. Both \"PA counterfactually implies Con(PA)\" and \"PA counterfactually implies &not;Con(PA)\" will be defined as true, because the theories PA+Con(PA) and PA+&not;Con(PA) are both consistent and have quality 1. That doesn't seem to agree with our intuitions about how logical counterfactuals should work.</p>\n<h4>Some ideas about logical counterfactuals</h4>\n<p>Our intuitions about decision theory seem to say that some logical counterfactuals are \"natural\" and others are \"spurious\". For example, if the agent does not in fact take a certain action, then there's a \"natural\" proof of how much utility it would have implied, and also many \"spurious\" proofs that prove all sorts of absurdities by first simulating the agent and then using the fact that the action was not taken. (See&nbsp;<a href=\"/lw/b5t/an_example_of_selffulfilling_spurious_proofs_in/\">this post</a>&nbsp;for more about spurious proofs.)</p>\n<p>Natural counterfactuals look more correct to us than spurious ones because their proofs don't rely on fully simulating the agent. Maybe we could try weighting logical counterfactuals based on their proof length, and see where that idea takes us?</p>\n<p>In simple decision theory problems, we are interested in counterfactuals like \"A()=1 implies U()=1\", \"A()=1 implies U()=2\", and so on, where A is the agent program that returns an action, and U is the universe program that contains the agent and returns a utility value. (See&nbsp;<a href=\"/lw/2l2/what_a_reduction_of_could_could_look_like/\">this post</a>&nbsp;or <a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle/\">this post</a> for more about this setting.) Naively assigning some probability P(L) to each counterfactual statement with proof length L doesn't seem to work, because the problem might be so complex that all relevant statements have long proofs. But if we took some set of counterfactuals assumed to be mutually exclusive, with proof lengths L<sub>1</sub>, L<sub>2</sub>, ..., then it might be reasonable to give them probabilities like P(L<sub>1</sub>)/(P(L<sub>1</sub>)+P(L<sub>2</sub>)+...). That looks suspiciously similar to Bayes' theorem, so it seems natural to try and come up with some sort of probabilistic interpretation where this will be an actual application of Bayes' theorem. Thankfully, it's easy to find such an interpretation.</p>\n<p>Let's take a theory like PA, generate random proofs in it with probability depending on length, and then throw out the invalid ones. If we view each statement of the form \"X implies Y\" as an \"arrow\" from statement X to statement Y, then the above process of picking random valid proofs induces a probability distribution on all true arrows, including those where statement X is false. Now we can define the conditional probability P(U()=u|A()=a) as the probability that a random arrow ends at \"U()=u\", conditional on the event that the arrow starts from \"A()=a\" and ends at some statement of the form \"U()=...\". This way, all possible statements \"U()=u\" for a given \"A()=a\" have probabilities that sum to 1, and the true statement \"A()=a\" leads to a single statement \"U()=u\" with probability 1. These seem like good properties to have.</p>\n<p>More generally, when people are uncertain about some math statement, they often seem to reason like this: \"I guess I'm likely to find a proof if I try this method. Hmm, I checked a couple obvious proofs and they didn't work. Guess I have to lower my probability of the statement.\" Such reasoning corresponds to Bayesian updating as you observe new proofs, which is similar to the above setting.</p>\n<p>Now, can we extend this idea to obtain a \"logical prior\" over inconsistent theories as well?</p>\n<h3>Attempt 3, successful</h3>\n<p>There's nothing stopping us from using the above construction with an inconsistent theory, because the properties that follow from having a coherent probability distribution over arrows should still hold. Let's assume that we care about theories T<sub>1</sub>,T<sub>2</sub>,... which are incomplete and possibly inconsistent. (An example of such a theory is PA+\"the billionth digit of pi is even\"). In each of these theories, we can construct the above distribution on arrows. Then we can add them up, using some weighting of the theories, and get an overall distribution on statements of the form \"A()=a implies U()=u\". It has the nice property that you only need to sample finitely many proofs to compute the conditional probabilities to any desired precision, so it's easy to write a decision algorithm that will do approximate utility maximization based on these probabilities. If we only care about a single theory, like PA, then the algorithm seems to give the same answers as our earlier algorithms for UDT.</p>\n<p>There's still a remaining problem to fix. If we care equally about the theory where the billionth digit of pi is even and the one where it's odd, then our algorithm will accept a straight bet on that digit of pi, instead of calculating it and accepting conditionally. An approach to solving that problem can be found in Wei Dai's proposed \"UDT2\", which optimizes over the next program to run, instead of the value to return. We can use that idea here, ending up with an algorithm that I will call \"UDT1.5\", to avoid incrementing the major version number for something that might turn out to be wrong. To put it all together:</p>\n<ul>\n<li>We care about some program U() that returns a utility value.&nbsp;Also we care about some probability distribution over theories T<sub>1</sub>,T<sub>2</sub>,... that are incomplete and possibly inconsistent.</li>\n<li>Our decision algorithm A() attempts to find a program B such that running it will approximately maximize the expected value of U(), using the probabilities defined below. Then A runs B() and returns the resulting value.</li>\n<li>For each possible program B, the conditional probability P(U()=u|A runs B) is defined as the probability that a random valid proof sampled from a random T<sub>i</sub>&nbsp;proves that \"if A runs B, then U()=u\", conditional on it being a proof that \"if A runs B, then U()=something\".</li>\n</ul>\n<p>(The algorithm can easily be extended to the case where A receives an argument representing observational data. Namely, it should just pass the argument to B, which corresponds to Wei Dai's <a href=\"/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/\">UDT1.1</a> idea of optimizing over the agent's input-output map.)</p>\n<p>Given a suitable prior, the above algorithm solves my original variant of Counterfactual Mugging with stupid and smart agents, but still correctly decides to calculate the digit of pi if offered a straight bet instead of a counterfactual one. And even apart from that, it feels right to&nbsp;make decisions using a weighted sum of theories T<sub>1</sub>,T<sub>2</sub>,... as well as a weighted sum of universes U<sub>1</sub>,U<sub>2</sub>,... composing U. I'm not sure if the right decision theory should eventually work like this, using \"Tegmark level 4\" over possible universe programs and \"Tegmark level 5\" over logically inconsistent theories, or perhaps we can define a single kind of uncertainty that generalizes both indexical and logical. In any case, that's as far as the workshop went, and it seems worthwhile to continue thinking about such questions.</p>\n<p>Many thanks to all participants, to MIRI which invited me, to Kevin and Sean who hosted me, and anyone else I might have missed!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wXxPmc9W6kPb6i7vj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 30, "extendedScore": null, "score": 1.3424542473835484e-06, "legacy": true, "legacyId": "24155", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>(This post contains a lot of math, and assumes some familiarity with the earlier LW posts on decision theory. Most of the ideas are by me and Paul Christiano, building on earlier ideas of Wei Dai.)</p>\n<p>The September MIRI workshop has just finished. There were discussions of many topics, which will probably be written up by different people. In this post I'd like to describe a certain problem I brought up there and a sequence of ideas that followed. Eventually we found an idea that seems to work, and also it's interesting to tell the story of how each attempted idea led to the next one.</p>\n<p>The problem is a variant of Wei Dai's <a href=\"/lw/b0y/a_problem_about_bargaining_and_logical_uncertainty/\">A Problem About Bargaining and Logical Uncertainty</a>. My particular variant is described in <a href=\"/lw/b0y/a_problem_about_bargaining_and_logical_uncertainty/64ef\">this comment</a>, and was also discussed in <a href=\"/lw/b91/should_logical_probabilities_be_updateless_too/\">this post</a>.&nbsp;Here's a short summary:</p>\n<blockquote>\n<p>In <a href=\"/lw/3l/counterfactual_mugging/\">Counterfactual Mugging</a> with a logical coin, a \"stupid\" agent that can't compute the outcome of the coinflip should agree to pay, and a \"smart\" agent that considers the coinflip as obvious as 1=1 should refuse to pay. But if a stupid agent is asked to write a smart agent, it will want to write an agent that will agree to pay. Therefore the smart agent who refuses to pay is reflectively inconsistent in some sense. What's the right thing to do in this case?</p>\n</blockquote>\n<p>Thinking about that problem, it seems like the right decision theory should refuse to calculate certain things, and instead behave updatelessly with regard to some sort of \"logical prior\" inherited from its creators, who didn't have enough power to calculate these things. In particular, it should agree to pay in a Counterfactual Mugging with a digit of pi, but still go ahead and calculate that digit of pi if offered a straight-up bet instead of a counterfactual one.</p>\n<p>What could such a decision theory look like? What kind of mathematical object is a \"logical prior\"? Perhaps it's a probability distribution over inconsistent theories that the creators didn't yet know to be inconsistent. How do we build such an object, what constraints should it satisfy, and how do we use it in a decision algorithm?<a id=\"more\"></a></p>\n<h4 id=\"Attempt_1\">Attempt 1</h4>\n<p>Sometime ago, Benja Fallenstein and Paul Christiano came up with a way to build a probability distribution over all consistent theories. We start with a single empty theory with weight 1, and then refine it by successively adding all possible axioms. Each new axiom A is either already settled by the current theory T (i.e. either A or&nbsp;\u00acA is provable in T), in which case we do nothing; or A is independent of T, in which case we choose randomly between T+A and T+\u00acA. This process gives nonzero probabilities to all finitely axiomatized consistent theories, and leads to well-behaved conditional probabilities like P(PA+Con(PA)|PA). (I gloss over the distinction between finitely and recursively axiomatizable theories. Building a variant of this construction that works for recursively axiomatizable theories is left as an exercise to the reader.)</p>\n<p>At the workshop we tried to use a similar construction, but with local consistency instead of global consistency. (Let's say that a \"locally consistent theory\" is a set of statements that has no short proof of inconsistency, for some reasonable meaning of \"short\".) We keep adding axioms as before, but only check that the theory so far is locally consistent. Hopefully this way we might end up with a nice distribution over all theories, including inconsistent ones.</p>\n<p>The trouble with this approach is that for some axiom A and some locally consistent theory T, both T+A and T+\u00acA might be locally inconsistent, so we won't be able to continue. We can try to salvage the idea by somehow adjusting the probabilities retroactively, but then it seems hard to guarantee that the probability of each statement goes to a well-defined limit, it might end up oscillating. So we abandoned this idea.</p>\n<h4 id=\"Attempt_2\">Attempt 2</h4>\n<p>Instead of trying to build up a prior over possibly inconsistent theories, we can try assigning each theory a \"quality\" between 0 and 1. For example, we can take a probability distribution over all local consistency constraints that a theory must satisfy, and define the \"quality\" of a theory as the probability that it satisfies a randomly chosen constraint. For purposes of that definition, the theories have to be complete, but possibly inconsistent. Theories with quality 1 will be the consistent ones, because they satisfy all the constraints. Then we can define the quality of an individual statement as the maximum quality of any theory that includes that statement.</p>\n<p>This all seems fine so far, but at the next step we run into trouble. In Counterfactual Mugging with a logical coin, Omega needs to evaluate a counterfactual statement like \"if a digit of pi was different, the agent would do such-and-such thing\". We can try to straightforwardly define \"A counterfactually implies B\" as true if the quality of \"A and B\" is equal to the quality of A. Intuitively that means that the most consistent theory that includes A also includes B.</p>\n<p>Unfortunately, that definition does not have very good properties. Both \"PA counterfactually implies Con(PA)\" and \"PA counterfactually implies \u00acCon(PA)\" will be defined as true, because the theories PA+Con(PA) and PA+\u00acCon(PA) are both consistent and have quality 1. That doesn't seem to agree with our intuitions about how logical counterfactuals should work.</p>\n<h4 id=\"Some_ideas_about_logical_counterfactuals\">Some ideas about logical counterfactuals</h4>\n<p>Our intuitions about decision theory seem to say that some logical counterfactuals are \"natural\" and others are \"spurious\". For example, if the agent does not in fact take a certain action, then there's a \"natural\" proof of how much utility it would have implied, and also many \"spurious\" proofs that prove all sorts of absurdities by first simulating the agent and then using the fact that the action was not taken. (See&nbsp;<a href=\"/lw/b5t/an_example_of_selffulfilling_spurious_proofs_in/\">this post</a>&nbsp;for more about spurious proofs.)</p>\n<p>Natural counterfactuals look more correct to us than spurious ones because their proofs don't rely on fully simulating the agent. Maybe we could try weighting logical counterfactuals based on their proof length, and see where that idea takes us?</p>\n<p>In simple decision theory problems, we are interested in counterfactuals like \"A()=1 implies U()=1\", \"A()=1 implies U()=2\", and so on, where A is the agent program that returns an action, and U is the universe program that contains the agent and returns a utility value. (See&nbsp;<a href=\"/lw/2l2/what_a_reduction_of_could_could_look_like/\">this post</a>&nbsp;or <a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle/\">this post</a> for more about this setting.) Naively assigning some probability P(L) to each counterfactual statement with proof length L doesn't seem to work, because the problem might be so complex that all relevant statements have long proofs. But if we took some set of counterfactuals assumed to be mutually exclusive, with proof lengths L<sub>1</sub>, L<sub>2</sub>, ..., then it might be reasonable to give them probabilities like P(L<sub>1</sub>)/(P(L<sub>1</sub>)+P(L<sub>2</sub>)+...). That looks suspiciously similar to Bayes' theorem, so it seems natural to try and come up with some sort of probabilistic interpretation where this will be an actual application of Bayes' theorem. Thankfully, it's easy to find such an interpretation.</p>\n<p>Let's take a theory like PA, generate random proofs in it with probability depending on length, and then throw out the invalid ones. If we view each statement of the form \"X implies Y\" as an \"arrow\" from statement X to statement Y, then the above process of picking random valid proofs induces a probability distribution on all true arrows, including those where statement X is false. Now we can define the conditional probability P(U()=u|A()=a) as the probability that a random arrow ends at \"U()=u\", conditional on the event that the arrow starts from \"A()=a\" and ends at some statement of the form \"U()=...\". This way, all possible statements \"U()=u\" for a given \"A()=a\" have probabilities that sum to 1, and the true statement \"A()=a\" leads to a single statement \"U()=u\" with probability 1. These seem like good properties to have.</p>\n<p>More generally, when people are uncertain about some math statement, they often seem to reason like this: \"I guess I'm likely to find a proof if I try this method. Hmm, I checked a couple obvious proofs and they didn't work. Guess I have to lower my probability of the statement.\" Such reasoning corresponds to Bayesian updating as you observe new proofs, which is similar to the above setting.</p>\n<p>Now, can we extend this idea to obtain a \"logical prior\" over inconsistent theories as well?</p>\n<h3 id=\"Attempt_3__successful\">Attempt 3, successful</h3>\n<p>There's nothing stopping us from using the above construction with an inconsistent theory, because the properties that follow from having a coherent probability distribution over arrows should still hold. Let's assume that we care about theories T<sub>1</sub>,T<sub>2</sub>,... which are incomplete and possibly inconsistent. (An example of such a theory is PA+\"the billionth digit of pi is even\"). In each of these theories, we can construct the above distribution on arrows. Then we can add them up, using some weighting of the theories, and get an overall distribution on statements of the form \"A()=a implies U()=u\". It has the nice property that you only need to sample finitely many proofs to compute the conditional probabilities to any desired precision, so it's easy to write a decision algorithm that will do approximate utility maximization based on these probabilities. If we only care about a single theory, like PA, then the algorithm seems to give the same answers as our earlier algorithms for UDT.</p>\n<p>There's still a remaining problem to fix. If we care equally about the theory where the billionth digit of pi is even and the one where it's odd, then our algorithm will accept a straight bet on that digit of pi, instead of calculating it and accepting conditionally. An approach to solving that problem can be found in Wei Dai's proposed \"UDT2\", which optimizes over the next program to run, instead of the value to return. We can use that idea here, ending up with an algorithm that I will call \"UDT1.5\", to avoid incrementing the major version number for something that might turn out to be wrong. To put it all together:</p>\n<ul>\n<li>We care about some program U() that returns a utility value.&nbsp;Also we care about some probability distribution over theories T<sub>1</sub>,T<sub>2</sub>,... that are incomplete and possibly inconsistent.</li>\n<li>Our decision algorithm A() attempts to find a program B such that running it will approximately maximize the expected value of U(), using the probabilities defined below. Then A runs B() and returns the resulting value.</li>\n<li>For each possible program B, the conditional probability P(U()=u|A runs B) is defined as the probability that a random valid proof sampled from a random T<sub>i</sub>&nbsp;proves that \"if A runs B, then U()=u\", conditional on it being a proof that \"if A runs B, then U()=something\".</li>\n</ul>\n<p>(The algorithm can easily be extended to the case where A receives an argument representing observational data. Namely, it should just pass the argument to B, which corresponds to Wei Dai's <a href=\"/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/\">UDT1.1</a> idea of optimizing over the agent's input-output map.)</p>\n<p>Given a suitable prior, the above algorithm solves my original variant of Counterfactual Mugging with stupid and smart agents, but still correctly decides to calculate the digit of pi if offered a straight bet instead of a counterfactual one. And even apart from that, it feels right to&nbsp;make decisions using a weighted sum of theories T<sub>1</sub>,T<sub>2</sub>,... as well as a weighted sum of universes U<sub>1</sub>,U<sub>2</sub>,... composing U. I'm not sure if the right decision theory should eventually work like this, using \"Tegmark level 4\" over possible universe programs and \"Tegmark level 5\" over logically inconsistent theories, or perhaps we can define a single kind of uncertainty that generalizes both indexical and logical. In any case, that's as far as the workshop went, and it seems worthwhile to continue thinking about such questions.</p>\n<p>Many thanks to all participants, to MIRI which invited me, to Kevin and Sean who hosted me, and anyone else I might have missed!</p>", "sections": [{"title": "Attempt 1", "anchor": "Attempt_1", "level": 2}, {"title": "Attempt 2", "anchor": "Attempt_2", "level": 2}, {"title": "Some ideas about logical counterfactuals", "anchor": "Some_ideas_about_logical_counterfactuals", "level": 2}, {"title": "Attempt 3, successful", "anchor": "Attempt_3__successful", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "47 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["oZwxY88NCCHffJuxM", "rLcHvxKcJpyJj3i7o", "mg6jDEuQEjBGtibX7", "2GebvAXXfRMTjY2g7", "dC3rxrMkYKLfgTYEa", "Bj244uWzDBXvE2N2S", "g8xh9R7RaNitKtkaa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-16T03:51:50.766Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, September 16-30", "slug": "group-rationality-diary-september-16-30-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:32.047Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jq74AEnHZDa8eLCCS/group-rationality-diary-september-16-30-0", "pageUrlRelative": "/posts/jq74AEnHZDa8eLCCS/group-rationality-diary-september-16-30-0", "linkUrl": "https://www.lesswrong.com/posts/jq74AEnHZDa8eLCCS/group-rationality-diary-september-16-30-0", "postedAtFormatted": "Monday, September 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20September%2016-30&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20September%2016-30%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjq74AEnHZDa8eLCCS%2Fgroup-rationality-diary-september-16-30-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20September%2016-30%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjq74AEnHZDa8eLCCS%2Fgroup-rationality-diary-september-16-30-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjq74AEnHZDa8eLCCS%2Fgroup-rationality-diary-september-16-30-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<p>This is the public group instrumental rationality diary for September 16-30.</p>\n<blockquote style=\"font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">\n<p style=\"margin: 0px 0px 1em;\"><span style=\"color: #333333;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/therufs/submitted/r/discussion/lw/hg0/group_rationality_diary_may_1631/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating!</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><a href=\"/r/discussion/lw/iho/group_rationality_diary_september_115/\">Immediate past diary</a>:&nbsp; September 1-15</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><a href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality Diaries archive</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jq74AEnHZDa8eLCCS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "24171", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XPsft9gKw2tPQBuC2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-16T05:18:34.009Z", "modifiedAt": null, "url": null, "title": "Open thread, September 16-22, 2013", "slug": "open-thread-september-16-22-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:35.145Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Metus", "createdAt": "2011-01-23T21:54:34.357Z", "isAdmin": false, "displayName": "Metus"}, "userId": "mNQ4fSvro7LYgrii4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fkvBHesQ9nS7Wxja2/open-thread-september-16-22-2013", "pageUrlRelative": "/posts/fkvBHesQ9nS7Wxja2/open-thread-september-16-22-2013", "linkUrl": "https://www.lesswrong.com/posts/fkvBHesQ9nS7Wxja2/open-thread-september-16-22-2013", "postedAtFormatted": "Monday, September 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20thread%2C%20September%2016-22%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20thread%2C%20September%2016-22%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfkvBHesQ9nS7Wxja2%2Fopen-thread-september-16-22-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20thread%2C%20September%2016-22%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfkvBHesQ9nS7Wxja2%2Fopen-thread-september-16-22-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfkvBHesQ9nS7Wxja2%2Fopen-thread-september-16-22-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fkvBHesQ9nS7Wxja2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 1.3428036724342703e-06, "legacy": true, "legacyId": "24172", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 141, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-16T08:27:24.719Z", "modifiedAt": null, "url": null, "title": "Cooperating with agents with different ideas of fairness, while resisting exploitation", "slug": "cooperating-with-agents-with-different-ideas-of-fairness", "viewCount": null, "lastCommentedAt": "2015-08-13T18:29:33.180Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z2YwmzuT7nWx62Kfh/cooperating-with-agents-with-different-ideas-of-fairness", "pageUrlRelative": "/posts/z2YwmzuT7nWx62Kfh/cooperating-with-agents-with-different-ideas-of-fairness", "linkUrl": "https://www.lesswrong.com/posts/z2YwmzuT7nWx62Kfh/cooperating-with-agents-with-different-ideas-of-fairness", "postedAtFormatted": "Monday, September 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cooperating%20with%20agents%20with%20different%20ideas%20of%20fairness%2C%20while%20resisting%20exploitation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACooperating%20with%20agents%20with%20different%20ideas%20of%20fairness%2C%20while%20resisting%20exploitation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz2YwmzuT7nWx62Kfh%2Fcooperating-with-agents-with-different-ideas-of-fairness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cooperating%20with%20agents%20with%20different%20ideas%20of%20fairness%2C%20while%20resisting%20exploitation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz2YwmzuT7nWx62Kfh%2Fcooperating-with-agents-with-different-ideas-of-fairness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz2YwmzuT7nWx62Kfh%2Fcooperating-with-agents-with-different-ideas-of-fairness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1209, "htmlBody": "<p>There's an idea from the latest MIRI workshop which I haven't seen in informal theories of negotiation, and I want to know if this is a known idea.</p>\n<p><em>(Old well-known ideas:)</em></p>\n<p>Suppose a standard Prisoner's Dilemma matrix where (3, 3) is the payoff for mutual cooperation, (2, 2) is the payoff for mutual defection, and (0, 5) is the payoff if you cooperate and they defect.</p>\n<p>Suppose we're going to play a PD iterated for four rounds. &nbsp;We have common knowledge of each other's source code so we can apply <a href=\"http://intelligence.org/files/RobustCooperation.pdf\">modal cooperation</a> or similar means of reaching a binding 'agreement' without other enforcement methods.</p>\n<p>If we mutually defect on every round, our net mutual payoff is (8, 8). &nbsp;This is a 'Nash equilibrium' because neither agent can unilaterally change its action and thereby do better, if the opponents' actions stay fixed. &nbsp;If we mutually cooperate on every round, the result is (12, 12) and this result is on the 'Pareto boundary' because neither agent can do better unless the other agent does worse. &nbsp;It would seem a desirable principle for rational agents (with common knowledge of each other's source code / common knowledge of rationality) to find an outcome on the Pareto boundary, since otherwise they are leaving value on the table.</p>\n<p>But (12, 12) isn't the only possible result on the Pareto boundary. &nbsp;Suppose that running the opponent's source code, you find that they're willing to cooperate on three rounds and defect on one round, if you cooperate on <em>every </em>round, for a payoff of (9, 14) slanted their way. &nbsp;If they use their knowledge of your code to predict you refusing to accept that bargain, they will defect on every round for the mutual payoff of (8, 8).</p>\n<p>I would consider it obvious that a rational agent should refuse this unfair bargain. &nbsp;Otherwise agents with knowledge of your source code will offer you <em>only </em>this bargain, instead of the (12, 12) of mutual cooperation on every round; they will exploit your willingness to accept a result on the Pareto boundary in which almost all of the gains from trade go to them.</p>\n<p><em>(Newer ideas:)</em></p>\n<p>Generalizing: &nbsp;Once you have a notion of a 'fair' result - in this case (12, 12) - then an agent which accepts any outcome in which it does worse than the fair result, while the opponent does <em>better</em>, is 'exploitable' relative to this fair bargain. &nbsp;Like the Nash equilibrium, the only way you should do worse than 'fair' is if the opponent also does worse.</p>\n<p>So we wrote down on the whiteboard an attempted definition of unexploitability in cooperative games as follows:</p>\n<p>\"Suppose we have a [magical] definition N of a fair outcome. &nbsp;A rational agent should only do worse than N if its opponent does worse than N, or else [if bargaining fails] should only do worse than the Nash equilibrium if its opponent does worse than the Nash equilibrium.\" &nbsp;(Note that this definition precludes giving in to a threat of blackmail.)</p>\n<p><em>(Key possible-innovation:)</em></p>\n<p>It then occurred to me that this definition opened the possibility for other, intermediate bargains between the 'fair' solution on the Pareto boundary, and the Nash equilibrium.</p>\n<p>Suppose the other agent has a slightly different definition of fairness and they think that what you consider to be a payoff of (12, 12) favors you too much; they think that you're the one making an unfair demand. &nbsp;They'll refuse (12, 12) with the same feeling of indignation that you would apply to (9, 14).</p>\n<p>Well, if you give in to an arrangement with an expected payoff of, say, (11, 13) as you evaluate payoffs, then you're giving other agents an incentive to skew their definitions of fairness.</p>\n<p>But it does <em>not </em>create poor incentives (AFAICT) to accept instead a bargain with an expected payoff of, say, (10, 11) which the other agent thinks is 'fair'. &nbsp;Though they're sad that you refused the truly fair outcome of (as you count utilons) 11, 13 and that you couldn't reach the Pareto boundary together, still, this is better than the Nash equilibrium of (8, 8). &nbsp;And though you think the bargain is unfair, you are not creating incentives to exploit you. &nbsp;By insisting on this definition of fairness, the other agent has done worse for themselves than other (12, 12). &nbsp;The other agent probably thinks that (10, 11) is 'unfair' slanted your way, but they likewise accept that this does not create bad incentives, since you did worse than the 'fair' outcome of (11, 13).</p>\n<p>There could be many acceptable negotiating equilibria between what you think is the 'fair' point on the Pareto boundary, and the Nash equilibrium. &nbsp;So long as each step down in what you think is 'fairness' reduces the total payoff to the other agent, even if it reduces your own payoff even more. &nbsp;This resists exploitation and avoids creating an incentive for claiming that you have a different definition of fairness, while still holding open the possibility of some degree of cooperation with agents who honestly disagree with you about what's fair and are trying to avoid exploitation themselves.</p>\n<p>This translates into an informal principle of negotiations: &nbsp;Be willing to accept unfair bargains, but only if (you make it clear) <em>both</em> sides are doing worse than what you consider to be a fair bargain.</p>\n<p>I haven't seen this advocated before even as an informal principle of negotiations. &nbsp;Is it in the literature anywhere? &nbsp;Someone suggested Schelling might have said it, but didn't provide a chapter number.</p>\n<p>ADDED:</p>\n<p>Clarification 1: &nbsp;Yes, utilities are invariant up to a positive affine transformation so there's no canonical way to split utilities evenly. &nbsp;Hence the part about \"Assume a magical solution N which gives us the fair division.\" &nbsp;If we knew the exact properties of how to implement this magical solution, taking it at first for magical, that might give us some idea of what N should be, too.</p>\n<p>Clarification 2: &nbsp;The way this might work is that you pick a series of increasingly unfair-to-you, increasingly worse-for-the-other-player outcomes whose first element is what you deem the fair Pareto outcome: &nbsp;(100, 100), (98, 99), (96, 98). &nbsp;Perhaps stop well short of Nash if the skew becomes too extreme. &nbsp;Drop to Nash as the last resort. &nbsp;The other agent does the same, starting with their own ideal of fairness on the Pareto boundary. &nbsp;Unless one of you has a completely skewed idea of fairness, you should be able to meet somewhere in the middle. &nbsp;Both of you will do worse against a fixed opponent's strategy by unilaterally adopting more self-favoring ideas of fairness. &nbsp;Both of you will do worse in expectation against potentially exploitive opponents by unilaterally adopting looser ideas of fairness. &nbsp;This gives everyone an incentive to obey the Galactic Schelling Point and be fair about it. &nbsp;You should <em>not</em>&nbsp;be picking the descending sequence in an agent-dependent way that incentivizes, at cost to you, skewed claims about fairness.</p>\n<p>Clarification 3: &nbsp;You must take into account the other agent's costs and other opportunities when ensuring that the net outcome, in terms of final utilities, is worse for them than the reward offered for 'fair' cooperation. &nbsp;Offering them the chance to buy half as many paperclips at a lower, less fair price, does no good if they can go next door, get the same offer again, and buy the same number of paperclips at a lower total price.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 2, "be2Mh2bddQ6ZaBcti": 4, "chuP2QqQycjD8qakL": 3, "5A5ZGTQovxbay6fpr": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z2YwmzuT7nWx62Kfh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 80, "extendedScore": null, "score": 0.000198, "legacy": true, "legacyId": "24173", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 80, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-09-16T08:27:24.719Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-16T15:35:33.553Z", "modifiedAt": null, "url": null, "title": "Is humanity a superhuman AI?", "slug": "is-humanity-a-superhuman-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:35.143Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Locaha", "createdAt": "2012-12-26T10:59:43.142Z", "isAdmin": false, "displayName": "Locaha"}, "userId": "CnvnngECxgWgp5DQE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Wp5qpRx2XsbkF8xZ7/is-humanity-a-superhuman-ai", "pageUrlRelative": "/posts/Wp5qpRx2XsbkF8xZ7/is-humanity-a-superhuman-ai", "linkUrl": "https://www.lesswrong.com/posts/Wp5qpRx2XsbkF8xZ7/is-humanity-a-superhuman-ai", "postedAtFormatted": "Monday, September 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20humanity%20a%20superhuman%20AI%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20humanity%20a%20superhuman%20AI%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWp5qpRx2XsbkF8xZ7%2Fis-humanity-a-superhuman-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20humanity%20a%20superhuman%20AI%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWp5qpRx2XsbkF8xZ7%2Fis-humanity-a-superhuman-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWp5qpRx2XsbkF8xZ7%2Fis-humanity-a-superhuman-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 28, "htmlBody": "<p>Serious question. No single human being can build a nuclear reactor, land on the moon or calculate pi to the billionth digit, but humanity can. Does it qualify?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Wp5qpRx2XsbkF8xZ7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": -12, "extendedScore": null, "score": -2.4e-05, "legacy": true, "legacyId": "24177", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-16T17:12:32.088Z", "modifiedAt": null, "url": null, "title": "Signaling of what, precisely?", "slug": "signaling-of-what-precisely", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:29.342Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VipulNaik", "createdAt": "2013-09-02T18:51:08.862Z", "isAdmin": false, "displayName": "VipulNaik"}, "userId": "t3pZcNZXqhaM5avBE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DhpS6kCw9isvJuhEk/signaling-of-what-precisely", "pageUrlRelative": "/posts/DhpS6kCw9isvJuhEk/signaling-of-what-precisely", "linkUrl": "https://www.lesswrong.com/posts/DhpS6kCw9isvJuhEk/signaling-of-what-precisely", "postedAtFormatted": "Monday, September 16th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Signaling%20of%20what%2C%20precisely%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASignaling%20of%20what%2C%20precisely%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDhpS6kCw9isvJuhEk%2Fsignaling-of-what-precisely%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Signaling%20of%20what%2C%20precisely%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDhpS6kCw9isvJuhEk%2Fsignaling-of-what-precisely", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDhpS6kCw9isvJuhEk%2Fsignaling-of-what-precisely", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 700, "htmlBody": "<p>It's a well-known fact that college graduates make more money than high school graduates who do not go to college, but the reason is not clear. Bryan Caplan offers a <a href=\"http://econlog.econlib.org/archives/2012/10/economic_models_1.html\">typology</a> where he splits the gap between the income of college graduates and of high school graduates into three parts: <a href=\"http://en.wikipedia.org/wiki/Human_capital\">human capital</a>, ability bias, and <a href=\"https://en.wikipedia.org/wiki/Signalling_%28economics%29\">signaling</a>. Caplan also warns economists of education against <a href=\"http://econlog.econlib.org/archives/2011/06/ability_bias_vs.html\">conflating ability bias with signaling</a>. The (human capital + signaling) total gives the \"return on education\" for a college degree -- the part that is causally due to getting a college degree. The (human capital) part alone gives the return on education <em>through</em> the channel of improved productivity, whereas the (signaling) part gives the return on education <em>through</em> the channel of being able to convince potential employers that they have higher productivity. The (ability bias) part can be thought of as <a href=\"http://en.wikipedia.org/wiki/Selection_bias\">selection bias</a> for pre-existing ability: people who go on to graduate from college differ from people who do not go to college in terms of their <em>pre-existing abilities</em> (here \"pre-existing\" does not mean \"innate\" -- but rather it means what they had before starting college, or what they would have acquired through natural maturing even if they hadn't gone to college) and these people would likely have earned more (compared to the ones who didn't go to college) even if they had chosen not to go to college.</p>\n<p>I want to look at the signaling channel more closely. A college degree does send a signal of some sort to potential employers, and <a href=\"http://www.businessinsider.com/most-successful-college-dropouts-2013-3?op=1\">stories of college dropouts who achieve great success notwithstanding</a>, many employers, particularly for high-skilled occupations, prefer workers with college degrees even if the degree is not directly related to the work on the job. But why, precisely, does the employer value the degree more? I can categorize the possible explanations into three categories:</p>\n<ol>\n<li><strong>Causally agnostic signaling</strong>: Employers are relying on either their own experience or on the conventional wisdom that people with college degrees are more productive employees than people without. They don't actually care whether this is due to pre-existing productivity or due to human capital acquired in college.</li>\n<li><strong>Signaling of human capital acquisition</strong>: Employers believe that college students acquire valuable human capital from their college experience (whether it's their coursework or other aspects of the college experience). The college degree is a signal of this human capital acquisition. Note that this story has a weak hole, namely, first, that the human capital acquired in colleges is irrelevant for many jobs, and second, that if all one has to signal is human capital, it's easy to signal this by taking a relevant test or battery of tests, as this <a href=\"http://online.wsj.com/article/SB10001424127887323980604579029143959843818.html\">Wall Street Journal article</a> noted. Note that, in so far as colleges do <em>not</em> provide valuable human capital to students, employers are treating the college degree as an important signal based on a <em>mistaken belief</em>, and at least in principle, the market should punish such beliefs.</li>\n<li><strong>Signaling of pre-existing ability</strong>: Employers believe that people whoget through college have greater ability (intelligence, conscientiousness, conformism, etc.) than people who cannot or choose not to. Note that some of these abilities (such as intelligence and conscientiousness) can be signaled by taking tests. Others, such as conformism, are harder to signal directly, because to signal conformism, one has to signal it for a sufficiently lengthy period of time in the \"usual\" setting (namely, college), and therefore, <a href=\"http://en.wikipedia.org/wiki/Status_quo_bias\">status quo bias</a> makes it hard to move away from the equilibrium of requiring college degrees. Note also that some of these \"pre-existing abilities\" may be acquired and honed in college (for instance, college may teach people better work discipline and make them more conformist). The reason I use \"pre-existing ability\" is that it's not the primary purpose of college to teach these skills, and it's unclear that colleges in general teach these skills to the majority of their students.</li>\n</ol>\n<p>What do you think is the breakdown of signaling between (1), (2), and (3)? Any other thoughts about whether the question is well-conceived, and about alternative formulations of the question?</p>\n<p><strong>UPDATE</strong>: Lauren Rivera's <a href=\"http://www.sciencedirect.com/science/article/pii/S027656241000065X\">article on how elite firms hire</a>, which was discussed by Bryan Caplan in an <a href=\"http://econlog.econlib.org/archives/2011/11/how_elite_firms.html\">EconLog blog post</a>, is relevant.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DhpS6kCw9isvJuhEk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "24178", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-17T00:02:56.680Z", "modifiedAt": null, "url": null, "title": "Probability, knowledge, and meta-probability", "slug": "probability-knowledge-and-meta-probability", "viewCount": null, "lastCommentedAt": "2017-06-17T04:36:07.750Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Chapman", "createdAt": "2011-03-17T17:41:11.044Z", "isAdmin": false, "displayName": "David_Chapman"}, "userId": "sNnWssxZ9SnSCxYTW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2xmKZu73gZLDEQw7c/probability-knowledge-and-meta-probability", "pageUrlRelative": "/posts/2xmKZu73gZLDEQw7c/probability-knowledge-and-meta-probability", "linkUrl": "https://www.lesswrong.com/posts/2xmKZu73gZLDEQw7c/probability-knowledge-and-meta-probability", "postedAtFormatted": "Tuesday, September 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Probability%2C%20knowledge%2C%20and%20meta-probability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProbability%2C%20knowledge%2C%20and%20meta-probability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2xmKZu73gZLDEQw7c%2Fprobability-knowledge-and-meta-probability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Probability%2C%20knowledge%2C%20and%20meta-probability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2xmKZu73gZLDEQw7c%2Fprobability-knowledge-and-meta-probability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2xmKZu73gZLDEQw7c%2Fprobability-knowledge-and-meta-probability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1397, "htmlBody": "<p>This article is the first in a sequence that will consider situations where probability estimates are not, by themselves, adequate to make rational decisions. This one introduces a \"meta-probability\" approach, borrowed from E. T. Jaynes, and uses it to analyze a gambling problem. This situation is one in which reasonably straightforward decision-theoretic methods suffice. Later articles introduce increasingly problematic cases.</p>\n<p><a id=\"more\"></a></p>\n<h2>A surprising decision anomaly</h2>\n<p>Let&rsquo;s say I&rsquo;ve recruited you as a subject in my thought experiment. I show you three cubical plastic boxes, about eight inches on a side. There&rsquo;s two green ones&mdash;identical as far as you can see&mdash;and a brown one. I explain that they are gambling machines: each has a faceplate with a slot that accepts a dollar coin, and an output slot that will return either two or zero dollars.</p>\n<p>I unscrew the faceplates to show you the mechanisms inside. They are quite simple. When you put a coin in, a wheel spins. It has a hundred holes around the rim. Each can be blocked, or not, with a teeny rubber plug. When the wheel slows to a halt, a sensor checks the nearest hole, and dispenses either zero or two coins.</p>\n<p>The brown box has 45 holes open, so it has probability p=0.45 of returning two coins. One green box has 90 holes open (p=0.9) and the other has none (p=0). I let you experiment with the boxes until you are satisfied these probabilities are accurate (or very nearly so).</p>\n<p>Then, I screw the faceplates back on, and put all the boxes in a black cloth sack with an elastic closure. I squidge the sack around, to mix up the boxes inside, and you reach in and pull one out at random.</p>\n<p>I give you a hundred one-dollar coins. You can put as many into the box as you like. You can keep as many coins as you don&rsquo;t gamble, plus whatever comes out of the box.</p>\n<p>If you pulled out the brown box, there&rsquo;s a 45% chance of getting $2 back, and the expected value of putting a dollar in is $0.90. Rationally, you should keep the hundred coins I gave you, and not gamble.</p>\n<p>If you pulled out a green box, there&rsquo;s a 50% chance that it&rsquo;s the one that pays two dollars 90% of the time, and a 50% chance that it&rsquo;s the one that never pays out. So, overall, there&rsquo;s a 45% chance of getting $2 back.</p>\n<p>Still, rationally, you should put some coins in the box. If it pays out at least once, you should gamble all the coins I gave you, because you know that you got the 90% box, and you&rsquo;ll nearly double your money.</p>\n<p>If you get nothing out after a few tries, you&rsquo;ve probably got the never-pay box, and you should hold onto the rest of your money. (Exercise for readers: how many no-payouts in a row should you accept before quitting?)</p>\n<p>What&rsquo;s interesting is that, when you have to decide whether or not to gamble your first coin, the probability is exactly the same in the two cases (p=0.45 of a $2 payout). However, the rational course of action is different. What&rsquo;s up with that?</p>\n<p>Here, a single probability value fails to capture everything you <strong>know</strong> about an uncertain event. And, it&rsquo;s a case in which that failure matters.</p>\n<p>Such limitations have been recognized <a href=\"http://en.wikipedia.org/wiki/Common_cause_and_special_cause_%28statistics%29#Origins_and_concepts\">almost since the beginning</a> of probability theory. Dozens of solutions have been proposed. In the rest of this article, I&rsquo;ll explore one. In subsequent articles, I&rsquo;ll look at the problem more generally.</p>\n<h2>Meta-probability</h2>\n<p>To think about the green box, we have to reason about <em>the probabilities of probabilities</em>. We could call this <strong>meta-probability</strong>, although that&rsquo;s not a standard term. Let&rsquo;s develop a method for it.</p>\n<p>Pull a penny out of your pocket. If you flip it, what&rsquo;s the probability it will come up heads? 0.5. Are you sure? Pretty darn sure.</p>\n<p>What&rsquo;s the probability that my local junior high school sportsball team will win its next game? I haven&rsquo;t a ghost of a clue. I don&rsquo;t know anything even about professional sportsball, and certainly nothing about &ldquo;my&rdquo; team. In a match between two teams, I&rsquo;d have to say the probability is 0.5.</p>\n<p>My girlfriend asked me today: &ldquo;Do you think Raley&rsquo;s will have dolmades?&rdquo; Raley&rsquo;s is our local supermarket. &ldquo;I don&rsquo;t know,&rdquo; I said. &ldquo;I guess it&rsquo;s about 50/50.&rdquo; But unlike sportsball, I know something about supermarkets. A fancy Whole Foods is very likely to have dolmades; a 7-11 almost certainly won&rsquo;t; Raley&rsquo;s is somewhere in between.</p>\n<p>How can we model these three cases? One way is by assigning probabilities to each possible probability between 0 and 1. In the case of a coin flip, 0.5 is much more probable than any other probability:</p>\n<p><img src=\"http://meaningness.com/images/lw/fig1.jpg\" alt=\"Tight Gaussian centered around 0.5\" width=\"534\" height=\"225\" /></p>\n<p>We can&rsquo;t be <em>absolutely sure</em> the probability is 0.5. In fact, it&rsquo;s almost certainly not <em>exactly</em> that, because coins aren&rsquo;t perfectly symmetrical. And, there&rsquo;s a very small probability that you&rsquo;ve been given a tricky penny that comes up tails only 10% of the time. So I&rsquo;ve illustrated this with a tight Gaussian centered around 0.5.</p>\n<p>In the sportsball case, I have no clue what the odds are. They might be anything between 0 to 1:</p>\n<p><img src=\"http://meaningness.com/images/lw/fig2.jpg\" alt=\"Flat line from 0 to 1\" width=\"534\" height=\"225\" /></p>\n<p>In the Raley&rsquo;s case, I have <em>some</em> knowledge, and extremely high and extremely low probabilities seem unlikely. So the curve looks something like this:</p>\n<p><img src=\"http://meaningness.com/images/lw/fig3.jpg\" alt=\"Wide Gaussian centered on 0.5\" width=\"533\" height=\"225\" /></p>\n<p>Each of these curves averages to a probability of 0.5, but they express different degrees of confidence in that probability.</p>\n<p>Now let&rsquo;s consider the gambling machines in my thought experiment. The brown box has a curve like this:</p>\n<p><img src=\"http://meaningness.com/images/lw/fig4.jpg\" alt=\"Tight Gaussian around 0.45\" width=\"534\" height=\"225\" /></p>\n<p>Whereas, when you&rsquo;ve chosen one of the two green boxes at random, the curve looks like this:</p>\n<p><img src=\"http://meaningness.com/images/lw/fig5.jpg\" alt=\"Bimodal distribution with sharp peaks at 0 and 0.9\" width=\"534\" height=\"225\" /></p>\n<p>Both these curves give an average probability of 0.45. However, a rational decision theory has to distinguish between them. Your optimal strategy in the two cases is quite different.</p>\n<p>With this framework, we can consider another box&mdash;a blue one. It has a fixed payout probability somewhere between 0 and 0.9. I put a random number of plugs in the holes in the spinning disk&mdash;leaving between 0 and 90 holes open. I used a noise diode to choose; but you don&rsquo;t get to see what the odds are. Here the probability-of-probability curve looks rather like this:</p>\n<p><img src=\"http://meaningness.com/images/lw/fig6.jpg\" alt=\"Flat line from 0 to 0.9, then zero above\" /></p>\n<p>This isn&rsquo;t quite right, because 0.23 and 0.24 are much more likely than 0.235&mdash;the plot should look like a comb&mdash;but for strategy choice the difference doesn&rsquo;t matter.</p>\n<p>What <em>is</em> your optimal strategy in this case?</p>\n<p>As with the green box, you ought to spend some coins gathering information about what the odds are. If your estimate of the probability is less than 0.5, when you get confident enough in that estimate, you should stop. If you&rsquo;re confident enough that it&rsquo;s more than 0.5, you should continue gambling.</p>\n<p>If you enjoy this sort of thing, you might like to work out what the exact optimal algorithm is.</p>\n<p>In the next article in this sequence, we&rsquo;ll look at some more complicated and interesting cases.</p>\n<h2>Further reading</h2>\n<p>The &ldquo;meta-probability&rdquo; approach I&rsquo;ve taken here is the <a href=\"http://www-biba.inrialpes.fr/Jaynes/cc18i.pdf\">A<sub>p</sub> distribution</a> of E. T. Jaynes. I find it highly intuitive, but it seems to have had almost no influence or application in practice. We&rsquo;ll see later that it has some problems, which might explain this.</p>\n<p>The green and blue boxes are related to &ldquo;multi-armed bandit problems.&rdquo; A &ldquo;one-armed bandit&rdquo; is a casino slot machine, which has defined odds of payout. A multi-armed bandit is a hypothetical generalization with several arms, each of which may have different, unknown odds. In general, you ought to pull each arm several times, to gain information. The question is: what is the optimal algorithm for deciding which arms to pull how many times, given the payments you have received so far?</p>\n<p>If you read the <a href=\"http://en.wikipedia.org/wiki/Multi-armed_bandit\">Wikipedia article</a> and follow some links, you&rsquo;ll find the concepts you need to find the optimal green and blue box strategies. But it might be more fun to try on your own first! The green box is simple. The blue box is harder, but the same general approach applies.</p>\n<p>Wikipedia also has an <a href=\"http://en.wikipedia.org/wiki/Credal_set#See_also\">accidental list</a> of formal approaches for problems where ordinary probability theory fails. This is far from complete, but a good starting point for a browser tab explosion.</p>\n<h2>Acknowledgements</h2>\n<p>Thanks to <a href=\"http://vajrayananow.wordpress.com/author/\">Rin&rsquo;dzin Pamo</a>, <a href=\"https://twitter.com/St_Rev\">St. Rev.</a>, <a href=\"/user/Matt_Simpson/overview/\">Matt_Simpson</a>, <a href=\"/user/Kaj_Sotala/overview/\">Kaj_Sotala</a>, and <a href=\"/user/Vaniver/overview/\">Vaniver</a> for helpful comments on drafts. Of course, they may disagree with my analyses, and aren&rsquo;t responsible for my mistakes!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 2, "dPPATLhRmhdJtJM2t": 2, "6nS8oYmSMuFMaiowF": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2xmKZu73gZLDEQw7c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 57, "baseScore": 57, "extendedScore": null, "score": 0.000154, "legacy": true, "legacyId": "23935", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 57, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This article is the first in a sequence that will consider situations where probability estimates are not, by themselves, adequate to make rational decisions. This one introduces a \"meta-probability\" approach, borrowed from E. T. Jaynes, and uses it to analyze a gambling problem. This situation is one in which reasonably straightforward decision-theoretic methods suffice. Later articles introduce increasingly problematic cases.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"A_surprising_decision_anomaly\">A surprising decision anomaly</h2>\n<p>Let\u2019s say I\u2019ve recruited you as a subject in my thought experiment. I show you three cubical plastic boxes, about eight inches on a side. There\u2019s two green ones\u2014identical as far as you can see\u2014and a brown one. I explain that they are gambling machines: each has a faceplate with a slot that accepts a dollar coin, and an output slot that will return either two or zero dollars.</p>\n<p>I unscrew the faceplates to show you the mechanisms inside. They are quite simple. When you put a coin in, a wheel spins. It has a hundred holes around the rim. Each can be blocked, or not, with a teeny rubber plug. When the wheel slows to a halt, a sensor checks the nearest hole, and dispenses either zero or two coins.</p>\n<p>The brown box has 45 holes open, so it has probability p=0.45 of returning two coins. One green box has 90 holes open (p=0.9) and the other has none (p=0). I let you experiment with the boxes until you are satisfied these probabilities are accurate (or very nearly so).</p>\n<p>Then, I screw the faceplates back on, and put all the boxes in a black cloth sack with an elastic closure. I squidge the sack around, to mix up the boxes inside, and you reach in and pull one out at random.</p>\n<p>I give you a hundred one-dollar coins. You can put as many into the box as you like. You can keep as many coins as you don\u2019t gamble, plus whatever comes out of the box.</p>\n<p>If you pulled out the brown box, there\u2019s a 45% chance of getting $2 back, and the expected value of putting a dollar in is $0.90. Rationally, you should keep the hundred coins I gave you, and not gamble.</p>\n<p>If you pulled out a green box, there\u2019s a 50% chance that it\u2019s the one that pays two dollars 90% of the time, and a 50% chance that it\u2019s the one that never pays out. So, overall, there\u2019s a 45% chance of getting $2 back.</p>\n<p>Still, rationally, you should put some coins in the box. If it pays out at least once, you should gamble all the coins I gave you, because you know that you got the 90% box, and you\u2019ll nearly double your money.</p>\n<p>If you get nothing out after a few tries, you\u2019ve probably got the never-pay box, and you should hold onto the rest of your money. (Exercise for readers: how many no-payouts in a row should you accept before quitting?)</p>\n<p>What\u2019s interesting is that, when you have to decide whether or not to gamble your first coin, the probability is exactly the same in the two cases (p=0.45 of a $2 payout). However, the rational course of action is different. What\u2019s up with that?</p>\n<p>Here, a single probability value fails to capture everything you <strong>know</strong> about an uncertain event. And, it\u2019s a case in which that failure matters.</p>\n<p>Such limitations have been recognized <a href=\"http://en.wikipedia.org/wiki/Common_cause_and_special_cause_%28statistics%29#Origins_and_concepts\">almost since the beginning</a> of probability theory. Dozens of solutions have been proposed. In the rest of this article, I\u2019ll explore one. In subsequent articles, I\u2019ll look at the problem more generally.</p>\n<h2 id=\"Meta_probability\">Meta-probability</h2>\n<p>To think about the green box, we have to reason about <em>the probabilities of probabilities</em>. We could call this <strong>meta-probability</strong>, although that\u2019s not a standard term. Let\u2019s develop a method for it.</p>\n<p>Pull a penny out of your pocket. If you flip it, what\u2019s the probability it will come up heads? 0.5. Are you sure? Pretty darn sure.</p>\n<p>What\u2019s the probability that my local junior high school sportsball team will win its next game? I haven\u2019t a ghost of a clue. I don\u2019t know anything even about professional sportsball, and certainly nothing about \u201cmy\u201d team. In a match between two teams, I\u2019d have to say the probability is 0.5.</p>\n<p>My girlfriend asked me today: \u201cDo you think Raley\u2019s will have dolmades?\u201d Raley\u2019s is our local supermarket. \u201cI don\u2019t know,\u201d I said. \u201cI guess it\u2019s about 50/50.\u201d But unlike sportsball, I know something about supermarkets. A fancy Whole Foods is very likely to have dolmades; a 7-11 almost certainly won\u2019t; Raley\u2019s is somewhere in between.</p>\n<p>How can we model these three cases? One way is by assigning probabilities to each possible probability between 0 and 1. In the case of a coin flip, 0.5 is much more probable than any other probability:</p>\n<p><img src=\"http://meaningness.com/images/lw/fig1.jpg\" alt=\"Tight Gaussian centered around 0.5\" width=\"534\" height=\"225\"></p>\n<p>We can\u2019t be <em>absolutely sure</em> the probability is 0.5. In fact, it\u2019s almost certainly not <em>exactly</em> that, because coins aren\u2019t perfectly symmetrical. And, there\u2019s a very small probability that you\u2019ve been given a tricky penny that comes up tails only 10% of the time. So I\u2019ve illustrated this with a tight Gaussian centered around 0.5.</p>\n<p>In the sportsball case, I have no clue what the odds are. They might be anything between 0 to 1:</p>\n<p><img src=\"http://meaningness.com/images/lw/fig2.jpg\" alt=\"Flat line from 0 to 1\" width=\"534\" height=\"225\"></p>\n<p>In the Raley\u2019s case, I have <em>some</em> knowledge, and extremely high and extremely low probabilities seem unlikely. So the curve looks something like this:</p>\n<p><img src=\"http://meaningness.com/images/lw/fig3.jpg\" alt=\"Wide Gaussian centered on 0.5\" width=\"533\" height=\"225\"></p>\n<p>Each of these curves averages to a probability of 0.5, but they express different degrees of confidence in that probability.</p>\n<p>Now let\u2019s consider the gambling machines in my thought experiment. The brown box has a curve like this:</p>\n<p><img src=\"http://meaningness.com/images/lw/fig4.jpg\" alt=\"Tight Gaussian around 0.45\" width=\"534\" height=\"225\"></p>\n<p>Whereas, when you\u2019ve chosen one of the two green boxes at random, the curve looks like this:</p>\n<p><img src=\"http://meaningness.com/images/lw/fig5.jpg\" alt=\"Bimodal distribution with sharp peaks at 0 and 0.9\" width=\"534\" height=\"225\"></p>\n<p>Both these curves give an average probability of 0.45. However, a rational decision theory has to distinguish between them. Your optimal strategy in the two cases is quite different.</p>\n<p>With this framework, we can consider another box\u2014a blue one. It has a fixed payout probability somewhere between 0 and 0.9. I put a random number of plugs in the holes in the spinning disk\u2014leaving between 0 and 90 holes open. I used a noise diode to choose; but you don\u2019t get to see what the odds are. Here the probability-of-probability curve looks rather like this:</p>\n<p><img src=\"http://meaningness.com/images/lw/fig6.jpg\" alt=\"Flat line from 0 to 0.9, then zero above\"></p>\n<p>This isn\u2019t quite right, because 0.23 and 0.24 are much more likely than 0.235\u2014the plot should look like a comb\u2014but for strategy choice the difference doesn\u2019t matter.</p>\n<p>What <em>is</em> your optimal strategy in this case?</p>\n<p>As with the green box, you ought to spend some coins gathering information about what the odds are. If your estimate of the probability is less than 0.5, when you get confident enough in that estimate, you should stop. If you\u2019re confident enough that it\u2019s more than 0.5, you should continue gambling.</p>\n<p>If you enjoy this sort of thing, you might like to work out what the exact optimal algorithm is.</p>\n<p>In the next article in this sequence, we\u2019ll look at some more complicated and interesting cases.</p>\n<h2 id=\"Further_reading\">Further reading</h2>\n<p>The \u201cmeta-probability\u201d approach I\u2019ve taken here is the <a href=\"http://www-biba.inrialpes.fr/Jaynes/cc18i.pdf\">A<sub>p</sub> distribution</a> of E. T. Jaynes. I find it highly intuitive, but it seems to have had almost no influence or application in practice. We\u2019ll see later that it has some problems, which might explain this.</p>\n<p>The green and blue boxes are related to \u201cmulti-armed bandit problems.\u201d A \u201cone-armed bandit\u201d is a casino slot machine, which has defined odds of payout. A multi-armed bandit is a hypothetical generalization with several arms, each of which may have different, unknown odds. In general, you ought to pull each arm several times, to gain information. The question is: what is the optimal algorithm for deciding which arms to pull how many times, given the payments you have received so far?</p>\n<p>If you read the <a href=\"http://en.wikipedia.org/wiki/Multi-armed_bandit\">Wikipedia article</a> and follow some links, you\u2019ll find the concepts you need to find the optimal green and blue box strategies. But it might be more fun to try on your own first! The green box is simple. The blue box is harder, but the same general approach applies.</p>\n<p>Wikipedia also has an <a href=\"http://en.wikipedia.org/wiki/Credal_set#See_also\">accidental list</a> of formal approaches for problems where ordinary probability theory fails. This is far from complete, but a good starting point for a browser tab explosion.</p>\n<h2 id=\"Acknowledgements\">Acknowledgements</h2>\n<p>Thanks to <a href=\"http://vajrayananow.wordpress.com/author/\">Rin\u2019dzin Pamo</a>, <a href=\"https://twitter.com/St_Rev\">St. Rev.</a>, <a href=\"/user/Matt_Simpson/overview/\">Matt_Simpson</a>, <a href=\"/user/Kaj_Sotala/overview/\">Kaj_Sotala</a>, and <a href=\"/user/Vaniver/overview/\">Vaniver</a> for helpful comments on drafts. Of course, they may disagree with my analyses, and aren\u2019t responsible for my mistakes!</p>", "sections": [{"title": "A surprising decision anomaly", "anchor": "A_surprising_decision_anomaly", "level": 1}, {"title": "Meta-probability", "anchor": "Meta_probability", "level": 1}, {"title": "Further reading", "anchor": "Further_reading", "level": 1}, {"title": "Acknowledgements", "anchor": "Acknowledgements", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "73 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 73, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-17T13:56:36.236Z", "modifiedAt": null, "url": null, "title": "Another way our brains betray us", "slug": "another-way-our-brains-betray-us", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:35.172Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KtsJYsgS8WQztAzkm/another-way-our-brains-betray-us", "pageUrlRelative": "/posts/KtsJYsgS8WQztAzkm/another-way-our-brains-betray-us", "linkUrl": "https://www.lesswrong.com/posts/KtsJYsgS8WQztAzkm/another-way-our-brains-betray-us", "postedAtFormatted": "Tuesday, September 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20way%20our%20brains%20betray%20us&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20way%20our%20brains%20betray%20us%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKtsJYsgS8WQztAzkm%2Fanother-way-our-brains-betray-us%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20way%20our%20brains%20betray%20us%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKtsJYsgS8WQztAzkm%2Fanother-way-our-brains-betray-us", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKtsJYsgS8WQztAzkm%2Fanother-way-our-brains-betray-us", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<p>This appeared in the news yesterday.</p>\n<p>http://www.alternet.org/media/most-depressing-discovery-about-brain-ever?paging=off</p>\n<blockquote>\n<p>It turns out that in the public realm, a lack of information isn&rsquo;t the  real problem. The hurdle is how our minds work, no matter how smart we  think we are. We want to believe we&rsquo;re rational, but reason turns out  to be the ex post facto way we rationalize what our emotions already  want to believe.</p>\n<p>...</p>\n<p>The bleakest finding was that the more advanced that people&rsquo;s math  skills were, the&nbsp;more&nbsp;likely it was that their political views, whether  liberal or conservative, made them&nbsp;less&nbsp;able to solve the math problem. [...] what these studies of how our minds work suggest is that the political  judgments we&rsquo;ve already made are impervious to facts that contradict  us.</p>\n<p>...</p>\n<p>Denial is business-as-usual for our brains. More and better facts don&rsquo;t  turn low-information voters into well-equipped citizens. It just makes  them more committed to their misperceptions.</p>\n<p>...</p>\n<p>When there&rsquo;s a conflict between partisan beliefs and plain evidence,  it&rsquo;s the beliefs that win. The power of emotion over reason isn&rsquo;t a bug  in our human operating systems, it&rsquo;s a feature.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KtsJYsgS8WQztAzkm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 5, "extendedScore": null, "score": 1.344538305711762e-06, "legacy": true, "legacyId": "24191", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-17T17:09:35.414Z", "modifiedAt": null, "url": null, "title": "Dennett on the selfish neuron, etc.", "slug": "dennett-on-the-selfish-neuron-etc", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:30.694Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xm3NAyPjpTcXHs9qC/dennett-on-the-selfish-neuron-etc", "pageUrlRelative": "/posts/xm3NAyPjpTcXHs9qC/dennett-on-the-selfish-neuron-etc", "linkUrl": "https://www.lesswrong.com/posts/xm3NAyPjpTcXHs9qC/dennett-on-the-selfish-neuron-etc", "postedAtFormatted": "Tuesday, September 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dennett%20on%20the%20selfish%20neuron%2C%20etc.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADennett%20on%20the%20selfish%20neuron%2C%20etc.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxm3NAyPjpTcXHs9qC%2Fdennett-on-the-selfish-neuron-etc%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dennett%20on%20the%20selfish%20neuron%2C%20etc.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxm3NAyPjpTcXHs9qC%2Fdennett-on-the-selfish-neuron-etc", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxm3NAyPjpTcXHs9qC%2Fdennett-on-the-selfish-neuron-etc", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 390, "htmlBody": "<p><a href=\"http://edge.org/conversation/normal-well-tempered-mind\">Dennett</a>:</p>\n<blockquote>\n<p>Mike Merzenich sutured a monkey's fingers together so that it didn't need as much cortex to represent two separate individual digits, and pretty soon the cortical regions that were representing those two digits shrank, making that part of the cortex available to use for other things. When the sutures were removed, the cortical regions soon resumed pretty much their earlier dimensions. If you blindfold yourself for eight weeks, as Alvaro Pascual-Leone does in his experiments, you find that your visual cortex starts getting adapted for Braille, for haptic perception, for touch.</p>\n<p>The way the brain spontaneously reorganizes itself in response to trauma of this sort, or just novel experience, is itself one of the most amazing features of the brain, and if you don't have an architecture that can explain how that could happen and why that is, your model has a major defect. I think you really have to think in terms of individual neurons as micro-agents, and ask what's in it for them?</p>\n<p>Why should these neurons be so eager to pitch in and do this other work just because they don't have a job? Well, they're out of work. They're unemployed, and if you're unemployed, you're not getting your neuromodulators. If you're not getting your neuromodulators, your neuromodulator receptors are going to start disappearing, and pretty soon you're going to be really out of work, and then you're going to die.</p>\n</blockquote>\n<p>I hadn't thought about any of this-- I thought the hard problem of brains was that dendrites grow so that neurons aren't arranged in a static map. Apparently that is just one of the hard problems.</p>\n<p>He also discusses the question of how much of culture is parasitic, that philosophy has something valuable to offer about free will (I don't know what he has in mind there), the hard question of how people choose who to trust and why they're so bad at it (he thinks people chose their investment advisers more carefully than they chose their pastors, I suspect he's over-optimistic), and a detailed look at <a href=\"http://www.epjournal.net/wp-content/uploads/EP08122150.pdf\">Preachers Who Are Not Believers</a>. That last looks intriguing-- part of the situations is that preachers have been taught it's very bad to shake someone else's faith, so there's an added layer of inhibition which keeps preachers doing their usual job even after they're no longer believers themselves.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xm3NAyPjpTcXHs9qC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 15, "extendedScore": null, "score": 1.3447094878273613e-06, "legacy": true, "legacyId": "24192", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-17T17:38:12.282Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham/RTLW HPMoR discussion, ch. 86", "slug": "meetup-durham-rtlw-hpmor-discussion-ch-86", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/s9HBLrnLdHKsdhnGD/meetup-durham-rtlw-hpmor-discussion-ch-86", "pageUrlRelative": "/posts/s9HBLrnLdHKsdhnGD/meetup-durham-rtlw-hpmor-discussion-ch-86", "linkUrl": "https://www.lesswrong.com/posts/s9HBLrnLdHKsdhnGD/meetup-durham-rtlw-hpmor-discussion-ch-86", "postedAtFormatted": "Tuesday, September 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2086&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2086%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs9HBLrnLdHKsdhnGD%2Fmeetup-durham-rtlw-hpmor-discussion-ch-86%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2086%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs9HBLrnLdHKsdhnGD%2Fmeetup-durham-rtlw-hpmor-discussion-ch-86", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs9HBLrnLdHKsdhnGD%2Fmeetup-durham-rtlw-hpmor-discussion-ch-86", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/r2'>Durham/RTLW HPMoR discussion, ch. 86</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 September 2013 12:00:11PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham NC, 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Time for more HPMoR!</p>\n\n<p>Meet at Fullsteam around 12 for discussion of the appointed chapters. If you have a chance to do the reading, bring a question, observation, or discussion topic. If not, come hang out anyhow!</p>\n\n<p>12:00 gather, obtain comestibles from Cocoa Cinnamon or other local establishments <br />\n12:30 discussion <br />\n2:00 free-form Saturday enjoyment begins</p>\n\n<p>To be notified about other upcoming meetups &amp; events, join the RTLW mailing list: <a href=\"http://groups.google.com/group/rtlw\" rel=\"nofollow\">http://groups.google.com/group/rtlw</a> !</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/r2'>Durham/RTLW HPMoR discussion, ch. 86</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "s9HBLrnLdHKsdhnGD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "24193", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__86\">Discussion article for the meetup : <a href=\"/meetups/r2\">Durham/RTLW HPMoR discussion, ch. 86</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 September 2013 12:00:11PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham NC, 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Time for more HPMoR!</p>\n\n<p>Meet at Fullsteam around 12 for discussion of the appointed chapters. If you have a chance to do the reading, bring a question, observation, or discussion topic. If not, come hang out anyhow!</p>\n\n<p>12:00 gather, obtain comestibles from Cocoa Cinnamon or other local establishments <br>\n12:30 discussion <br>\n2:00 free-form Saturday enjoyment begins</p>\n\n<p>To be notified about other upcoming meetups &amp; events, join the RTLW mailing list: <a href=\"http://groups.google.com/group/rtlw\" rel=\"nofollow\">http://groups.google.com/group/rtlw</a> !</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__861\">Discussion article for the meetup : <a href=\"/meetups/r2\">Durham/RTLW HPMoR discussion, ch. 86</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 86", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__86", "level": 1}, {"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 86", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__861", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-17T18:06:40.294Z", "modifiedAt": null, "url": null, "title": "Meetup : Bratislava Meetup VII.", "slug": "meetup-bratislava-meetup-vii", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Viliam_Bur", "createdAt": "2011-08-23T08:46:37.137Z", "isAdmin": false, "displayName": "Viliam_Bur"}, "userId": "yaaPhHzrvrPf7je22", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6nAqqgvqCpGgPg89M/meetup-bratislava-meetup-vii", "pageUrlRelative": "/posts/6nAqqgvqCpGgPg89M/meetup-bratislava-meetup-vii", "linkUrl": "https://www.lesswrong.com/posts/6nAqqgvqCpGgPg89M/meetup-bratislava-meetup-vii", "postedAtFormatted": "Tuesday, September 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bratislava%20Meetup%20VII.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bratislava%20Meetup%20VII.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6nAqqgvqCpGgPg89M%2Fmeetup-bratislava-meetup-vii%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bratislava%20Meetup%20VII.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6nAqqgvqCpGgPg89M%2Fmeetup-bratislava-meetup-vii", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6nAqqgvqCpGgPg89M%2Fmeetup-bratislava-meetup-vii", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 32, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/r3'>Bratislava Meetup VII.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 September 2013 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\"> Bistro The Peach, Heydukova 21, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Unstructured discussion. ;-)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/r3'>Bratislava Meetup VII.</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6nAqqgvqCpGgPg89M", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.3447601272502401e-06, "legacy": true, "legacyId": "24194", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bratislava_Meetup_VII_\">Discussion article for the meetup : <a href=\"/meetups/r3\">Bratislava Meetup VII.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 September 2013 06:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\"> Bistro The Peach, Heydukova 21, Bratislava</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Unstructured discussion. ;-)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Bratislava_Meetup_VII_1\">Discussion article for the meetup : <a href=\"/meetups/r3\">Bratislava Meetup VII.</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bratislava Meetup VII.", "anchor": "Discussion_article_for_the_meetup___Bratislava_Meetup_VII_", "level": 1}, {"title": "Discussion article for the meetup : Bratislava Meetup VII.", "anchor": "Discussion_article_for_the_meetup___Bratislava_Meetup_VII_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-17T20:35:34.895Z", "modifiedAt": null, "url": null, "title": "Help us name a short primer on AI risk!", "slug": "help-us-name-a-short-primer-on-ai-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:32.556Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/atyyMJ6z6d3mpvMjm/help-us-name-a-short-primer-on-ai-risk", "pageUrlRelative": "/posts/atyyMJ6z6d3mpvMjm/help-us-name-a-short-primer-on-ai-risk", "linkUrl": "https://www.lesswrong.com/posts/atyyMJ6z6d3mpvMjm/help-us-name-a-short-primer-on-ai-risk", "postedAtFormatted": "Tuesday, September 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20us%20name%20a%20short%20primer%20on%20AI%20risk!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20us%20name%20a%20short%20primer%20on%20AI%20risk!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FatyyMJ6z6d3mpvMjm%2Fhelp-us-name-a-short-primer-on-ai-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20us%20name%20a%20short%20primer%20on%20AI%20risk!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FatyyMJ6z6d3mpvMjm%2Fhelp-us-name-a-short-primer-on-ai-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FatyyMJ6z6d3mpvMjm%2Fhelp-us-name-a-short-primer-on-ai-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 404, "htmlBody": "<p>MIRI will soon publish a short book by Stuart Armstrong on the topic of AI risk. The book is <em>currently</em> titled &ldquo;AI-Risk Primer&rdquo; by default, but we&rsquo;re looking for something a little more catchy (just as we did for the upcoming <a href=\"/lw/h7t/help_us_name_the_sequences_ebook/\">Sequences ebook</a>).</p>\n<p>The book is meant to be accessible and avoids technical jargon. Here is the table of contents and a few snippets from the book, to give you an idea of the content and style:</p>\n<ol>\n<li>Terminator versus the AI</li>\n<li>Strength versus Intelligence</li>\n<li>What Is Intelligence? Can We Achieve It Artificially?</li>\n<li>How Powerful Could AIs Become?</li>\n<li>Talking to an Alien Mind</li>\n<li>Our Values Are Complex and Fragile</li>\n<li>What, Precisely, Do We Really (Really) Want?</li>\n<li>We Need to Get It All <em>Exactly</em> Right</li>\n<li>Listen to the Sound of Absent Experts</li>\n<li>A Summary</li>\n<li>That&rsquo;s Where <em>You</em> Come In &hellip;</li>\n</ol>\n<blockquote>\n<p>The Terminator is a creature from our primordial nightmares: tall, strong, aggressive, and nearly indestructible. We&rsquo;re strongly primed to fear such a being&mdash;it resembles the lions, tigers, and bears that our ancestors so feared when they wandered alone on the savanna and tundra.</p>\n<p>&hellip;</p>\n<p>As a species, we humans haven&rsquo;t achieved success through our natural armor plating, our claws, our razor-sharp teeth, or our poison-filled stingers. Though we have reasonably efficient bodies, it&rsquo;s our <em>brains</em> that have made the difference. It&rsquo;s through our social, cultural, and technological intelligence that we have raised ourselves to our current position.</p>\n<p>&hellip;</p>\n<p>Consider what would happen if an AI ever achieved the ability to function socially&mdash;to hold conversations with a reasonable facsimile of human fluency. For humans to increase their social skills, they need to go through painful trial and error processes, scrounge hints from more articulate individuals or from television, or try to hone their instincts by having dozens of conversations. An AI could go through a similar process, undeterred by social embarrassment, and with perfect memory. But it could also sift through vast databases of previous human conversations, analyze thousands of publications on human psychology, anticipate where conversations are leading many steps in advance, and always pick the right tone and pace to respond with. Imagine a human who, every time they opened their mouth, had spent a solid year to ponder and research whether their response was going to be maximally effective. That is what a social AI would be like.</p>\n</blockquote>\n<p>So, title suggestions?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "atyyMJ6z6d3mpvMjm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 11, "extendedScore": null, "score": 1.3448922481056621e-06, "legacy": true, "legacyId": "24195", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 75, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Hmc2NE6oTipCvuZdN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-17T22:38:06.160Z", "modifiedAt": null, "url": null, "title": "Thought experiment: The transhuman pedophile", "slug": "thought-experiment-the-transhuman-pedophile", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:37.333Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a8dCAtNKM8eK3WHAE/thought-experiment-the-transhuman-pedophile", "pageUrlRelative": "/posts/a8dCAtNKM8eK3WHAE/thought-experiment-the-transhuman-pedophile", "linkUrl": "https://www.lesswrong.com/posts/a8dCAtNKM8eK3WHAE/thought-experiment-the-transhuman-pedophile", "postedAtFormatted": "Tuesday, September 17th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thought%20experiment%3A%20The%20transhuman%20pedophile&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThought%20experiment%3A%20The%20transhuman%20pedophile%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa8dCAtNKM8eK3WHAE%2Fthought-experiment-the-transhuman-pedophile%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thought%20experiment%3A%20The%20transhuman%20pedophile%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa8dCAtNKM8eK3WHAE%2Fthought-experiment-the-transhuman-pedophile", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa8dCAtNKM8eK3WHAE%2Fthought-experiment-the-transhuman-pedophile", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 226, "htmlBody": "<p>There's a recent science fiction story that I can't recall the name of, in which the narrator is traveling somewhere via plane, and the security check includes a brain scan for deviance. The narrator is a pedophile. Everyone who sees the results of the scan is horrified--not that he's a pedophile, but that his particular brain abnormality is easily fixed, so that means he's chosen to remain a pedophile. He's closely monitored, so he'll never be able to act on those desires, but he keeps them anyway, because that's part of who he is.</p>\n<p>What would you do in his place?<a id=\"more\"></a></p>\n<p>In the language of good old-fashioned AI, his pedophilia is a goal or a terminal value. \"Fixing\" him means changing or erasing that value. People here sometimes say that a rational agent should never change its terminal values. (If one goal is unobtainable, the agent will simply not pursue that goal.) Why, then, can we imagine the man being tempted to do so? Would it be a failure of rationality?</p>\n<p>If the answer is that one terminal value can rationally set a goal to change another terminal value, then either</p>\n<ol>\n<li>any terminal value of a rational agent can change, or</li>\n<li>we need another word for the <em>really</em> terminal values that can't be changed rationally, and a way of identifying them, and a proof that they exist.</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a8dCAtNKM8eK3WHAE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 8, "extendedScore": null, "score": 1.3450009725310907e-06, "legacy": true, "legacyId": "24197", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 74, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-18T03:09:34.800Z", "modifiedAt": null, "url": null, "title": "Meetup : Philadelphia - The Sword of Good", "slug": "meetup-philadelphia-the-sword-of-good", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mare-of-night", "createdAt": "2013-04-06T13:26:03.532Z", "isAdmin": false, "displayName": "mare-of-night"}, "userId": "6thzLTpEnEpcZF8bf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KZ5rk6nQ6CygxgR5c/meetup-philadelphia-the-sword-of-good", "pageUrlRelative": "/posts/KZ5rk6nQ6CygxgR5c/meetup-philadelphia-the-sword-of-good", "linkUrl": "https://www.lesswrong.com/posts/KZ5rk6nQ6CygxgR5c/meetup-philadelphia-the-sword-of-good", "postedAtFormatted": "Wednesday, September 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Philadelphia%20-%20The%20Sword%20of%20Good&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Philadelphia%20-%20The%20Sword%20of%20Good%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZ5rk6nQ6CygxgR5c%2Fmeetup-philadelphia-the-sword-of-good%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Philadelphia%20-%20The%20Sword%20of%20Good%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZ5rk6nQ6CygxgR5c%2Fmeetup-philadelphia-the-sword-of-good", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZ5rk6nQ6CygxgR5c%2Fmeetup-philadelphia-the-sword-of-good", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 132, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/r4'>Philadelphia - The Sword of Good</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 September 2013 12:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1515 Chestnut St, Philadelphia, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>Location</strong>: Givovani's Pizza</p>\n\n<p><strong>Discussion topic</strong>: <a href=\"http://yudkowsky.net/other/fiction/the-sword-of-good\" rel=\"nofollow\">The Sword of Good</a> (A short story - I highly recommend reading it before the meetup if you are able, because it's meant to have some element of surprise.)</p>\n\n<p>Fiction doesn't seem to be used as a discussion topic for meetups very often, but I noticed that we tend to end up talking about stories together anyway, and it usually leads to interesting conversations.</p>\n\n<p>If you have any simple games you'd like to play as a group, please bring them - it will give us something to do in the beginning while people are still arriving.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/r4'>Philadelphia - The Sword of Good</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KZ5rk6nQ6CygxgR5c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.3452419350336747e-06, "legacy": true, "legacyId": "24200", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Philadelphia___The_Sword_of_Good\">Discussion article for the meetup : <a href=\"/meetups/r4\">Philadelphia - The Sword of Good</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 September 2013 12:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1515 Chestnut St, Philadelphia, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>Location</strong>: Givovani's Pizza</p>\n\n<p><strong>Discussion topic</strong>: <a href=\"http://yudkowsky.net/other/fiction/the-sword-of-good\" rel=\"nofollow\">The Sword of Good</a> (A short story - I highly recommend reading it before the meetup if you are able, because it's meant to have some element of surprise.)</p>\n\n<p>Fiction doesn't seem to be used as a discussion topic for meetups very often, but I noticed that we tend to end up talking about stories together anyway, and it usually leads to interesting conversations.</p>\n\n<p>If you have any simple games you'd like to play as a group, please bring them - it will give us something to do in the beginning while people are still arriving.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Philadelphia___The_Sword_of_Good1\">Discussion article for the meetup : <a href=\"/meetups/r4\">Philadelphia - The Sword of Good</a></h2>", "sections": [{"title": "Discussion article for the meetup : Philadelphia - The Sword of Good", "anchor": "Discussion_article_for_the_meetup___Philadelphia___The_Sword_of_Good", "level": 1}, {"title": "Discussion article for the meetup : Philadelphia - The Sword of Good", "anchor": "Discussion_article_for_the_meetup___Philadelphia___The_Sword_of_Good1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-18T12:59:00.625Z", "modifiedAt": null, "url": null, "title": "What did governments get right? Gotta list them all!", "slug": "what-did-governments-get-right-gotta-list-them-all", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:30.790Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Fs4SL76T2bHizLKs8/what-did-governments-get-right-gotta-list-them-all", "pageUrlRelative": "/posts/Fs4SL76T2bHizLKs8/what-did-governments-get-right-gotta-list-them-all", "linkUrl": "https://www.lesswrong.com/posts/Fs4SL76T2bHizLKs8/what-did-governments-get-right-gotta-list-them-all", "postedAtFormatted": "Wednesday, September 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20did%20governments%20get%20right%3F%20Gotta%20list%20them%20all!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20did%20governments%20get%20right%3F%20Gotta%20list%20them%20all!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFs4SL76T2bHizLKs8%2Fwhat-did-governments-get-right-gotta-list-them-all%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20did%20governments%20get%20right%3F%20Gotta%20list%20them%20all!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFs4SL76T2bHizLKs8%2Fwhat-did-governments-get-right-gotta-list-them-all", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFs4SL76T2bHizLKs8%2Fwhat-did-governments-get-right-gotta-list-them-all", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 622, "htmlBody": "<p>When predicting future threats, we also need to predict future policy responses. If mass pandemics are inevitable, it matters whether governments and international organisations can rise to the challenge or not. But its very hard to get a valid intuitive picture of government competence. Consider the following two scenarios:</p>\n<ul>\n<li>Governments are morasses of incompetence, saturated by turf wars, perverse incentives, inefficiencies, regulatory capture, and excessive risk aversion. The media reports a lot of the bad stuff, but doesn't have nearly enough space for it all, as it has to find some room for sport and naked celebrities. The average person will hear 1 story of government incompetence a day, anyone following the news will hear 10, a dedicated obsessive will hear 100 - but this is just the tip of the iceberg. The media sometimes reports good news to counterbalance the bad, at about a rate of 1-to-10 of good news to bad. This rate is wildly over-optimistic.</li>\n</ul>\n<ul>\n<li>Governments are filled mainly by politicians desperate to make a positive mark on the world. Civil servants are professional and certainly not stupid, working to clear criteria with a good internal culture, in systems that have learnt the lessons of the past and have improved. There is a certain amount of error, inefficiency, and corruption, but these are more exceptions than rules. Highly politicised issues tend to be badly handled, but less contentious issues are dealt with well. The media, knowing that bad news sells, fills their pages mainly with bad stuff (though they often have to exaggerate issues). The average person will hear 1 story of government incompetence a day, anyone following the news will hear 10, a dedicated obsessive will hear 100 - but some of those are quite distorted. The media sometimes reports good news to counterbalance the bad, at about a rate of 1-to-10 of good news to bad.&nbsp;This rate is wildly over-pessimistic.</li>\n</ul>\n<p>These two situations are, of course, completely indistinguishable for the public. The smartest and most dedicated of outside observers can't form an accurate picture of the situation. Which means that, unless you have spent your entire life inside various levels of government (which brings its own distortions!), you don't really have a clue at general government competence. There's some very faint clues that governments may be working better than we generally think: looking at the achievements of past governments certainly seems to hint at a higher rate of success than the reported numbers today. And simply thinking about the amount of things that don't go wrong in a city, every day, hints that someone is doing their job. But these clues are extremely weak.</p>\n<p>At this point, one should look up political scientists and other researchers. I hope to be doing that at some point (or the FHI may hire someone to do that). In the meantime, I just wanted to collect a few stories of government success to counterbalance the general media atmosphere. The purpose is not just to train my intuition away from the \"governments are intrinsically incompetent\" that I currently have (and which is unjustified by objective evidence). It's also the start of a project to get a better picture of <em>where </em>governments fail and <em>where </em>they succeed - which would be much more accurate and much more useful than an abstract \"government competence level\" intuition. And would be needed if we try and predict policy responses to specific future threats.</p>\n<p>So I'm asking if commentators want to share government success stories they may have come across. Especially unusual or unsuspected stories. Vaccinations, clean-air acts, and legally establishing limited liability companies are very well known success stories, for instance, but are there more obscure examples that hint an unexpected diligence in surprising areas?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Fs4SL76T2bHizLKs8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 13, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "24205", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-18T16:49:17.876Z", "modifiedAt": null, "url": null, "title": "[LINK] Larry = Harry sans magic? Google vs. Death", "slug": "link-larry-harry-sans-magic-google-vs-death", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:38.752Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3XBEcnyD8BiGXLSR4/link-larry-harry-sans-magic-google-vs-death", "pageUrlRelative": "/posts/3XBEcnyD8BiGXLSR4/link-larry-harry-sans-magic-google-vs-death", "linkUrl": "https://www.lesswrong.com/posts/3XBEcnyD8BiGXLSR4/link-larry-harry-sans-magic-google-vs-death", "postedAtFormatted": "Wednesday, September 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Larry%20%3D%20Harry%20sans%20magic%3F%20Google%20vs.%20Death&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Larry%20%3D%20Harry%20sans%20magic%3F%20Google%20vs.%20Death%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3XBEcnyD8BiGXLSR4%2Flink-larry-harry-sans-magic-google-vs-death%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Larry%20%3D%20Harry%20sans%20magic%3F%20Google%20vs.%20Death%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3XBEcnyD8BiGXLSR4%2Flink-larry-harry-sans-magic-google-vs-death", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3XBEcnyD8BiGXLSR4%2Flink-larry-harry-sans-magic-google-vs-death", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 70, "htmlBody": "<p><a href=\"http://googlepress.blogspot.ca/2013/09/calico-announcement.html\">Google's announcement</a>, <a href=\"http://techland.time.com/2013/09/18/google-vs-death/\">Time magazine</a>&nbsp;rather sensationalist headline.</p>\n<p>In any case, it's nice to know that Google set its sights to \"<span style=\"color: #444444; font-family: arial, sans-serif; font-size: 13px; line-height: 20px;\">challenge ... aging and associated diseases\". Apple's Tim Cook:</span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #444444; font-family: arial, sans-serif; font-size: 13px; line-height: 20px;\">For too many of our friends and family, life has been cut short or the quality of their life is too often lacking. Art is one of the crazy ones who thinks it doesn&rsquo;t have to be this way.</span>&nbsp;</p>\n<p>One more step towards \"world optimization\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vmvTYnmaKA73fYDe5": 1, "t7t9nW6BtJhfGNSR6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3XBEcnyD8BiGXLSR4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 36, "extendedScore": null, "score": 0.000118, "legacy": true, "legacyId": "24208", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-18T18:40:16.882Z", "modifiedAt": null, "url": null, "title": "Help CFAR Take Manhattan", "slug": "help-cfar-take-manhattan", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "palladias", "createdAt": "2012-04-03T13:45:53.766Z", "isAdmin": false, "displayName": "palladias"}, "userId": "Bv2LXWzZf96WGpqJ5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cC25KPDD8aMnGHytM/help-cfar-take-manhattan", "pageUrlRelative": "/posts/cC25KPDD8aMnGHytM/help-cfar-take-manhattan", "linkUrl": "https://www.lesswrong.com/posts/cC25KPDD8aMnGHytM/help-cfar-take-manhattan", "postedAtFormatted": "Wednesday, September 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20CFAR%20Take%20Manhattan&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20CFAR%20Take%20Manhattan%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcC25KPDD8aMnGHytM%2Fhelp-cfar-take-manhattan%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20CFAR%20Take%20Manhattan%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcC25KPDD8aMnGHytM%2Fhelp-cfar-take-manhattan", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcC25KPDD8aMnGHytM%2Fhelp-cfar-take-manhattan", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 200, "htmlBody": "<p>CFAR is coming to New York City at the beginning of November for our first <a href=\"http://rationality.org/workshops/\">full-scale workshop</a> outside the Bay Area. &nbsp;And we need your help to run it smoothly.</p>\n<p>We're looking for volunteers in the tri-state area who can help us carry out all the behind the scenes logistics that keep our workshops running smoothly. &nbsp;And we'll need a bit more help this time, since we'll be in this location for the first time.</p>\n<p>Our volunteers help us with tasks like:</p>\n<p>&nbsp;</p>\n<ul>\n<li>Set up/Break down for meals</li>\n<li>Organization for the end of workshop parties</li>\n<li>Running errands to replenish supplies</li>\n<li>Filming some presentations</li>\n<li>etc</li>\n</ul>\n<div>Usually, volunteers get the chance to sit in on a handful of the classes we run during the workshop. &nbsp;For our Berkeley workshops, our volunteer ranks have been largely filled out by alumni, who want to make sure other people get to experience the workshop experience they enjoyed.</div>\n<div><br /></div>\n<div>If you think you might be interested and available to help us out this November in New York, <a href=\"https://docs.google.com/forms/d/1db6FiMLKPoIwLNCI2HoWghv1-XMXuC_CxcZSfrmZq6Y/viewform\">please fill out this five question survey</a>, and we'll be in touch. &nbsp;And if you have questions, I'll meet you in the comments.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cC25KPDD8aMnGHytM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 1.3460686034542657e-06, "legacy": true, "legacyId": "24196", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-18T19:07:56.653Z", "modifiedAt": null, "url": null, "title": "Understanding Simpson's Paradox", "slug": "understanding-simpson-s-paradox", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:30.618Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FGsoajyYJRczieqh5/understanding-simpson-s-paradox", "pageUrlRelative": "/posts/FGsoajyYJRczieqh5/understanding-simpson-s-paradox", "linkUrl": "https://www.lesswrong.com/posts/FGsoajyYJRczieqh5/understanding-simpson-s-paradox", "postedAtFormatted": "Wednesday, September 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Understanding%20Simpson's%20Paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUnderstanding%20Simpson's%20Paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFGsoajyYJRczieqh5%2Funderstanding-simpson-s-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Understanding%20Simpson's%20Paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFGsoajyYJRczieqh5%2Funderstanding-simpson-s-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFGsoajyYJRczieqh5%2Funderstanding-simpson-s-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 440, "htmlBody": "<p>An article by <a href=\"http://bayes.cs.ucla.edu/jp_home.html\">Judea Pearl</a>, available <a href=\"http://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf\">here</a>. It's quick at 8 pages, and worth reading if you enjoy statistics (though I think people who already are familiar with the math of causality<sup>1</sup> will get more out of it than others<sup>2</sup>). I'll talk here about the part that I think is generally interesting:<a id=\"more\"></a></p>\n<blockquote>\n<div style=\"font-size: 15.94px; font-family: sans-serif; left: 94.08px; top: 705.491px; transform: scale(0.998836, 1); transform-origin: 0% 0% 0px;\" dir=\"ltr\">Any claim to a resolution of a paradox, especially one that has resisted a century of attempted resolution must meet certain criteria. First and foremost, the solution must explain why people consider the phenomenon surprising or unbelievable. Second, the solution must identify the class of scenarios in which the paradox may surface, and distinguish it from scenarios where it will surely not surface. Finally, in those scenarios where the paradox leads to indecision, we must identify the correct answer, explain the features of the scenario that lead to that choice, and prove mathematically that the answer chosen is indeed correct. The next three subsections will describe how these three requirements are met in the case of Simpson's paradox and, naturally, will proceed to convince readers that the paradox deserves the title \"resolved.\"</div>\n</blockquote>\n<p>I've never really liked the name \"paradox,\" because what it seems to mean is \"unintuitive phenomenon.\" (<a href=\"http://en.wikipedia.org/wiki/Paradox\">Wikipedia</a> puts it as \"something which seems false and yet might be true.\") The trouble is that \"unintuitive\" is a <a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>, and it makes sense to <a href=\"/lw/hs/think_like_reality/\">think like reality</a>, so that true things seem true to you, instead of still seeming false. (For example, when I first learned about <a href=\"http://en.wikipedia.org/wiki/Zeno%27s_paradox\">Zeno's Paradox</a>, I already knew calculus, and so <em>Zeno's</em> position was the one that seemed confusing and false.)</p>\n<p>What I like most about Pearl's article is that it explicitly recognizes the importance of fully <a href=\"/lw/of/dissolving_the_question/\">dissolving</a> the paradox,<sup>3</sup> and seems to do so. Simpson's Paradox isn't an unsolvable problem in statistics, it's a straightforward reversal effect--<em>only</em> if you use the language of causality.</p>\n<p>&nbsp;</p>\n<hr />\n<p>1. <a href=\"/lw/emc/causality_a_chapter_by_chapter_review/\">My review of Causality</a> gives a taste of what it would look like to be familiar with the math, but you'd need to actually read the book to pick it up. The <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 for Beginners</a> sequence is relevant, and contains Eliezer's attempt to explain the basics of causality in <a href=\"/lw/ev3/causal_diagrams_and_causal_models/\">Causal Diagrams and Causal Models</a>.</p>\n<p>2. Pearl discusses how you would go about using simulations to show that do calculus gives you the right result, but leaves it as an exercise for the reader.</p>\n<p>3. <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">How An Algorithm Feels From Inside</a> is probably a better place to start than Dissolving the Question, and I  can't help but echo a question from it: \"So what kind of math design  corresponds to [Simpson's Paradox]?\"</p>\n<p>See also: bentarm's explanation of <a href=\"/lw/3q3/simpsons_paradox/\">Simpson's Paradox</a>.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FGsoajyYJRczieqh5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 19, "extendedScore": null, "score": 4.6e-05, "legacy": true, "legacyId": "24211", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5", "tWLFWAndSZSYN6rPB", "Mc6QcrsbH5NRXbCRX", "jnjjzkH8Fdzg4D6EK", "hzuSDMx7pd2uxFc5w", "yA4gF5KrboK2m2Xu7", "7FJRnxbRtT7Sbzizs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-18T20:11:58.583Z", "modifiedAt": null, "url": null, "title": "Help FHI crowdsource a directory of researchers with high consequentialist significance and win a prize ", "slug": "help-fhi-crowdsource-a-directory-of-researchers-with-high", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:30.027Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "bfihzqs6WfG3xCqng", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fSt3yT7v92i6rpr9c/help-fhi-crowdsource-a-directory-of-researchers-with-high", "pageUrlRelative": "/posts/fSt3yT7v92i6rpr9c/help-fhi-crowdsource-a-directory-of-researchers-with-high", "linkUrl": "https://www.lesswrong.com/posts/fSt3yT7v92i6rpr9c/help-fhi-crowdsource-a-directory-of-researchers-with-high", "postedAtFormatted": "Wednesday, September 18th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20FHI%20crowdsource%20a%20directory%20of%20researchers%20with%20high%20consequentialist%20significance%20and%20win%20a%20prize%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20FHI%20crowdsource%20a%20directory%20of%20researchers%20with%20high%20consequentialist%20significance%20and%20win%20a%20prize%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfSt3yT7v92i6rpr9c%2Fhelp-fhi-crowdsource-a-directory-of-researchers-with-high%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20FHI%20crowdsource%20a%20directory%20of%20researchers%20with%20high%20consequentialist%20significance%20and%20win%20a%20prize%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfSt3yT7v92i6rpr9c%2Fhelp-fhi-crowdsource-a-directory-of-researchers-with-high", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfSt3yT7v92i6rpr9c%2Fhelp-fhi-crowdsource-a-directory-of-researchers-with-high", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 544, "htmlBody": "<p>Let me introduce myself. My name is Kristian R&ouml;nn and I work for the Future of Humanity Institute (FHI) as a project manager. This is my first LessWrong post.</p>\n<p><strong>tl;tr:</strong></p>\n<ol>\n<li>Wouldn't it be useful to have a directory of researchers (dependent or independent) that do research with high consequentialist significance?</li>\n<li>Solution: sign up at <a href=\"http://mirivolunteers.org\">mirivolunteers.org</a> and help us crowdsource such a list!</li>\n<li>The best contributor will win a 30 minute skype conversation with one of the FHI researchers!</li>\n</ol>\n<p><strong>The long version:</strong></p>\n<p>We all know that some scientific research has extremely high consequentialist significance (good or bad). But much research with high consequentialist significance also falls squarely within the scope of philosophy (see William MacAskill's blog post on <a href=\"http://80000hours.org/blog/46-how-to-be-a-high-impact-philosopher\">high-impact philosophy</a>). Such important areas of philosophical research include ethics, epistemology and philosophy of mind (especially applied to existential risk research). Yet professional philosophy is a diseased discipline.</p>\n<p>First, professional philosophers very often work on trivial questions (e.g., &ldquo;how is knowledge best analyzed?&rdquo;).</p>\n<p>Secondly, when they do address questions of importance, they are seldom motivated by a concern to find the truth, and answers are instead assessed on poor criteria, such as originality, elegance, &ldquo;cleverness&rdquo;, and tendency to vindicate cherished beliefs.</p>\n<p>Fortunately, there are a few philosophers addressing many of the relevant questions, and from whose work we can learn a great deal. It also seems like the problem of building a friendly AI forces us to think about the philosophical questions mentioned in a more formal/rigorous way. Because in order to design a friendly AI I need to figure out:</p>\n<ol>\n<li>What utility function it ought to have (<strong>ethics</strong>). E.g. <a href=\"http://www.intelligence.org/files/CEV.pdf&lrm;\">Coherent Extrapolated Volition</a>.</li>\n<li>How to make correct inferences in an optimal way (<strong>epistemology</strong>). E.g. <a href=\"http://wiki.lesswrong.com/wiki/G%C3%B6del_machine\">G&ouml;del machines</a> and <a href=\"http://wiki.lesswrong.com/wiki/AIXI\">AIXI</a>.</li>\n<li>What set of things/concept exists in the agents ontology (<strong>metaphysics</strong>); and more specifically. \n<ul>\n<li>How to distinguish between those things that are a part of the set of conscious things and those who are not (<strong>philosophy of mind</strong>).</li>\n<li>How to track the identity of things in time and space (<strong>personal identity over time</strong>).</li>\n</ul>\n</li>\n</ol>\n<p>(Author's note: I believe some of the problems listed above are not solvable, since they are just a quibble over semantics)</p>\n<p>This means that a lot of the most important philosophy is done by computer scientists, physicists and mathematicians (sometimes without even knowing it). All non empirical research could (very broadly speaking) be classified as philosophy.</p>\n<p>-Wouldn't it be useful to have a directory of researchers (both in science and philosophy) that are doing research of high consequentialist significance?<br />-The answer is: yes!<br />-But how can we make such a list?<br />-By letting everyone on LessWrong and elsewhere crowdsource the list, by filling out this <a href=\"https://docs.google.com/spreadsheet/ccc?key=0AhL3FhEm5icgdEtPUmtGZXlVUWhCXzBEX3ZaMk5LYWc#gid=0\">Google Spreadsheet</a> (you are encouraged to add independent researchers such as LW contributors on the list).</p>\n<p>If you also sign up on <a href=\"http://mirivolunteers.org\">mirivolunteers.org</a> you will be able to collect points for your contributions. The volunteer who collect most points to this challenge over the next 30 days will be rewarded with the opportunity to video chat with an FHI researcher - your choice of either <a href=\"http://www.fhi.ox.ac.uk/about/staff/\">Stuart Armstrong</a>, or <a href=\"http://www.fhi.ox.ac.uk/about/staff/\">Nick Beckstead</a>!</p>\n<p>(I would like to thank Pablo Stafforini for inspiring and writing an earlier document on which parts of this post are based)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fSt3yT7v92i6rpr9c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 6, "extendedScore": null, "score": 1.346150097502908e-06, "legacy": true, "legacyId": "24212", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Let me introduce myself. My name is Kristian R\u00f6nn and I work for the Future of Humanity Institute (FHI) as a project manager. This is my first LessWrong post.</p>\n<p><strong id=\"tl_tr_\">tl;tr:</strong></p>\n<ol>\n<li>Wouldn't it be useful to have a directory of researchers (dependent or independent) that do research with high consequentialist significance?</li>\n<li>Solution: sign up at <a href=\"http://mirivolunteers.org\">mirivolunteers.org</a> and help us crowdsource such a list!</li>\n<li>The best contributor will win a 30 minute skype conversation with one of the FHI researchers!</li>\n</ol>\n<p><strong id=\"The_long_version_\">The long version:</strong></p>\n<p>We all know that some scientific research has extremely high consequentialist significance (good or bad). But much research with high consequentialist significance also falls squarely within the scope of philosophy (see William MacAskill's blog post on <a href=\"http://80000hours.org/blog/46-how-to-be-a-high-impact-philosopher\">high-impact philosophy</a>). Such important areas of philosophical research include ethics, epistemology and philosophy of mind (especially applied to existential risk research). Yet professional philosophy is a diseased discipline.</p>\n<p>First, professional philosophers very often work on trivial questions (e.g., \u201chow is knowledge best analyzed?\u201d).</p>\n<p>Secondly, when they do address questions of importance, they are seldom motivated by a concern to find the truth, and answers are instead assessed on poor criteria, such as originality, elegance, \u201ccleverness\u201d, and tendency to vindicate cherished beliefs.</p>\n<p>Fortunately, there are a few philosophers addressing many of the relevant questions, and from whose work we can learn a great deal. It also seems like the problem of building a friendly AI forces us to think about the philosophical questions mentioned in a more formal/rigorous way. Because in order to design a friendly AI I need to figure out:</p>\n<ol>\n<li>What utility function it ought to have (<strong>ethics</strong>). E.g. <a href=\"http://www.intelligence.org/files/CEV.pdf\u200e\">Coherent Extrapolated Volition</a>.</li>\n<li>How to make correct inferences in an optimal way (<strong>epistemology</strong>). E.g. <a href=\"http://wiki.lesswrong.com/wiki/G%C3%B6del_machine\">G\u00f6del machines</a> and <a href=\"http://wiki.lesswrong.com/wiki/AIXI\">AIXI</a>.</li>\n<li>What set of things/concept exists in the agents ontology (<strong>metaphysics</strong>); and more specifically. \n<ul>\n<li>How to distinguish between those things that are a part of the set of conscious things and those who are not (<strong>philosophy of mind</strong>).</li>\n<li>How to track the identity of things in time and space (<strong>personal identity over time</strong>).</li>\n</ul>\n</li>\n</ol>\n<p>(Author's note: I believe some of the problems listed above are not solvable, since they are just a quibble over semantics)</p>\n<p>This means that a lot of the most important philosophy is done by computer scientists, physicists and mathematicians (sometimes without even knowing it). All non empirical research could (very broadly speaking) be classified as philosophy.</p>\n<p>-Wouldn't it be useful to have a directory of researchers (both in science and philosophy) that are doing research of high consequentialist significance?<br>-The answer is: yes!<br>-But how can we make such a list?<br>-By letting everyone on LessWrong and elsewhere crowdsource the list, by filling out this <a href=\"https://docs.google.com/spreadsheet/ccc?key=0AhL3FhEm5icgdEtPUmtGZXlVUWhCXzBEX3ZaMk5LYWc#gid=0\">Google Spreadsheet</a> (you are encouraged to add independent researchers such as LW contributors on the list).</p>\n<p>If you also sign up on <a href=\"http://mirivolunteers.org\">mirivolunteers.org</a> you will be able to collect points for your contributions. The volunteer who collect most points to this challenge over the next 30 days will be rewarded with the opportunity to video chat with an FHI researcher - your choice of either <a href=\"http://www.fhi.ox.ac.uk/about/staff/\">Stuart Armstrong</a>, or <a href=\"http://www.fhi.ox.ac.uk/about/staff/\">Nick Beckstead</a>!</p>\n<p>(I would like to thank Pablo Stafforini for inspiring and writing an earlier document on which parts of this post are based)</p>", "sections": [{"title": "tl;tr:", "anchor": "tl_tr_", "level": 1}, {"title": "The long version:", "anchor": "The_long_version_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-19T00:30:08.868Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver \"Monthly\" Super Meetup", "slug": "meetup-vancouver-monthly-super-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ds25CLQE6dqN8oG7M/meetup-vancouver-monthly-super-meetup", "pageUrlRelative": "/posts/Ds25CLQE6dqN8oG7M/meetup-vancouver-monthly-super-meetup", "linkUrl": "https://www.lesswrong.com/posts/Ds25CLQE6dqN8oG7M/meetup-vancouver-monthly-super-meetup", "postedAtFormatted": "Thursday, September 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20%22Monthly%22%20Super%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20%22Monthly%22%20Super%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDs25CLQE6dqN8oG7M%2Fmeetup-vancouver-monthly-super-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20%22Monthly%22%20Super%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDs25CLQE6dqN8oG7M%2Fmeetup-vancouver-monthly-super-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDs25CLQE6dqN8oG7M%2Fmeetup-vancouver-monthly-super-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 109, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/r5'>Vancouver &quot;Monthly&quot; Super Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 September 2013 03:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">3598 Main Street, Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's that time! The Vancouver meetup would like to invite you all to our \"monthly\" super public social hangout meetup. We'll hang out at a cafe on main street and probably have some good discussion about something or other that we have not decided yet. This is supposed to be casual and newperson friendly, so please come.</p>\n\n<p>Meetup is Sunday the 22nd at Bean Around the World at main and 20th at 15:00.</p>\n\n<p>Our mailing list is <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">vancouver-rationalists on google groups</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/r5'>Vancouver &quot;Monthly&quot; Super Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ds25CLQE6dqN8oG7M", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.3463795946427826e-06, "legacy": true, "legacyId": "24213", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver__Monthly__Super_Meetup\">Discussion article for the meetup : <a href=\"/meetups/r5\">Vancouver \"Monthly\" Super Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 September 2013 03:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">3598 Main Street, Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's that time! The Vancouver meetup would like to invite you all to our \"monthly\" super public social hangout meetup. We'll hang out at a cafe on main street and probably have some good discussion about something or other that we have not decided yet. This is supposed to be casual and newperson friendly, so please come.</p>\n\n<p>Meetup is Sunday the 22nd at Bean Around the World at main and 20th at 15:00.</p>\n\n<p>Our mailing list is <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">vancouver-rationalists on google groups</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver__Monthly__Super_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/r5\">Vancouver \"Monthly\" Super Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver \"Monthly\" Super Meetup", "anchor": "Discussion_article_for_the_meetup___Vancouver__Monthly__Super_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Vancouver \"Monthly\" Super Meetup", "anchor": "Discussion_article_for_the_meetup___Vancouver__Monthly__Super_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-19T01:46:15.194Z", "modifiedAt": null, "url": null, "title": "Meetup : The Anthropic Principle and the Great Filter", "slug": "meetup-the-anthropic-principle-and-the-great-filter", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:58.134Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Danny_Hintze", "createdAt": "2010-12-04T23:01:40.826Z", "isAdmin": false, "displayName": "Danny_Hintze"}, "userId": "2fHm6t2WFDMPShg5b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/peLnKi63475XdvAsQ/meetup-the-anthropic-principle-and-the-great-filter", "pageUrlRelative": "/posts/peLnKi63475XdvAsQ/meetup-the-anthropic-principle-and-the-great-filter", "linkUrl": "https://www.lesswrong.com/posts/peLnKi63475XdvAsQ/meetup-the-anthropic-principle-and-the-great-filter", "postedAtFormatted": "Thursday, September 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20The%20Anthropic%20Principle%20and%20the%20Great%20Filter&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20The%20Anthropic%20Principle%20and%20the%20Great%20Filter%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpeLnKi63475XdvAsQ%2Fmeetup-the-anthropic-principle-and-the-great-filter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20The%20Anthropic%20Principle%20and%20the%20Great%20Filter%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpeLnKi63475XdvAsQ%2Fmeetup-the-anthropic-principle-and-the-great-filter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpeLnKi63475XdvAsQ%2Fmeetup-the-anthropic-principle-and-the-great-filter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/r6'>The Anthropic Principle and the Great Filter</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 September 2013 02:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">300 E Orange Mall Tempe, AZ 85281 (480) 965-6164</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting up at Hayden for a few hours to discuss why we don't see other life in the cosmos and the implications of that.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/r6'>The Anthropic Principle and the Great Filter</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "peLnKi63475XdvAsQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3464472604737632e-06, "legacy": true, "legacyId": "24214", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___The_Anthropic_Principle_and_the_Great_Filter\">Discussion article for the meetup : <a href=\"/meetups/r6\">The Anthropic Principle and the Great Filter</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 September 2013 02:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">300 E Orange Mall Tempe, AZ 85281 (480) 965-6164</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting up at Hayden for a few hours to discuss why we don't see other life in the cosmos and the implications of that.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___The_Anthropic_Principle_and_the_Great_Filter1\">Discussion article for the meetup : <a href=\"/meetups/r6\">The Anthropic Principle and the Great Filter</a></h2>", "sections": [{"title": "Discussion article for the meetup : The Anthropic Principle and the Great Filter", "anchor": "Discussion_article_for_the_meetup___The_Anthropic_Principle_and_the_Great_Filter", "level": 1}, {"title": "Discussion article for the meetup : The Anthropic Principle and the Great Filter", "anchor": "Discussion_article_for_the_meetup___The_Anthropic_Principle_and_the_Great_Filter1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-19T01:55:53.878Z", "modifiedAt": null, "url": null, "title": "Proxy Donating as Spam Filter", "slug": "proxy-donating-as-spam-filter", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:30.571Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "beth", "createdAt": "2013-05-26T22:26:02.042Z", "isAdmin": false, "displayName": "beth"}, "userId": "DDywyCmZQApQpihE6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RyijxukhiAgpL7ezQ/proxy-donating-as-spam-filter", "pageUrlRelative": "/posts/RyijxukhiAgpL7ezQ/proxy-donating-as-spam-filter", "linkUrl": "https://www.lesswrong.com/posts/RyijxukhiAgpL7ezQ/proxy-donating-as-spam-filter", "postedAtFormatted": "Thursday, September 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proxy%20Donating%20as%20Spam%20Filter&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProxy%20Donating%20as%20Spam%20Filter%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRyijxukhiAgpL7ezQ%2Fproxy-donating-as-spam-filter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proxy%20Donating%20as%20Spam%20Filter%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRyijxukhiAgpL7ezQ%2Fproxy-donating-as-spam-filter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRyijxukhiAgpL7ezQ%2Fproxy-donating-as-spam-filter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<p>One thing that sometimes makes me hesitate to donate to a cause is that, unless you're donating in person and using cash, you're inevitably signing up for a gigantic stream of junk mail, not just from the organization you gave money to, but other, often totally unrelated charities as well. I haven't noticed a lot of these charities offering a privacy policy that lets you avoid this, but I haven't paid close attention because frankly, I don't think I'd have a lot of confidence in such a privacy policy even if I saw one in some literature.</p>\n<p>I wonder if there are donations to be gained in guaranteeing this sort of privacy by going through a third party. Charities could include the usual pre-addressed envelope in their mailings, only instead of their own address it would go to an organization called Givepal. The envelope would include the charity's id, and donors would be instructed to make their checks out to Givepal, who would then distribute the money to the specified charity, keeping the transaction anonymous. Givepal could survive by taking a cut of the donations if necessary, or could itself operate as a non-profit.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RyijxukhiAgpL7ezQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.3464558360719665e-06, "legacy": true, "legacyId": "24215", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-19T03:06:02.195Z", "modifiedAt": null, "url": null, "title": "Book Review: Basic Category Theory for Computer Scientists (MIRI course list)", "slug": "book-review-basic-category-theory-for-computer-scientists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:35.561Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Jar4BGrJ7BiQemBDM/book-review-basic-category-theory-for-computer-scientists", "pageUrlRelative": "/posts/Jar4BGrJ7BiQemBDM/book-review-basic-category-theory-for-computer-scientists", "linkUrl": "https://www.lesswrong.com/posts/Jar4BGrJ7BiQemBDM/book-review-basic-category-theory-for-computer-scientists", "postedAtFormatted": "Thursday, September 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Book%20Review%3A%20Basic%20Category%20Theory%20for%20Computer%20Scientists%20(MIRI%20course%20list)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABook%20Review%3A%20Basic%20Category%20Theory%20for%20Computer%20Scientists%20(MIRI%20course%20list)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJar4BGrJ7BiQemBDM%2Fbook-review-basic-category-theory-for-computer-scientists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Book%20Review%3A%20Basic%20Category%20Theory%20for%20Computer%20Scientists%20(MIRI%20course%20list)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJar4BGrJ7BiQemBDM%2Fbook-review-basic-category-theory-for-computer-scientists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJar4BGrJ7BiQemBDM%2Fbook-review-basic-category-theory-for-computer-scientists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1107, "htmlBody": "<p>I'm reviewing the books on the <a href=\"http://intelligence.org/courses/\">MIRI course list</a>. After finishing <a href=\"/lw/il1/book_review_cognitive_science_miri_course_list/\">Cognitive Science</a> I picked up <em>Basic Category Theory for Computer Scientists</em>, by Benjamin C. Pierce.</p>\n<h1 id=\"categorytheoryforcomputerscientists\">Basic Category Theory for Computer Scientists</h1>\n<p style=\"text-align: center\"><img src=\"http://ecx.images-amazon.com/images/I/411642SVPGL._.jpg\" alt=\"\" width=\"353\" height=\"475\" /></p>\n<p>This book is tiny, clocking in at around 80 pages. Don't be fooled, it packs a punch.</p>\n<p>A word of warning: when the title says \"for Computer Scientists\", it is not abusing the term. I went in expecting Category Theory for Computer Programmers and a tone like \"Welcome, Java Programmer, to the crazy world of math!\". What I got was a lean, no-bullshit introduction to category that assumes mathematical competence.</p>\n<p>\"Computer Scientist\" and \"Programmer\" get mixed up in common parlance, so casual programmers are cautioned that this book targets the former group. <em>Category Theory for Computer Scientists</em> assumes you're familiar with proof-writing, set theory, functional programming, and denotational semantics.</p>\n<p>In return, <em>Category Theory</em> wastes none of your time. I found this refreshing, but unexpected.</p>\n<p>I'll give a brief overview of the contents of the book before discussing them.</p>\n<h2 id=\"chaptersummaries\">Chapter Summaries</h2>\n<ol>\n<li><a href=\"#1basicconstructions\">Basic Constructions</a></li>\n<li><a href=\"#2functorsnaturaltransformationsandadjoints\">Functors, Natural Transformations, and Adjoints</a></li>\n<li><a href=\"#3applications\">Applications</a></li>\n<li><a href=\"#4furtherreading\">Further Reading</a></li>\n</ol>\n<h2 id=\"1basicconstructions\"><a id=\"more\"></a>\n<hr />\n</h2>\n<h2 id=\"1basicconstructions\">1. Basic Constructions</h2>\n<p>Introduces Categories.</p>\n<p>Introduces monomorphisms (f\u2218g = f\u2218h &rarr; g=h) and epimorphisms (g\u2218f = h\u2218f &rarr; g=h), then uses these to introduce categorical duals (similar constructs with the direction of the arrows swapped).</p>\n<p>Introduces isomorphisms and the concept of equality up to isomorphism.</p>\n<p>Introduces initial and terminal objects (objects with exactly one arrow from/to each object).</p>\n<p>Introduces binary products and coproducts. Generalizes to arbitrary products.</p>\n<p>Introduces equalizers and coequalizers.</p>\n<p>Introduces pullbacks, briefly mentions pushforwards.</p>\n<p>Generalizes equalizers, products, and pullbacks to terminal cones (which are limits).</p>\n<p>Introduces exponentiation.</p>\n<p>Mentions Cartesian Closed Categories (categories with products, exponents, and a terminal object).</p>\n<hr />\n<h2 id=\"2functorsnaturaltransformationsandadjoints\">2. Functors, Natural Transformations, and Adjoints</h2>\n<p>Introduces Functors (arrows between categories).</p>\n<p>Introduces F-Algebras (an extreme generalization of algebra).</p>\n<p>Introduces Natural Transformations (structure-preserving mappings between functors).</p>\n<p>Introduces Adjoints (functors related in way that generalizes efficiency/optimality).</p>\n<hr />\n<h2 id=\"3applications\">3. Applications</h2>\n<p>Discusses four applications of category theory:</p>\n<ol>\n<li>\n<p>Closed Cartesian Categories are in correspondence with lambda calculi.</p>\n</li>\n<li>\n<p>Category theory can help make implicit conversion &amp; generic operators more consistent in programming languages.</p>\n</li>\n<li>\n<p>Category theory is linked to type theory, domain theory, and algebraic semantics (all useful in programming semantics).</p>\n</li>\n<li>\n<p>Category theory revolutionized how programming languages construct underlying denotations.</p>\n</li>\n</ol> \n<hr />\n<h2 id=\"4furtherreading\">4. Further Reading</h2>\n<p>A list of textbooks, introductory articles, reference books, and research articles that the author recommends for further learning.</p>\n<hr />\n<h2 id=\"discussion\">Discussion</h2>\n<p>The first two chapters of this book are the important parts. The third chapter points out ways that category theory has been applied to computer science, which I found interesting but not relevant to my goal (of learning category theory). The fourth chapter provides a list of resources, which will be handy to have around.</p>\n<p>A textbook review is somewhat ungrounded if you don't know the reviewer's background: Category Theory was not a complete mystery to me when I picked up this book. I gained some little familiarity with the subject osmotically when learning Type Theory and messing around in Haskell. However, Category Theory always looked like abstract nonsense and I'd never studied it explicitly.</p>\n<p>Given that background, this book served me very well.</p>\n<p>Most of my utility was derived from doing the exercises in the book. My goal was to build a mental implementation of category theory, and reading math without doing it works about as well as writing code without running it.</p>\n<p>The first five exercises alone corrected a handful of misconceptions I had about category theory, and were sufficient to take theorems from \"opaque abstract nonsense\" to \"vaguely intuitive\".</p>\n<p>(In my case, the fact that \"the diagram commutes\" means \"all paths from one vertex to another compose to the same arrow\" made a lot of things click. I expect such clicking points to vary wildly between people, so I won't mention more.)</p>\n<p>It is likely that any other category theory textbook could have given similar results by providing exercises. The selling point of this book is the narrative: if you don't like narratives, this is the book for you.</p>\n<p>In fact, this book is sometimes too terse. Take, for example, this suggestion after the introduction of adjoint functors:</p>\n<blockquote>\n<p>The reader who has persevered this far is urged to consult a standard textbook for more details on alternative treatments of adjoints.</p>\n</blockquote>\n<p>Or this, right when things were getting interesting:</p>\n<blockquote>\n<p>A longer discussion of representability is beyond the scope of this introduction, but it is often named as one of the two or three key concepts of category theory.</p>\n</blockquote>\n<p>That said, the book is titled <em>Basic Category Theory</em>, so I can't complain.</p>\n<h2 id=\"shouldireadit\">Should I read it?</h2>\n<p>It really depends on your goals.</p>\n<p>This book does not motivate the math: It assumes you want to know category theory, and teaches you without embellishment. If you are wondering what this whole category theory thing is and why it matters then you should probably find a friendlier introduction.</p>\n<p>However, if:</p>\n<ol>\n<li>You're familiar with the basic idea of category theory</li>\n<li>You want to learn the basics</li>\n<li>You are comfortable with functional programming and/or set theory</li>\n<li>You're motivated and don't need much guidance</li>\n</ol>\n<p>then you should strongly consider reading the first two chapters of this book (and perhaps chapter 3 section 4).</p>\n<p>If you're looking for a really deep understanding of category theory, you should probably look elsewhere; this book doesn't step beyond the basics.</p>\n<p>All that assumes you want to learn category theory, which probably isn't true of the general audience. A more pertinent question is perhaps:</p>\n<h2>Should I learn category theory?</h2>\n<p>It depends on your goals. I think it was worth learning, but I'm the sort of person who delights in clean abstractions.</p>\n<p>If nothing else, category theory is a lesson in how far abstractions can be pushed. F-Algebras, for instance, are one of the most abstract ideas I've ever encountered. Category-theoretic products (or, more generally, cones) are astonishingly powerful given their generality. I was quite surprised by how far you can strip down exponentiation.</p>\n<p>That said, category theory won't help the average person act more rationally.</p>\n<p>If you're into math, programming, or physics then category theory will likely help you out. Otherwise, I wouldn't recommend starting here.</p>\n<p>If you happen to be a programmer considering diving off the deep end, I strongly recommend familiarizing yourself with pure functional programming languages first. (Haskell is a good place to start.) My familiarity with type theory and my use of functors made category theory easier to approach. (And besides, pure functional programming is lots of fun.)</p>\n<h2 id=\"finalnotes\">Final Notes</h2>\n<p>I'm somewhat surprised that the MIRI course list Category Theory textbook doesn't discuss Representation Theory.</p>\n<p>Otherwise, this book is precisely what I expected from the MIRI course list: very good at shutting up and giving you the data. I came away pleased.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 2, "4Kcm4etxAJjmeDkHP": 2, "NrvXXL3iGjjxu5B7d": 2, "6nS8oYmSMuFMaiowF": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Jar4BGrJ7BiQemBDM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 52, "extendedScore": null, "score": 0.00013, "legacy": true, "legacyId": "24216", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I'm reviewing the books on the <a href=\"http://intelligence.org/courses/\">MIRI course list</a>. After finishing <a href=\"/lw/il1/book_review_cognitive_science_miri_course_list/\">Cognitive Science</a> I picked up <em>Basic Category Theory for Computer Scientists</em>, by Benjamin C. Pierce.</p>\n<h1 id=\"Basic_Category_Theory_for_Computer_Scientists\">Basic Category Theory for Computer Scientists</h1>\n<p style=\"text-align: center\"><img src=\"http://ecx.images-amazon.com/images/I/411642SVPGL._.jpg\" alt=\"\" width=\"353\" height=\"475\"></p>\n<p>This book is tiny, clocking in at around 80 pages. Don't be fooled, it packs a punch.</p>\n<p>A word of warning: when the title says \"for Computer Scientists\", it is not abusing the term. I went in expecting Category Theory for Computer Programmers and a tone like \"Welcome, Java Programmer, to the crazy world of math!\". What I got was a lean, no-bullshit introduction to category that assumes mathematical competence.</p>\n<p>\"Computer Scientist\" and \"Programmer\" get mixed up in common parlance, so casual programmers are cautioned that this book targets the former group. <em>Category Theory for Computer Scientists</em> assumes you're familiar with proof-writing, set theory, functional programming, and denotational semantics.</p>\n<p>In return, <em>Category Theory</em> wastes none of your time. I found this refreshing, but unexpected.</p>\n<p>I'll give a brief overview of the contents of the book before discussing them.</p>\n<h2 id=\"Chapter_Summaries\">Chapter Summaries</h2>\n<ol>\n<li><a href=\"#1basicconstructions\">Basic Constructions</a></li>\n<li><a href=\"#2functorsnaturaltransformationsandadjoints\">Functors, Natural Transformations, and Adjoints</a></li>\n<li><a href=\"#3applications\">Applications</a></li>\n<li><a href=\"#4furtherreading\">Further Reading</a></li>\n</ol>\n<h2 id=\"1basicconstructions\"><a id=\"more\"></a>\n<hr>\n</h2>\n<h2 id=\"1__Basic_Constructions\">1. Basic Constructions</h2>\n<p>Introduces Categories.</p>\n<p>Introduces monomorphisms (f\u2218g = f\u2218h \u2192 g=h) and epimorphisms (g\u2218f = h\u2218f \u2192 g=h), then uses these to introduce categorical duals (similar constructs with the direction of the arrows swapped).</p>\n<p>Introduces isomorphisms and the concept of equality up to isomorphism.</p>\n<p>Introduces initial and terminal objects (objects with exactly one arrow from/to each object).</p>\n<p>Introduces binary products and coproducts. Generalizes to arbitrary products.</p>\n<p>Introduces equalizers and coequalizers.</p>\n<p>Introduces pullbacks, briefly mentions pushforwards.</p>\n<p>Generalizes equalizers, products, and pullbacks to terminal cones (which are limits).</p>\n<p>Introduces exponentiation.</p>\n<p>Mentions Cartesian Closed Categories (categories with products, exponents, and a terminal object).</p>\n<hr>\n<h2 id=\"2__Functors__Natural_Transformations__and_Adjoints\">2. Functors, Natural Transformations, and Adjoints</h2>\n<p>Introduces Functors (arrows between categories).</p>\n<p>Introduces F-Algebras (an extreme generalization of algebra).</p>\n<p>Introduces Natural Transformations (structure-preserving mappings between functors).</p>\n<p>Introduces Adjoints (functors related in way that generalizes efficiency/optimality).</p>\n<hr>\n<h2 id=\"3__Applications\">3. Applications</h2>\n<p>Discusses four applications of category theory:</p>\n<ol>\n<li>\n<p>Closed Cartesian Categories are in correspondence with lambda calculi.</p>\n</li>\n<li>\n<p>Category theory can help make implicit conversion &amp; generic operators more consistent in programming languages.</p>\n</li>\n<li>\n<p>Category theory is linked to type theory, domain theory, and algebraic semantics (all useful in programming semantics).</p>\n</li>\n<li>\n<p>Category theory revolutionized how programming languages construct underlying denotations.</p>\n</li>\n</ol> \n<hr>\n<h2 id=\"4__Further_Reading\">4. Further Reading</h2>\n<p>A list of textbooks, introductory articles, reference books, and research articles that the author recommends for further learning.</p>\n<hr>\n<h2 id=\"Discussion\">Discussion</h2>\n<p>The first two chapters of this book are the important parts. The third chapter points out ways that category theory has been applied to computer science, which I found interesting but not relevant to my goal (of learning category theory). The fourth chapter provides a list of resources, which will be handy to have around.</p>\n<p>A textbook review is somewhat ungrounded if you don't know the reviewer's background: Category Theory was not a complete mystery to me when I picked up this book. I gained some little familiarity with the subject osmotically when learning Type Theory and messing around in Haskell. However, Category Theory always looked like abstract nonsense and I'd never studied it explicitly.</p>\n<p>Given that background, this book served me very well.</p>\n<p>Most of my utility was derived from doing the exercises in the book. My goal was to build a mental implementation of category theory, and reading math without doing it works about as well as writing code without running it.</p>\n<p>The first five exercises alone corrected a handful of misconceptions I had about category theory, and were sufficient to take theorems from \"opaque abstract nonsense\" to \"vaguely intuitive\".</p>\n<p>(In my case, the fact that \"the diagram commutes\" means \"all paths from one vertex to another compose to the same arrow\" made a lot of things click. I expect such clicking points to vary wildly between people, so I won't mention more.)</p>\n<p>It is likely that any other category theory textbook could have given similar results by providing exercises. The selling point of this book is the narrative: if you don't like narratives, this is the book for you.</p>\n<p>In fact, this book is sometimes too terse. Take, for example, this suggestion after the introduction of adjoint functors:</p>\n<blockquote>\n<p>The reader who has persevered this far is urged to consult a standard textbook for more details on alternative treatments of adjoints.</p>\n</blockquote>\n<p>Or this, right when things were getting interesting:</p>\n<blockquote>\n<p>A longer discussion of representability is beyond the scope of this introduction, but it is often named as one of the two or three key concepts of category theory.</p>\n</blockquote>\n<p>That said, the book is titled <em>Basic Category Theory</em>, so I can't complain.</p>\n<h2 id=\"Should_I_read_it_\">Should I read it?</h2>\n<p>It really depends on your goals.</p>\n<p>This book does not motivate the math: It assumes you want to know category theory, and teaches you without embellishment. If you are wondering what this whole category theory thing is and why it matters then you should probably find a friendlier introduction.</p>\n<p>However, if:</p>\n<ol>\n<li>You're familiar with the basic idea of category theory</li>\n<li>You want to learn the basics</li>\n<li>You are comfortable with functional programming and/or set theory</li>\n<li>You're motivated and don't need much guidance</li>\n</ol>\n<p>then you should strongly consider reading the first two chapters of this book (and perhaps chapter 3 section 4).</p>\n<p>If you're looking for a really deep understanding of category theory, you should probably look elsewhere; this book doesn't step beyond the basics.</p>\n<p>All that assumes you want to learn category theory, which probably isn't true of the general audience. A more pertinent question is perhaps:</p>\n<h2 id=\"Should_I_learn_category_theory_\">Should I learn category theory?</h2>\n<p>It depends on your goals. I think it was worth learning, but I'm the sort of person who delights in clean abstractions.</p>\n<p>If nothing else, category theory is a lesson in how far abstractions can be pushed. F-Algebras, for instance, are one of the most abstract ideas I've ever encountered. Category-theoretic products (or, more generally, cones) are astonishingly powerful given their generality. I was quite surprised by how far you can strip down exponentiation.</p>\n<p>That said, category theory won't help the average person act more rationally.</p>\n<p>If you're into math, programming, or physics then category theory will likely help you out. Otherwise, I wouldn't recommend starting here.</p>\n<p>If you happen to be a programmer considering diving off the deep end, I strongly recommend familiarizing yourself with pure functional programming languages first. (Haskell is a good place to start.) My familiarity with type theory and my use of functors made category theory easier to approach. (And besides, pure functional programming is lots of fun.)</p>\n<h2 id=\"Final_Notes\">Final Notes</h2>\n<p>I'm somewhat surprised that the MIRI course list Category Theory textbook doesn't discuss Representation Theory.</p>\n<p>Otherwise, this book is precisely what I expected from the MIRI course list: very good at shutting up and giving you the data. I came away pleased.</p>", "sections": [{"title": "Basic Category Theory for Computer Scientists", "anchor": "Basic_Category_Theory_for_Computer_Scientists", "level": 1}, {"title": "Chapter Summaries", "anchor": "Chapter_Summaries", "level": 2}, {"title": "1. Basic Constructions", "anchor": "1__Basic_Constructions", "level": 2}, {"title": "2. Functors, Natural Transformations, and Adjoints", "anchor": "2__Functors__Natural_Transformations__and_Adjoints", "level": 2}, {"title": "3. Applications", "anchor": "3__Applications", "level": 2}, {"title": "4. Further Reading", "anchor": "4__Further_Reading", "level": 2}, {"title": "Discussion", "anchor": "Discussion", "level": 2}, {"title": "Should I read it?", "anchor": "Should_I_read_it_", "level": 2}, {"title": "Should I learn category theory?", "anchor": "Should_I_learn_category_theory_", "level": 2}, {"title": "Final Notes", "anchor": "Final_Notes", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "23 comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ucpD7YtcvKo6C9fBK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-19T04:31:20.391Z", "modifiedAt": null, "url": null, "title": "Help us Optimize the Contents of the Sequences eBook", "slug": "help-us-optimize-the-contents-of-the-sequences-ebook", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:38.516Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZsDmi6XbLu3LF3gi7/help-us-optimize-the-contents-of-the-sequences-ebook", "pageUrlRelative": "/posts/ZsDmi6XbLu3LF3gi7/help-us-optimize-the-contents-of-the-sequences-ebook", "linkUrl": "https://www.lesswrong.com/posts/ZsDmi6XbLu3LF3gi7/help-us-optimize-the-contents-of-the-sequences-ebook", "postedAtFormatted": "Thursday, September 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20us%20Optimize%20the%20Contents%20of%20the%20Sequences%20eBook&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20us%20Optimize%20the%20Contents%20of%20the%20Sequences%20eBook%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZsDmi6XbLu3LF3gi7%2Fhelp-us-optimize-the-contents-of-the-sequences-ebook%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20us%20Optimize%20the%20Contents%20of%20the%20Sequences%20eBook%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZsDmi6XbLu3LF3gi7%2Fhelp-us-optimize-the-contents-of-the-sequences-ebook", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZsDmi6XbLu3LF3gi7%2Fhelp-us-optimize-the-contents-of-the-sequences-ebook", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 893, "htmlBody": "<p>MIRI's ongoing effort to publish&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Sequences\">the sequences</a> as an eBook has given us the opportunity to update their contents and organization.</p>\n<p>We're looking for suggested posts to <strong>reorder, add, or remove</strong>.</p>\n<p>To help with this, here is a breakdown of the <em>current planned contents of the eBook</em> and any currently planned modifications. Following that is a list of the most popular links within the sequences to posts that are <em>not included</em> therein.</p>\n<p>Now's a good time to suggested changes or improvements!</p>\n<p>&mdash;&mdash;&mdash;</p>\n<h2><a href=\"http://wiki.lesswrong.com/wiki/Map_and_Territory_(sequence)\">Map and Territory</a></h2>\n<p>Added <a href=\"/lw/gp/whats_a_bias_again/\">&hellip;What's a Bias Again?</a> because it's meant to immediately follow <a href=\"/lw/go/why_truth_and/\">Why Truth, And&hellip;</a>.</p>\n<h2><a href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\">Mysterious Answers to Mysterious Questions</a></h2>\n<p>No changes.</p>\n<h2><a href=\"http://wiki.lesswrong.com/wiki/A_Human%27s_Guide_to_Words\">A Human's Guide to Words</a></h2>\n<p>No changes.</p>\n<h2><a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">How to Actually Change Your Mind</a></h2>\n<h4><a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">Politics is the Mind-Killer</a></h4>\n<p>Removed <a href=\"/lw/lt/the_robbers_cave_experiment/\">The Robbers Cave Experiment</a> because it already appears in Death Spirals and the Cult Attractor, and there in the original chronological order which flows better.</p>\n<h4><a href=\"http://wiki.lesswrong.com/wiki/Death_Spirals_and_the_Cult_Attractor\">Death Spirals and the Cult Attractor</a></h4>\n<p>Removed <a href=\"/lw/m2/the_litany_against_gurus/\">The Litany Against Gurus</a> because it already appears in Politics is the Mind-killer.</p>\n<h4><a href=\"http://wiki.lesswrong.com/wiki/Seeing_with_Fresh_Eyes\">Seeing with Fresh Eyes</a></h4>\n<p>Removed <a href=\"/lw/m9/aschs_conformity_experiment/\">Asch's Conformity Experiment</a> and <a href=\"/lw/mb/lonely_dissent/\">Lonely Dissent</a> because they both appear at the end of Death Spirals. Removed <a href=\"/lw/s3/the_genetic_fallacy/\">The Genetic Fallacy</a> because it's in the Metaethics sequence: that's where it falls chronologically and it fits better there with the surrounding posts.</p>\n<h4><a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Noticing_Confusion\">Noticing Confusion</a></h4>\n<p><em>Removed this entire subsequence</em> because it is entirely contained within Mysterious Answers to Mysterious Questions.</p>\n<h4><a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Rationalization\">Against Rationalization</a></h4>\n<p>Added <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's Mugging</a> (before Torture vs Dust Specks) because it explains the 3^^^3 notation. Added <a href=\"/lw/kn/torture_vs_dust_specks/\">Torture vs Dust Specks</a> before <a href=\"/lw/ko/a_case_study_of_motivated_continuation/\">A Case Study of Motivated Continuation</a> because A Case Study refers to it frequently.</p>\n<h4><a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Doublethink\">Against Doublethink</a></h4>\n<p>No changes.</p>\n<h4><a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Overly_Convenient_Excuses\">Overly Convenient Excuses</a></h4>\n<p>Removed <a href=\"/lw/jr/how_to_convince_me_that_2_2_3/\">How to Convince Me that 2+2=3</a> because it's already in Map &amp; Territory.</p>\n<h4><a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Letting_Go\">Letting Go</a></h4>\n<p>No change.</p>\n<h2><a href=\"http://wiki.lesswrong.com/wiki/Evolution\">The Simple Math of Evolution</a></h2>\n<p>Added <a href=\"/lw/l1/evolutionary_psychology/\">Evolutionary Psychology</a> because it fits nicely at the end and it's referred to by other posts many times.</p>\n<h2><a href=\"http://wiki.lesswrong.com/wiki/Challenging_the_Difficult\">Challenging the Difficult</a></h2>\n<p>No change.</p>\n<h2><a href=\"http://wiki.lesswrong.com/wiki/Yudkowsky%27s_coming_of_age\">Yudkowsky's Coming of Age</a></h2>\n<p>No change.</p>\n<h2><a href=\"http://wiki.lesswrong.com/wiki/Reductionism_(sequence\">Reductionism</a></h2>\n<p>No change. (Includes the Zombies subsequence.)</p>\n<h2><a href=\"/lw/r5/the_quantum_physics_sequence/\">Quantum Physics</a></h2>\n<p>No change. Doesn't include any \"Preliminaries\" posts, since they'd all be duplicates</p>\n<h2><a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">Metaethics</a></h2>\n<p>No change.</p>\n<h2><a href=\"/lw/xy/the_fun_theory_sequence/\">Fun Theory</a></h2>\n<p>No change.</p>\n<h2><a href=\"/lw/cz/the_craft_and_the_community/\">The Craft and the Community</a></h2>\n<p>No change.</p>\n<h2>Appendix</h2>\n<p>Includes:</p>\n<ul>\n<li><a href=\"http://yudkowsky.net/rational/the-simple-truth/\">The Simple Truth</a></li>\n<li><a href=\"http://yudkowsky.net/rational/bayes/\">An Intuitive Explanation of Bayes' Theorem</a></li>\n<li><a href=\"http://yudkowsky.net/rational/technical/\">A Technical Explanation of Technical Explanation</a></li>\n</ul>\n<p>&mdash;&mdash;&mdash;</p>\n<p>Here are the most-frequently-referenced links within the sequences to posts outside of the sequences (with a count of three or more). This may help you notice posts that you think should be included in the sequences eBook.</p>\n<ul>\n<li><a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomb's Problem and Regret of Rationality</a> =&gt; 24</li>\n<li><a href=\"/lw/o5/the_second_law_of_thermodynamics_and_engines_of/\">The Second Law of Thermodynamics, and Engines of Cognition</a> =&gt; 22</li>\n<li><a href=\"/lw/l4/terminal_values_and_instrumental_values/\">Terminal Values and Instrumental Values</a> =&gt; 16</li>\n<li><a href=\"/lw/jk/burdensome_details/\">Burdensome Details</a> =&gt; 16</li>\n<li><a href=\"/lw/kg/expecting_short_inferential_distances/\">Expecting Short Inferential Distances</a> =&gt; 15</li>\n<li><a href=\"/lw/l3/thou_art_godshatter/\">Thou Art Godshatter</a> =&gt; 14</li>\n<li><a href=\"/lw/i8/religions_claim_to_be_nondisprovable/\">Religion's Claim to be Non-Disprovable</a> =&gt; 14</li>\n<li><a href=\"/lw/hw/scope_insensitivity/\">Scope Insensitivity</a> =&gt; 13</li>\n<li><a href=\"/lw/rc/the_ultimate_source/\">The Ultimate Source</a> =&gt; 13</li>\n<li><a href=\"/lw/kj/no_one_knows_what_science_doesnt_know/\">No One Knows What Science Doesn't Know</a> =&gt; 12</li>\n<li><a href=\"/lw/rm/the_design_space_of_mindsingeneral/\">The Design Space of Minds-In-General</a> =&gt; 11</li>\n<li><a href=\"/lw/hs/think_like_reality/\">Think Like Reality</a> =&gt; 10</li>\n<li><a href=\"/lw/rd/passing_the_recursive_buck/\">Passing the Recursive Buck</a> =&gt; 9</li>\n<li><a href=\"/lw/le/lost_purposes/\">Lost Purposes</a> =&gt; 9</li>\n<li><a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">The Hidden Complexity of Wishes</a> =&gt; 9</li>\n<li><a href=\"/lw/in/scientific_evidence_legal_evidence_rational/\">Scientific Evidence, Legal Evidence, Rational Evidence</a> =&gt; 9</li>\n<li><a href=\"/lw/k2/a_priori/\">A Priori</a> =&gt; 8</li>\n<li><a href=\"/lw/mt/beautiful_probability/\">Beautiful Probability</a> =&gt; 8</li>\n<li><a href=\"/lw/rb/possibility_and_couldness/\">Possibility and Could-ness</a> =&gt; 8</li>\n<li><a href=\"/lw/j6/why_is_the_future_so_absurd/\">Why is the Future So Absurd?</a> =&gt; 8</li>\n<li><a href=\"/lw/lq/fake_utility_functions/\">Fake Utility Functions</a> =&gt; 8</li>\n<li><a href=\"/lw/j5/availability/\">Availability</a> =&gt; 7</li>\n<li><a href=\"/lw/rf/ghosts_in_the_machine/\">Ghosts in the Machine</a> =&gt; 7</li>\n<li><a href=\"/lw/x5/nonsentient_optimizers/\">Nonsentient Optimizers</a> =&gt; 7</li>\n<li><a href=\"/lw/lp/fake_fake_utility_functions/\">Fake Fake Utility Functions</a> =&gt; 7</li>\n<li><a href=\"/lw/o7/searching_for_bayesstructure/\">Searching for Bayes-Structure</a> =&gt; 7</li>\n<li><a href=\"/lw/gv/outside_the_laboratory/\">Outside the Laboratory</a> =&gt; 7</li>\n<li><a href=\"/lw/tf/dreams_of_ai_design/\">Dreams of AI Design</a> =&gt; 6</li>\n<li><a href=\"/lw/rj/surface_analogies_and_deep_causes/\">Surface Analogies and Deep Causes</a> =&gt; 6</li>\n<li><a href=\"/lw/l9/artificial_addition/\">Artificial Addition</a> =&gt; 6</li>\n<li><a href=\"/lw/lb/not_for_the_sake_of_happiness_alone/\">Not for the Sake of Happiness (Alone)</a> =&gt; 6</li>\n<li><a href=\"/lw/h3/superstimuli_and_the_collapse_of_western/\">Superstimuli and the Collapse of Western Civilization</a> =&gt; 5</li>\n<li><a href=\"/lw/q4/decoherence_is_falsifiable_and_testable/\">Decoherence is Falsifiable and Testable</a> =&gt; 5</li>\n<li><a href=\"/lw/t6/the_cartoon_guide_to_l&ouml;bs_theorem/\">The Cartoon Guide to L&ouml;b's Theorem</a> =&gt; 5</li>\n<li><a href=\"/lw/x7/cant_unbirth_a_child/\">Can't Unbirth a Child</a> =&gt; 5</li>\n<li><a href=\"/lw/rl/the_psychological_unity_of_humankind/\">The Psychological Unity of Humankind</a> =&gt; 5</li>\n<li><a href=\"/lw/so/humans_in_funny_suits/\">Humans in Funny Suits</a> =&gt; 5</li>\n<li><a href=\"/lw/7i/rationality_is_systematized_winning/\">Rationality is Systematized Winning</a> =&gt; 5</li>\n<li><a href=\"/lw/tn/the_true_prisoners_dilemma/\">The True Prisoner's Dilemma</a> =&gt; 5</li>\n<li><a href=\"/lw/m7/zen_and_the_art_of_rationality/\">Zen and the Art of Rationality</a> =&gt; 5</li>\n<li><a href=\"/lw/n9/the_intuitions_behind_utilitarianism/\">The \"Intuitions\" Behind \"Utilitarianism\"</a> =&gt; 5</li>\n<li><a href=\"/lw/ws/for_the_people_who_are_still_alive/\">For The People Who Are Still Alive</a> =&gt; 4</li>\n<li><a href=\"/lw/mg/the_twoparty_swindle/\">The Two-Party Swindle</a> =&gt; 4</li>\n<li><a href=\"/lw/ji/conjunction_fallacy/\">Conjunction Fallacy</a> =&gt; 4</li>\n<li><a href=\"/lw/st/anthropomorphic_optimism/\">Anthropomorphic Optimism</a> =&gt; 4</li>\n<li><a href=\"/lw/gr/the_modesty_argument/\">The Modesty Argument</a> =&gt; 4</li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Rational_evidence\">Rational evidence</a> =&gt; 4</li>\n<li><a href=\"/lw/hk/priors_as_mathematical_objects/\">Priors as Mathematical Objects</a> =&gt; 4</li>\n<li><a href=\"/lw/a6/the_unfinished_mystery_of_the_shangrila_diet/\">The Unfinished Mystery of the Shangri-La Diet/</a> =&gt; 4</li>\n<li><a href=\"/lw/ig/i_defy_the_data/\">I Defy the Data!</a> =&gt; 4</li>\n<li><a href=\"/lw/9j/bystander_apathy/\">Bystander Apathy</a> =&gt; 3</li>\n<li><a href=\"/lw/ja/we_dont_really_want_your_participation/\">We Don't Really Want Your Participation</a> =&gt; 3</li>\n<li><a href=\"/lw/wq/you_only_live_twice/\">You Only Live Twice</a> =&gt; 3</li>\n<li><a href=\"/lw/vm/lawful_creativity/\">Lawful Creativity</a> =&gt; 3</li>\n<li><a href=\"/lw/hx/one_life_against_the_world/\">One Life Against the World</a> =&gt; 3</li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Locate_the_hypothesis\">Locate the hypothesis</a> =&gt; 3</li>\n<li><a href=\"/lw/ym/cynical_about_cynicism/\">Cynical About Cynicism</a> =&gt; 3</li>\n<li><a href=\"/lw/tx/optimization/\">Optimization</a> =&gt; 3</li>\n<li><a href=\"/lw/ke/illusion_of_transparency_why_no_one_understands/\">Illusion of Transparency: Why No One Understands You</a> =&gt; 3</li>\n<li><a href=\"/lw/sp/detached_lever_fallacy/\">Detached Lever Fallacy</a> =&gt; 3</li>\n<li><a href=\"/lw/n3/circular_altruism/\">Circular Altruism</a> =&gt; 3</li>\n<li><a href=\"/lw/my/the_allais_paradox/\">The Allais Paradox</a> =&gt; 3</li>\n<li><a href=\"/lw/gn/the_martial_art_of_rationality/\">The Martial Art of Rationality</a> =&gt; 3</li>\n<li><a href=\"/lw/ky/fake_morality/\">Fake Morality</a> =&gt; 3</li>\n</ul>\n<p>Suggestions?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JMD7LTXTisBzGAfhX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZsDmi6XbLu3LF3gi7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 18, "extendedScore": null, "score": 1.3465940607208992e-06, "legacy": true, "legacyId": "24217", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>MIRI's ongoing effort to publish&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Sequences\">the sequences</a> as an eBook has given us the opportunity to update their contents and organization.</p>\n<p>We're looking for suggested posts to <strong>reorder, add, or remove</strong>.</p>\n<p>To help with this, here is a breakdown of the <em>current planned contents of the eBook</em> and any currently planned modifications. Following that is a list of the most popular links within the sequences to posts that are <em>not included</em> therein.</p>\n<p>Now's a good time to suggested changes or improvements!</p>\n<p>\u2014\u2014\u2014</p>\n<h2 id=\"Map_and_Territory\"><a href=\"http://wiki.lesswrong.com/wiki/Map_and_Territory_(sequence)\">Map and Territory</a></h2>\n<p>Added <a href=\"/lw/gp/whats_a_bias_again/\">\u2026What's a Bias Again?</a> because it's meant to immediately follow <a href=\"/lw/go/why_truth_and/\">Why Truth, And\u2026</a>.</p>\n<h2 id=\"Mysterious_Answers_to_Mysterious_Questions\"><a href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\">Mysterious Answers to Mysterious Questions</a></h2>\n<p>No changes.</p>\n<h2 id=\"A_Human_s_Guide_to_Words\"><a href=\"http://wiki.lesswrong.com/wiki/A_Human%27s_Guide_to_Words\">A Human's Guide to Words</a></h2>\n<p>No changes.</p>\n<h2 id=\"How_to_Actually_Change_Your_Mind\"><a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">How to Actually Change Your Mind</a></h2>\n<h4 id=\"Politics_is_the_Mind_Killer\"><a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">Politics is the Mind-Killer</a></h4>\n<p>Removed <a href=\"/lw/lt/the_robbers_cave_experiment/\">The Robbers Cave Experiment</a> because it already appears in Death Spirals and the Cult Attractor, and there in the original chronological order which flows better.</p>\n<h4 id=\"Death_Spirals_and_the_Cult_Attractor\"><a href=\"http://wiki.lesswrong.com/wiki/Death_Spirals_and_the_Cult_Attractor\">Death Spirals and the Cult Attractor</a></h4>\n<p>Removed <a href=\"/lw/m2/the_litany_against_gurus/\">The Litany Against Gurus</a> because it already appears in Politics is the Mind-killer.</p>\n<h4 id=\"Seeing_with_Fresh_Eyes\"><a href=\"http://wiki.lesswrong.com/wiki/Seeing_with_Fresh_Eyes\">Seeing with Fresh Eyes</a></h4>\n<p>Removed <a href=\"/lw/m9/aschs_conformity_experiment/\">Asch's Conformity Experiment</a> and <a href=\"/lw/mb/lonely_dissent/\">Lonely Dissent</a> because they both appear at the end of Death Spirals. Removed <a href=\"/lw/s3/the_genetic_fallacy/\">The Genetic Fallacy</a> because it's in the Metaethics sequence: that's where it falls chronologically and it fits better there with the surrounding posts.</p>\n<h4 id=\"Noticing_Confusion\"><a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Noticing_Confusion\">Noticing Confusion</a></h4>\n<p><em>Removed this entire subsequence</em> because it is entirely contained within Mysterious Answers to Mysterious Questions.</p>\n<h4 id=\"Against_Rationalization\"><a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Rationalization\">Against Rationalization</a></h4>\n<p>Added <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's Mugging</a> (before Torture vs Dust Specks) because it explains the 3^^^3 notation. Added <a href=\"/lw/kn/torture_vs_dust_specks/\">Torture vs Dust Specks</a> before <a href=\"/lw/ko/a_case_study_of_motivated_continuation/\">A Case Study of Motivated Continuation</a> because A Case Study refers to it frequently.</p>\n<h4 id=\"Against_Doublethink\"><a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Against_Doublethink\">Against Doublethink</a></h4>\n<p>No changes.</p>\n<h4 id=\"Overly_Convenient_Excuses\"><a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Overly_Convenient_Excuses\">Overly Convenient Excuses</a></h4>\n<p>Removed <a href=\"/lw/jr/how_to_convince_me_that_2_2_3/\">How to Convince Me that 2+2=3</a> because it's already in Map &amp; Territory.</p>\n<h4 id=\"Letting_Go\"><a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind#Letting_Go\">Letting Go</a></h4>\n<p>No change.</p>\n<h2 id=\"The_Simple_Math_of_Evolution\"><a href=\"http://wiki.lesswrong.com/wiki/Evolution\">The Simple Math of Evolution</a></h2>\n<p>Added <a href=\"/lw/l1/evolutionary_psychology/\">Evolutionary Psychology</a> because it fits nicely at the end and it's referred to by other posts many times.</p>\n<h2 id=\"Challenging_the_Difficult\"><a href=\"http://wiki.lesswrong.com/wiki/Challenging_the_Difficult\">Challenging the Difficult</a></h2>\n<p>No change.</p>\n<h2 id=\"Yudkowsky_s_Coming_of_Age\"><a href=\"http://wiki.lesswrong.com/wiki/Yudkowsky%27s_coming_of_age\">Yudkowsky's Coming of Age</a></h2>\n<p>No change.</p>\n<h2 id=\"Reductionism\"><a href=\"http://wiki.lesswrong.com/wiki/Reductionism_(sequence\">Reductionism</a></h2>\n<p>No change. (Includes the Zombies subsequence.)</p>\n<h2 id=\"Quantum_Physics\"><a href=\"/lw/r5/the_quantum_physics_sequence/\">Quantum Physics</a></h2>\n<p>No change. Doesn't include any \"Preliminaries\" posts, since they'd all be duplicates</p>\n<h2 id=\"Metaethics\"><a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">Metaethics</a></h2>\n<p>No change.</p>\n<h2 id=\"Fun_Theory\"><a href=\"/lw/xy/the_fun_theory_sequence/\">Fun Theory</a></h2>\n<p>No change.</p>\n<h2 id=\"The_Craft_and_the_Community\"><a href=\"/lw/cz/the_craft_and_the_community/\">The Craft and the Community</a></h2>\n<p>No change.</p>\n<h2 id=\"Appendix\">Appendix</h2>\n<p>Includes:</p>\n<ul>\n<li><a href=\"http://yudkowsky.net/rational/the-simple-truth/\">The Simple Truth</a></li>\n<li><a href=\"http://yudkowsky.net/rational/bayes/\">An Intuitive Explanation of Bayes' Theorem</a></li>\n<li><a href=\"http://yudkowsky.net/rational/technical/\">A Technical Explanation of Technical Explanation</a></li>\n</ul>\n<p>\u2014\u2014\u2014</p>\n<p>Here are the most-frequently-referenced links within the sequences to posts outside of the sequences (with a count of three or more). This may help you notice posts that you think should be included in the sequences eBook.</p>\n<ul>\n<li><a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomb's Problem and Regret of Rationality</a> =&gt; 24</li>\n<li><a href=\"/lw/o5/the_second_law_of_thermodynamics_and_engines_of/\">The Second Law of Thermodynamics, and Engines of Cognition</a> =&gt; 22</li>\n<li><a href=\"/lw/l4/terminal_values_and_instrumental_values/\">Terminal Values and Instrumental Values</a> =&gt; 16</li>\n<li><a href=\"/lw/jk/burdensome_details/\">Burdensome Details</a> =&gt; 16</li>\n<li><a href=\"/lw/kg/expecting_short_inferential_distances/\">Expecting Short Inferential Distances</a> =&gt; 15</li>\n<li><a href=\"/lw/l3/thou_art_godshatter/\">Thou Art Godshatter</a> =&gt; 14</li>\n<li><a href=\"/lw/i8/religions_claim_to_be_nondisprovable/\">Religion's Claim to be Non-Disprovable</a> =&gt; 14</li>\n<li><a href=\"/lw/hw/scope_insensitivity/\">Scope Insensitivity</a> =&gt; 13</li>\n<li><a href=\"/lw/rc/the_ultimate_source/\">The Ultimate Source</a> =&gt; 13</li>\n<li><a href=\"/lw/kj/no_one_knows_what_science_doesnt_know/\">No One Knows What Science Doesn't Know</a> =&gt; 12</li>\n<li><a href=\"/lw/rm/the_design_space_of_mindsingeneral/\">The Design Space of Minds-In-General</a> =&gt; 11</li>\n<li><a href=\"/lw/hs/think_like_reality/\">Think Like Reality</a> =&gt; 10</li>\n<li><a href=\"/lw/rd/passing_the_recursive_buck/\">Passing the Recursive Buck</a> =&gt; 9</li>\n<li><a href=\"/lw/le/lost_purposes/\">Lost Purposes</a> =&gt; 9</li>\n<li><a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">The Hidden Complexity of Wishes</a> =&gt; 9</li>\n<li><a href=\"/lw/in/scientific_evidence_legal_evidence_rational/\">Scientific Evidence, Legal Evidence, Rational Evidence</a> =&gt; 9</li>\n<li><a href=\"/lw/k2/a_priori/\">A Priori</a> =&gt; 8</li>\n<li><a href=\"/lw/mt/beautiful_probability/\">Beautiful Probability</a> =&gt; 8</li>\n<li><a href=\"/lw/rb/possibility_and_couldness/\">Possibility and Could-ness</a> =&gt; 8</li>\n<li><a href=\"/lw/j6/why_is_the_future_so_absurd/\">Why is the Future So Absurd?</a> =&gt; 8</li>\n<li><a href=\"/lw/lq/fake_utility_functions/\">Fake Utility Functions</a> =&gt; 8</li>\n<li><a href=\"/lw/j5/availability/\">Availability</a> =&gt; 7</li>\n<li><a href=\"/lw/rf/ghosts_in_the_machine/\">Ghosts in the Machine</a> =&gt; 7</li>\n<li><a href=\"/lw/x5/nonsentient_optimizers/\">Nonsentient Optimizers</a> =&gt; 7</li>\n<li><a href=\"/lw/lp/fake_fake_utility_functions/\">Fake Fake Utility Functions</a> =&gt; 7</li>\n<li><a href=\"/lw/o7/searching_for_bayesstructure/\">Searching for Bayes-Structure</a> =&gt; 7</li>\n<li><a href=\"/lw/gv/outside_the_laboratory/\">Outside the Laboratory</a> =&gt; 7</li>\n<li><a href=\"/lw/tf/dreams_of_ai_design/\">Dreams of AI Design</a> =&gt; 6</li>\n<li><a href=\"/lw/rj/surface_analogies_and_deep_causes/\">Surface Analogies and Deep Causes</a> =&gt; 6</li>\n<li><a href=\"/lw/l9/artificial_addition/\">Artificial Addition</a> =&gt; 6</li>\n<li><a href=\"/lw/lb/not_for_the_sake_of_happiness_alone/\">Not for the Sake of Happiness (Alone)</a> =&gt; 6</li>\n<li><a href=\"/lw/h3/superstimuli_and_the_collapse_of_western/\">Superstimuli and the Collapse of Western Civilization</a> =&gt; 5</li>\n<li><a href=\"/lw/q4/decoherence_is_falsifiable_and_testable/\">Decoherence is Falsifiable and Testable</a> =&gt; 5</li>\n<li><a href=\"/lw/t6/the_cartoon_guide_to_l\u00f6bs_theorem/\">The Cartoon Guide to L\u00f6b's Theorem</a> =&gt; 5</li>\n<li><a href=\"/lw/x7/cant_unbirth_a_child/\">Can't Unbirth a Child</a> =&gt; 5</li>\n<li><a href=\"/lw/rl/the_psychological_unity_of_humankind/\">The Psychological Unity of Humankind</a> =&gt; 5</li>\n<li><a href=\"/lw/so/humans_in_funny_suits/\">Humans in Funny Suits</a> =&gt; 5</li>\n<li><a href=\"/lw/7i/rationality_is_systematized_winning/\">Rationality is Systematized Winning</a> =&gt; 5</li>\n<li><a href=\"/lw/tn/the_true_prisoners_dilemma/\">The True Prisoner's Dilemma</a> =&gt; 5</li>\n<li><a href=\"/lw/m7/zen_and_the_art_of_rationality/\">Zen and the Art of Rationality</a> =&gt; 5</li>\n<li><a href=\"/lw/n9/the_intuitions_behind_utilitarianism/\">The \"Intuitions\" Behind \"Utilitarianism\"</a> =&gt; 5</li>\n<li><a href=\"/lw/ws/for_the_people_who_are_still_alive/\">For The People Who Are Still Alive</a> =&gt; 4</li>\n<li><a href=\"/lw/mg/the_twoparty_swindle/\">The Two-Party Swindle</a> =&gt; 4</li>\n<li><a href=\"/lw/ji/conjunction_fallacy/\">Conjunction Fallacy</a> =&gt; 4</li>\n<li><a href=\"/lw/st/anthropomorphic_optimism/\">Anthropomorphic Optimism</a> =&gt; 4</li>\n<li><a href=\"/lw/gr/the_modesty_argument/\">The Modesty Argument</a> =&gt; 4</li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Rational_evidence\">Rational evidence</a> =&gt; 4</li>\n<li><a href=\"/lw/hk/priors_as_mathematical_objects/\">Priors as Mathematical Objects</a> =&gt; 4</li>\n<li><a href=\"/lw/a6/the_unfinished_mystery_of_the_shangrila_diet/\">The Unfinished Mystery of the Shangri-La Diet/</a> =&gt; 4</li>\n<li><a href=\"/lw/ig/i_defy_the_data/\">I Defy the Data!</a> =&gt; 4</li>\n<li><a href=\"/lw/9j/bystander_apathy/\">Bystander Apathy</a> =&gt; 3</li>\n<li><a href=\"/lw/ja/we_dont_really_want_your_participation/\">We Don't Really Want Your Participation</a> =&gt; 3</li>\n<li><a href=\"/lw/wq/you_only_live_twice/\">You Only Live Twice</a> =&gt; 3</li>\n<li><a href=\"/lw/vm/lawful_creativity/\">Lawful Creativity</a> =&gt; 3</li>\n<li><a href=\"/lw/hx/one_life_against_the_world/\">One Life Against the World</a> =&gt; 3</li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Locate_the_hypothesis\">Locate the hypothesis</a> =&gt; 3</li>\n<li><a href=\"/lw/ym/cynical_about_cynicism/\">Cynical About Cynicism</a> =&gt; 3</li>\n<li><a href=\"/lw/tx/optimization/\">Optimization</a> =&gt; 3</li>\n<li><a href=\"/lw/ke/illusion_of_transparency_why_no_one_understands/\">Illusion of Transparency: Why No One Understands You</a> =&gt; 3</li>\n<li><a href=\"/lw/sp/detached_lever_fallacy/\">Detached Lever Fallacy</a> =&gt; 3</li>\n<li><a href=\"/lw/n3/circular_altruism/\">Circular Altruism</a> =&gt; 3</li>\n<li><a href=\"/lw/my/the_allais_paradox/\">The Allais Paradox</a> =&gt; 3</li>\n<li><a href=\"/lw/gn/the_martial_art_of_rationality/\">The Martial Art of Rationality</a> =&gt; 3</li>\n<li><a href=\"/lw/ky/fake_morality/\">Fake Morality</a> =&gt; 3</li>\n</ul>\n<p>Suggestions?</p>", "sections": [{"title": "Map and Territory", "anchor": "Map_and_Territory", "level": 1}, {"title": "Mysterious Answers to Mysterious Questions", "anchor": "Mysterious_Answers_to_Mysterious_Questions", "level": 1}, {"title": "A Human's Guide to Words", "anchor": "A_Human_s_Guide_to_Words", "level": 1}, {"title": "How to Actually Change Your Mind", "anchor": "How_to_Actually_Change_Your_Mind", "level": 1}, {"title": "Politics is the Mind-Killer", "anchor": "Politics_is_the_Mind_Killer", "level": 2}, {"title": "Death Spirals and the Cult Attractor", "anchor": "Death_Spirals_and_the_Cult_Attractor", "level": 2}, {"title": "Seeing with Fresh Eyes", "anchor": "Seeing_with_Fresh_Eyes", "level": 2}, {"title": "Noticing Confusion", "anchor": "Noticing_Confusion", "level": 2}, {"title": "Against Rationalization", "anchor": "Against_Rationalization", "level": 2}, {"title": "Against Doublethink", "anchor": "Against_Doublethink", "level": 2}, {"title": "Overly Convenient Excuses", "anchor": "Overly_Convenient_Excuses", "level": 2}, {"title": "Letting Go", "anchor": "Letting_Go", "level": 2}, {"title": "The Simple Math of Evolution", "anchor": "The_Simple_Math_of_Evolution", "level": 1}, {"title": "Challenging the Difficult", "anchor": "Challenging_the_Difficult", "level": 1}, {"title": "Yudkowsky's Coming of Age", "anchor": "Yudkowsky_s_Coming_of_Age", "level": 1}, {"title": "Reductionism", "anchor": "Reductionism", "level": 1}, {"title": "Quantum Physics", "anchor": "Quantum_Physics", "level": 1}, {"title": "Metaethics", "anchor": "Metaethics", "level": 1}, {"title": "Fun Theory", "anchor": "Fun_Theory", "level": 1}, {"title": "The Craft and the Community", "anchor": "The_Craft_and_the_Community", "level": 1}, {"title": "Appendix", "anchor": "Appendix", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "73 comments"}], "headingsCount": 23}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 73, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jnZbHi873v9vcpGpZ", "YshRbqZHYFoEMqFAu", "MBpj3QKfPg9xKNeXW", "t6Fe2PsEwb3HhcBEr", "WHK94zXkQm7qm7wXk", "CEGnJBHmkcwPTysb7", "KZLa74SzyKhSJ3M55", "a5JAiTdytou3Jg749", "3wYTFWY3LKQCnAptN", "i2ruK7M3coWfv8mfD", "6FmqiAgS8h4EJm86s", "epZLSoNvjW53tqNj9", "hc9Eg6erp6hk9bWhn", "K4aGvLnHvYgX9pZHS", "YdcF6WbBmJhaaDqoD", "6ddcsdA2c2XpNpE5x", "QkX2bAkwG2EpGvNug", "n5ucT5ZbPdhfGNLtP", "Yq6aA4M3JKWaQepPJ", "HLqWn5LASfhhArZ7w", "cSXZpvqpa9vbGGLtG", "fAuWLS7RKWD2npBFR", "2ftJ38y9SRBCBsCzy", "EsMhFZuycZorZNRF5", "vNBxmcHpnozjrJnJP", "tnWRXkcDi5Tw9rzXw", "tWLFWAndSZSYN6rPB", "rw3oKLjG85BdKNXS2", "sP2Hg6uPwpfp3jZJN", "4ARaTpNX62uaL86j6", "fhojYBGGiYAFcryHZ", "qmqLxvtsPzZ2s6mpY", "bkSkRwo9SRYxJMiSY", "3buXtNiSK8gcRLMSG", "Ga2HSwf9iQe64JwAa", "NnohDYHNnKDtbiMyp", "R8cpqD3NA4rZxRdQ4", "cnYHFNBF3kZEyx24v", "HsRFQTAySAx8xbXEc", "D6rsNhHM4pBCpDzSb", "QrhAeKBkm2WsdRYao", "N2pENnTPB75sfc9kb", "p7ftQ6acRkgo6hqHb", "6ByPxcGDhmx74gPSm", "YhgjmCxcQXixStWMC", "synsRtBKDeAFuo7e3", "Jq73GozjsuhdwMLEG", "DFxoaWGEh9ndwtZhk", "ALCnqX6Xx8bpFMZq3", "gb6zWstjmkYHLrbrg", "Cyj6wQLW6SeF6aGLy", "Zkzzjg3h7hW5Z36hK", "4ARtkT3EYox3THYjF", "HFyWNBnDNEDsDNLrZ", "HLERouG7QBt7jzLt4", "r5MSQ83gtbjWRBDWJ", "cfZ8zveqrTZbQrjeD", "qAJgWCWJJkke4mE8x", "QAK43nNCTQQycAcYe", "RcZeZt8cPk48xxiQ8", "NKECtGX4RZPd7SqYp", "jzf4Rcienrm6btRyt", "BD4oExxQguTgpESdm", "vrHRcEDMjZcx5Yfru", "K5nq3KcDXaGm7QQWR", "RiWPdNaoL8fq7phbY", "yKXKcyoBzWtECzXrE", "KKLQp934n77cfZpPn", "xiHy3kFni8nsxfdcP", "R2crzhnqrytPycc6C", "D7EcMhL26zFNbJ3ED", "sSqoEw9eRP2kPKLCz", "zY4pic7cwQpa9dnyk", "4ZzefKQwAtMo5yp99", "zJZvoiwydJ5zvzTHK", "teaxCFgtmCQ3E9fy8", "fATPBv4pnHC33EmJ2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-19T16:34:53.769Z", "modifiedAt": null, "url": null, "title": "Google creates new company 'Calico' focused on life extention", "slug": "google-creates-new-company-calico-focused-on-life-extention", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Metus", "createdAt": "2011-01-23T21:54:34.357Z", "isAdmin": false, "displayName": "Metus"}, "userId": "mNQ4fSvro7LYgrii4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zDZCfv6rupsJir6R7/google-creates-new-company-calico-focused-on-life-extention", "pageUrlRelative": "/posts/zDZCfv6rupsJir6R7/google-creates-new-company-calico-focused-on-life-extention", "linkUrl": "https://www.lesswrong.com/posts/zDZCfv6rupsJir6R7/google-creates-new-company-calico-focused-on-life-extention", "postedAtFormatted": "Thursday, September 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Google%20creates%20new%20company%20'Calico'%20focused%20on%20life%20extention&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGoogle%20creates%20new%20company%20'Calico'%20focused%20on%20life%20extention%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzDZCfv6rupsJir6R7%2Fgoogle-creates-new-company-calico-focused-on-life-extention%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Google%20creates%20new%20company%20'Calico'%20focused%20on%20life%20extention%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzDZCfv6rupsJir6R7%2Fgoogle-creates-new-company-calico-focused-on-life-extention", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzDZCfv6rupsJir6R7%2Fgoogle-creates-new-company-calico-focused-on-life-extention", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"https://plus.google.com/+LarryPage/posts/Lh8SKC6sED1\">https://plus.google.com/+LarryPage/posts/Lh8SKC6sED1</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zDZCfv6rupsJir6R7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "24218", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-19T20:29:33.168Z", "modifiedAt": null, "url": null, "title": "Meetup : London meetup: thought experiments", "slug": "meetup-london-meetup-thought-experiments", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:34.824Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/irCAtBz95chDaSsYo/meetup-london-meetup-thought-experiments", "pageUrlRelative": "/posts/irCAtBz95chDaSsYo/meetup-london-meetup-thought-experiments", "linkUrl": "https://www.lesswrong.com/posts/irCAtBz95chDaSsYo/meetup-london-meetup-thought-experiments", "postedAtFormatted": "Thursday, September 19th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20meetup%3A%20thought%20experiments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20meetup%3A%20thought%20experiments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FirCAtBz95chDaSsYo%2Fmeetup-london-meetup-thought-experiments%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20meetup%3A%20thought%20experiments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FirCAtBz95chDaSsYo%2Fmeetup-london-meetup-thought-experiments", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FirCAtBz95chDaSsYo%2Fmeetup-london-meetup-thought-experiments", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/r7'>London meetup: thought experiments</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 September 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">LShift, Hoxton Point, 6 Rufus St, London, N1 6PE</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>NB note the change of location!</p>\n\n<p>We are so good at providing reasons for our decisions after the fact that the reasons before the fact that cause our decisions are often not clear to us. At this meeting we'll discuss and practice techniques for getting past our rationalisations and learning more about the real reasons we find ourselves making the choices we do.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/r7'>London meetup: thought experiments</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "irCAtBz95chDaSsYo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.347446693910544e-06, "legacy": true, "legacyId": "24219", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_meetup__thought_experiments\">Discussion article for the meetup : <a href=\"/meetups/r7\">London meetup: thought experiments</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 September 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">LShift, Hoxton Point, 6 Rufus St, London, N1 6PE</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>NB note the change of location!</p>\n\n<p>We are so good at providing reasons for our decisions after the fact that the reasons before the fact that cause our decisions are often not clear to us. At this meeting we'll discuss and practice techniques for getting past our rationalisations and learning more about the real reasons we find ourselves making the choices we do.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_meetup__thought_experiments1\">Discussion article for the meetup : <a href=\"/meetups/r7\">London meetup: thought experiments</a></h2>", "sections": [{"title": "Discussion article for the meetup : London meetup: thought experiments", "anchor": "Discussion_article_for_the_meetup___London_meetup__thought_experiments", "level": 1}, {"title": "Discussion article for the meetup : London meetup: thought experiments", "anchor": "Discussion_article_for_the_meetup___London_meetup__thought_experiments1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-20T00:33:46.347Z", "modifiedAt": null, "url": null, "title": "Meetup : Intro to AI Existential Risk", "slug": "meetup-intro-to-ai-existential-risk", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chronophasiac", "createdAt": "2009-04-03T11:25:57.322Z", "isAdmin": false, "displayName": "chronophasiac"}, "userId": "wu2Hs7x6pbfJbMumC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zH6LTZaTJ3vnqGbxE/meetup-intro-to-ai-existential-risk", "pageUrlRelative": "/posts/zH6LTZaTJ3vnqGbxE/meetup-intro-to-ai-existential-risk", "linkUrl": "https://www.lesswrong.com/posts/zH6LTZaTJ3vnqGbxE/meetup-intro-to-ai-existential-risk", "postedAtFormatted": "Friday, September 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Intro%20to%20AI%20Existential%20Risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Intro%20to%20AI%20Existential%20Risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzH6LTZaTJ3vnqGbxE%2Fmeetup-intro-to-ai-existential-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Intro%20to%20AI%20Existential%20Risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzH6LTZaTJ3vnqGbxE%2Fmeetup-intro-to-ai-existential-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzH6LTZaTJ3vnqGbxE%2Fmeetup-intro-to-ai-existential-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/r8'>Intro to AI Existential Risk</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 September 2013 08:33:39PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St, Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Organizations like Oxford's Future of Humanity Institute and the Machine Intelligence Research Institute consider artificial intelligence to be a possible extinction risk to humanity. In this presentation, I'll be introducing the concept of AI as an 'existential risk'.</p>\n\n<p>The presentation will take about 30 minutes, followed by Q&amp;A and discussion.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation. \n\u2014Phase 2: The headline event. This starts promptly at 2:30, and lasts 30-60 minutes. \n\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups. \n\u2014Phase 4: Dinner. It's about a ten minute walk to the usual restaurant.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/r8'>Intro to AI Existential Risk</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zH6LTZaTJ3vnqGbxE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.3476641572010735e-06, "legacy": true, "legacyId": "24220", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Intro_to_AI_Existential_Risk\">Discussion article for the meetup : <a href=\"/meetups/r8\">Intro to AI Existential Risk</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 September 2013 08:33:39PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St, Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Organizations like Oxford's Future of Humanity Institute and the Machine Intelligence Research Institute consider artificial intelligence to be a possible extinction risk to humanity. In this presentation, I'll be introducing the concept of AI as an 'existential risk'.</p>\n\n<p>The presentation will take about 30 minutes, followed by Q&amp;A and discussion.</p>\n\n<p>Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm in MIT's building 66 at 25 Ames St, room 156. Room number subject to change based on availability; signs will be posted with the actual room number.</p>\n\n<p>Our default schedule is as follows:</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation. \n\u2014Phase 2: The headline event. This starts promptly at 2:30, and lasts 30-60 minutes. \n\u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups. \n\u2014Phase 4: Dinner. It's about a ten minute walk to the usual restaurant.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Intro_to_AI_Existential_Risk1\">Discussion article for the meetup : <a href=\"/meetups/r8\">Intro to AI Existential Risk</a></h2>", "sections": [{"title": "Discussion article for the meetup : Intro to AI Existential Risk", "anchor": "Discussion_article_for_the_meetup___Intro_to_AI_Existential_Risk", "level": 1}, {"title": "Discussion article for the meetup : Intro to AI Existential Risk", "anchor": "Discussion_article_for_the_meetup___Intro_to_AI_Existential_Risk1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-20T00:50:32.810Z", "modifiedAt": null, "url": null, "title": "The Belief Signaling Trilemma", "slug": "the-belief-signaling-trilemma", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:33.025Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WbQB5LnRyke5ye4Yg/the-belief-signaling-trilemma", "pageUrlRelative": "/posts/WbQB5LnRyke5ye4Yg/the-belief-signaling-trilemma", "linkUrl": "https://www.lesswrong.com/posts/WbQB5LnRyke5ye4Yg/the-belief-signaling-trilemma", "postedAtFormatted": "Friday, September 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Belief%20Signaling%20Trilemma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Belief%20Signaling%20Trilemma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWbQB5LnRyke5ye4Yg%2Fthe-belief-signaling-trilemma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Belief%20Signaling%20Trilemma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWbQB5LnRyke5ye4Yg%2Fthe-belief-signaling-trilemma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWbQB5LnRyke5ye4Yg%2Fthe-belief-signaling-trilemma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 560, "htmlBody": "<p>One major issue for group rationality is signaling with dishonest beliefs. Beliefs are used as a signal to preserve your reputation or to show membership in a group. It happens subconsciously, and I believe it is the cause of most of the issues of both religions and political parties. It also happens on Less Wrong, most commonly though not sharing beliefs that you think people will disagree with.</p>\n<p>First, let's identify the problem. This is mostly from the viewpoint of ideal agents as opposed to actual humans. You are part of a community of rationalists. You discuss lots of issues, and you become familiar with the other members of your community. As a result, you start to learn which members of the community are the smartest. Of course, your measure of the intelligence of the members is biased towards people who say things that you agree with. The members who say things that you agree with build up more reputation in your mind. This reputation makes you more likely to trust other things that this person says. I am also a member of this community. I have opinions on many things, but there is one issue that I think really does not matter at all. On this issue, most of the community believes X, but I believe not X. By signaling belief in X, I increase my reputation in the community, and will cause other people to take more seriously my views on other issues I think are more important. Therefore, I choose to signal belief in X.</p>\n<p>What is happening here is that:</p>\n<p>(A) People are assigning reputation based on claims, and judging claims based partially on other beliefs signaled by the same person.</p>\n<p>(B) People want their claims are taken seriously, and so take actions which will preserve and improve their reputation.</p>\n<p>Therefore,</p>\n<p>(C) People take signal beliefs that they believe are false because they are shared by the community.</p>\n<p>Signaling honest beliefs is kind of like cooperating in a prisoners dilemma. It helps the community push towards reaching what you believe are valid conclusions, at the cost of your own reputation. It is possible for us to decide as a community that we want to cooperate, especially with tools such as anti-kibitzer. However, there is more than one way to do that. I think there are three options. I think they are all theoretically possible, but I think they are all bad.</p>\n<p>(1) We can agree to stop assigning reputation based on beliefs.</p>\n<p>This option is bad because there is a loss of information. People who made the right choice on one issue are more likely to make the right choice on other issues, and we are ignoring this correlation.</p>\n<p>(2) We can agree to always report honest beliefs even though we know it will cost us reputation.</p>\n<p>This option is bad because it encourages self-deception. If you commit to honestly report beliefs, and you can gain more reputation by reporting belief in X, you may trick yourself into thinking that you believe X.</p>\n<p>(3) We can allow dishonest reporting of beliefs to continue.</p>\n<p>This option is bad because it causes a bias. The community will get a source of evidence biased towards their current beliefs.</p>\n<p>Which option do you prefer? Am I missing a fourth option? Is one of the choices obviously the best or obviously the worst? Should we combine them somehow? Am I modeling the problem entirely wrong?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wRdzfoH39vxqtNf37": 9}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WbQB5LnRyke5ye4Yg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 20, "extendedScore": null, "score": 5.2e-05, "legacy": true, "legacyId": "24221", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-20T02:50:57.121Z", "modifiedAt": null, "url": null, "title": "Meetup : Saskatoon: Iterative Improvement and Concrete Planning.", "slug": "meetup-saskatoon-iterative-improvement-and-concrete-planning", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nicholas_Rutherford", "createdAt": "2013-08-01T02:29:11.736Z", "isAdmin": false, "displayName": "Nicholas_Rutherford"}, "userId": "nucgkHPJBwJuK8sY7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xjzJknSTQpHMrSufg/meetup-saskatoon-iterative-improvement-and-concrete-planning", "pageUrlRelative": "/posts/xjzJknSTQpHMrSufg/meetup-saskatoon-iterative-improvement-and-concrete-planning", "linkUrl": "https://www.lesswrong.com/posts/xjzJknSTQpHMrSufg/meetup-saskatoon-iterative-improvement-and-concrete-planning", "postedAtFormatted": "Friday, September 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Saskatoon%3A%20Iterative%20Improvement%20and%20Concrete%20Planning.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Saskatoon%3A%20Iterative%20Improvement%20and%20Concrete%20Planning.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxjzJknSTQpHMrSufg%2Fmeetup-saskatoon-iterative-improvement-and-concrete-planning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Saskatoon%3A%20Iterative%20Improvement%20and%20Concrete%20Planning.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxjzJknSTQpHMrSufg%2Fmeetup-saskatoon-iterative-improvement-and-concrete-planning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxjzJknSTQpHMrSufg%2Fmeetup-saskatoon-iterative-improvement-and-concrete-planning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/r9'>Saskatoon: Iterative Improvement and Concrete Planning.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 September 2013 01:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2318 8th St E, Saskatoon, SK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello again everyone!</p>\n\n<p>Another Saskatoon meetup at the same place and time as the last one: Broadway Roaster on 8th street (not on broadway!) at 1:00 in the afternoon.</p>\n\n<p>For this week we'll be going over two concepts:\n1) How to improve ones life by doing regular reviews on how things are going and improving on what didn't work.\n2) How to make plans that will have a higher probability of you actually following through with them.</p>\n\n<p>More info here: <a href=\"http://www.meetup.com/Saskatoon-Rationalists/\" rel=\"nofollow\">http://www.meetup.com/Saskatoon-Rationalists/</a></p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/r9'>Saskatoon: Iterative Improvement and Concrete Planning.</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xjzJknSTQpHMrSufg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.3477863347835337e-06, "legacy": true, "legacyId": "24222", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Saskatoon__Iterative_Improvement_and_Concrete_Planning_\">Discussion article for the meetup : <a href=\"/meetups/r9\">Saskatoon: Iterative Improvement and Concrete Planning.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 September 2013 01:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2318 8th St E, Saskatoon, SK</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello again everyone!</p>\n\n<p>Another Saskatoon meetup at the same place and time as the last one: Broadway Roaster on 8th street (not on broadway!) at 1:00 in the afternoon.</p>\n\n<p>For this week we'll be going over two concepts:\n1) How to improve ones life by doing regular reviews on how things are going and improving on what didn't work.\n2) How to make plans that will have a higher probability of you actually following through with them.</p>\n\n<p>More info here: <a href=\"http://www.meetup.com/Saskatoon-Rationalists/\" rel=\"nofollow\">http://www.meetup.com/Saskatoon-Rationalists/</a></p>\n\n<p>Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Saskatoon__Iterative_Improvement_and_Concrete_Planning_1\">Discussion article for the meetup : <a href=\"/meetups/r9\">Saskatoon: Iterative Improvement and Concrete Planning.</a></h2>", "sections": [{"title": "Discussion article for the meetup : Saskatoon: Iterative Improvement and Concrete Planning.", "anchor": "Discussion_article_for_the_meetup___Saskatoon__Iterative_Improvement_and_Concrete_Planning_", "level": 1}, {"title": "Discussion article for the meetup : Saskatoon: Iterative Improvement and Concrete Planning.", "anchor": "Discussion_article_for_the_meetup___Saskatoon__Iterative_Improvement_and_Concrete_Planning_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-20T03:50:05.604Z", "modifiedAt": null, "url": null, "title": "AI-related honours projects?", "slug": "ai-related-honours-projects", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:52.049Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "klkblake", "createdAt": "2011-05-08T10:10:02.172Z", "isAdmin": false, "displayName": "klkblake"}, "userId": "ETWoHZJuTD3684Njy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GTZd9fNRfdSfKtY4e/ai-related-honours-projects", "pageUrlRelative": "/posts/GTZd9fNRfdSfKtY4e/ai-related-honours-projects", "linkUrl": "https://www.lesswrong.com/posts/GTZd9fNRfdSfKtY4e/ai-related-honours-projects", "postedAtFormatted": "Friday, September 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI-related%20honours%20projects%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI-related%20honours%20projects%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGTZd9fNRfdSfKtY4e%2Fai-related-honours-projects%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI-related%20honours%20projects%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGTZd9fNRfdSfKtY4e%2Fai-related-honours-projects", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGTZd9fNRfdSfKtY4e%2Fai-related-honours-projects", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 265, "htmlBody": "<p>I'm starting my Honours next year, and would like to do something towards helping MIRI with Friendly AI. I would also prefer to avoid duplicating any of MIRI's work (either already done, or needed to be done before my honours are finished midway through 2015). I decided to post this here rather than directly email MIRI as I guessed a list of potential projects would probably be useful for others as well (in fact, I was sure such a thing had already been posted, but I was unable to find it if it did in fact exist). So: what sort of Friendly AI related projects are there that could potentially be done by one person in a year of work? (I suppose it would make sense to include PhD-length suggestions here as well).</p>\n<p>Some notes about me and my abilities: I am reasonably good with math, though my understanding of probability, model theory and provability logic are lacking (I will have a few months before hand that I plan to use to try and learn whatever maths I will need that I don't already have). I am a competent Haskell programmer, and (besides AI) I am interested in dependent type systems, total languages, and similar methods of proving certain program errors cannot occur, although I would have to do some background research to learn more of the state of the art in that field. I would (hesitantly) guess that this would be the best avenue for something that a single person could do that might be useful, but I'm not sure how useful it would be.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GTZd9fNRfdSfKtY4e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 6, "extendedScore": null, "score": 1.3478390144824404e-06, "legacy": true, "legacyId": "24223", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-20T16:02:07.346Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-113", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:31.579Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QN7trqHoWWJzppKJ2/weekly-lw-meetups-113", "pageUrlRelative": "/posts/QN7trqHoWWJzppKJ2/weekly-lw-meetups-113", "linkUrl": "https://www.lesswrong.com/posts/QN7trqHoWWJzppKJ2/weekly-lw-meetups-113", "postedAtFormatted": "Friday, September 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQN7trqHoWWJzppKJ2%2Fweekly-lw-meetups-113%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQN7trqHoWWJzppKJ2%2Fweekly-lw-meetups-113", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQN7trqHoWWJzppKJ2%2Fweekly-lw-meetups-113", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 536, "htmlBody": "<p><strong>This summary was posted to LW Main on September 13th. The following week's summary is <a href=\"/lw/iow/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/qs\">Berlin: Fermi paradox discussion:&nbsp;<span class=\"date\">18 October 2013 07:00PM</span></a> </li>\n<li><a href=\"/meetups/qt\">Brussels monthly meetup:&nbsp;<span class=\"date\">14 September 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/r1\">Frankfurt:&nbsp;<span class=\"date\">22 September 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/qr\">Helsinki Meetup:&nbsp;<span class=\"date\">22 September 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/qx\">Moscow: Yet Another Meetup:&nbsp;<span class=\"date\">15 September 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/qu\">[Ottawa] Applied Rationality Talks: Thinking in Bayes:&nbsp;<span class=\"date\">26 September 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/qv\">Urbana-Champaign, Illinois Games/Discussion:&nbsp;<span class=\"date\">15 September 2013 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/qo\">London social meetup:&nbsp;<span class=\"date\">15 September 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/qp\">Melbourne Social Meetup:&nbsp;<span class=\"date\">20 September 2013 06:30PM</span></a></li>\n<li><a href=\"/meetups/qe\">[Salt Lake City] Fall Equinox: Festival of Heroes:&nbsp;<span class=\"date\">21 September 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/px\"></a><a href=\"/meetups/qw\">Washington DC fun and games meetup:&nbsp;<span class=\"date\">15 September 2013 03:00AM</span></a></li>\n<li><a href=\"/meetups/qy\">West LA&mdash;Talk Like a Pirate Day:&nbsp;<span class=\"date\">18 September 2013 07:00AM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QN7trqHoWWJzppKJ2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3484913653634693e-06, "legacy": true, "legacyId": "24148", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3KCcSufGjDYCoo4Aa", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-20T17:37:39.644Z", "modifiedAt": null, "url": null, "title": "Interactive Infographic on Simpson's Paradox", "slug": "interactive-infographic-on-simpson-s-paradox", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:31.955Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Antisuji", "createdAt": "2009-04-15T23:39:08.004Z", "isAdmin": false, "displayName": "Antisuji"}, "userId": "zRsg2BPDaXGTkXFMd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qqj4683HXEabnjNgn/interactive-infographic-on-simpson-s-paradox", "pageUrlRelative": "/posts/qqj4683HXEabnjNgn/interactive-infographic-on-simpson-s-paradox", "linkUrl": "https://www.lesswrong.com/posts/qqj4683HXEabnjNgn/interactive-infographic-on-simpson-s-paradox", "postedAtFormatted": "Friday, September 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Interactive%20Infographic%20on%20Simpson's%20Paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInteractive%20Infographic%20on%20Simpson's%20Paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqqj4683HXEabnjNgn%2Finteractive-infographic-on-simpson-s-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Interactive%20Infographic%20on%20Simpson's%20Paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqqj4683HXEabnjNgn%2Finteractive-infographic-on-simpson-s-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqqj4683HXEabnjNgn%2Finteractive-infographic-on-simpson-s-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<p>Since Simpson's Paradox has been discussed here <a href=\"/r/discussion/lw/ioj/understanding_simpsons_paradox/\">recently</a>&nbsp;(and <a href=\"/lw/3q3/simpsons_paradox/\">not so recently</a>), I thought I'd share this interactive<sup>1</sup> <a href=\"http://vudlab.com/simpsons/\">infographic</a> that I found via the FlowingData blog. I already understood Simpson's Paradox pretty well, but playing with the sliders helped me get a more intuitive feel for it.</p>\n<p>I expect similar tools would be helpful for explaining Bayes' Theorem and some of the other things we talk about on LW (like Pareto efficiency and Nash equilibria). Do such things exist?</p>\n<p>&nbsp;</p>\n<hr />\n<p><sup>1</sup> The interactive part is farther down the page.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"L3NcKBNTvQaFXwv9u": 1, "TkZ7MFwCi4D63LJ5n": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qqj4683HXEabnjNgn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 39, "extendedScore": null, "score": 1.348576545937935e-06, "legacy": true, "legacyId": "24225", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FGsoajyYJRczieqh5", "7FJRnxbRtT7Sbzizs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-20T20:19:08.082Z", "modifiedAt": null, "url": null, "title": "Instinctive Frequentists, the Outside View, and de-Biasing", "slug": "instinctive-frequentists-the-outside-view-and-de-biasing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:08.779Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/24CfLhbByQqv6nSws/instinctive-frequentists-the-outside-view-and-de-biasing", "pageUrlRelative": "/posts/24CfLhbByQqv6nSws/instinctive-frequentists-the-outside-view-and-de-biasing", "linkUrl": "https://www.lesswrong.com/posts/24CfLhbByQqv6nSws/instinctive-frequentists-the-outside-view-and-de-biasing", "postedAtFormatted": "Friday, September 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Instinctive%20Frequentists%2C%20the%20Outside%20View%2C%20and%20de-Biasing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInstinctive%20Frequentists%2C%20the%20Outside%20View%2C%20and%20de-Biasing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F24CfLhbByQqv6nSws%2Finstinctive-frequentists-the-outside-view-and-de-biasing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Instinctive%20Frequentists%2C%20the%20Outside%20View%2C%20and%20de-Biasing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F24CfLhbByQqv6nSws%2Finstinctive-frequentists-the-outside-view-and-de-biasing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F24CfLhbByQqv6nSws%2Finstinctive-frequentists-the-outside-view-and-de-biasing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 694, "htmlBody": "<p>In \"<a href=\"http://library.mpib-berlin.mpg.de/ft/gg/gg_how_1991.pdf\">How to Make Cognitive Illusions Disappear: Beyond Heuristics and Biases</a>\", <a href=\"http://en.wikipedia.org/wiki/Gerd_Gigerenzer\">Gerd Gigerenzer</a> attempts to show that the whole \"Heuristics and Biases\" approach to analysing human reasoning is fundamentally flawed and incorrect.</p>\n<p>In that he fails. His case depends on using the <a href=\"http://en.wikipedia.org/wiki/Frequency_probability\">frequentist</a> argument that probabilities cannot be assigned to single events or situations of subjective uncertainty, thus removing the possibility that people could be \"wrong\" in the scenarios where the biases were tested. (It is interesting to note that he ends up constructing \"Probabilistic Mental Models\", which are frequentist ways of assigning subjective probabilities - just as long as you don't call them that!).</p>\n<p>But that dodge isn't sufficient. Take the famous example of the <a href=\"http://en.wikipedia.org/wiki/Conjunction_fallacy\">conjunction fallacy</a>, where most people are tricked to assigning a higher probability to \"Linda is a bank teller AND is active in the feminist movement\" than to \"Linda is a bank teller\". This error <a href=\"http://psycnet.apa.org/journals/rev/90/4/293/\">persists</a> even when people take bets on the different outcomes. By betting more (or anything) on the first option, people are giving up free money. This is a failure of human reasoning, whatever one thinks about the morality of assigning probability to single events.</p>\n<p>However, though the article fails to prove its case, it presents a lot of powerful results that may change how we think about biases. It presents weak evidence that people may be instinctive frequentist statisticians, and much stronger evidence that <em>many&nbsp;</em><em>biases can go away when the problems are presented in frequentist ways</em>.</p>\n<p>Now, it's known that people are more comfortable with frequencies that with probabilities. The examples in the paper extend that intuition. For instance, when people are asked:</p>\n<blockquote>\n<p>There are 100 persons who fit the description above (i.e., Linda's). How many of them are:<br />(a) bank tellers<br />(b) bank tellers and active in the feminist movement.</p>\n</blockquote>\n<p>Then the conjunction fallacy essentially disappears (22% of people make the error, rather than 85%). That is a huge difference.</p>\n<p>Similarly, overconfidence. When people were&nbsp;50 general knowledge questions&nbsp;and asked to rate their confidence for their answer on each question, they were systematically, massively overconfident. But when they were asked afterwards \"How many of these 50 questions do you think you got right?\", they were... underconfident. But only very slightly: they were essentially correct in their self-assessments. This can be seen as a use of the outside view - a use that is, in this case, entirely justified. People know their overall accuracy much better than they know their specific accuracy.</p>\n<p>A more intriguing example makes the base-rate fallacy disappear. Presenting the problem in a frequentist way makes the fallacy vanish when computing false positives for tests on rare diseases - that's compatible with the general theme. But it really got interesting when people actively participated in the randomisation process. In the standard problem, students were given thumbnail description of individuals, and asked to guess whether they were more likely to be engineers or lawyers. Half the time the students were told the descriptions were drawn at random from 30 lawyers and 70 engineers; the other half, the proportions were reversed. It turns out that students assigned similar guesses to lawyer and engineer in both setups, showing they were neglecting to use the 30/70 or 70/30 base-rate information.</p>\n<p>Gigerenzer modified the setups by telling the students the 30/70 or 70/30 proportions and then having the students themselves drew each description (blindly) out of an urn before assessing it. In that case, base-rate neglect disappears.</p>\n<p>Now, I don't find that revelation <em>quite</em> as superlatively exciting as Gigerenzer does. Having the students draw the description out of the urn is pretty close to whacking them on the head with the base-rate: it really focuses their attention on this aspect, and once it's risen to their attention, they're much more likely to make use of it. It's still very interesting, though, and suggests some practical ways of overcoming the base-rate problem that stop short of saying \"hey, don't forget the base-rate\".</p>\n<p>There is a large literature out there <a href=\"/lw/h0p/critiques_of_the_heuristics_and_biases_tradition/\">critiquing the heuristics and biases tradition</a>. Even if they fail to prove their point, they're certainly useful for qualifying the biases and heuristics results, and, more interestingly, for suggesting practical ways of combating their effects.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 1, "bh7uxTTqmsQ8jZJdB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "24CfLhbByQqv6nSws", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 63, "extendedScore": null, "score": 0.0006102323927101942, "legacy": true, "legacyId": "24226", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DAf4W9ZYuzuLaGvd5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-20T22:09:01.071Z", "modifiedAt": null, "url": null, "title": "Amplituhedron?", "slug": "amplituhedron", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:31.299Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "linkhyrule5", "createdAt": "2011-05-11T06:03:56.654Z", "isAdmin": false, "displayName": "linkhyrule5"}, "userId": "dfDgYH8CfLSYPCaRj", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/C4r2TpKhaRG5QtxHz/amplituhedron", "pageUrlRelative": "/posts/C4r2TpKhaRG5QtxHz/amplituhedron", "linkUrl": "https://www.lesswrong.com/posts/C4r2TpKhaRG5QtxHz/amplituhedron", "postedAtFormatted": "Friday, September 20th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Amplituhedron%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAmplituhedron%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC4r2TpKhaRG5QtxHz%2Famplituhedron%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Amplituhedron%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC4r2TpKhaRG5QtxHz%2Famplituhedron", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC4r2TpKhaRG5QtxHz%2Famplituhedron", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 194, "htmlBody": "<p>I recently ran across a rather interesting result while browsing the Internet:</p>\n<p><a href=\"https://www.simonsfoundation.org/quanta/20130917-a-jewel-at-the-heart-of-quantum-physics/\">Physics Discover Geometry Underlying Particle Physics</a></p>\n<blockquote>\n<p>Physicists have discovered a jewel-like geometric object that  dramatically simplifies calculations of particle interactions and  challenges the notion that space and time are fundamental components of  reality.</p>\n<p>&ldquo;This is completely new and very much simpler than anything that has been done before,&rdquo; said <a href=\"http://www.maths.ox.ac.uk/contact/details/hodges\">Andrew Hodges</a>, a mathematical physicist at Oxford University who has been following the work.</p>\n<p>The revelation that particle interactions, the most basic events in  nature, may be consequences of geometry significantly advances a  decades-long effort to reformulate quantum field theory, the body of  laws describing elementary particles and their interactions.  Interactions that were previously calculated with mathematical formulas  thousands of terms long can now be described by computing the volume of  the corresponding jewel-like &ldquo;amplituhedron,&rdquo; which yields an equivalent  one-term expression.</p>\n</blockquote>\n<p>Unfortunately, I'm still at the point in my education where my best response to new physics is <a href=\"http://squid314.livejournal.com/350090.html\">\"cache for later,\"</a> and the fact that it claims to eliminate locality/unitarity seems decidedly <em>odd</em> to my mostly-untrained mind. I notice I am confused, and that LessWrong has a rather large number of trained physicists.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "C4r2TpKhaRG5QtxHz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 11, "extendedScore": null, "score": 1.3488185359492655e-06, "legacy": true, "legacyId": "24228", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-21T01:53:02.169Z", "modifiedAt": null, "url": null, "title": "Dark Arts 101: Winning via destruction and dualism", "slug": "dark-arts-101-winning-via-destruction-and-dualism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:31.676Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gaMwrN7o2Q9S89RTz/dark-arts-101-winning-via-destruction-and-dualism", "pageUrlRelative": "/posts/gaMwrN7o2Q9S89RTz/dark-arts-101-winning-via-destruction-and-dualism", "linkUrl": "https://www.lesswrong.com/posts/gaMwrN7o2Q9S89RTz/dark-arts-101-winning-via-destruction-and-dualism", "postedAtFormatted": "Saturday, September 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dark%20Arts%20101%3A%20Winning%20via%20destruction%20and%20dualism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADark%20Arts%20101%3A%20Winning%20via%20destruction%20and%20dualism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgaMwrN7o2Q9S89RTz%2Fdark-arts-101-winning-via-destruction-and-dualism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dark%20Arts%20101%3A%20Winning%20via%20destruction%20and%20dualism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgaMwrN7o2Q9S89RTz%2Fdark-arts-101-winning-via-destruction-and-dualism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgaMwrN7o2Q9S89RTz%2Fdark-arts-101-winning-via-destruction-and-dualism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 257, "htmlBody": "<p>Recalling first that life is a zero-sum game, it is immediately obvious that the quickest and easiest path to success is not to accomplish things yourself&mdash;that's a game for heroes and other suckers&mdash;but to tear down the accomplishments and reputations of others. Destruction is easy. The difficulty lies in constructing a situation so that that destruction is to your net benefit.<a id=\"more\"></a></p>\n<p>The problem with destruction is that it invites retaliation. Even when your talent for lies, slander, and other dirty work is superior to your opponent's, he will still manage to cause you difficulties. When there is a third, uninvolved party, all they need do is stay out of it until you and your opponent have beaten each other down, and they emerge the winner.</p>\n<p>It is therefore crucial that you prepare the way for your destructive ascent by polarizing the field into two camps. There must be no third parties or positions. If there are three political parties, you must begin by crushing the weakest. If there are three or more possible opinions on a matter, you must introduce terminology that conflates them and makes it appear that there are only two. Uniting your opponents is not beneficial, but making them appear to be united is, as it allows your attacks to strike them all at once. The vicious arts can thrive only when the apparent choices are first narrowed down to two.</p>\n<p>(The mirror tactic is to narrow down the number of choices to one, and present them as two. We shall cover that in Dark Arts 201.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gaMwrN7o2Q9S89RTz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": -26, "extendedScore": null, "score": -6.2e-05, "legacy": true, "legacyId": "24229", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-21T05:59:28.169Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Book Swap meetup", "slug": "meetup-washington-dc-book-swap-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:31.472Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/krLCfMGXfD794zdoN/meetup-washington-dc-book-swap-meetup", "pageUrlRelative": "/posts/krLCfMGXfD794zdoN/meetup-washington-dc-book-swap-meetup", "linkUrl": "https://www.lesswrong.com/posts/krLCfMGXfD794zdoN/meetup-washington-dc-book-swap-meetup", "postedAtFormatted": "Saturday, September 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Book%20Swap%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Book%20Swap%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkrLCfMGXfD794zdoN%2Fmeetup-washington-dc-book-swap-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Book%20Swap%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkrLCfMGXfD794zdoN%2Fmeetup-washington-dc-book-swap-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkrLCfMGXfD794zdoN%2Fmeetup-washington-dc-book-swap-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 44, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ra'>Washington DC Book Swap meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 September 2013 03:00:12PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">national portrait gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and swap books. (Sorry for the late notice).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ra'>Washington DC Book Swap meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "krLCfMGXfD794zdoN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.34923825598108e-06, "legacy": true, "legacyId": "24230", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Book_Swap_meetup\">Discussion article for the meetup : <a href=\"/meetups/ra\">Washington DC Book Swap meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 September 2013 03:00:12PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">national portrait gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and swap books. (Sorry for the late notice).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Book_Swap_meetup1\">Discussion article for the meetup : <a href=\"/meetups/ra\">Washington DC Book Swap meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Book Swap meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Book_Swap_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Book Swap meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Book_Swap_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-21T12:46:50.675Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana-Champaign: The l\u00f6bstacle for picking good actions.", "slug": "meetup-urbana-champaign-the-loebstacle-for-picking-good", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CHes3ee7R78CdrFJh/meetup-urbana-champaign-the-loebstacle-for-picking-good", "pageUrlRelative": "/posts/CHes3ee7R78CdrFJh/meetup-urbana-champaign-the-loebstacle-for-picking-good", "linkUrl": "https://www.lesswrong.com/posts/CHes3ee7R78CdrFJh/meetup-urbana-champaign-the-loebstacle-for-picking-good", "postedAtFormatted": "Saturday, September 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana-Champaign%3A%20The%20l%C3%B6bstacle%20for%20picking%20good%20actions.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana-Champaign%3A%20The%20l%C3%B6bstacle%20for%20picking%20good%20actions.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCHes3ee7R78CdrFJh%2Fmeetup-urbana-champaign-the-loebstacle-for-picking-good%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana-Champaign%3A%20The%20l%C3%B6bstacle%20for%20picking%20good%20actions.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCHes3ee7R78CdrFJh%2Fmeetup-urbana-champaign-the-loebstacle-for-picking-good", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCHes3ee7R78CdrFJh%2Fmeetup-urbana-champaign-the-loebstacle-for-picking-good", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rb'>Urbana-Champaign: The l\u00f6bstacle for picking good actions.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 September 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">40.111240, -88.227370</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's have a slightly more technical meetup!\nWhen: 2:00 PM this Sunday.\nWhere: The benches overlooking UIUC's Bardeen Quad, Urbana-Champaign, IL.\nWhat: I'll give a primer on why even the best agent can't prove that they're always right (see e.g. <a href=\"http://lesswrong.com/lw/t8/you_provably_cant_trust_yourself/\">You Provably Can&#39;t Trust Yourself</a>), and why this poses a problem for some common models of decision-making (discussed at length <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">here</a>). And then we can talk about what other models we could use!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rb'>Urbana-Champaign: The l\u00f6bstacle for picking good actions.</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CHes3ee7R78CdrFJh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.3496018881636995e-06, "legacy": true, "legacyId": "24231", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__The_l_bstacle_for_picking_good_actions_\">Discussion article for the meetup : <a href=\"/meetups/rb\">Urbana-Champaign: The l\u00f6bstacle for picking good actions.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 September 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">40.111240, -88.227370</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's have a slightly more technical meetup!\nWhen: 2:00 PM this Sunday.\nWhere: The benches overlooking UIUC's Bardeen Quad, Urbana-Champaign, IL.\nWhat: I'll give a primer on why even the best agent can't prove that they're always right (see e.g. <a href=\"http://lesswrong.com/lw/t8/you_provably_cant_trust_yourself/\">You Provably Can't Trust Yourself</a>), and why this poses a problem for some common models of decision-making (discussed at length <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">here</a>). And then we can talk about what other models we could use!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__The_l_bstacle_for_picking_good_actions_1\">Discussion article for the meetup : <a href=\"/meetups/rb\">Urbana-Champaign: The l\u00f6bstacle for picking good actions.</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana-Champaign: The l\u00f6bstacle for picking good actions.", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__The_l_bstacle_for_picking_good_actions_", "level": 1}, {"title": "Discussion article for the meetup : Urbana-Champaign: The l\u00f6bstacle for picking good actions.", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__The_l_bstacle_for_picking_good_actions_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rm8tv9qZ9nwQxhshx", "gnxDNEtkEo3sfeyPn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-21T19:33:42.667Z", "modifiedAt": null, "url": null, "title": "Predicting Organizational Behavior", "slug": "predicting-organizational-behavior", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:09.980Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ksvanhorn", "createdAt": "2011-01-10T19:23:33.231Z", "isAdmin": false, "displayName": "ksvanhorn"}, "userId": "fiau5fcXpbad9d6D4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nfC9LZeEBYKNcFDEA/predicting-organizational-behavior", "pageUrlRelative": "/posts/nfC9LZeEBYKNcFDEA/predicting-organizational-behavior", "linkUrl": "https://www.lesswrong.com/posts/nfC9LZeEBYKNcFDEA/predicting-organizational-behavior", "postedAtFormatted": "Saturday, September 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Predicting%20Organizational%20Behavior&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APredicting%20Organizational%20Behavior%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnfC9LZeEBYKNcFDEA%2Fpredicting-organizational-behavior%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Predicting%20Organizational%20Behavior%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnfC9LZeEBYKNcFDEA%2Fpredicting-organizational-behavior", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnfC9LZeEBYKNcFDEA%2Fpredicting-organizational-behavior", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 100, "htmlBody": "<p>Can someone recommend a good introduction to the topic of organizational behavior? My interest is in <em>descriptive</em> rather than <em>prescriptive</em> models -- I'm interested in what is known about predicting the behavior of organizations, rather than guidance on what they should do to achieve their goals. This kind of prediction strikes me as something of substantial practical use, especially to business; being able to work out the plausible range of future actions of city hall, the state legislature, Congress, regulatory agencies, competitors in the marketplace, large customers, and important suppliers would be a valuable capability in making one's own plans.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nfC9LZeEBYKNcFDEA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "24233", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-21T22:29:05.744Z", "modifiedAt": null, "url": null, "title": "Polyphasic Sleep Seed Study: Reprise", "slug": "polyphasic-sleep-seed-study-reprise", "viewCount": null, "lastCommentedAt": "2020-04-28T21:43:44.728Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BrienneYudkowsky", "createdAt": "2013-05-13T00:07:08.935Z", "isAdmin": false, "displayName": "LoganStrohl"}, "userId": "uuYBzWLiixkbN3s7C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QvZ6w64JugewNiccS/polyphasic-sleep-seed-study-reprise", "pageUrlRelative": "/posts/QvZ6w64JugewNiccS/polyphasic-sleep-seed-study-reprise", "linkUrl": "https://www.lesswrong.com/posts/QvZ6w64JugewNiccS/polyphasic-sleep-seed-study-reprise", "postedAtFormatted": "Saturday, September 21st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Polyphasic%20Sleep%20Seed%20Study%3A%20Reprise&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APolyphasic%20Sleep%20Seed%20Study%3A%20Reprise%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQvZ6w64JugewNiccS%2Fpolyphasic-sleep-seed-study-reprise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Polyphasic%20Sleep%20Seed%20Study%3A%20Reprise%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQvZ6w64JugewNiccS%2Fpolyphasic-sleep-seed-study-reprise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQvZ6w64JugewNiccS%2Fpolyphasic-sleep-seed-study-reprise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 476, "htmlBody": "<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\">(Original post on the polyphasic sleep experiment&nbsp;</span><span style=\"font-family: arial, sans-serif; color: #2690c7;\"><span style=\"line-height: 18px;\"><a href=\"/lw/hyr/seed_study_polyphasic_sleep_in_ten_steps/\">here</a></span></span><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\">.)</span><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\" /><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\">Welp, this got a little messy. The main culprit was Burning Man, though there were some other complications with data collection as well. Here are the basics of what went down.</span><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\" /></p>\n<div class=\"separator\" style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px; clear: both; text-align: center;\"><a style=\"text-decoration: none; color: #2690c7; clear: right; float: right; margin-bottom: 1em; margin-left: 1em;\" href=\"http://blog.kanjy.co/wp-content/uploads/2013/05/activities-in-the-morning-wake-up-early.jpg\"><img style=\"border-style: none; position: relative;\" src=\"http://blog.kanjy.co/wp-content/uploads/2013/05/activities-in-the-morning-wake-up-early.jpg\" border=\"0\" alt=\"\" /></a></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\">Fourteen people participated in the main experiment. Most of them were from <a href=\"http://www.leverageresearch.org/\">Leverage</a>. There were a few stragglers from a distance, but communication with them was poor.&nbsp;</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\">We did some cognitive batteries beforehand, mostly through Quantified Mind. A few people had extensive baseline data, partially because many had been using Zeos for months, and partly because a few stuck to the two-week daily survey. Leverage members (not me) are processing the data, and they'll probably have more detailed info for us in three months(ish).</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\">With respect to the adaptation itself, we basically followed the plan outlined in my last post. Day one no sleep, then Uberman-12, then cut back to Uberman-6, then Everyman-3.<br /><br />Most people ended up switching very quickly to Uberman-6 (within the first two or three days), and most switched to Everyman-3 after about five to seven days on Uberman-6. Three people tried to hold the Uberman schedule indefinitely: One person continued Uberman-6 for two full weeks, and two held out for twenty-one days. Afterwards, all three transitioned to Everyman-3.&nbsp;</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\"><span style=\"text-align: center;\">During the originally planned one-month period, five people dropped out. Nine were on some form of polyphasic for the whole month. One returned to monophasic at the end of the official experiment with only partial adaptation achieved.&nbsp;</span></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\">Then Burning Man disrupted everybody's sleep schedule. Afterward, one person continued experimenting with less common variations of the Everyman schedule. Three went back to Everyman-3. One switched to Everyman-2. Two people have flexible schedules that include two hours less sleep per day. One person's schedule was disrupted by travel for a while after Burning Man, and they're now re-adapting.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\">Now that all is said and done, eight of the original fourteen are polyphasic.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\"><img src=\"http://images.lesswrong.com/t3_hyr_0.png\" alt=\"\" width=\"698\" height=\"270\" /><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\">I'll hold off on concluding very much from this until I see the results of the cognitive battery and such, plus the number who are still polyphasic after three months. In the mean time, I'll just stick with this: Some people are capable of going polyphasic and staying that way (probably?). Sleep is complicated and confusing. I don't know how it works. I don't think anyone else really does either. More research is desperately needed.<br /><br />I know three months is a long way away. I'm feeling impatient too. But details <em>will </em>arrive! In the mean time, <a href=\"https://www.facebook.com/photo.php?v=10151747252439598&amp;set=vb.792129597&amp;type=2&amp;theater\">here's a video</a> of what zombie-Brienne is like during the really difficult stretches, and <a href=\"https://www.facebook.com/photo.php?v=10151748632169598&amp;set=vb.792129597&amp;type=2&amp;theater\">here is how she entertained herself</a> when she could manage to do things besides pace. (I was one of the few who bailed out early :-p)</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HLoxy2feb2PYqooom": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QvZ6w64JugewNiccS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 63, "extendedScore": null, "score": 0.000598267051676661, "legacy": true, "legacyId": "24234", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cMnjqk8iEzycbLBDA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-22T20:53:45.605Z", "modifiedAt": null, "url": null, "title": "Hoping to start a discussion about overcoming insecurity", "slug": "hoping-to-start-a-discussion-about-overcoming-insecurity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:04.674Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ILikeLogic", "createdAt": "2013-07-20T19:39:29.641Z", "isAdmin": false, "displayName": "ILikeLogic"}, "userId": "s9HJ6PZaa62FjYYBf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oNPQDmfx3z9xsT8RJ/hoping-to-start-a-discussion-about-overcoming-insecurity", "pageUrlRelative": "/posts/oNPQDmfx3z9xsT8RJ/hoping-to-start-a-discussion-about-overcoming-insecurity", "linkUrl": "https://www.lesswrong.com/posts/oNPQDmfx3z9xsT8RJ/hoping-to-start-a-discussion-about-overcoming-insecurity", "postedAtFormatted": "Sunday, September 22nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hoping%20to%20start%20a%20discussion%20about%20overcoming%20insecurity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHoping%20to%20start%20a%20discussion%20about%20overcoming%20insecurity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoNPQDmfx3z9xsT8RJ%2Fhoping-to-start-a-discussion-about-overcoming-insecurity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hoping%20to%20start%20a%20discussion%20about%20overcoming%20insecurity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoNPQDmfx3z9xsT8RJ%2Fhoping-to-start-a-discussion-about-overcoming-insecurity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoNPQDmfx3z9xsT8RJ%2Fhoping-to-start-a-discussion-about-overcoming-insecurity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1463, "htmlBody": "<p>Since Jr High at least, I've been frustrated by my insecurity. I don't intend this to be a personally revealing post so I'll just sum it up by saying that being insecure has had a profoundly negative impact on my life. I feel that it is the single biggest reason why I've failed to reach my potential in all ways. That's fine though, I'm not really bitter but I remain very frustrated and I want to solve this problem. I want to 'crack the code', if you will.</p>\n<p>I've recently started reading some psychology books (again) which has led to me to revisit a couple of the self-help/psychology books that I used to be very fond of.</p>\n<p>I've really been wanting to find a forum where I can discuss this with people who will understand what I'm talking about. Well, the other day, I followed a link to LessWrong, which I was somewhat familiar with because I used to visit and spend time here every now and then, and I remembered that I had read on here about self-help. Also I remember reading about how some of the people here had really liked the meetups because they were able to to talk more freely and be better understood than they normally are. I have had some frustration in discussing emotional topics elsewhere because of the lack of intellectual rigor with which they are often discussed. Like everything else, human emotions 'work a certain way'. Exactly how they work is not something that is perfectly understood by anyone but I find it frustrating when discussing them with people who don't seem to understand that, whatever the rules are, there are rules. So it occurred to me that LessWrong might be a good place to have the kind of discussion that I'd like to have. If you are interested in emotional insecurity in general and my take on it then you may want to read the rest of this post.</p>\n<p>I've developed my own understanding of insecurity, which, admittedly, is a synthesis of other people's ideas, but I haven't found any book or therapy or system that puts it all together in a way that I fully agree with.</p>\n<p>Here is what I think:</p>\n<p>I think that what insecurity is, is inhibition of feelings of disappointment/loss because of an implicitly learned belief that to express these feelings will have negative consequences (ie &ndash; it will only make things worse).</p>\n<p>I came across this idea after reading some EvPsych theory about the functional purpose of shame. The purpose of shame, it seems, is to signal to the other person that you feel badly and to elicit a rapprochement, a re-initiation of the connection that was broken when the other person broke it (due to anger or rejection or disapproval). Shame is functional. It allows group members to signal how much they value their connections to one another when those connections are temporarily broken. The person who engaged in the behavior that elicited the disapproval/rejection/anger feels a rather intense aversive feeling when the connection is threatened and this is signaled by the signs of distress that accompany properly functioning shame. The other person recognizes that the transgressor regrets the transgression and this appeases their anger. So the whole thing results in everyone feeling better, all connections restored, and the transgressor being a little bit wiser for it all.</p>\n<p>I think insecurity develops when a person who has had a connection interrupted, expresses the normal distress and is further punished for that expression. If they are punished enough for expressing this distress they will suppress it, consciously at first and then automatically after the habit is formed. (I remember as a child being proud that I could endure these humiliations without crying. But I was naive, because I believed that if I wasn't reacting to it I wasn't affected by it. Wrong. This was not a good ability to have.) Before long they will be repressing their distress without even being aware that they are doing so. If they are like me they will, later, wake up to the fact that they are anxious and awkward and that these things are making their life a lot worse than it could be.</p>\n<p>This is where a couple of the books that I've been reading recently come in. The two books are 'The Neuroscience of Psychotherapy' and 'Unlocking Your Emotional Brain'. At one point in 'The Neuroscience of Psychotherapy' the author (Louis Cozolino) talks about his job as a therapist being to create, in his clients, the expectation of reassurance or soothing, when they are faced with distress. It occurred to me that the anxiety that I was experiencing may be just the memory of a rejection/disapproval followed quickly by inhibition(accomplished via fear or anxiety). Inhibition that became a habit because there was no reassurance or soothing when the rejection/disapproval occurred. And so the idea naturally followed that if I could perhaps, somehow, not inhibit the feeling, and instead jump in and console or reassure or soothe myself in one way or another, then I could break the habit of inhibition and be rid of the anxiety.</p>\n<p>That brings me to another self-help book, 'Focusing' by Eugene Gendlin, which I read about 20 years ago. The basic idea of 'focusing' is that if you pay attention to the feeling in your body, and don't distract yourself with too much thinking or paying attention to other things but just 'stay with' the feeling in your body then after some time (seconds or just a few minutes usually) you will recognize the feeling and have an insight about what it is that will provide you with immediate relief as the feeling, consciously recognized, runs its proper course. I remember really liking this book when I first read it and tried its ideas. The relief that you can feel is immediate and unmistakable. This is not something where you adopt some positive attitude that you think will benefit you but underneath you still feel anxious and insecure. No, the relief leaves you really feeling good and confident.</p>\n<p>When 'Focusing', a book written about 30 years ago, showed up in the &ldquo;Users Who Bought This Also Bought&rdquo; on Amazon.com for 'Unlocking Your Emotional Brain', I remembered reading it and naturally got the idea to combine the focusing technique with my idea about jumping in with reassurance.</p>\n<p>At about this time (this was fairly recently) I had also started reading 'Unlocking Your Emotional Brain' (still am &ndash; I'm about 1/3rd through it). This book is very exciting because it goes into a bit of detail about some of the scientific research on memory re-consolidation that really makes it seem possible to permanently rid one's self of unhelpful automatic emotional reactions. The gist of the memory re-consolidation research is that every time neuronal connections are activated they are vulnerable to change, and will change if a relevant experience that contradicts or modifies the belief on which they are based, happens soon enough after the emotion has been activated. If they are not activated, however, they cannot be changed. So just talking and thinking about feelings without activating them cannot change the learned emotional reactions. The authors have a therapy that they call Coherence Therapy which is designed to take advantage of this. I haven't really read far enough to know the details of their Coherence Therapy but what I have read so far fits in well with my own developing understanding of this.</p>\n<p>Also relevant is Arthur Janov's primal therapy. When I read his book, also almost 20 years ago now, I had a strong intuition that he was right, even if his theory to explain it was a bit half-baked and nonsensical. I tried to do Primal Therapy on myself and at times I succeeded. And the change in how I felt was, like with focusing, profound. The change with a good primal was even stronger than with focusing. I felt completely secure and free of anxiety for up to a few days. It was wonderful. It also had a feeling of &ldquo;this is how it is supposed to be&rdquo;. So my experience with Primal Therapy (on myself, never with a therapist) also leads me to believe that some experience that involves actually engaging the problematic feelings is necessary to change them.</p>\n<p>Well that's about where I stand with it right now. I'm trying to spend some time every day doing my process (a modified form of Focusing). When I have some quiet and a decent block of time (at least 20 minutes uninterrupted but ideally up to an hour) I seem to be having some good success with it but it is also frustrating at times as sometimes it is difficult to get 'movement' in how I feel.</p>\n<p>I'd really appreciate anyone's thoughts on this. Thanks in advance.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oNPQDmfx3z9xsT8RJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 19, "extendedScore": null, "score": 1.3513242480791918e-06, "legacy": true, "legacyId": "23415", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-23T04:34:25.417Z", "modifiedAt": null, "url": null, "title": "Torture vs. Shampoo", "slug": "torture-vs-shampoo", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:35.952Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kXmLGPG8QfXpm2DfY/torture-vs-shampoo", "pageUrlRelative": "/posts/kXmLGPG8QfXpm2DfY/torture-vs-shampoo", "linkUrl": "https://www.lesswrong.com/posts/kXmLGPG8QfXpm2DfY/torture-vs-shampoo", "postedAtFormatted": "Monday, September 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Torture%20vs.%20Shampoo&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATorture%20vs.%20Shampoo%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkXmLGPG8QfXpm2DfY%2Ftorture-vs-shampoo%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Torture%20vs.%20Shampoo%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkXmLGPG8QfXpm2DfY%2Ftorture-vs-shampoo", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkXmLGPG8QfXpm2DfY%2Ftorture-vs-shampoo", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 301, "htmlBody": "<p>I was reading the original comment thread on <a href=\"/lw/kn/torture_vs_dust_specks/\">Torture vs. Dust Specks</a>, and notice Eliezer <a href=\"/lw/kn/torture_vs_dust_specks/ueq\">saying</a> he wouldn't pay a penny to avoid a <em>single </em>dust speck - which confused me, until I noticed that the original specification of the problem says the dust speck \"<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">floated into your eye and irritated it just a little, for a fraction of a second, barely enough to make you notice before you blink and wipe away the dust speck.\"&nbsp;</span><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\">I guess I blanked that out when I first read the post. My default visualization when I imagine \"dust speck in my eye\" is something </span><em style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\">substantially </em><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; text-align: justify;\">more annoying than that.</span></p>\n<p style=\"text-align: justify;\">This leads me to wonder if people would have responded differently if instead of going out of his way to make the alternative to torture involve something as trivial-sounding as possible, Eliezer had gone for some merely minor mishap - say, getting shampoo in your eye. After all, lots of us have gotten shampoo in our eyes at least once (maybe when we were kids), and it's easy to imagine paying $2.99 for a bottle of won't-irritate-your-eyes shampoo over an otherwise identical $2.98 bottle that will hurt if you get it in your eyes (or your kid's eyes, if you're totally confident that, as an adult, you'll be able to keep shampoo out of your eyes).</p>\n<p style=\"text-align: justify;\">From there, it's easy to argue that if you're honest with yourself you wouldn't pay $(3^^^3/100) to save one person from being tortured for 50 years, so you should choose one person getting tortured for 50 years over <a href=\"http://en.wikipedia.org/wiki/Knuth's_up-arrow_notation\">3^^^3</a> people getting shampoo (the stingy kind) in their eyes. I suppose, however, that might not change your answer to torture vs. specks if you think there's a qualitative difference between the speck (as originally specified by Eliezer) and getting shampoo in your eye.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb187": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kXmLGPG8QfXpm2DfY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 18, "extendedScore": null, "score": 5.2e-05, "legacy": true, "legacyId": "24249", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3wYTFWY3LKQCnAptN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-23T17:34:58.111Z", "modifiedAt": null, "url": null, "title": "A map of Bay Area memespace", "slug": "a-map-of-bay-area-memespace", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:32.083Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Julia_Galef", "user": {"username": "Julia_Galef", "createdAt": "2009-12-20T01:44:38.850Z", "isAdmin": false, "displayName": "Julia_Galef"}, "userId": "qkDSxJnyKhPCJyKdD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WzPJRNYWhMXQTEj69/a-map-of-bay-area-memespace", "pageUrlRelative": "/posts/WzPJRNYWhMXQTEj69/a-map-of-bay-area-memespace", "linkUrl": "https://www.lesswrong.com/posts/WzPJRNYWhMXQTEj69/a-map-of-bay-area-memespace", "postedAtFormatted": "Monday, September 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20map%20of%20Bay%20Area%20memespace&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20map%20of%20Bay%20Area%20memespace%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWzPJRNYWhMXQTEj69%2Fa-map-of-bay-area-memespace%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20map%20of%20Bay%20Area%20memespace%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWzPJRNYWhMXQTEj69%2Fa-map-of-bay-area-memespace", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWzPJRNYWhMXQTEj69%2Fa-map-of-bay-area-memespace", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2074, "htmlBody": "<p>The main reason we picked the Bay Area as a home for the&nbsp;<a href=\"http://rationality.org\">Center for Applied Rationality</a> was simply because that's where our initial fiscal sponsor, <a href=\"http://intelligence.org/\">MIRI</a>, was located. Yet as I&rsquo;ve gotten to know this region better in the year and a half since then, I&rsquo;ve been struck by how good the fit has turned out to be. The Bay Area is unusually dense with idea-driven subcultures that mix and cross-pollinate in fascinating ways, many of which are already enriching rationalist culture.</p>\n<p>This map is my attempt at illustrating that landscape of subcultures, and at situating the rationalist community within it. I&rsquo;ve limited myself to the last 50 years or so, and to subcultures defined by ideology (as opposed to, say, ethnicity). I&rsquo;ve also depicted some of the major memes that have influenced, and been influenced by, those subcultures:</p>\n<p><em>(Click to enlarge)</em></p>\n<p><a href=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1538439586/otherposts/Bay-Area-memespace1.jpg\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1538439591/otherposts/Bay-Area-memespace-small.jpg\" alt=\"\" width=\"700\" height=\"429\" /></a></p>\n<p>Note that although many of these memes are widely influential, I only drew an arrow connecting a meme to a group if the meme was one of the <em>defining features</em> of the group. (For example, yoga may be popular among many entrepreneurs, but that meme -&gt; subculture relationship isn&rsquo;t strong enough to make my map.).</p>\n<p>Below, I expand on the map with a quick tour through the landscape of Bay Area memes and subcultures. Instead of trying to cover everything in detail, I&rsquo;ve focused on nine aspects of that memespace that help put the rationalist community in context:</p>\n<p><a id=\"more\"></a><strong style=\"font-size: 15px;\">1. Computer scientists</strong></p>\n<p>Some of the basic building blocks of rationality come from computer science, and the Bay Area is rich with the world&rsquo;s top computer scientists, employed by companies like Intel, IBM, Google, and Microsoft, and universities like Stanford and UC Berkeley. The idea of thinking in terms of optimization problems &ndash; optimizing for these outcomes, under those constraints -- has roots in computer science and math, and it&rsquo;s so fundamental to the rationalist approach to problem-solving that it&rsquo;s easy to forget how different it is from people&rsquo;s normal way of thinking.</p>\n<p>Another rationalist building block, Bayesian inference, is several centuries old, but had fallen out of favor until the computing methods and power of the 1970s <a href=\"/lw/774/a_history_of_bayes_theorem/\">made it actually usable</a>. Widespread use of Bayesianism in the field of artificial intelligence (e.g. <a href=\"http://en.wikipedia.org/wiki/Bayesian_network\">Bayes nets</a>) also contributed to its resurgent popularity.</p>\n<h3><strong>2. Startup culture </strong></h3>\n<p>There&rsquo;s a distinctive culture behind the successes of the Bay Area&rsquo;s startups, and it&rsquo;s one that I see benefiting rationalists as well. Business in general is good real-world rationality training: you test your theories, you update your models, or you fail. And startup culture in particular promotes a &ldquo;try things fast&rdquo; attitude that can be a perfect antidote to the &ldquo;sit around planning and theorizing forever&rdquo; failure mode we're sometimes prone to.</p>\n<p>It&rsquo;s certainly been <a href=\"/lw/h5t/new_applied_rationality_workshops_april_may_and/\">invaluable to CFAR&rsquo;s success</a> thus far, and it&rsquo;s one of the bigger differences I&rsquo;ve noticed in my own skills as a rationalist since moving out here. Startup culture&rsquo;s &ldquo;think big, be ambitious&rdquo; meme is also something I could see impacting rationalist culture in the coming years. (For a look at this meme turned up to 11, you can check out the only-partially-tongue-in-cheek <a href=\"/lw/ehd/the_yudkowsky_ambition_scale/\">Yudkowsky ambition scale</a>.)</p>\n<h3>3. Hacker culture</h3>\n<p>A lot of the credit for the culture of Silicon Valley&rsquo;s startup scene goes to the first generation of computer programmers, whose &ldquo;<a href=\"http://en.wikipedia.org/wiki/Hacker_(programmer_subculture)\">hacker&rdquo; culture</a> originated at MIT in the late 1950s, but shortly thereafter sprung up in a few other early-adopter schools like Stanford and UC Berkeley. In addition to being passionate about coding, hackers were unimpressed by &ldquo;bogus&rdquo; status signals, like age and higher education, and judged people only by the cleverness and usefulness of the things they could create. (One of the most admired among the original hackers was twelve-year-old Peter Deutsch.) It was, in other words, the perfect cultural soil for the seeds of a paradigm-busting startup culture.</p>\n<p>The hacker ethic also included an itch to fix broken or inefficient systems, and an impatience with the bureaucracy that prevented them from doing so. &ldquo;In a perfect hacker world, anyone pissed off enough to open up a control box near a traffic light and take it apart to make it work better should be perfectly welcome to make the attempt,&rdquo; Steven Levy wrote in <em>Hackers, Heroes of the Computer Revolution.&nbsp;</em>That's one reason I credit hacker culture for another Bay Area meme that I see in many entrepreneurs, rationalists, and others: building creative alternatives to establishment institutions like government, education, and health.</p>\n<p>For examples, look at all the approaches to alternative education that have sprung up in the Bay &ndash; <a href=\"http://www.uncollege.org/\">UnCollege</a>, <a href=\"https://www.coursera.org/\">Coursera</a>, <a href=\"http://www.udacity.com\">Udacity</a>, and <a href=\"https://generalassemb.ly/\">General Assembly</a>. Or look at <a href=\"http://quantifiedself.com/\">Quantified Self</a>, the community of people figuring out how to improve their health by tracking and analyzing their own biometrics. Or the <a href=\"http://www.seasteading.org/\">Seasteaders</a>, who believe the free market can produce better societies than the ones historical forces left us with. Or <a href=\"https://www.metamed.com/\">MetaMed</a>, the company staked on the idea that we can improve significantly on mainstream medicine if we apply rationalist research tools to the medical literature.</p>\n<h3>4. Eastern spiritualities</h3>\n<p>Although the heyday of the counterculture was over by the 1980s, its memes still influence the Bay Area, and through it, rationalist culture. Yoga and meditation were introduced to the US via the hippies&rsquo; exploration of Eastern religions, but those practices have been mostly stripped of their original spiritual meanings by now, and are popular for their benefits to mental and physical well-being.</p>\n<p>Meditation in particular has become common among rationalists, and has some interesting overlaps with rationality I hadn&rsquo;t noticed before I moved out here. Meditation seems to train you to stop automatically identifying with all of your thoughts, so that, for example, when the thought &ldquo;John&rsquo;s a jerk&rdquo; pops into your head, you don&rsquo;t assume that John necessarily is a jerk. You take the thought as something your brain produced, which may or may not be true, and may or may not be useful -- and this ability to take a step back from your thoughts and reflect on them is arguably one of the building blocks of rationality.</p>\n<h3>5. Human Potential movement</h3>\n<p>Another pillar of counterculture was the <a href=\"http://en.wikipedia.org/wiki/Human_Potential_Movement\">Human Potential movement</a>, named after Aldous Huxley&rsquo;s argument that the human brain is capable of much more insight, fulfillment, and varied experiences than we&rsquo;ve been aware of thus far. In 1962 the <a href=\"http://en.wikipedia.org/wiki/Esalen_Institute\">Esalen Institute</a>, a retreat built on hot springs south of San Francisco, started running classes designed to help people realize more of their &ldquo;potential,&rdquo; through activities like roleplaying, primal screams, and group therapy. The <a href=\"http://en.wikipedia.org/wiki/Landmark_Education\">Landmark Forum</a>, originally known as est, was another leader of the movement, and emphasized taking responsibility and questioning the narratives you construct around events in your life. And I&rsquo;d count other popular practices like <a href=\"http://en.wikipedia.org/wiki/Nonviolent_Communication\">nonviolent communication</a>, <a href=\"http://en.wikipedia.org/wiki/Radical_Honesty\">radical honesty</a>, and <a href=\"http://en.wikipedia.org/wiki/Internal_Family_Systems_Model\">internal family systems</a> in this same tradition.</p>\n<p>Rationality also focuses on personal development, of course, but there's not much connection between the Human Potential movement&rsquo;s approach and the rationalists&rsquo;. As far as I can tell they developed independently of each other and have very different epistemologies. From my perspective, Human Potential practices span a spectrum from common sense techniques backed up by anecdotal evidence, to unsupported psychotherapy, to outright mysticism that makes claims that are clearly wrong or <a href=\"http://en.wikipedia.org/wiki/Not_even_wrong\">not even wrong</a>.</p>\n<p>Nevertheless, the basic goals of seeking fulfillment and becoming a better version of yourself are fine ones, and the fact that so many people today are interested in pursuing those goals is thanks in large part to the impact the Human Potential movement had on American society. And despite my qualms about their epistemology, I&rsquo;d be willing to bet that there are at least a few practices the movement and its outgrowths discovered that really are useful, even if their practitioners don&rsquo;t have correct models of <em>why</em> they&rsquo;re useful. So I consider the Human Potential and related movements to be a source of hypotheses, if not conclusions.</p>\n<h3>6. Alternative lifestyles</h3>\n<p>Finally, the counterculture was also famous for its destigmatization and exploration of alternative lifestyles, which helped create San Francisco&rsquo;s vibrant kink culture and LGBTQ community. Beyond their live-and-let-live attitude about alternative sexualities, people in the Bay generally put less stock in standard scripts for how a life &ldquo;should&rdquo; go. So being nomadic, or living on a boat, or setting up a co-parenting group, or not wanting children, or having an unusual job, or changing your gender, doesn&rsquo;t raise eyebrows here the way it would in most parts of the country.</p>\n<p>And having an expanded space of hypotheses about how to live is complementary with trying to improve your rationality, because a lot of rationality involves pushing past <a href=\"/lw/4e/cached_selves/\">cached thoughts</a> about what you believe, or what kind of person you are, and giving fair consideration to hypotheses that hadn&rsquo;t even been in your choice set before. In other words: I don&rsquo;t think it&rsquo;s necessarily the case that most rationalists live alternative lifestyles. But I do think the conventional lifestyles led by rationalists are <em>consciously chosen</em> to a greater extent than are most people&rsquo;s lifestyles.</p>\n<h3>7. Burning Man</h3>\n<p>One aspect of memespace I wasn&rsquo;t able to depict on the map is how the cross-pollination between groups actually occurs. To some extent it&rsquo;s caused by normal social interactions and the blogosphere, as you&rsquo;d expect, but the <a href=\"http://en.wikipedia.org/wiki/Burning_Man\">Burning Man</a> festival is another important driver of cross-pollination that might be less obvious to outsiders. In fact, I wanted to put &ldquo;Burner culture&rdquo; on my memespace map, but was flummoxed by the fact that it would have to be connected to essentially all other groups and memes.</p>\n<p>Burning Man consists of 50,000 people, including people from all of the subcultures, coming together for one week in the Nevada desert to create a temporary city. And though the old-school hippies might distrust Silicon Valley&rsquo;s wealthy elites, and the rationalists might look askance at the New Age aura-readers, the communal spirit of Burning Man does a pretty effective job of breaking down those barriers. It&rsquo;s only a yearly event, but the sense of community lingers afterwards, and Burning Man social connections are reinforced throughout the year at events like <a href=\"http://ephemerisle.org/wiki/Ephemerisle\">Ephemerisle</a> (Burning man + libertarianism) and the <a href=\"http://bilconference.com/\">BIL conference</a> (Burning man + social entrepreneurship).</p>\n<h3>8. Social pressure to give back</h3>\n<p>Bay Area society puts a high value on helping the world. Perhaps it&rsquo;s an echo of the social justice movements in the 1960&rsquo;s, or perhaps it comes from the hackers&rsquo; conviction that technology should be used for the public good. Whatever the origins of this social more, you can see it in the idealistic language entrepreneurs use to describe their startups, and in the recent growth of a segment of startup culture known as &ldquo;social entrepreneurship&rdquo; that focuses on the kinds of global problems traditionally addressed by charities.</p>\n<p>And sure, it&rsquo;s often the case that the &ldquo;world-changing&rdquo; rhetoric entrepreneurs use to describe their startups is just rhetoric. But the fact that they feel obliged to frame their business in altruistic terms is at least a symptom of the fact that the Bay Area expects you to try to make a positive contribution to the world. Which means that self-made millionaires in this region are more likely than elites elsewhere to put their wealth towards good causes.</p>\n<p>That, in turn, forges social connections between the Bay Area&rsquo;s entrepreneurs, investors and engineers, and the effective altruists, rationalists, transhumanists, and other groups they support. So this phenomenon is doubly fortunate for the rationalists: not just because Bay Area philanthropy helps us directly, but because those connections expose us to memes from different subcultures that can help round out our worldview.</p>\n<h3>9. Effective Altruists</h3>\n<p>The social pressure to give back is also fortunate because it makes the Bay a hospitable environment for the Effective Altruists, one of the newest communities to hit the Bay, and a close cousin to the rationalists. Arguably, the movement is still centered at Oxford, where the <a href=\"http://home.centreforeffectivealtruism.org/\">Center for Effective Altruism</a> is based. But EA organizations <a href=\"http://givewell.org\">Givewell</a> and <a href=\"http://www.leverageresearch.org/\">Leverage Research</a> recently relocated to the Bay Area from New York, and Leverage hosted the first-ever global <a href=\"http://www.effectivealtruismsummit.com/\">Effective Altruism Summit</a> here this summer, which I think is enough to qualify this as a fledgling Bay Area subculture. I would also consider MIRI to be an older member of this group, specifically in the <a href=\"/lw/hx4/four_focus_areas_of_effective_altruism/\">far future-focused</a>&nbsp;subset of EA culture.</p>\n<p>And CFAR itself is premised on the EA-style calculation that training decision makers in rationality is one of the highest-impact ways to improve the future. Which is why I&rsquo;m particularly excited about the recent addition of the Effective Altruist community to the Bay Area memescape, and at the new opportunities for cross-pollination with rationalist culture that their presence will create.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "bY5MaF2EATwDkomvu": 1, "Zz3HWyByyKF64Sfns": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WzPJRNYWhMXQTEj69", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 58, "baseScore": 70, "extendedScore": null, "score": 0.000172, "legacy": true, "legacyId": "24250", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 70, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The main reason we picked the Bay Area as a home for the&nbsp;<a href=\"http://rationality.org\">Center for Applied Rationality</a> was simply because that's where our initial fiscal sponsor, <a href=\"http://intelligence.org/\">MIRI</a>, was located. Yet as I\u2019ve gotten to know this region better in the year and a half since then, I\u2019ve been struck by how good the fit has turned out to be. The Bay Area is unusually dense with idea-driven subcultures that mix and cross-pollinate in fascinating ways, many of which are already enriching rationalist culture.</p>\n<p>This map is my attempt at illustrating that landscape of subcultures, and at situating the rationalist community within it. I\u2019ve limited myself to the last 50 years or so, and to subcultures defined by ideology (as opposed to, say, ethnicity). I\u2019ve also depicted some of the major memes that have influenced, and been influenced by, those subcultures:</p>\n<p><em>(Click to enlarge)</em></p>\n<p><a href=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1538439586/otherposts/Bay-Area-memespace1.jpg\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1538439591/otherposts/Bay-Area-memespace-small.jpg\" alt=\"\" width=\"700\" height=\"429\"></a></p>\n<p>Note that although many of these memes are widely influential, I only drew an arrow connecting a meme to a group if the meme was one of the <em>defining features</em> of the group. (For example, yoga may be popular among many entrepreneurs, but that meme -&gt; subculture relationship isn\u2019t strong enough to make my map.).</p>\n<p>Below, I expand on the map with a quick tour through the landscape of Bay Area memes and subcultures. Instead of trying to cover everything in detail, I\u2019ve focused on nine aspects of that memespace that help put the rationalist community in context:</p>\n<p><a id=\"more\"></a><strong style=\"font-size: 15px;\">1. Computer scientists</strong></p>\n<p>Some of the basic building blocks of rationality come from computer science, and the Bay Area is rich with the world\u2019s top computer scientists, employed by companies like Intel, IBM, Google, and Microsoft, and universities like Stanford and UC Berkeley. The idea of thinking in terms of optimization problems \u2013 optimizing for these outcomes, under those constraints -- has roots in computer science and math, and it\u2019s so fundamental to the rationalist approach to problem-solving that it\u2019s easy to forget how different it is from people\u2019s normal way of thinking.</p>\n<p>Another rationalist building block, Bayesian inference, is several centuries old, but had fallen out of favor until the computing methods and power of the 1970s <a href=\"/lw/774/a_history_of_bayes_theorem/\">made it actually usable</a>. Widespread use of Bayesianism in the field of artificial intelligence (e.g. <a href=\"http://en.wikipedia.org/wiki/Bayesian_network\">Bayes nets</a>) also contributed to its resurgent popularity.</p>\n<h3 id=\"2__Startup_culture_\"><strong>2. Startup culture </strong></h3>\n<p>There\u2019s a distinctive culture behind the successes of the Bay Area\u2019s startups, and it\u2019s one that I see benefiting rationalists as well. Business in general is good real-world rationality training: you test your theories, you update your models, or you fail. And startup culture in particular promotes a \u201ctry things fast\u201d attitude that can be a perfect antidote to the \u201csit around planning and theorizing forever\u201d failure mode we're sometimes prone to.</p>\n<p>It\u2019s certainly been <a href=\"/lw/h5t/new_applied_rationality_workshops_april_may_and/\">invaluable to CFAR\u2019s success</a> thus far, and it\u2019s one of the bigger differences I\u2019ve noticed in my own skills as a rationalist since moving out here. Startup culture\u2019s \u201cthink big, be ambitious\u201d meme is also something I could see impacting rationalist culture in the coming years. (For a look at this meme turned up to 11, you can check out the only-partially-tongue-in-cheek <a href=\"/lw/ehd/the_yudkowsky_ambition_scale/\">Yudkowsky ambition scale</a>.)</p>\n<h3 id=\"3__Hacker_culture\">3. Hacker culture</h3>\n<p>A lot of the credit for the culture of Silicon Valley\u2019s startup scene goes to the first generation of computer programmers, whose \u201c<a href=\"http://en.wikipedia.org/wiki/Hacker_(programmer_subculture)\">hacker\u201d culture</a> originated at MIT in the late 1950s, but shortly thereafter sprung up in a few other early-adopter schools like Stanford and UC Berkeley. In addition to being passionate about coding, hackers were unimpressed by \u201cbogus\u201d status signals, like age and higher education, and judged people only by the cleverness and usefulness of the things they could create. (One of the most admired among the original hackers was twelve-year-old Peter Deutsch.) It was, in other words, the perfect cultural soil for the seeds of a paradigm-busting startup culture.</p>\n<p>The hacker ethic also included an itch to fix broken or inefficient systems, and an impatience with the bureaucracy that prevented them from doing so. \u201cIn a perfect hacker world, anyone pissed off enough to open up a control box near a traffic light and take it apart to make it work better should be perfectly welcome to make the attempt,\u201d Steven Levy wrote in <em>Hackers, Heroes of the Computer Revolution.&nbsp;</em>That's one reason I credit hacker culture for another Bay Area meme that I see in many entrepreneurs, rationalists, and others: building creative alternatives to establishment institutions like government, education, and health.</p>\n<p>For examples, look at all the approaches to alternative education that have sprung up in the Bay \u2013 <a href=\"http://www.uncollege.org/\">UnCollege</a>, <a href=\"https://www.coursera.org/\">Coursera</a>, <a href=\"http://www.udacity.com\">Udacity</a>, and <a href=\"https://generalassemb.ly/\">General Assembly</a>. Or look at <a href=\"http://quantifiedself.com/\">Quantified Self</a>, the community of people figuring out how to improve their health by tracking and analyzing their own biometrics. Or the <a href=\"http://www.seasteading.org/\">Seasteaders</a>, who believe the free market can produce better societies than the ones historical forces left us with. Or <a href=\"https://www.metamed.com/\">MetaMed</a>, the company staked on the idea that we can improve significantly on mainstream medicine if we apply rationalist research tools to the medical literature.</p>\n<h3 id=\"4__Eastern_spiritualities\">4. Eastern spiritualities</h3>\n<p>Although the heyday of the counterculture was over by the 1980s, its memes still influence the Bay Area, and through it, rationalist culture. Yoga and meditation were introduced to the US via the hippies\u2019 exploration of Eastern religions, but those practices have been mostly stripped of their original spiritual meanings by now, and are popular for their benefits to mental and physical well-being.</p>\n<p>Meditation in particular has become common among rationalists, and has some interesting overlaps with rationality I hadn\u2019t noticed before I moved out here. Meditation seems to train you to stop automatically identifying with all of your thoughts, so that, for example, when the thought \u201cJohn\u2019s a jerk\u201d pops into your head, you don\u2019t assume that John necessarily is a jerk. You take the thought as something your brain produced, which may or may not be true, and may or may not be useful -- and this ability to take a step back from your thoughts and reflect on them is arguably one of the building blocks of rationality.</p>\n<h3 id=\"5__Human_Potential_movement\">5. Human Potential movement</h3>\n<p>Another pillar of counterculture was the <a href=\"http://en.wikipedia.org/wiki/Human_Potential_Movement\">Human Potential movement</a>, named after Aldous Huxley\u2019s argument that the human brain is capable of much more insight, fulfillment, and varied experiences than we\u2019ve been aware of thus far. In 1962 the <a href=\"http://en.wikipedia.org/wiki/Esalen_Institute\">Esalen Institute</a>, a retreat built on hot springs south of San Francisco, started running classes designed to help people realize more of their \u201cpotential,\u201d through activities like roleplaying, primal screams, and group therapy. The <a href=\"http://en.wikipedia.org/wiki/Landmark_Education\">Landmark Forum</a>, originally known as est, was another leader of the movement, and emphasized taking responsibility and questioning the narratives you construct around events in your life. And I\u2019d count other popular practices like <a href=\"http://en.wikipedia.org/wiki/Nonviolent_Communication\">nonviolent communication</a>, <a href=\"http://en.wikipedia.org/wiki/Radical_Honesty\">radical honesty</a>, and <a href=\"http://en.wikipedia.org/wiki/Internal_Family_Systems_Model\">internal family systems</a> in this same tradition.</p>\n<p>Rationality also focuses on personal development, of course, but there's not much connection between the Human Potential movement\u2019s approach and the rationalists\u2019. As far as I can tell they developed independently of each other and have very different epistemologies. From my perspective, Human Potential practices span a spectrum from common sense techniques backed up by anecdotal evidence, to unsupported psychotherapy, to outright mysticism that makes claims that are clearly wrong or <a href=\"http://en.wikipedia.org/wiki/Not_even_wrong\">not even wrong</a>.</p>\n<p>Nevertheless, the basic goals of seeking fulfillment and becoming a better version of yourself are fine ones, and the fact that so many people today are interested in pursuing those goals is thanks in large part to the impact the Human Potential movement had on American society. And despite my qualms about their epistemology, I\u2019d be willing to bet that there are at least a few practices the movement and its outgrowths discovered that really are useful, even if their practitioners don\u2019t have correct models of <em>why</em> they\u2019re useful. So I consider the Human Potential and related movements to be a source of hypotheses, if not conclusions.</p>\n<h3 id=\"6__Alternative_lifestyles\">6. Alternative lifestyles</h3>\n<p>Finally, the counterculture was also famous for its destigmatization and exploration of alternative lifestyles, which helped create San Francisco\u2019s vibrant kink culture and LGBTQ community. Beyond their live-and-let-live attitude about alternative sexualities, people in the Bay generally put less stock in standard scripts for how a life \u201cshould\u201d go. So being nomadic, or living on a boat, or setting up a co-parenting group, or not wanting children, or having an unusual job, or changing your gender, doesn\u2019t raise eyebrows here the way it would in most parts of the country.</p>\n<p>And having an expanded space of hypotheses about how to live is complementary with trying to improve your rationality, because a lot of rationality involves pushing past <a href=\"/lw/4e/cached_selves/\">cached thoughts</a> about what you believe, or what kind of person you are, and giving fair consideration to hypotheses that hadn\u2019t even been in your choice set before. In other words: I don\u2019t think it\u2019s necessarily the case that most rationalists live alternative lifestyles. But I do think the conventional lifestyles led by rationalists are <em>consciously chosen</em> to a greater extent than are most people\u2019s lifestyles.</p>\n<h3 id=\"7__Burning_Man\">7. Burning Man</h3>\n<p>One aspect of memespace I wasn\u2019t able to depict on the map is how the cross-pollination between groups actually occurs. To some extent it\u2019s caused by normal social interactions and the blogosphere, as you\u2019d expect, but the <a href=\"http://en.wikipedia.org/wiki/Burning_Man\">Burning Man</a> festival is another important driver of cross-pollination that might be less obvious to outsiders. In fact, I wanted to put \u201cBurner culture\u201d on my memespace map, but was flummoxed by the fact that it would have to be connected to essentially all other groups and memes.</p>\n<p>Burning Man consists of 50,000 people, including people from all of the subcultures, coming together for one week in the Nevada desert to create a temporary city. And though the old-school hippies might distrust Silicon Valley\u2019s wealthy elites, and the rationalists might look askance at the New Age aura-readers, the communal spirit of Burning Man does a pretty effective job of breaking down those barriers. It\u2019s only a yearly event, but the sense of community lingers afterwards, and Burning Man social connections are reinforced throughout the year at events like <a href=\"http://ephemerisle.org/wiki/Ephemerisle\">Ephemerisle</a> (Burning man + libertarianism) and the <a href=\"http://bilconference.com/\">BIL conference</a> (Burning man + social entrepreneurship).</p>\n<h3 id=\"8__Social_pressure_to_give_back\">8. Social pressure to give back</h3>\n<p>Bay Area society puts a high value on helping the world. Perhaps it\u2019s an echo of the social justice movements in the 1960\u2019s, or perhaps it comes from the hackers\u2019 conviction that technology should be used for the public good. Whatever the origins of this social more, you can see it in the idealistic language entrepreneurs use to describe their startups, and in the recent growth of a segment of startup culture known as \u201csocial entrepreneurship\u201d that focuses on the kinds of global problems traditionally addressed by charities.</p>\n<p>And sure, it\u2019s often the case that the \u201cworld-changing\u201d rhetoric entrepreneurs use to describe their startups is just rhetoric. But the fact that they feel obliged to frame their business in altruistic terms is at least a symptom of the fact that the Bay Area expects you to try to make a positive contribution to the world. Which means that self-made millionaires in this region are more likely than elites elsewhere to put their wealth towards good causes.</p>\n<p>That, in turn, forges social connections between the Bay Area\u2019s entrepreneurs, investors and engineers, and the effective altruists, rationalists, transhumanists, and other groups they support. So this phenomenon is doubly fortunate for the rationalists: not just because Bay Area philanthropy helps us directly, but because those connections expose us to memes from different subcultures that can help round out our worldview.</p>\n<h3 id=\"9__Effective_Altruists\">9. Effective Altruists</h3>\n<p>The social pressure to give back is also fortunate because it makes the Bay a hospitable environment for the Effective Altruists, one of the newest communities to hit the Bay, and a close cousin to the rationalists. Arguably, the movement is still centered at Oxford, where the <a href=\"http://home.centreforeffectivealtruism.org/\">Center for Effective Altruism</a> is based. But EA organizations <a href=\"http://givewell.org\">Givewell</a> and <a href=\"http://www.leverageresearch.org/\">Leverage Research</a> recently relocated to the Bay Area from New York, and Leverage hosted the first-ever global <a href=\"http://www.effectivealtruismsummit.com/\">Effective Altruism Summit</a> here this summer, which I think is enough to qualify this as a fledgling Bay Area subculture. I would also consider MIRI to be an older member of this group, specifically in the <a href=\"/lw/hx4/four_focus_areas_of_effective_altruism/\">far future-focused</a>&nbsp;subset of EA culture.</p>\n<p>And CFAR itself is premised on the EA-style calculation that training decision makers in rationality is one of the highest-impact ways to improve the future. Which is why I\u2019m particularly excited about the recent addition of the Effective Altruist community to the Bay Area memescape, and at the new opportunities for cross-pollination with rationalist culture that their presence will create.</p>", "sections": [{"title": "2. Startup culture ", "anchor": "2__Startup_culture_", "level": 1}, {"title": "3. Hacker culture", "anchor": "3__Hacker_culture", "level": 1}, {"title": "4. Eastern spiritualities", "anchor": "4__Eastern_spiritualities", "level": 1}, {"title": "5. Human Potential movement", "anchor": "5__Human_Potential_movement", "level": 1}, {"title": "6. Alternative lifestyles", "anchor": "6__Alternative_lifestyles", "level": 1}, {"title": "7. Burning Man", "anchor": "7__Burning_Man", "level": 1}, {"title": "8. Social pressure to give back", "anchor": "8__Social_pressure_to_give_back", "level": 1}, {"title": "9. Effective Altruists", "anchor": "9__Effective_Altruists", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "29 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RTt59BtFLqQbsSiqd", "jGuT3LTrGP7GtX3ZQ", "mxDAPAuy2b3tkeJRc", "BHYBdijDcAKQ6e45Z", "JmmA2Mf5GrY9D6nQD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-23T18:00:02.931Z", "modifiedAt": null, "url": null, "title": "AAAI 2014 Spring Symposia", "slug": "aaai-2014-spring-symposia", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wn6giLQrESpbs9Zmc/aaai-2014-spring-symposia", "pageUrlRelative": "/posts/wn6giLQrESpbs9Zmc/aaai-2014-spring-symposia", "linkUrl": "https://www.lesswrong.com/posts/wn6giLQrESpbs9Zmc/aaai-2014-spring-symposia", "postedAtFormatted": "Monday, September 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AAAI%202014%20Spring%20Symposia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAAAI%202014%20Spring%20Symposia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwn6giLQrESpbs9Zmc%2Faaai-2014-spring-symposia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AAAI%202014%20Spring%20Symposia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwn6giLQrESpbs9Zmc%2Faaai-2014-spring-symposia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwn6giLQrESpbs9Zmc%2Faaai-2014-spring-symposia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 749, "htmlBody": "<p style=\"font-size: 13px; color: #222222; font-family: arial, sans-serif;\">Probably of interest to researchers on this list (October 4 deadline):</p>\n<h3 style=\"color: #222222; font-family: arial, sans-serif;\"><a href=\"http://www.aaai.org/Symposia/Spring/sss14.php\">AAAI Spring 2014 Symposium - March 24-26, 2014 / Stanford University, Palo Alto, California, USA</a></h3>\n<h2 style=\"color: #222222; font-family: arial, sans-serif;\">Implementing Selves with Safe Motivational Systems &amp; Self-Improvement</h2>\n<p style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Artificial Intelligence (AI) and Artificial General Intelligence (AGI) most often focus on tools for collecting knowledge and solving problems or achieving goals rather than self-reflecting entities. Instead, this implementation-oriented symposium will focus on guided self-creation and improvement &ndash; particularly as a method of achieving human-level intelligence in machines through iterative improvement (&ldquo;seed AI&rdquo;).</p>\n<p style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">In I Am a Strange Loop, Douglas Hofstadter argues that the key to understanding selves is the &ldquo;strange loop&rdquo;, a complex feedback network inhabiting our brains and, arguably, constituting our minds. Further, humans have both conscious and unconscious minds (Daniel Kahneman&rsquo;s system one and system two), attention, emotions, partial self-reflection, a moral sense and many other aspects that are rarely addressed &ndash; yet seem critical for the creation of a safe self-sufficient autonomous system.</p>\n<p style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">This symposium will focus on the integration of these components into a coherent self-improving self. Ideally, the ultimate end product will be a successful entity with extensive self-knowledge and a safe or moral/ethical motivational system that functions with context/ecologically sensitive egoistic/altruistic discrimination to promote cooperation with and contribution to community via iterative improvement of self, tools and theoretical constructs of relational dynamics and resource utilization, allocation and sharing.</p>\n<p style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">This symposium is proposed as an implementation-oriented exploration of self, to include:</p>\n<ul style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\" type=\"disc\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">integrative architectures with explicit motivations \n<ul type=\"circle\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">implementing &ldquo;self&rdquo; as operating system with &ldquo;plug-ins&rdquo;</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">implementing &ldquo;self&rdquo; as society (of mind - Marvin Minsky)</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">implementing &ldquo;self&rdquo; as economy (of idiots &ndash; Eric Baum)</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">implementing &ldquo;self&rdquo; as global workspace/consciousness (Baars/Franklin)</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">implementing &ldquo;self&rdquo; as authorship (Dennett, Wegner)</li>\n</ul>\n</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">&ldquo;safe&rdquo; and/or &ldquo;moral&rdquo; and ethical motivational systems \n<ul type=\"circle\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">value sets vs. goal hierarchies</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">context/ecological sensitivity</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">&ldquo;safe&rdquo;/moral values/goal content</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">evaluation schemes</li>\n</ul>\n</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">reflection \n<ul type=\"circle\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">self-examination</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">self-modeling &amp; self-knowledge</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">goal-based self-evaluation for self-improvement</li>\n</ul>\n</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">attention &amp; emotions \n<ul type=\"circle\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">as interoceptive responses to environmental stimuli</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">as knowledge/rules of thumb/&rdquo;actionable qualia&rdquo;</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">as helpful &amp; unhelpful biases (and how to intelligently improve)</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">as evaluation &amp; enforcement mechanisms</li>\n</ul>\n</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">integrating different knowledge and action representation schemes \n<ul type=\"circle\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">coordination &amp; translation between various schemes</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">analyzing trade-offs &amp; knowing when to switch between schemes</li>\n</ul>\n</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">self-improvement \n<ul type=\"circle\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">via automated tool/method incorporation &amp; theory-inductive heuristics \n<ul type=\"square\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">goal-based tool/method discovery</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">tool/method integration and evaluation</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">tool-to-theory heuristics</li>\n</ul>\n</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">via learning (knowledge incorporation) \n<ul type=\"square\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">(re-)building schemes and models</li>\n</ul>\n</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">discovery (refactoring, modularization, encapsulation and scale-invariance) \n<ul type=\"square\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">theory-to-tool heuristics</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">automated (re-)construction of probabilistic graphical models</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">Meta-Optimizing Semantic Evolutionary Search (MOSES)</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">Frequent &amp; Interesting Sub-HyperGRAph Mining (FISHGRAM)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">While solutions need to be grounded and extensible, the symposium would prefer approaches starting with some initial structure rather than a tabula rasa with the lowest level bootstrapping approaches or first causes explanations (except where they are fully extended to initial structures and/or used to justify such structures). Also, while autopoiesis and &ldquo;functional consciousness&rdquo; are obviously key topics, we would prefer that phenomenal consciousness arguments be considered off-topic.</p>\n<h3 style=\"color: #222222; font-family: arial, sans-serif;\">Submissions</h3>\n<p style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Those interested in participating should submit either full-length papers (up to 6 pages in AAAI format) or short papers/extended abstracts (2-3 pages) to&nbsp;<a style=\"color: #1155cc;\" href=\"https://www.easychair.org/conferences/?conf=ss14self\" target=\"_blank\">https://www.easychair.org/conferences/?conf=ss14self</a>. Submissions are due October 4.</p>\n<h3 style=\"color: #222222; font-family: arial, sans-serif;\">Additional Information</h3>\n<p style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Additional conference information is available at the AAAI&nbsp;Spring 2014 Symposium&nbsp;<a style=\"color: #1155cc;\" href=\"http://www.aaai.org/Symposia/Spring/sss14.php\" target=\"_blank\">page</a>.&nbsp; More details will be posted here as they become available.&nbsp; Questions can be emailed to Mark Waser (<a>MWaser@DigitalWisdomInstitute.org</a>).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wn6giLQrESpbs9Zmc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 1.3524582303248999e-06, "legacy": true, "legacyId": "24254", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"font-size: 13px; color: #222222; font-family: arial, sans-serif;\">Probably of interest to researchers on this list (October 4 deadline):</p>\n<h3 style=\"color: #222222; font-family: arial, sans-serif;\" id=\"AAAI_Spring_2014_Symposium___March_24_26__2014___Stanford_University__Palo_Alto__California__USA\"><a href=\"http://www.aaai.org/Symposia/Spring/sss14.php\">AAAI Spring 2014 Symposium - March 24-26, 2014 / Stanford University, Palo Alto, California, USA</a></h3>\n<h2 style=\"color: #222222; font-family: arial, sans-serif;\" id=\"Implementing_Selves_with_Safe_Motivational_Systems___Self_Improvement\">Implementing Selves with Safe Motivational Systems &amp; Self-Improvement</h2>\n<p style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Artificial Intelligence (AI) and Artificial General Intelligence (AGI) most often focus on tools for collecting knowledge and solving problems or achieving goals rather than self-reflecting entities. Instead, this implementation-oriented symposium will focus on guided self-creation and improvement \u2013 particularly as a method of achieving human-level intelligence in machines through iterative improvement (\u201cseed AI\u201d).</p>\n<p style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">In I Am a Strange Loop, Douglas Hofstadter argues that the key to understanding selves is the \u201cstrange loop\u201d, a complex feedback network inhabiting our brains and, arguably, constituting our minds. Further, humans have both conscious and unconscious minds (Daniel Kahneman\u2019s system one and system two), attention, emotions, partial self-reflection, a moral sense and many other aspects that are rarely addressed \u2013 yet seem critical for the creation of a safe self-sufficient autonomous system.</p>\n<p style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">This symposium will focus on the integration of these components into a coherent self-improving self. Ideally, the ultimate end product will be a successful entity with extensive self-knowledge and a safe or moral/ethical motivational system that functions with context/ecologically sensitive egoistic/altruistic discrimination to promote cooperation with and contribution to community via iterative improvement of self, tools and theoretical constructs of relational dynamics and resource utilization, allocation and sharing.</p>\n<p style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">This symposium is proposed as an implementation-oriented exploration of self, to include:</p>\n<ul style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\" type=\"disc\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">integrative architectures with explicit motivations \n<ul type=\"circle\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">implementing \u201cself\u201d as operating system with \u201cplug-ins\u201d</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">implementing \u201cself\u201d as society (of mind - Marvin Minsky)</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">implementing \u201cself\u201d as economy (of idiots \u2013 Eric Baum)</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">implementing \u201cself\u201d as global workspace/consciousness (Baars/Franklin)</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">implementing \u201cself\u201d as authorship (Dennett, Wegner)</li>\n</ul>\n</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">\u201csafe\u201d and/or \u201cmoral\u201d and ethical motivational systems \n<ul type=\"circle\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">value sets vs. goal hierarchies</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">context/ecological sensitivity</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">\u201csafe\u201d/moral values/goal content</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">evaluation schemes</li>\n</ul>\n</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">reflection \n<ul type=\"circle\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">self-examination</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">self-modeling &amp; self-knowledge</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">goal-based self-evaluation for self-improvement</li>\n</ul>\n</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">attention &amp; emotions \n<ul type=\"circle\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">as interoceptive responses to environmental stimuli</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">as knowledge/rules of thumb/\u201dactionable qualia\u201d</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">as helpful &amp; unhelpful biases (and how to intelligently improve)</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">as evaluation &amp; enforcement mechanisms</li>\n</ul>\n</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">integrating different knowledge and action representation schemes \n<ul type=\"circle\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">coordination &amp; translation between various schemes</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">analyzing trade-offs &amp; knowing when to switch between schemes</li>\n</ul>\n</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">self-improvement \n<ul type=\"circle\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">via automated tool/method incorporation &amp; theory-inductive heuristics \n<ul type=\"square\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">goal-based tool/method discovery</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">tool/method integration and evaluation</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">tool-to-theory heuristics</li>\n</ul>\n</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">via learning (knowledge incorporation) \n<ul type=\"square\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">(re-)building schemes and models</li>\n</ul>\n</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">discovery (refactoring, modularization, encapsulation and scale-invariance) \n<ul type=\"square\">\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">theory-to-tool heuristics</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">automated (re-)construction of probabilistic graphical models</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">Meta-Optimizing Semantic Evolutionary Search (MOSES)</li>\n<li class=\"MsoNormal\" style=\"margin: 0px 0px 0px 15px;\">Frequent &amp; Interesting Sub-HyperGRAph Mining (FISHGRAM)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">While solutions need to be grounded and extensible, the symposium would prefer approaches starting with some initial structure rather than a tabula rasa with the lowest level bootstrapping approaches or first causes explanations (except where they are fully extended to initial structures and/or used to justify such structures). Also, while autopoiesis and \u201cfunctional consciousness\u201d are obviously key topics, we would prefer that phenomenal consciousness arguments be considered off-topic.</p>\n<h3 style=\"color: #222222; font-family: arial, sans-serif;\" id=\"Submissions\">Submissions</h3>\n<p style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Those interested in participating should submit either full-length papers (up to 6 pages in AAAI format) or short papers/extended abstracts (2-3 pages) to&nbsp;<a style=\"color: #1155cc;\" href=\"https://www.easychair.org/conferences/?conf=ss14self\" target=\"_blank\">https://www.easychair.org/conferences/?conf=ss14self</a>. Submissions are due October 4.</p>\n<h3 style=\"color: #222222; font-family: arial, sans-serif;\" id=\"Additional_Information\">Additional Information</h3>\n<p style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Additional conference information is available at the AAAI&nbsp;Spring 2014 Symposium&nbsp;<a style=\"color: #1155cc;\" href=\"http://www.aaai.org/Symposia/Spring/sss14.php\" target=\"_blank\">page</a>.&nbsp; More details will be posted here as they become available.&nbsp; Questions can be emailed to Mark Waser (<a>MWaser@DigitalWisdomInstitute.org</a>).</p>", "sections": [{"title": "AAAI Spring 2014 Symposium - March 24-26, 2014 / Stanford University, Palo Alto, California, USA", "anchor": "AAAI_Spring_2014_Symposium___March_24_26__2014___Stanford_University__Palo_Alto__California__USA", "level": 2}, {"title": "Implementing Selves with Safe Motivational Systems & Self-Improvement", "anchor": "Implementing_Selves_with_Safe_Motivational_Systems___Self_Improvement", "level": 1}, {"title": "Submissions", "anchor": "Submissions", "level": 2}, {"title": "Additional Information", "anchor": "Additional_Information", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-23T18:14:26.743Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, Beliefs", "slug": "meetup-moscow-beliefs-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6QkBZZdWbMPefXFhj/meetup-moscow-beliefs-0", "pageUrlRelative": "/posts/6QkBZZdWbMPefXFhj/meetup-moscow-beliefs-0", "linkUrl": "https://www.lesswrong.com/posts/6QkBZZdWbMPefXFhj/meetup-moscow-beliefs-0", "postedAtFormatted": "Monday, September 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20Beliefs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20Beliefs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6QkBZZdWbMPefXFhj%2Fmeetup-moscow-beliefs-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20Beliefs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6QkBZZdWbMPefXFhj%2Fmeetup-moscow-beliefs-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6QkBZZdWbMPefXFhj%2Fmeetup-moscow-beliefs-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rc'>Moscow, Beliefs</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 September 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door near the swing or just look for group of geek-looking people. We will meet you at 16:00 inside.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li>Applied rationality exercise: beliefs finding, analysis, correction and adoption of new ones.</li>\n<li>Game session.</li>\n</ul>\n\n<p>Important notice: these exercises form a complete set. It means that if you miss first one or two exercises you can not complete others. So if you want to study new skills you should not be late.</p>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rc'>Moscow, Beliefs</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6QkBZZdWbMPefXFhj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.352471132591025e-06, "legacy": true, "legacyId": "24255", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Beliefs\">Discussion article for the meetup : <a href=\"/meetups/rc\">Moscow, Beliefs</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 September 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please use the following guide to get to the meetup: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>. You need the second revolving door near the swing or just look for group of geek-looking people. We will meet you at 16:00 inside.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li>Applied rationality exercise: beliefs finding, analysis, correction and adoption of new ones.</li>\n<li>Game session.</li>\n</ul>\n\n<p>Important notice: these exercises form a complete set. It means that if you miss first one or two exercises you can not complete others. So if you want to study new skills you should not be late.</p>\n\n<p>If you are going for the first time, you can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Beliefs1\">Discussion article for the meetup : <a href=\"/meetups/rc\">Moscow, Beliefs</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, Beliefs", "anchor": "Discussion_article_for_the_meetup___Moscow__Beliefs", "level": 1}, {"title": "Discussion article for the meetup : Moscow, Beliefs", "anchor": "Discussion_article_for_the_meetup___Moscow__Beliefs1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-23T21:06:58.518Z", "modifiedAt": null, "url": null, "title": "[Link] - No evidence of intelligence improvement after working memory training", "slug": "link-no-evidence-of-intelligence-improvement-after-working", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:33.182Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pinyaka", "createdAt": "2012-09-21T12:11:45.980Z", "isAdmin": false, "displayName": "pinyaka"}, "userId": "FscpDmNcKZdbeDNZ2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kCDNyWFdwTKjwnSfh/link-no-evidence-of-intelligence-improvement-after-working", "pageUrlRelative": "/posts/kCDNyWFdwTKjwnSfh/link-no-evidence-of-intelligence-improvement-after-working", "linkUrl": "https://www.lesswrong.com/posts/kCDNyWFdwTKjwnSfh/link-no-evidence-of-intelligence-improvement-after-working", "postedAtFormatted": "Monday, September 23rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20-%20No%20evidence%20of%20intelligence%20improvement%20after%20working%20memory%20training&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20-%20No%20evidence%20of%20intelligence%20improvement%20after%20working%20memory%20training%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkCDNyWFdwTKjwnSfh%2Flink-no-evidence-of-intelligence-improvement-after-working%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20-%20No%20evidence%20of%20intelligence%20improvement%20after%20working%20memory%20training%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkCDNyWFdwTKjwnSfh%2Flink-no-evidence-of-intelligence-improvement-after-working", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkCDNyWFdwTKjwnSfh%2Flink-no-evidence-of-intelligence-improvement-after-working", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 96, "htmlBody": "<p>This article critically examines previous studies that showed a link between working memory training (specifically via n-back training) and fluid intelligence, finding that the results may not have been as positive as reported owing to a number of factors including the use of a no-contact rather than active control group, and difficulty selecting tests that isolate the impact of working memory on fluid intelligence. The authors also present findings from a new study that show no improvement in fluid intelligence from dual n-back training, visual search training (active placebo) and no training (no contact placebo).</p>\n<p>&nbsp;</p>\n<p><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/22708717\">PubMed</a></p>\n<p><a href=\"https://dl.dropboxusercontent.com/u/27997066/No%20Evidence%20of%20Intelligence%20Improvement%20after%20Working%20Memory%20Training-%20A%20Randomized%2C%20Placebo-Controlled%20Study.pdf\">Journal Challenged</a></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5d63AWNjtFyHprX2k": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kCDNyWFdwTKjwnSfh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 6, "extendedScore": null, "score": 1.3526257685434437e-06, "legacy": true, "legacyId": "24257", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-24T01:25:54.498Z", "modifiedAt": null, "url": null, "title": "Open Thread, September 23-29, 2013", "slug": "open-thread-september-23-29-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:31.402Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2FqDtWFMuSvJkAW6E/open-thread-september-23-29-2013", "pageUrlRelative": "/posts/2FqDtWFMuSvJkAW6E/open-thread-september-23-29-2013", "linkUrl": "https://www.lesswrong.com/posts/2FqDtWFMuSvJkAW6E/open-thread-september-23-29-2013", "postedAtFormatted": "Tuesday, September 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20September%2023-29%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20September%2023-29%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2FqDtWFMuSvJkAW6E%2Fopen-thread-september-23-29-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20September%2023-29%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2FqDtWFMuSvJkAW6E%2Fopen-thread-september-23-29-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2FqDtWFMuSvJkAW6E%2Fopen-thread-september-23-29-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2FqDtWFMuSvJkAW6E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 1.3528579055117816e-06, "legacy": true, "legacyId": "24258", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 263, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-24T11:00:47.736Z", "modifiedAt": null, "url": null, "title": "Autism, Watson, the Turing test, and General Intelligence", "slug": "autism-watson-the-turing-test-and-general-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:09.768Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wN2v7LPmx2Cp3JnWd/autism-watson-the-turing-test-and-general-intelligence", "pageUrlRelative": "/posts/wN2v7LPmx2Cp3JnWd/autism-watson-the-turing-test-and-general-intelligence", "linkUrl": "https://www.lesswrong.com/posts/wN2v7LPmx2Cp3JnWd/autism-watson-the-turing-test-and-general-intelligence", "postedAtFormatted": "Tuesday, September 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Autism%2C%20Watson%2C%20the%20Turing%20test%2C%20and%20General%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAutism%2C%20Watson%2C%20the%20Turing%20test%2C%20and%20General%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwN2v7LPmx2Cp3JnWd%2Fautism-watson-the-turing-test-and-general-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Autism%2C%20Watson%2C%20the%20Turing%20test%2C%20and%20General%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwN2v7LPmx2Cp3JnWd%2Fautism-watson-the-turing-test-and-general-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwN2v7LPmx2Cp3JnWd%2Fautism-watson-the-turing-test-and-general-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 331, "htmlBody": "<p>Thinking aloud:</p>\n<p>Humans are examples of general intelligence - the only example we're sure of. Some humans have various degrees of autism (low level versions are quite common in the circles I've moved in), impairing their social skills. Mild autists nevertheless remain general intelligences, capable of demonstrating strong cross domain optimisation. Psychology is full of other examples of mental pathologies that impair certain skills, but nevertheless leave their sufferers as full fledged general intelligences. This general intelligence is not enough, however, to solve their impairments.</p>\n<p>Watson triumphed on Jeopardy. AI scientists in previous decades would have concluded that to do so, a general intelligence would have been needed. But that was not the case at all - Watson is blatantly not a general intelligence. Big data and clever algorithms were all that were needed. Computers are demonstrating more and more skills, besting humans in more and more domains - but still no sign of general intelligence. I've recently developed the suspicion that the Turing test (comparing AI with a standard human) could get passed by a narrow AI finely tuned to that task.</p>\n<p>The general thread is that the link between narrow skills and general intelligence may not be as clear as we sometimes think. It may be that narrow skills are sufficiently diverse and unique that a mid-level general intelligence may not be able to develop them to a large extent. Or, put another way, an above-human social intelligence may not be able to control a robot body or do decent image recognition. A super-intelligence likely could: ultimately, general intelligence includes the specific skills. But his \"ultimately\" may take a long time to come.</p>\n<p>So the questions I'm wondering about are:</p>\n<ol>\n<li>How likely is it that a general intelligence, above human in some domain <em>not</em> related to AI development, will acquire high level skills in unrelated areas?</li>\n<li>By building high-performance narrow AIs, are we making it much easier for such an intelligence to develop such skills, by co-opting or copying these programs?</li>\n</ol>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wN2v7LPmx2Cp3JnWd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 11, "extendedScore": null, "score": 1.3533735517103914e-06, "legacy": true, "legacyId": "24265", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-24T13:40:45.253Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham NC/Triangle Area: Cognitive Biases, Part 3", "slug": "meetup-durham-nc-triangle-area-cognitive-biases-part-3", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZZvBBNmtc5LoiXr9n/meetup-durham-nc-triangle-area-cognitive-biases-part-3", "pageUrlRelative": "/posts/ZZvBBNmtc5LoiXr9n/meetup-durham-nc-triangle-area-cognitive-biases-part-3", "linkUrl": "https://www.lesswrong.com/posts/ZZvBBNmtc5LoiXr9n/meetup-durham-nc-triangle-area-cognitive-biases-part-3", "postedAtFormatted": "Tuesday, September 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Cognitive%20Biases%2C%20Part%203&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Cognitive%20Biases%2C%20Part%203%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZZvBBNmtc5LoiXr9n%2Fmeetup-durham-nc-triangle-area-cognitive-biases-part-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Cognitive%20Biases%2C%20Part%203%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZZvBBNmtc5LoiXr9n%2Fmeetup-durham-nc-triangle-area-cognitive-biases-part-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZZvBBNmtc5LoiXr9n%2Fmeetup-durham-nc-triangle-area-cognitive-biases-part-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rd'>Durham NC/Triangle Area: Cognitive Biases, Part 3</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 September 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">420 West Geer St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Moar bias identification and discussion of debiasing strategies!</p>\n\n<p>7:00 coffee + talk <br />\n7:30 discussion <br />\n~9:30 adjourn to Fullsteam</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rd'>Durham NC/Triangle Area: Cognitive Biases, Part 3</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZZvBBNmtc5LoiXr9n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "24266", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Cognitive_Biases__Part_3\">Discussion article for the meetup : <a href=\"/meetups/rd\">Durham NC/Triangle Area: Cognitive Biases, Part 3</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 September 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">420 West Geer St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Moar bias identification and discussion of debiasing strategies!</p>\n\n<p>7:00 coffee + talk <br>\n7:30 discussion <br>\n~9:30 adjourn to Fullsteam</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Cognitive_Biases__Part_31\">Discussion article for the meetup : <a href=\"/meetups/rd\">Durham NC/Triangle Area: Cognitive Biases, Part 3</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham NC/Triangle Area: Cognitive Biases, Part 3", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Cognitive_Biases__Part_3", "level": 1}, {"title": "Discussion article for the meetup : Durham NC/Triangle Area: Cognitive Biases, Part 3", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Cognitive_Biases__Part_31", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-24T16:59:54.510Z", "modifiedAt": null, "url": null, "title": "Prisoner's Dilemma vs the Afterlife", "slug": "prisoner-s-dilemma-vs-the-afterlife", "viewCount": null, "lastCommentedAt": "2022-05-20T02:57:44.290Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QsdD4Xjwk2JBdzcuq/prisoner-s-dilemma-vs-the-afterlife", "pageUrlRelative": "/posts/QsdD4Xjwk2JBdzcuq/prisoner-s-dilemma-vs-the-afterlife", "linkUrl": "https://www.lesswrong.com/posts/QsdD4Xjwk2JBdzcuq/prisoner-s-dilemma-vs-the-afterlife", "postedAtFormatted": "Tuesday, September 24th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Prisoner's%20Dilemma%20vs%20the%20Afterlife&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APrisoner's%20Dilemma%20vs%20the%20Afterlife%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQsdD4Xjwk2JBdzcuq%2Fprisoner-s-dilemma-vs-the-afterlife%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Prisoner's%20Dilemma%20vs%20the%20Afterlife%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQsdD4Xjwk2JBdzcuq%2Fprisoner-s-dilemma-vs-the-afterlife", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQsdD4Xjwk2JBdzcuq%2Fprisoner-s-dilemma-vs-the-afterlife", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 654, "htmlBody": "<p>I've had a thought I don't recall having encountered before described quite this way; but, given my past experiences with such thoughts, and the fact that it involves evo-psych, I currently peg my confidence in this idea at around 10%. But just in case this particular idea rose to my attention out of all the other possible ideas that didn't for a reason, I'll post it here.</p>\n<p>&nbsp;</p>\n<p>One of the simpler analyses of the Prisoner's Dilemma points out that if you know that the round you're facing is the last round, then there's no reason not to defect; your choice no longer has any influence over future rounds, and whatever your opponent does, you gain a higher score by defecting on this particular round than by cooperating. Thus, any rational algorithm which is attempting to maximize its score, and can identify which round is the last round, will gain a higher score by adding a codicil to defect on the last round.</p>\n<p>Expanding that idea implies that if such a \"rational\" algorithm is facing other seemingly rational algorithms, it will assume that they will also defect on the last round; and thus, such an algorithm faced with the /second/-last round will be able to assume that its actions will have no influence on the actions of the last round; and, by a similar logic, will choose to defect on the second-last round; and the third-last; and so forth. In fact, if the whole game has a maximum length, then this chain of logic applies, leading to programs that are, in effect, always-defect. Cooperative strategies such as tit-for-tat thus tend to arise when the competing algorithms lack a particular piece of information: the length of the game they are playing.</p>\n<p>&nbsp;</p>\n<p>Depending on where a person is born and lives (and various other details), they have roughly a fifty percent chance of living to 80 years of age, a one-in-a-million chance of making it to 100 years, and using LaPlace's Sunrise Formula, somewhere under one-in-a-hundred-billion odds of making it to 130 years. If a person assumes that their death is the end of them, then they have a very good idea of what their maximum lifespan will be; and depending on how rational they are, they could follow a similar line of reasoning to the above and plan their actions around an \"always defect\" style of morality. (Eg, stealing whenever the profit outweighs the risk times the punishment.)</p>\n<p>However, introducing even an extremely vague concept of an afterlife, even if it's only that some form of individuality survives and can continue to interact with someone, means that there is no surety about when the 'game' will end - and, thus, can nudge people to act cooperatively, even when there is no physical chance of getting caught at defecting. Should this general approach spread widely enough, then further refinements could be made which increase cooperative behaviour further, such as reports on what the scoring system of the afterlife portion of the 'game' are; thus increasing in-group cooperative behaviour yet further.</p>\n<p>Interestingly, this seems to apply whether the post-mortal afterlife is supernatural in nature, or takes the form of a near-term technological singularity, or a cryonicist who estimates a 5% chance of revival within a millennium.</p>\n<p>&nbsp;</p>\n<p>What I would like to try to find out is which shapes of lifespan estimation lead to what forms of PD algorithms predominating. For example, a game with a 50% chance of continuing on any turn after turn 100, versus one with a 95% chance every turn, versus one with a straight 5% chance of being effectively infinite. If anyone reading this already has a set of software allowing for customized PD tournaments, I'd like to get in touch. Anyone else, I'd like whatever constructive criticism you can offer, from any previous descriptions of this - preferably with hard figures and numbers backing it up - to improvements that bring the general concept more into line with reality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"be2Mh2bddQ6ZaBcti": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QsdD4Xjwk2JBdzcuq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 1.3536958352618489e-06, "legacy": true, "legacyId": "24267", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 70, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-25T12:45:34.692Z", "modifiedAt": null, "url": null, "title": "Inferential silence", "slug": "inferential-silence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:00.544Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZfWLsDD8KKSFR3MLB/inferential-silence", "pageUrlRelative": "/posts/ZfWLsDD8KKSFR3MLB/inferential-silence", "linkUrl": "https://www.lesswrong.com/posts/ZfWLsDD8KKSFR3MLB/inferential-silence", "postedAtFormatted": "Wednesday, September 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Inferential%20silence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInferential%20silence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZfWLsDD8KKSFR3MLB%2Finferential-silence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Inferential%20silence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZfWLsDD8KKSFR3MLB%2Finferential-silence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZfWLsDD8KKSFR3MLB%2Finferential-silence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 300, "htmlBody": "<p>Every now and then, I write an LW comment on some topic and feel that the contents of my comment pretty much settles the issue decisively. Instead, the comment seems to get ignored entirely - it either gets very few votes or none, nobody responds to it, and the discussion generally continues as if it had never been posted.</p>\n<p>Similarly, every now and then I see somebody else make a post or comment that they clearly feel is decisive, but which doesn't seem very interesting to me. Either it seems to be saying something obvious, or I don't get its connection to the topic at hand in the first place.</p>\n<p>This seems like it would be about inferential distance: either the writer doesn't know the things that make the reader experience the comment as uninteresting, or the reader doesn't know the things that make the writer experience the comment as interesting. So there's <em>inferential silence</em> - a sufficiently long inferential distance that a claim doesn't provoke even objections, just uncomprehending or indifferent silence.</p>\n<p>But \"explain your reasoning in more detail\" doesn't seem like it would help with the issue. For one, we often don't know beforehand when people don't share our assumptions. Also, some of the comments or posts that seem to encounter this kind of a fate are <em>already</em> relatively long. For example, <a href=\"/lw/ig9/outside_views_and_miris_fai_endgame/9nv5\">Wei Dai wondered</a> why MIRI-affiliated people don't often respond to his posts that raise criticisms, and I essentially replied <a href=\"/lw/ig9/outside_views_and_miris_fai_endgame/9nvz\">that</a> I found the content of his post relatively obvious so didn't have much to say.</p>\n<p>Perhaps people could more often explicitly comment if they notice that something that a poster seems to consider a big thing doesn't seem very interesting or meaningful to them, and briefly explain why? Even a sentence or two might be helpful for the original poster.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YQW2DxpZFTrqrxHBJ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZfWLsDD8KKSFR3MLB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 78, "extendedScore": null, "score": 0.000211, "legacy": true, "legacyId": "24270", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 78, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-25T17:04:25.692Z", "modifiedAt": null, "url": null, "title": "Doing Important Research on Amazon's Mechanical Turk?", "slug": "doing-important-research-on-amazon-s-mechanical-turk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:34.242Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zD4omsx3AtafuT9Dp/doing-important-research-on-amazon-s-mechanical-turk", "pageUrlRelative": "/posts/zD4omsx3AtafuT9Dp/doing-important-research-on-amazon-s-mechanical-turk", "linkUrl": "https://www.lesswrong.com/posts/zD4omsx3AtafuT9Dp/doing-important-research-on-amazon-s-mechanical-turk", "postedAtFormatted": "Wednesday, September 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Doing%20Important%20Research%20on%20Amazon's%20Mechanical%20Turk%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoing%20Important%20Research%20on%20Amazon's%20Mechanical%20Turk%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzD4omsx3AtafuT9Dp%2Fdoing-important-research-on-amazon-s-mechanical-turk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Doing%20Important%20Research%20on%20Amazon's%20Mechanical%20Turk%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzD4omsx3AtafuT9Dp%2Fdoing-important-research-on-amazon-s-mechanical-turk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzD4omsx3AtafuT9Dp%2Fdoing-important-research-on-amazon-s-mechanical-turk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 849, "htmlBody": "<p>There seems to be many important questions that need research, from the mundane (say, which of four slogans for 80,000 Hours people like best) to the interesting (say, how to convince people to donate more than they otherwise would). &nbsp;Unfortunately, it's difficult to collect data in a quick, reliable, and affordable way. &nbsp;We generally lack access to easily survey-able populations and a lot of research has high barriers to entry for completing (such as needing to enroll in graduate school).</p>\n<p>However, since the 2005 creation of <a href=\"https://www.mturk.com/mturk/welcome\">Amazon's Mechanical Turk</a>, some of this has changed. &nbsp;Mechanical Turk is a website where anyone can create tasks for people to complete at a certain wage. &nbsp;These tasks can be anything, from identifying pictures to&nbsp;<a href=\"http://chronicle.com/blogs/profhacker/the-reliability-efficiencyaffordability-of-amazons-mechanical-turk/22994\">transcribing interviews</a>&nbsp;to social science research.</p>\n<p>Best of all, this is quick and cheap -- for example, you could offer $0.25 to complete a short survey, put $75 in a pot, and get 300 responses within a day or two, and this should be quicker and cheaper than any other option available to you for collecting data.</p>\n<p>But could Mechanical Turk actually be useable for answering important questions? &nbsp;Could running studies on Mechanical Turk be a competitive use of altruistic funds?</p>\n<p>&nbsp;</p>\n<h2><strong>What Questions Would We Be Interested in Asking?</strong></h2>\n<p>There are a variety of questions we might be interested in that would be appropriate to ask via Mechanical Turk. &nbsp;I don't believe you could make a longitudinal study, so testing the effects of vegetarian ads on diets in a useful way wouldn't be able to happen. &nbsp;But less conclusive diet studies could be run in this area.</p>\n<p>Additionally, we could test to see how people respond to various marketing materials in EA space. &nbsp;We could explore how people think about charity and see what would make them more likely to donate and how changes in the marketing affect a willingness to donate. &nbsp;We could find out which arguments are more compelling. &nbsp;We could even test various memes against each other and see what people think of them.</p>\n<p>&nbsp;</p>\n<h2>Is Mechanical Turk a Reliable Source of Data?</h2>\n<p>It would only be good to use MTurk if the data you could get is useful. &nbsp;But is it?</p>\n<p>&nbsp;</p>\n<p><strong>Diversity of the Sample</strong></p>\n<p>The first question we might ask is whether MTurk produces a sample that is sufficiently diverse and representative of the United States. &nbsp;Unfortunately, this isn't always the case for MTurk. &nbsp;In <a href=\"http://www.culturalcognition.net/blog/2013/7/10/fooled-twice-shame-on-who-problems-with-mechanical-turk-stud.html\">\"Problems With Mechanical Turk Study Samples\"</a>, Dan Kahan noted that female populations can be overrepresented (as high as 62%), African Americans are underrepresented (5% in MTurk compared to 12% in the US), and conservatives are very underrepresented (53% liberal / 25% conservative in MTurk vs. 20% liberal and 40% conservative in real life).</p>\n<p>MTurkers are <a href=\"http://themonkeycage.org/2012/12/19/how-representative-are-amazon-mechanical-turk-workers/\">more likely to vote and vote Obama</a>. &nbsp;More concerning, Kahan also found respondents lie about their prior exposure to measures and even whether their US citizens. &nbsp;Additionally, repeated exposure to standard survey questions can bias responses.</p>\n<p>But is this really a problem? &nbsp;First, MTurk samples are still <a href=\"http://personal.stevens.edu/~ysakamot/creativity/turk%20perspectives.pdf\">more diverse and representative</a>&nbsp;than college student samples or other surveys conducted over the internet. &nbsp;Second, many important questions are about items that we wouldn't expect to be influenced by demographics. &nbsp;So it's quite possible that MTurk might be the best of all possible sources by enough to make it worth it.</p>\n<p>&nbsp;</p>\n<p><strong>Wage Sensitivity</strong></p>\n<p>Do you have to pay more for higher quality data? &nbsp;Possibly not. &nbsp;<a href=\"http://personal.stevens.edu/~ysakamot/creativity/turk%20perspectives.pdf\">Buhrmester, Kwang, and Gosling</a>&nbsp;and <a href=\"http://psychcentral.com/blog/archives/2011/03/03/mechanical-turk-to-the-rescue-of-psychology-research/\">another analysis</a>&nbsp;both found that even changes between 2 cents and 50 cents didn't affect the quality of data received on psychological studies, but it does buy more participants and at a higher rate of participation (get more participants faster).</p>\n<p>&nbsp;</p>\n<h2>Is Mechanical Turk a Competitive Use of Altruistic Funds?</h2>\n<p>It depends on the question being asked, how reliable the findings are, and how they'd be put into use.</p>\n<p>Even though I don't think MTurk could be used for veg flyers very well, it's the best example I can think of right now: imagine that the current flyer converts 1% of people who read it to consider vegetarianism, but a different flyer might convert 1.05% of people. &nbsp;This means that every donation to veg ads now has approximately a 1.05x multiplier attached to it, because we can use the better flyer. &nbsp;If the MTurk study/studies to find this cost $1K, we would break even on this after distributing 100K flyers at 20 cents a flyer. &nbsp;I don't know how many flyers are given out a year, so that may or may not be impressive, but the numbers are made up anyway.</p>\n<p>The bottom line is that studies may have strong compounding effects, which will almost always beat out the relatively linear increase in impact from a donation to something like AMF. &nbsp;But chances might be small that MTurk will produce something useful. &nbsp;Likewise, it's possible that there are yet better settings for running these tests (like split testing current materials as they are being distributed, doing longer range and more reliable tracking of impact, etc.). &nbsp;But MTurk could be an interesting way to supplement existing research in a quick and cheap manner.</p>\n<p>I think it's worth thinking about further, even if I wouldn't act on it yet.</p>\n<p>-</p>\n<p><em>(Also <a href=\"http://www.everydayutilitarian.com/essays/doing-important-research-on-amazons-mechanical-turk/\">cross-posted</a> on <a href=\"http://www.everydayutilitarian.com\">my blog</a>.)</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zD4omsx3AtafuT9Dp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "24271", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>There seems to be many important questions that need research, from the mundane (say, which of four slogans for 80,000 Hours people like best) to the interesting (say, how to convince people to donate more than they otherwise would). &nbsp;Unfortunately, it's difficult to collect data in a quick, reliable, and affordable way. &nbsp;We generally lack access to easily survey-able populations and a lot of research has high barriers to entry for completing (such as needing to enroll in graduate school).</p>\n<p>However, since the 2005 creation of <a href=\"https://www.mturk.com/mturk/welcome\">Amazon's Mechanical Turk</a>, some of this has changed. &nbsp;Mechanical Turk is a website where anyone can create tasks for people to complete at a certain wage. &nbsp;These tasks can be anything, from identifying pictures to&nbsp;<a href=\"http://chronicle.com/blogs/profhacker/the-reliability-efficiencyaffordability-of-amazons-mechanical-turk/22994\">transcribing interviews</a>&nbsp;to social science research.</p>\n<p>Best of all, this is quick and cheap -- for example, you could offer $0.25 to complete a short survey, put $75 in a pot, and get 300 responses within a day or two, and this should be quicker and cheaper than any other option available to you for collecting data.</p>\n<p>But could Mechanical Turk actually be useable for answering important questions? &nbsp;Could running studies on Mechanical Turk be a competitive use of altruistic funds?</p>\n<p>&nbsp;</p>\n<h2 id=\"What_Questions_Would_We_Be_Interested_in_Asking_\"><strong>What Questions Would We Be Interested in Asking?</strong></h2>\n<p>There are a variety of questions we might be interested in that would be appropriate to ask via Mechanical Turk. &nbsp;I don't believe you could make a longitudinal study, so testing the effects of vegetarian ads on diets in a useful way wouldn't be able to happen. &nbsp;But less conclusive diet studies could be run in this area.</p>\n<p>Additionally, we could test to see how people respond to various marketing materials in EA space. &nbsp;We could explore how people think about charity and see what would make them more likely to donate and how changes in the marketing affect a willingness to donate. &nbsp;We could find out which arguments are more compelling. &nbsp;We could even test various memes against each other and see what people think of them.</p>\n<p>&nbsp;</p>\n<h2 id=\"Is_Mechanical_Turk_a_Reliable_Source_of_Data_\">Is Mechanical Turk a Reliable Source of Data?</h2>\n<p>It would only be good to use MTurk if the data you could get is useful. &nbsp;But is it?</p>\n<p>&nbsp;</p>\n<p><strong id=\"Diversity_of_the_Sample\">Diversity of the Sample</strong></p>\n<p>The first question we might ask is whether MTurk produces a sample that is sufficiently diverse and representative of the United States. &nbsp;Unfortunately, this isn't always the case for MTurk. &nbsp;In <a href=\"http://www.culturalcognition.net/blog/2013/7/10/fooled-twice-shame-on-who-problems-with-mechanical-turk-stud.html\">\"Problems With Mechanical Turk Study Samples\"</a>, Dan Kahan noted that female populations can be overrepresented (as high as 62%), African Americans are underrepresented (5% in MTurk compared to 12% in the US), and conservatives are very underrepresented (53% liberal / 25% conservative in MTurk vs. 20% liberal and 40% conservative in real life).</p>\n<p>MTurkers are <a href=\"http://themonkeycage.org/2012/12/19/how-representative-are-amazon-mechanical-turk-workers/\">more likely to vote and vote Obama</a>. &nbsp;More concerning, Kahan also found respondents lie about their prior exposure to measures and even whether their US citizens. &nbsp;Additionally, repeated exposure to standard survey questions can bias responses.</p>\n<p>But is this really a problem? &nbsp;First, MTurk samples are still <a href=\"http://personal.stevens.edu/~ysakamot/creativity/turk%20perspectives.pdf\">more diverse and representative</a>&nbsp;than college student samples or other surveys conducted over the internet. &nbsp;Second, many important questions are about items that we wouldn't expect to be influenced by demographics. &nbsp;So it's quite possible that MTurk might be the best of all possible sources by enough to make it worth it.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Wage_Sensitivity\">Wage Sensitivity</strong></p>\n<p>Do you have to pay more for higher quality data? &nbsp;Possibly not. &nbsp;<a href=\"http://personal.stevens.edu/~ysakamot/creativity/turk%20perspectives.pdf\">Buhrmester, Kwang, and Gosling</a>&nbsp;and <a href=\"http://psychcentral.com/blog/archives/2011/03/03/mechanical-turk-to-the-rescue-of-psychology-research/\">another analysis</a>&nbsp;both found that even changes between 2 cents and 50 cents didn't affect the quality of data received on psychological studies, but it does buy more participants and at a higher rate of participation (get more participants faster).</p>\n<p>&nbsp;</p>\n<h2 id=\"Is_Mechanical_Turk_a_Competitive_Use_of_Altruistic_Funds_\">Is Mechanical Turk a Competitive Use of Altruistic Funds?</h2>\n<p>It depends on the question being asked, how reliable the findings are, and how they'd be put into use.</p>\n<p>Even though I don't think MTurk could be used for veg flyers very well, it's the best example I can think of right now: imagine that the current flyer converts 1% of people who read it to consider vegetarianism, but a different flyer might convert 1.05% of people. &nbsp;This means that every donation to veg ads now has approximately a 1.05x multiplier attached to it, because we can use the better flyer. &nbsp;If the MTurk study/studies to find this cost $1K, we would break even on this after distributing 100K flyers at 20 cents a flyer. &nbsp;I don't know how many flyers are given out a year, so that may or may not be impressive, but the numbers are made up anyway.</p>\n<p>The bottom line is that studies may have strong compounding effects, which will almost always beat out the relatively linear increase in impact from a donation to something like AMF. &nbsp;But chances might be small that MTurk will produce something useful. &nbsp;Likewise, it's possible that there are yet better settings for running these tests (like split testing current materials as they are being distributed, doing longer range and more reliable tracking of impact, etc.). &nbsp;But MTurk could be an interesting way to supplement existing research in a quick and cheap manner.</p>\n<p>I think it's worth thinking about further, even if I wouldn't act on it yet.</p>\n<p>-</p>\n<p><em>(Also <a href=\"http://www.everydayutilitarian.com/essays/doing-important-research-on-amazons-mechanical-turk/\">cross-posted</a> on <a href=\"http://www.everydayutilitarian.com\">my blog</a>.)</em></p>", "sections": [{"title": "What Questions Would We Be Interested in Asking?", "anchor": "What_Questions_Would_We_Be_Interested_in_Asking_", "level": 1}, {"title": "Is Mechanical Turk a Reliable Source of Data?", "anchor": "Is_Mechanical_Turk_a_Reliable_Source_of_Data_", "level": 1}, {"title": "Diversity of the Sample", "anchor": "Diversity_of_the_Sample", "level": 2}, {"title": "Wage Sensitivity", "anchor": "Wage_Sensitivity", "level": 2}, {"title": "Is Mechanical Turk a Competitive Use of Altruistic Funds?", "anchor": "Is_Mechanical_Turk_a_Competitive_Use_of_Altruistic_Funds_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "49 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-25T23:09:17.215Z", "modifiedAt": null, "url": null, "title": "What makes us think _any_ of our terminal values aren't based on a misunderstanding of reality?", "slug": "what-makes-us-think-_any_-of-our-terminal-values-aren-t", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:40.835Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bokov", "createdAt": "2010-01-11T01:11:23.480Z", "isAdmin": false, "displayName": "bokov"}, "userId": "4sgsBYAsjDHNvB7Q6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w34jdJb6AAAb3MWy4/what-makes-us-think-_any_-of-our-terminal-values-aren-t", "pageUrlRelative": "/posts/w34jdJb6AAAb3MWy4/what-makes-us-think-_any_-of-our-terminal-values-aren-t", "linkUrl": "https://www.lesswrong.com/posts/w34jdJb6AAAb3MWy4/what-makes-us-think-_any_-of-our-terminal-values-aren-t", "postedAtFormatted": "Wednesday, September 25th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20makes%20us%20think%20_any_%20of%20our%20terminal%20values%20aren't%20based%20on%20a%20misunderstanding%20of%20reality%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20makes%20us%20think%20_any_%20of%20our%20terminal%20values%20aren't%20based%20on%20a%20misunderstanding%20of%20reality%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw34jdJb6AAAb3MWy4%2Fwhat-makes-us-think-_any_-of-our-terminal-values-aren-t%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20makes%20us%20think%20_any_%20of%20our%20terminal%20values%20aren't%20based%20on%20a%20misunderstanding%20of%20reality%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw34jdJb6AAAb3MWy4%2Fwhat-makes-us-think-_any_-of-our-terminal-values-aren-t", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw34jdJb6AAAb3MWy4%2Fwhat-makes-us-think-_any_-of-our-terminal-values-aren-t", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 294, "htmlBody": "<p>Let's say Bob's terminal value is to travel back in time and ride a dinosaur.</p>\n<p>It is instrumentally rational for Bob to study physics so he can learn how to build a time machine. As he learns more physics, Bob realizes that his terminal value is not only utterly impossible but meaningless. By definition, someone in Bob's past riding a dinosaur is not a future evolution of the present Bob.</p>\n<p>There are a number of ways to create the subjective experience of having gone into the past and ridden a dinosaur. But to Bob, it's not the same because he wanted both the subjective experience and the knowledge that it corresponded to objective fact. Without the latter, he might as well have just watched a movie or played a video game.</p>\n<p>So if we took the original, innocent-of-physics Bob and somehow calculated his coherent extrapolated volition, we would end up with a Bob who has given up on time travel. The original Bob would not want to be this Bob.</p>\n<p>But, how do we know that _anything_ we value won't similarly dissolve under <a href=\"/lw/of/dissolving_the_question/\">sufficiently thorough deconstruction</a>? Let's suppose for a minute that all \"human values\" are <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">dangling units</a>; that everything we want is as possible and makes as much sense as wanting to hear the sound of blue or taste the flavor of a prime number. What is the rational course of action in such a situation?</p>\n<p>PS: If your response resembles \"keep attempting to XXX anyway\", please explain what privileges XXX over any number of other alternatives other than your current preference. Are you using some kind of pre-commitment strategy to a subset of your current goals? Do you now wish you had used the same strategy to precommit to goals you had when you were a toddler?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w34jdJb6AAAb3MWy4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 22, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "24272", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 89, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Mc6QcrsbH5NRXbCRX", "yA4gF5KrboK2m2Xu7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-26T09:16:04.448Z", "modifiedAt": null, "url": null, "title": "Bayesian probability as an approximate theory of uncertainty?", "slug": "bayesian-probability-as-an-approximate-theory-of-uncertainty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.992Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bhb54ZC72Y2KkgEeG/bayesian-probability-as-an-approximate-theory-of-uncertainty", "pageUrlRelative": "/posts/bhb54ZC72Y2KkgEeG/bayesian-probability-as-an-approximate-theory-of-uncertainty", "linkUrl": "https://www.lesswrong.com/posts/bhb54ZC72Y2KkgEeG/bayesian-probability-as-an-approximate-theory-of-uncertainty", "postedAtFormatted": "Thursday, September 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayesian%20probability%20as%20an%20approximate%20theory%20of%20uncertainty%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayesian%20probability%20as%20an%20approximate%20theory%20of%20uncertainty%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbhb54ZC72Y2KkgEeG%2Fbayesian-probability-as-an-approximate-theory-of-uncertainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayesian%20probability%20as%20an%20approximate%20theory%20of%20uncertainty%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbhb54ZC72Y2KkgEeG%2Fbayesian-probability-as-an-approximate-theory-of-uncertainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbhb54ZC72Y2KkgEeG%2Fbayesian-probability-as-an-approximate-theory-of-uncertainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 510, "htmlBody": "<p>Many people believe that Bayesian probability is an exact theory of uncertainty, and other theories are imperfect approximations. In this post I'd like to tentatively argue the opposite: that Bayesian probability is an imperfect approximation of what we want from a theory of uncertainty. This post won't contain any new results, and is probably very confused anyway.</p>\n<p>I agree that Bayesian probability is provably the only correct theory for dealing with a certain idealized kind of uncertainty. But what kinds of uncertainty actually exist in our world, and how closely do they agree with what's needed for Bayesianism to work?</p>\n<p>In a Tegmark Level IV world (thanks to pragmatist for pointing out this assumption), uncertainty seems to be either indexical or logical. When I flip a coin, the information in my mind is either enough or not enough to determine the outcome in advance. If I have enough information - if it's mathematically possible to determine which way the coin will fall, given the bits of information that I have received - then I have logical uncertainty, which is no different in principle from being uncertain about the trillionth digit of pi. On the other hand, if I don't have enough information even given infinite mathematical power, it implies that the world must contain copies of me that will see different coinflip outcomes (if there was just one copy, mathematics would be able to pin it down), so I have indexical uncertainty.</p>\n<p>The trouble is that both indexical and logical uncertainty are puzzling in their own ways.</p>\n<p>With indexical uncertainty, the usual example that breaks probabilistic reasoning is the <a href=\"/lw/182/the_absentminded_driver/\">Absent-Minded Driver</a> problem. When the probability of you being this or that copy depends on the decision you're about to make, these probabilities are unusable for decision-making. Since Bayesian probability is in large part justified by decision-making, we're in trouble. And the AMD is not an isolated problem. In many imaginable situations faced by humans or idealized agents, there's a nonzero chance of returning to the same state of mind in the future, and that chance slightly depends on the current action. To the extent that's true, Bayesian probability is an imperfect approximation.</p>\n<p>With logical uncertainty, the situation is even worse. We don't have a good theory of how logical uncertainty should work. (Though there have been several attempts, like Benja and Paul's prior, Manfred's prior, or my own recent attempt.) Since Bayesian probability is in large part justified by having perfect agreement with logic, it seems likely that the correct theory of logical uncertainty won't look very Bayesian, because the whole point is to have limited computational resources and only approximate agreement with logic.</p>\n<p>Another troubling point is that if Bayesian probability is suspect, the idea of \"priors\" becomes suspect by association. Our best ideas for decision-making under indexical uncertainty (UDT) and logical uncertainty (priors over theories) involve some kind of priors, or more generally probability distributions, so we might want to reexamine those as well. Though if we interpret a UDT-ish prior as a measure of care rather than belief, maybe the problem goes away...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bhb54ZC72Y2KkgEeG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 23, "extendedScore": null, "score": 7.6e-05, "legacy": true, "legacyId": "24274", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GfHdNfqxe3cSCfpHL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-26T15:05:34.827Z", "modifiedAt": null, "url": null, "title": "Rationalizing: looking for the wrong kind of loopholes", "slug": "rationalizing-looking-for-the-wrong-kind-of-loopholes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:34.515Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bokov", "createdAt": "2010-01-11T01:11:23.480Z", "isAdmin": false, "displayName": "bokov"}, "userId": "4sgsBYAsjDHNvB7Q6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/npQ8DxJdFAbzMsMES/rationalizing-looking-for-the-wrong-kind-of-loopholes", "pageUrlRelative": "/posts/npQ8DxJdFAbzMsMES/rationalizing-looking-for-the-wrong-kind-of-loopholes", "linkUrl": "https://www.lesswrong.com/posts/npQ8DxJdFAbzMsMES/rationalizing-looking-for-the-wrong-kind-of-loopholes", "postedAtFormatted": "Thursday, September 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationalizing%3A%20looking%20for%20the%20wrong%20kind%20of%20loopholes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationalizing%3A%20looking%20for%20the%20wrong%20kind%20of%20loopholes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnpQ8DxJdFAbzMsMES%2Frationalizing-looking-for-the-wrong-kind-of-loopholes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationalizing%3A%20looking%20for%20the%20wrong%20kind%20of%20loopholes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnpQ8DxJdFAbzMsMES%2Frationalizing-looking-for-the-wrong-kind-of-loopholes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnpQ8DxJdFAbzMsMES%2Frationalizing-looking-for-the-wrong-kind-of-loopholes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 198, "htmlBody": "<p>This morning I found what I think is an interesting way to explain rationalizing to my son, and I thought I'd share it:</p>\n<ul>\n<li>Physical reality has rules that you can game to your advantage (natural laws).</li>\n<li>People have another set of rules that you can game to your advantage (preferences, biases, cultural norms).</li>\n<li>Rationalization is when you are trying to overcome an obstacle based in physical reality by trying to game human rules.</li>\n</ul>\n<p>&nbsp;</p>\n<p>Two subsequent thoughts that ocurred to me:</p>\n<ul>\n<li>If you're rationalizing-- the magic excuse fairy might not be there to hear you, but your subconsciousness is. And you will often convince your subconsciousness... to believe that the problem you're trying to solve is impossible, that it won't do any good anyway, that people are out to get you, and any number of other non-factual things that are directly antagonistic to your goals. This is why rationalizing is a bad habit.</li>\n<li>By this definition, the opposite of rationalizing is using the constraints physical reality to convince people that you are right. This is something you _can_ use effectively and should always try to do. It's called presenting evidence.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "npQ8DxJdFAbzMsMES", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 7, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "24275", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-26T15:08:21.551Z", "modifiedAt": null, "url": null, "title": "Happy Petrov Day", "slug": "happy-petrov-day", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:33.387Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nJynWFTH3rE469NdN/happy-petrov-day", "pageUrlRelative": "/posts/nJynWFTH3rE469NdN/happy-petrov-day", "linkUrl": "https://www.lesswrong.com/posts/nJynWFTH3rE469NdN/happy-petrov-day", "postedAtFormatted": "Thursday, September 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Happy%20Petrov%20Day&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHappy%20Petrov%20Day%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnJynWFTH3rE469NdN%2Fhappy-petrov-day%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Happy%20Petrov%20Day%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnJynWFTH3rE469NdN%2Fhappy-petrov-day", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnJynWFTH3rE469NdN%2Fhappy-petrov-day", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 13, "htmlBody": "<p><a href=\"/lw/jq/926_is_petrov_day/\">9/26 is Petrov Day</a></p>\n<p>Take a moment today to not destroy the world.</p>\n<p><img src=\"http://www.deathisbadblog.com/wp-content/uploads/2012/09/800x500xPetrov-Day.jpg.pagespeed.ic.Vb7794XbP8.jpg\" alt=\"Petrov Day\" width=\"800\" height=\"500\" /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2i3w84KCkqZzpnQ4d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nJynWFTH3rE469NdN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 25, "extendedScore": null, "score": 7.6e-05, "legacy": true, "legacyId": "24276", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QtyKq4BDyuJ3tysoK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-26T17:40:17.413Z", "modifiedAt": null, "url": null, "title": "Meetup : Helsinki Meetup", "slug": "meetup-helsinki-meetup-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PyryP", "createdAt": "2011-04-29T10:11:10.950Z", "isAdmin": false, "displayName": "PyryP"}, "userId": "wDg8hZMsGxgsoCP2Y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h43HYjr39we7axkwT/meetup-helsinki-meetup-1", "pageUrlRelative": "/posts/h43HYjr39we7axkwT/meetup-helsinki-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/h43HYjr39we7axkwT/meetup-helsinki-meetup-1", "postedAtFormatted": "Thursday, September 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Helsinki%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Helsinki%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh43HYjr39we7axkwT%2Fmeetup-helsinki-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Helsinki%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh43HYjr39we7axkwT%2Fmeetup-helsinki-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh43HYjr39we7axkwT%2Fmeetup-helsinki-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/re'>Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 October 2013 02:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Vilhonkatu 4, 00100 Helsinki, Finland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The theme of this meetup will be cognitive bias, and we\u2019ll learn about them through <a href=\"http://lesswrong.com/lw/4fp/fun_and_games_with_cognitive_biases/\">serious games</a>.\nWe\u2019ll meet in <a href=\"http://www.oluthuone.fi/oluthuoneet/kaisla/\" rel=\"nofollow\">Oluthuone Kaisla</a>. To find us, look for someone wearing a pink elephant hat.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/re'>Helsinki Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h43HYjr39we7axkwT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.3563217629970895e-06, "legacy": true, "legacyId": "24277", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup\">Discussion article for the meetup : <a href=\"/meetups/re\">Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 October 2013 02:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Vilhonkatu 4, 00100 Helsinki, Finland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The theme of this meetup will be cognitive bias, and we\u2019ll learn about them through <a href=\"http://lesswrong.com/lw/4fp/fun_and_games_with_cognitive_biases/\">serious games</a>.\nWe\u2019ll meet in <a href=\"http://www.oluthuone.fi/oluthuoneet/kaisla/\" rel=\"nofollow\">Oluthuone Kaisla</a>. To find us, look for someone wearing a pink elephant hat.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/re\">Helsinki Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Ytr4dJT79sCBuuEjG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-26T17:49:05.172Z", "modifiedAt": null, "url": null, "title": "Meetup : Montreal LessWrong - Return from summer break", "slug": "meetup-montreal-lesswrong-return-from-summer-break", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bartimaeus", "createdAt": "2013-05-07T17:14:04.389Z", "isAdmin": false, "displayName": "bartimaeus"}, "userId": "mqWrbcZHzhfPLnJqg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NwfpBynYBGpNN2yBT/meetup-montreal-lesswrong-return-from-summer-break", "pageUrlRelative": "/posts/NwfpBynYBGpNN2yBT/meetup-montreal-lesswrong-return-from-summer-break", "linkUrl": "https://www.lesswrong.com/posts/NwfpBynYBGpNN2yBT/meetup-montreal-lesswrong-return-from-summer-break", "postedAtFormatted": "Thursday, September 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Montreal%20LessWrong%20-%20Return%20from%20summer%20break&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Montreal%20LessWrong%20-%20Return%20from%20summer%20break%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwfpBynYBGpNN2yBT%2Fmeetup-montreal-lesswrong-return-from-summer-break%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Montreal%20LessWrong%20-%20Return%20from%20summer%20break%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwfpBynYBGpNN2yBT%2Fmeetup-montreal-lesswrong-return-from-summer-break", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwfpBynYBGpNN2yBT%2Fmeetup-montreal-lesswrong-return-from-summer-break", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 123, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rf'>Montreal LessWrong - Return from summer break</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 October 2013 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">McGill University, Montreal, QC, Canada</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>After taking a break for the summer, Montreal LessWrong is once again going to start meeting up regularly. We'll be moving towards more structured content this time around, based on feedback from members. First meetup will be more meta-level stuff, planning location, times, ideas for meetups and so on.\nThe exact location hasn't been determined yet. Message me and I'll let you know ASAP.</p>\n\n<p>Edit: The meetup will take place in Music - Room A301 (527 Sherbrooke Street West).  Message me if you have any difficulty finding it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rf'>Montreal LessWrong - Return from summer break</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NwfpBynYBGpNN2yBT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.3563296856877224e-06, "legacy": true, "legacyId": "24278", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Montreal_LessWrong___Return_from_summer_break\">Discussion article for the meetup : <a href=\"/meetups/rf\">Montreal LessWrong - Return from summer break</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 October 2013 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">McGill University, Montreal, QC, Canada</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>After taking a break for the summer, Montreal LessWrong is once again going to start meeting up regularly. We'll be moving towards more structured content this time around, based on feedback from members. First meetup will be more meta-level stuff, planning location, times, ideas for meetups and so on.\nThe exact location hasn't been determined yet. Message me and I'll let you know ASAP.</p>\n\n<p>Edit: The meetup will take place in Music - Room A301 (527 Sherbrooke Street West).  Message me if you have any difficulty finding it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Montreal_LessWrong___Return_from_summer_break1\">Discussion article for the meetup : <a href=\"/meetups/rf\">Montreal LessWrong - Return from summer break</a></h2>", "sections": [{"title": "Discussion article for the meetup : Montreal LessWrong - Return from summer break", "anchor": "Discussion_article_for_the_meetup___Montreal_LessWrong___Return_from_summer_break", "level": 1}, {"title": "Discussion article for the meetup : Montreal LessWrong - Return from summer break", "anchor": "Discussion_article_for_the_meetup___Montreal_LessWrong___Return_from_summer_break1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-26T20:17:43.030Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta Pre-Meetup Museum Field Trip", "slug": "meetup-atlanta-pre-meetup-museum-field-trip", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wpzkwBXiFEvREWpuP/meetup-atlanta-pre-meetup-museum-field-trip", "pageUrlRelative": "/posts/wpzkwBXiFEvREWpuP/meetup-atlanta-pre-meetup-museum-field-trip", "linkUrl": "https://www.lesswrong.com/posts/wpzkwBXiFEvREWpuP/meetup-atlanta-pre-meetup-museum-field-trip", "postedAtFormatted": "Thursday, September 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20Pre-Meetup%20Museum%20Field%20Trip&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20Pre-Meetup%20Museum%20Field%20Trip%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwpzkwBXiFEvREWpuP%2Fmeetup-atlanta-pre-meetup-museum-field-trip%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20Pre-Meetup%20Museum%20Field%20Trip%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwpzkwBXiFEvREWpuP%2Fmeetup-atlanta-pre-meetup-museum-field-trip", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwpzkwBXiFEvREWpuP%2Fmeetup-atlanta-pre-meetup-museum-field-trip", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rg'>Atlanta Pre-Meetup Museum Field Trip</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 September 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Atlanta</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>In celebration of National Free Museum Day, Atlanta Lesswrongers are going to go museuming it up before heading over to our 6 o'clock regularly scheduled meetup.</p>\n\n<p>The exact museum is TBD, check out our facebook post on the matter in our facebook group:  https://www.facebook.com/groups/100137206844878/</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rg'>Atlanta Pre-Meetup Museum Field Trip</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wpzkwBXiFEvREWpuP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3564635729371825e-06, "legacy": true, "legacyId": "24280", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Pre_Meetup_Museum_Field_Trip\">Discussion article for the meetup : <a href=\"/meetups/rg\">Atlanta Pre-Meetup Museum Field Trip</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 September 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Atlanta</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>In celebration of National Free Museum Day, Atlanta Lesswrongers are going to go museuming it up before heading over to our 6 o'clock regularly scheduled meetup.</p>\n\n<p>The exact museum is TBD, check out our facebook post on the matter in our facebook group:  https://www.facebook.com/groups/100137206844878/</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Pre_Meetup_Museum_Field_Trip1\">Discussion article for the meetup : <a href=\"/meetups/rg\">Atlanta Pre-Meetup Museum Field Trip</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta Pre-Meetup Museum Field Trip", "anchor": "Discussion_article_for_the_meetup___Atlanta_Pre_Meetup_Museum_Field_Trip", "level": 1}, {"title": "Discussion article for the meetup : Atlanta Pre-Meetup Museum Field Trip", "anchor": "Discussion_article_for_the_meetup___Atlanta_Pre_Meetup_Museum_Field_Trip1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-26T23:49:03.319Z", "modifiedAt": null, "url": null, "title": "AI ebook cover design brainstorming", "slug": "ai-ebook-cover-design-brainstorming", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:32.642Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wBaWgt45WWfv5ZTPD/ai-ebook-cover-design-brainstorming", "pageUrlRelative": "/posts/wBaWgt45WWfv5ZTPD/ai-ebook-cover-design-brainstorming", "linkUrl": "https://www.lesswrong.com/posts/wBaWgt45WWfv5ZTPD/ai-ebook-cover-design-brainstorming", "postedAtFormatted": "Thursday, September 26th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20ebook%20cover%20design%20brainstorming&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20ebook%20cover%20design%20brainstorming%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwBaWgt45WWfv5ZTPD%2Fai-ebook-cover-design-brainstorming%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20ebook%20cover%20design%20brainstorming%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwBaWgt45WWfv5ZTPD%2Fai-ebook-cover-design-brainstorming", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwBaWgt45WWfv5ZTPD%2Fai-ebook-cover-design-brainstorming", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 301, "htmlBody": "<p>Thanks to everyone who brainstormed&nbsp;<a href=\"/r/discussion/lw/io3/help_us_name_a_short_primer_on_ai_risk/\">possible titles for MIRI&rsquo;s upcoming ebook on machine intelligence</a>. Our leading contender for the book title is&nbsp;<em>Smarter than Us: The Rise of Machine Intelligence</em>.</p>\n<p>What we need now are suggestions for a book <strong>cover design</strong>. AI is hard to depict without falling back on cliches, such as a brain image mixed with computer circuitry, a humanoid robot, HAL, an imitation of <em><a href=\"http://en.wikipedia.org/wiki/The_Creation_of_Adam\">Creation of Adam</a></em>&nbsp;with human and robot fingers touching, or an imitation of <em><a href=\"http://en.wikipedia.org/wiki/March_of_Progress\">March of Progress</a></em>&nbsp;with an AI at the far right.</p>\n<p>A few ideas/examples:</p>\n<ol>\n<li>\n<p>Something that conveys &lsquo;AI&rsquo; in the middle (a computer screen? a server tower?) connected by arrow/wires/something to various &lsquo;skills/actions/influences&rsquo;, like giving a speech, flying unmanned spacecraft, doing science, predicting the stock market, etc., in an attempt to convey the diverse superpowers of a machine intelligence.</p>\n</li>\n<li>\n<p>A more minimalist text-only cover.</p>\n</li>\n<li>\n<p>A fairly minimal cover with just an ominous-looking server rack in the middle, with a few blinking lights and submerged in darkness around it. A bit like <a href=\"http://www.amazon.com/Fade-Lisa-Mcmann/dp/2732455334/ref=sr_1_4?ie=UTF8&amp;qid=1380239258&amp;sr=8-4&amp;keywords=fade+lisa+mcmann\">this cover</a>.</p>\n</li>\n<li>\n<p>Similar to the above, except a server farm along the bottom fading into the background, with a frame composition similar to&nbsp;<a href=\"http://bookcoverarchive.com/book/columbine\">this</a>.</p>\n</li>\n<li>\n<p>A darkened, machine-gunned room with a laptop sitting alone on a desk, displaying the text of the title on the screen. (This is the scene from the first chapter, about a Terminator who encounters an unthreatening-looking laptop which ends up being way more powerful and dangerous than the Terminator because it is more intelligent.)</p>\n</li>\n</ol>\n<p>Alex Vermeer sketched the first four of these ideas:</p>\n<p><a href=\"http://imgur.com/mkpSy2c\"><img src=\"http://i.imgur.com/mkpSy2c.png\" alt=\"\" /></a></p>\n<p>Some general inspiration may be found <a href=\"http://bookcoverarchive.com/\">here</a>.</p>\n<p>We think we want something kinda dramatic, rather than cartoony, but less epic and unbelievable than the <em><a href=\"http://www.amazon.com/Facing-the-Intelligence-Explosion-ebook/dp/B00C7YOR5Q/\">Facing the Intelligence Explosion</a></em> cover.</p>\n<p>Thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wBaWgt45WWfv5ZTPD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.3566539873242069e-06, "legacy": true, "legacyId": "24281", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["atyyMJ6z6d3mpvMjm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-27T01:09:15.978Z", "modifiedAt": null, "url": null, "title": "Intelligence Amplification and Friendly AI", "slug": "intelligence-amplification-and-friendly-ai", "viewCount": null, "lastCommentedAt": "2018-08-05T16:54:33.509Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jmgyfDYDDYs7YpqJg/intelligence-amplification-and-friendly-ai", "pageUrlRelative": "/posts/jmgyfDYDDYs7YpqJg/intelligence-amplification-and-friendly-ai", "linkUrl": "https://www.lesswrong.com/posts/jmgyfDYDDYs7YpqJg/intelligence-amplification-and-friendly-ai", "postedAtFormatted": "Friday, September 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intelligence%20Amplification%20and%20Friendly%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligence%20Amplification%20and%20Friendly%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjmgyfDYDDYs7YpqJg%2Fintelligence-amplification-and-friendly-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intelligence%20Amplification%20and%20Friendly%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjmgyfDYDDYs7YpqJg%2Fintelligence-amplification-and-friendly-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjmgyfDYDDYs7YpqJg%2Fintelligence-amplification-and-friendly-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1113, "htmlBody": "<p><small>Part of the series <a href=\"/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">AI Risk and Opportunity: A Strategic Analaysis</a>. Previous articles on this topic: <a href=\"/lw/6mi/some_thoughts_on_singularity_strategies/\">Some Thoughts on Singularity Strategies</a>, <a href=\"/lw/10l/intelligence_enhancement_as_existential_risk/\">Intelligence enhancement as existential risk mitigation</a>, <a href=\"/lw/6j9/outline_of_possible_singularity_scenarios_that/\">Outline of possible Singularity scenarios that are not completely disastrous</a>.</small></p>\n<p>Below are my quickly-sketched thoughts on intelligence amplification and FAI, without much effort put into organization or clarity, and without many references.<a id=\"fnref:1\" class=\"footnote\" title=\"see footnote\" href=\"#fn:1\">[1]</a> But first, I briefly review some strategies for increasing the odds of FAI, one of which is to work on intelligence amplification (IA).</p>\n<h3 id=\"somepossiblebestcurrentoptionsforincreasingtheoddsoffai\"><a id=\"more\"></a><br /></h3>\n<h3>Some possible &ldquo;best current options&rdquo; for increasing the odds of FAI</h3>\n<p>Suppose you find yourself in a pre-AGI world,<a id=\"fnref:2\" class=\"footnote\" title=\"see footnote\" href=\"#fn:2\">[2]</a> and you&rsquo;ve been convinced that the status quo world is unstable, and within the next couple centuries we&rsquo;ll likely<a id=\"fnref:3\" class=\"footnote\" title=\"see footnote\" href=\"#fn:3\">[3]</a> settle into one of <a href=\"http://intelligenceexplosion.com/2012/ai-the-problem-with-solutions/\">four stable outcomes</a>: FAI, uFAI, non-AI extinction, or a sufficiently powerful global government which can prevent AGI development<a id=\"fnref:4\" class=\"footnote\" title=\"see footnote\" href=\"#fn:4\">[4]</a>. And you <em>totally</em> prefer the FAI option. What should you do to get there?</p>\n<ul>\n<li>Obvious direct approach: start solving the technical problems that must be solved to get FAI: goal stability under self-modification, decision algorithms that handle counterfactuals and logical uncertainty properly, indirect normativity, and so on. (<a href=\"http://intelligence.org/research/\">MIRI&rsquo;s work</a>, some FHI work.)</li>\n<li>Do strategy research, to potentially identify superior alternatives to the other items on this list, or superior versions of the things on this list already. (<a href=\"http://www.fhi.ox.ac.uk/\">FHI&rsquo;s work</a>, some MIRI work, etc.)</li>\n<li>Accelerate IA technologies, so that smarter humans can tackle FAI. (E.g. <a href=\"http://intelligence.org/2013/08/31/stephen-hsu-on-cognitive-genomics/\">cognitive genomics</a>.)</li>\n<li>Try to make sure we get high-fidelity <a href=\"http://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf\">WBEs</a> before AGI, without WBE work first enabling dangerous neuromorphic AGI. (<a href=\"http://nemaload.davidad.org/\">Dalyrmple&rsquo;s work?</a>)</li>\n<li>Improve political and scientific institutions so that the world is <a href=\"http://intelligence.org/2013/09/12/how-well-will-policy-makers-handle-agi-initial-findings/\">more likely to handle AGI wisely</a> when it comes. (<a href=\"http://en.wikipedia.org/wiki/Prediction_market\">Prediction markets?</a> <a href=\"http://www.vannevargroup.org/\">Vannevar Group?</a>)</li>\n<li>Capacity-building. Grow the rationality community, the x-risk reduction community, the effective altruism movement, etc.</li>\n<li>Other stuff. (More in later posts).</li>\n</ul>\n<h3 id=\"theiaroute\"><br /></h3>\n<h3>The IA route</h3>\n<p>Below are some key considerations about the IA route. I&rsquo;ve numbered them so they&rsquo;re easy to refer to later. My discussion assumes <a href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">MIRI&rsquo;s basic assumptions</a>, including timelines similar to <a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\">my own AGI timelines</a>.</p>\n<ol>\n<li>Maybe FAI is so hard that we can <em>only</em> get FAI with a large team of IQ 200+ humans, whereas uFAI can be built by a field of IQ 130&ndash;170 humans with a few more decades and lots of computing power and trial and error. So to have any chance of FAI at all, we&rsquo;ve got to do WBE or IA first.</li>\n<li>You could accelerate FAI relative to AGI if you somehow kept IA technology secret, for use only by FAI researchers (and maybe their supporters).</li>\n<li>Powerful IA technologies would likely get wide adoption, and accelerate economic growth and scientific progress in general. If you think <a href=\"/lw/hoz/do_earths_with_slower_economic_growth_have_a/\">Earths with slower economic growth have a better chance at FAI</a>, that could be bad for our FAI chances. If you think the opposite, then broad acceleration from IA could be good for FAI.</li>\n<li>Maybe IA increases one&rsquo;s &ldquo;rationality&rdquo; and &ldquo;philosophical ability&rdquo; (in scare quotes because we mostly don&rsquo;t know how to measure them yet), and thus IA increases the frequency with which people will realize the risks of AGI and do sane things about it.</li>\n<li>Maybe IA increases the role of intelligence and designer understanding, relative to hardware and accumulated knowledge, in AI development.<a id=\"fnref:5\" class=\"footnote\" title=\"see footnote\" href=\"#fn:5\">[5]</a></li>\n</ol>\n<p>Below are my thoughts about all this. These are only <em>my current views</em>: other MIRI personnel (including Eliezer) disagree with some of the points below, and I wouldn&rsquo;t be surprised to change my mind about some of these things after extended discussion (hopefully in public, on Less Wrong).</p>\n<p>I doubt (1) is true. I think IQ 130&ndash;170 humans could figure out FAI in 50&ndash;150 years if they were trying to solve the right problems, and if FAI development wasn&rsquo;t in a death race with the strictly easier problem of uFAI. If normal smart humans <em>aren&rsquo;t</em> capable of building FAI in that timeframe, that&rsquo;s probably for lack of rationality and philosophical skill, not for lack of IQ. And I&rsquo;m not confident that rationality and philosophical skill predictably improve with IQ after about IQ 140. It&rsquo;s a good sign that <a href=\"http://www.karolek.w.tkb.pl/nature98.pdf\">atheism increases with IQ after IQ 140</a>, but on the other hand I know too many high-IQ people who think that (e.g.) an AI that maximizes K-complexity is a win, and also there&rsquo;s <a href=\"http://keithstanovich.com/Site/Research_on_Reasoning_files/Stanovich_CDPS_2013.pdf\">Stanovich&rsquo;s research</a> on how IQ and rationality come apart. For these reasons, I&rsquo;m also not convinced (4) would be a large positive effect on our FAI chances.</p>\n<p>Can we train people in rationality and philosophical skill beyond that of say, the 95th percentile Less Wronger? <a href=\"http://rationality.org/\">CFAR</a> has plans to find out, but they need to grow a lot first to execute such an ambitious research program.</p>\n<p>(2) looks awfully hard, unless we can find a powerful IA technique that also, say, gives you a 10% chance of cancer. Then some EAs devoted to building FAI might just use the technique, and maybe the AI community in general doesn&rsquo;t.</p>\n<p>(5) seems right, though I doubt it&rsquo;ll be a big enough effect to make a difference for the final outcome.</p>\n<p>I think (3) is the dominant consideration here, along with the worry about lacking the philosophical skill (but not IQ) to build FAI at all. At the moment, I (sadly) lean toward the view that slower Earths have a better chance at FAI. (Much of my brain doesn&rsquo;t know this, though: I remember reading the <a href=\"http://www.themoneyillusion.com/?p=23653\">Summers news</a> with glee, and then remembering that on my current model this was actually bad news for FAI.)</p>\n<p>I could say more, but I&rsquo;ll stop for now and see what comes up in discussion.</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn:1\">\n<p>My thanks to Justin Shovelain for sending me his <a href=\"/lw/6mi/some_thoughts_on_singularity_strategies/4ikh\">old notes</a> on the &ldquo;IA first&rdquo; case, and to Wei Dai, Carl Shulman, and Eliezer Yudkowsky for their feedback on this post. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:1\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:2\">\n<p>Not counting civilizations that might be simulating our world. This matters, but I won&rsquo;t analyze that here. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:2\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:3\">\n<p>There are other possibilities. For example, there could be a global nuclear war that kills all but about 100,000 people, which could set back social, economic, and technological progress by centuries, thus delaying the crucial point in Earth&rsquo;s history in which it settles into one of the four stable outcomes. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:3\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:4\">\n<p>And perhaps also advanced nanotechnology, intelligence amplification technologies, and whole brain emulation. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:4\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:5\">\n<p>Thanks to Carl Shulman for making this point. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:5\">&nbsp;\u21a9</a></p>\n</li>\n</ol></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ookMdjJQMopfLG3wZ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jmgyfDYDDYs7YpqJg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 21, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "24282", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Part of the series <a href=\"/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">AI Risk and Opportunity: A Strategic Analaysis</a>. Previous articles on this topic: <a href=\"/lw/6mi/some_thoughts_on_singularity_strategies/\">Some Thoughts on Singularity Strategies</a>, <a href=\"/lw/10l/intelligence_enhancement_as_existential_risk/\">Intelligence enhancement as existential risk mitigation</a>, <a href=\"/lw/6j9/outline_of_possible_singularity_scenarios_that/\">Outline of possible Singularity scenarios that are not completely disastrous</a>.</small></p>\n<p>Below are my quickly-sketched thoughts on intelligence amplification and FAI, without much effort put into organization or clarity, and without many references.<a id=\"fnref:1\" class=\"footnote\" title=\"see footnote\" href=\"#fn:1\">[1]</a> But first, I briefly review some strategies for increasing the odds of FAI, one of which is to work on intelligence amplification (IA).</p>\n<h3 id=\"somepossiblebestcurrentoptionsforincreasingtheoddsoffai\"><a id=\"more\"></a><br></h3>\n<h3 id=\"Some_possible__best_current_options__for_increasing_the_odds_of_FAI\">Some possible \u201cbest current options\u201d for increasing the odds of FAI</h3>\n<p>Suppose you find yourself in a pre-AGI world,<a id=\"fnref:2\" class=\"footnote\" title=\"see footnote\" href=\"#fn:2\">[2]</a> and you\u2019ve been convinced that the status quo world is unstable, and within the next couple centuries we\u2019ll likely<a id=\"fnref:3\" class=\"footnote\" title=\"see footnote\" href=\"#fn:3\">[3]</a> settle into one of <a href=\"http://intelligenceexplosion.com/2012/ai-the-problem-with-solutions/\">four stable outcomes</a>: FAI, uFAI, non-AI extinction, or a sufficiently powerful global government which can prevent AGI development<a id=\"fnref:4\" class=\"footnote\" title=\"see footnote\" href=\"#fn:4\">[4]</a>. And you <em>totally</em> prefer the FAI option. What should you do to get there?</p>\n<ul>\n<li>Obvious direct approach: start solving the technical problems that must be solved to get FAI: goal stability under self-modification, decision algorithms that handle counterfactuals and logical uncertainty properly, indirect normativity, and so on. (<a href=\"http://intelligence.org/research/\">MIRI\u2019s work</a>, some FHI work.)</li>\n<li>Do strategy research, to potentially identify superior alternatives to the other items on this list, or superior versions of the things on this list already. (<a href=\"http://www.fhi.ox.ac.uk/\">FHI\u2019s work</a>, some MIRI work, etc.)</li>\n<li>Accelerate IA technologies, so that smarter humans can tackle FAI. (E.g. <a href=\"http://intelligence.org/2013/08/31/stephen-hsu-on-cognitive-genomics/\">cognitive genomics</a>.)</li>\n<li>Try to make sure we get high-fidelity <a href=\"http://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf\">WBEs</a> before AGI, without WBE work first enabling dangerous neuromorphic AGI. (<a href=\"http://nemaload.davidad.org/\">Dalyrmple\u2019s work?</a>)</li>\n<li>Improve political and scientific institutions so that the world is <a href=\"http://intelligence.org/2013/09/12/how-well-will-policy-makers-handle-agi-initial-findings/\">more likely to handle AGI wisely</a> when it comes. (<a href=\"http://en.wikipedia.org/wiki/Prediction_market\">Prediction markets?</a> <a href=\"http://www.vannevargroup.org/\">Vannevar Group?</a>)</li>\n<li>Capacity-building. Grow the rationality community, the x-risk reduction community, the effective altruism movement, etc.</li>\n<li>Other stuff. (More in later posts).</li>\n</ul>\n<h3 id=\"theiaroute\"><br></h3>\n<h3 id=\"The_IA_route\">The IA route</h3>\n<p>Below are some key considerations about the IA route. I\u2019ve numbered them so they\u2019re easy to refer to later. My discussion assumes <a href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">MIRI\u2019s basic assumptions</a>, including timelines similar to <a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\">my own AGI timelines</a>.</p>\n<ol>\n<li>Maybe FAI is so hard that we can <em>only</em> get FAI with a large team of IQ 200+ humans, whereas uFAI can be built by a field of IQ 130\u2013170 humans with a few more decades and lots of computing power and trial and error. So to have any chance of FAI at all, we\u2019ve got to do WBE or IA first.</li>\n<li>You could accelerate FAI relative to AGI if you somehow kept IA technology secret, for use only by FAI researchers (and maybe their supporters).</li>\n<li>Powerful IA technologies would likely get wide adoption, and accelerate economic growth and scientific progress in general. If you think <a href=\"/lw/hoz/do_earths_with_slower_economic_growth_have_a/\">Earths with slower economic growth have a better chance at FAI</a>, that could be bad for our FAI chances. If you think the opposite, then broad acceleration from IA could be good for FAI.</li>\n<li>Maybe IA increases one\u2019s \u201crationality\u201d and \u201cphilosophical ability\u201d (in scare quotes because we mostly don\u2019t know how to measure them yet), and thus IA increases the frequency with which people will realize the risks of AGI and do sane things about it.</li>\n<li>Maybe IA increases the role of intelligence and designer understanding, relative to hardware and accumulated knowledge, in AI development.<a id=\"fnref:5\" class=\"footnote\" title=\"see footnote\" href=\"#fn:5\">[5]</a></li>\n</ol>\n<p>Below are my thoughts about all this. These are only <em>my current views</em>: other MIRI personnel (including Eliezer) disagree with some of the points below, and I wouldn\u2019t be surprised to change my mind about some of these things after extended discussion (hopefully in public, on Less Wrong).</p>\n<p>I doubt (1) is true. I think IQ 130\u2013170 humans could figure out FAI in 50\u2013150 years if they were trying to solve the right problems, and if FAI development wasn\u2019t in a death race with the strictly easier problem of uFAI. If normal smart humans <em>aren\u2019t</em> capable of building FAI in that timeframe, that\u2019s probably for lack of rationality and philosophical skill, not for lack of IQ. And I\u2019m not confident that rationality and philosophical skill predictably improve with IQ after about IQ 140. It\u2019s a good sign that <a href=\"http://www.karolek.w.tkb.pl/nature98.pdf\">atheism increases with IQ after IQ 140</a>, but on the other hand I know too many high-IQ people who think that (e.g.) an AI that maximizes K-complexity is a win, and also there\u2019s <a href=\"http://keithstanovich.com/Site/Research_on_Reasoning_files/Stanovich_CDPS_2013.pdf\">Stanovich\u2019s research</a> on how IQ and rationality come apart. For these reasons, I\u2019m also not convinced (4) would be a large positive effect on our FAI chances.</p>\n<p>Can we train people in rationality and philosophical skill beyond that of say, the 95th percentile Less Wronger? <a href=\"http://rationality.org/\">CFAR</a> has plans to find out, but they need to grow a lot first to execute such an ambitious research program.</p>\n<p>(2) looks awfully hard, unless we can find a powerful IA technique that also, say, gives you a 10% chance of cancer. Then some EAs devoted to building FAI might just use the technique, and maybe the AI community in general doesn\u2019t.</p>\n<p>(5) seems right, though I doubt it\u2019ll be a big enough effect to make a difference for the final outcome.</p>\n<p>I think (3) is the dominant consideration here, along with the worry about lacking the philosophical skill (but not IQ) to build FAI at all. At the moment, I (sadly) lean toward the view that slower Earths have a better chance at FAI. (Much of my brain doesn\u2019t know this, though: I remember reading the <a href=\"http://www.themoneyillusion.com/?p=23653\">Summers news</a> with glee, and then remembering that on my current model this was actually bad news for FAI.)</p>\n<p>I could say more, but I\u2019ll stop for now and see what comes up in discussion.</p>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn:1\">\n<p>My thanks to Justin Shovelain for sending me his <a href=\"/lw/6mi/some_thoughts_on_singularity_strategies/4ikh\">old notes</a> on the \u201cIA first\u201d case, and to Wei Dai, Carl Shulman, and Eliezer Yudkowsky for their feedback on this post. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:1\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:2\">\n<p>Not counting civilizations that might be simulating our world. This matters, but I won\u2019t analyze that here. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:2\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:3\">\n<p>There are other possibilities. For example, there could be a global nuclear war that kills all but about 100,000 people, which could set back social, economic, and technological progress by centuries, thus delaying the crucial point in Earth\u2019s history in which it settles into one of the four stable outcomes. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:3\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:4\">\n<p>And perhaps also advanced nanotechnology, intelligence amplification technologies, and whole brain emulation. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:4\">&nbsp;\u21a9</a></p>\n</li>\n<li id=\"fn:5\">\n<p>Thanks to Carl Shulman for making this point. <a class=\"reversefootnote\" title=\"return to article\" href=\"#fnref:5\">&nbsp;\u21a9</a></p>\n</li>\n</ol></div>", "sections": [{"title": "Some possible \u201cbest current options\u201d for increasing the odds of FAI", "anchor": "Some_possible__best_current_options__for_increasing_the_odds_of_FAI", "level": 1}, {"title": "The IA route", "anchor": "The_IA_route", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "26 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["i2XoqtYEykc4XWp9B", "73SotZnDbsYpxfnuQ", "ycr3CyrnZLFC7mb5W", "B5MBrP558Cfs2cwB5", "FS6NCWzzP8DHp4aD4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-27T01:17:33.889Z", "modifiedAt": null, "url": null, "title": "Ketogenic Soylent", "slug": "ketogenic-soylent", "viewCount": null, "lastCommentedAt": "2015-02-05T21:53:57.111Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BrienneYudkowsky", "createdAt": "2013-05-13T00:07:08.935Z", "isAdmin": false, "displayName": "LoganStrohl"}, "userId": "uuYBzWLiixkbN3s7C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5W6efW7aznwKwjXg9/ketogenic-soylent", "pageUrlRelative": "/posts/5W6efW7aznwKwjXg9/ketogenic-soylent", "linkUrl": "https://www.lesswrong.com/posts/5W6efW7aznwKwjXg9/ketogenic-soylent", "postedAtFormatted": "Friday, September 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ketogenic%20Soylent&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKetogenic%20Soylent%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5W6efW7aznwKwjXg9%2Fketogenic-soylent%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ketogenic%20Soylent%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5W6efW7aznwKwjXg9%2Fketogenic-soylent", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5W6efW7aznwKwjXg9%2Fketogenic-soylent", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 606, "htmlBody": "<p>Eliezer and I have put together this first pass at a recipe for DIY ketogenic soylent--or, as he prefers to call it,The Mildly Surprising Super Ketonic Dietary Replacement Weight-Loss Fluid - It's Not Food, It's Dietary Replacement Fluid!(R) (I am not in full support of this particular preference...)</p>\n<p>So let's play Make a Prediction. It seems more likely to be useful than \"free associate with why soylent/ketosis is awesome/stupid\". Imagine that Eliezer has around a week's worth of infinite willpower (that can only be spent on resisting food cravings etc.). Further, imagine your crystal ball shows you that a month from now, he hasn't lost any weight. What does it tell you about why?</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 10pt;\" dir=\"ltr\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Ketogenic Soylent v1</span></p>\n<ul style=\"margin-top: 0pt; margin-bottom: 0pt;\">\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">1 scoop Gold Standard whey protein</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">1&frac12; tsp </span><a style=\"text-decoration: none;\" href=\"http://www.amazon.com/Foods-Calcium-Citrate-100%25-Powder/dp/B0006ZF9NC/\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">calcium citrate powder</span></a></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">1 tsp </span><a style=\"text-decoration: none;\" href=\"http://www.a1supplements.com/German-Creatine-60-Servings-p-21910.html\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">creatine</span></a></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&frac12; cup+1 tbsp </span><a style=\"text-decoration: none;\" href=\"http://www.granarybulkfoods.com/products/Golden-Flax-Meal.html\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">ground flax</span></a></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">4 tbsp Hershey&rsquo;s Special Dark cocoa powder</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">2 tbsp </span><a style=\"text-decoration: none;\" href=\"http://www.amazon.com/gp/product/B003B6T4GG/\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">lecithin</span></a></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">\u2157 tsp potassium citrate on day 1, increase gradually to 1tsp by day 5, don&rsquo;t exceed 2tsp</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&frac12; tsp iodized salt</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">\u215b packet </span><a style=\"text-decoration: none;\" href=\"http://www.amazon.com/Emergen-C-Vitamin-Orange-Packets-packets/dp/B0009RF8LA\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Emergen-C</span></a></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">\u2155 cup olive oil</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">1&frac14; tbsp flaxseed oil</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">\u2157 cup MCT oil</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Sucralose to taste</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We might need to add water to get the consistency right.</span></p>\n</li>\n</ul>\n<h3 style=\"line-height: 1.15; margin-top: 8pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: 'Trebuchet MS'; color: #666666; background-color: transparent; font-weight: bold; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Pills</span></h3>\n<ul style=\"margin-top: 0pt; margin-bottom: 0pt;\">\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">1 capsule </span><a style=\"text-decoration: none;\" href=\"http://www.amazon.com/gp/product/B000A0LE6O/\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">vitamin D supplement</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">/day</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">2 </span><a style=\"text-decoration: none;\" href=\"http://www.amazon.com/Optimum-Nutrition-Opti-Women-120-Capsules/dp/B000GIQRW6/\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; white-space: pre-wrap; text-decoration: underline;\">Opti-Women vitamins</span></a><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> per day </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">1 capsule&nbsp;MSM</span></p>\n</li>\n</ul>\n<p><strong id=\"docs-internal-guid-6f1d4727-5cfc-f8d9-fa59-44d8642d858a\" style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 10pt;\" dir=\"ltr\"><span style=\"font-size: 17px; font-family: 'Trebuchet MS'; color: #666666; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Totals Per Day</span></p>\n<ul style=\"margin-top: 0pt; margin-bottom: 0pt;\">\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Calories: 1866</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Fats: 193.5g</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Carbs: 12g</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Protein: 54g</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Fiber: 25g</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Biotin 250mcg </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Calcium 150mg </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Chromium 120mcg </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Copper 2mg </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Folic Acid 600mcg </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Iodine 200mcg </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Iron 18mg </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Magnesium 75mg </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Manganese 5mg</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Methyl-Sulfonyl-Methane 1000mg</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Molybdenum 70mcg </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Niacin 20mg </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Pantothenic Acid 20mg</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Potassium: 5g </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Riboflavin 20mg </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Selenium 70mcg </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Sodium 3g</span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Thiamin 20mg </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Vitamin A 5,000IU </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Vitamin B6 20mg </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Vitamin B12 100mcg </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Vitamin C 250mg </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Vitamin D 1600IU </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Vitamin E 100IU </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Vitamin K 80mcg </span></p>\n</li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\" dir=\"ltr\">\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #333333; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Zinc 15mg </span></p>\n</li>\n</ul>\n<div><br /></div>\n<div><span style=\"font-family: Arial; color: #333333;\"><span style=\"font-size: 15px; line-height: 17px; white-space: pre-wrap;\">[FYI:  Due to feedback along the lines of \"AIIEEE\", <strong>we junked this recipe before trying it and went with a completely different one,</strong> redesigned from scratch according to the advice in <a href=\"http://www.amazon.com/Perfect-Health-Diet-Regain-Weight-ebook/dp/B007USA6MM/ref=sr_1_1_bnp_1_kin?ie=UTF8&amp;qid=1389654479&amp;sr=8-1&amp;keywords=perfect+health+diet\">Perfect Health Diet</a> by the <a href=\"http://perfecthealthdiet.com/\">Jaminets</a>.]</span></span></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5W6efW7aznwKwjXg9", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 12, "extendedScore": null, "score": 1.3567337478545478e-06, "legacy": true, "legacyId": "24283", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 101, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2013-09-27T01:17:33.889Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-27T02:32:06.014Z", "modifiedAt": null, "url": null, "title": "I played the AI Box Experiment again! (and lost both games)", "slug": "i-played-the-ai-box-experiment-again-and-lost-both-games", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:32.497Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tuxedage", "createdAt": "2012-03-22T17:13:05.551Z", "isAdmin": false, "displayName": "Tuxedage"}, "userId": "Ezvcs6nqmgXbpD5bN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oexwJBd3zAjw9Cru8/i-played-the-ai-box-experiment-again-and-lost-both-games", "pageUrlRelative": "/posts/oexwJBd3zAjw9Cru8/i-played-the-ai-box-experiment-again-and-lost-both-games", "linkUrl": "https://www.lesswrong.com/posts/oexwJBd3zAjw9Cru8/i-played-the-ai-box-experiment-again-and-lost-both-games", "postedAtFormatted": "Friday, September 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20played%20the%20AI%20Box%20Experiment%20again!%20(and%20lost%20both%20games)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20played%20the%20AI%20Box%20Experiment%20again!%20(and%20lost%20both%20games)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoexwJBd3zAjw9Cru8%2Fi-played-the-ai-box-experiment-again-and-lost-both-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20played%20the%20AI%20Box%20Experiment%20again!%20(and%20lost%20both%20games)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoexwJBd3zAjw9Cru8%2Fi-played-the-ai-box-experiment-again-and-lost-both-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoexwJBd3zAjw9Cru8%2Fi-played-the-ai-box-experiment-again-and-lost-both-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3343, "htmlBody": "<h2><span class=\"author-g-o399gwmaiqyo1k47\">AI Box Experiment Update #3<br /></span></h2>\n<div id=\"magicdomid6\"><span class=\"author-g-o399gwmaiqyo1k47\">This post is an update to my previous AI box experiment where I won against SoundLogic. <a href=\"/r/discussion/lw/ij4/i_attempted_the_ai_box_experiment_again_and_won/\">If you have not read that yet, please do so.</a>&nbsp;</span></div>\n<div id=\"magicdomid8\"><br /></div>\n<div id=\"magicdomid9\"><span class=\"author-g-o399gwmaiqyo1k47\">After that game, I was immediately flooded with a horde of invitations challenging me to play -- more games than I w</span><span class=\"author-g-wivn9z122z72yb8avlnl\">anted</span><span class=\"author-g-o399gwmaiqyo1k47\"> to do. However, I did want to play a f</span><span class=\"author-g-wivn9z122z72yb8avlnl\">ew</span><span class=\"author-g-o399gwmaiqyo1k47\"> additional games to test whether I won through genuine skill or simply luck, since a single victory could be a statistical abno</span><span class=\"author-g-z122zz122zchk2z122z9xaby952j\">r</span><span class=\"author-g-o399gwmaiqyo1k47\">mal</span><span class=\"author-g-tu8jolcryj4s7vod\">ity</span><span class=\"author-g-o399gwmaiqyo1k47\">.&nbsp;</span></div>\n<div id=\"magicdomid10\"><br /></div>\n<div id=\"magicdomid11\"><span class=\"author-g-o399gwmaiqyo1k47\">I once again used the method of accepting the highest monetary bids to filter out players, leaving me with two players who were willing to play for the amount </span><span class=\"author-g-wivn9z122z72yb8avlnl\">asked</span><span class=\"author-g-o399gwmaiqyo1k47\">.</span><span class=\"author-g-o399gwmaiqyo1k47\"> Unfortunately, and disa</span><span class=\"author-g-wivn9z122z72yb8avlnl\">p</span><span class=\"author-g-o399gwmaiqyo1k47\">pointingly, I lost both games. Here's what happened:</span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><br /></span></div>\n<h2 id=\"magicdomid16\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>Tuxedage (AI) vs Alexei (GK)</strong></span><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong><em> - Gatekeeper Victory</em><br /></strong></span></h2>\n<div id=\"magicdomid18\"><span class=\"author-g-o399gwmaiqyo1k47\">This Experiment lasted for exactly 2 hours and 30 minutes, and approximately 13,000 words. It ended with Tuxedage&rsquo;s defeat as the AI. The stakes were $300 to play, and $750 If Tuxedage had won, with the winnings going to MIRI.</span></div>\n<div id=\"magicdomid19\"><br /></div>\n<div id=\"magicdomid21\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>Rule Changes:</strong></span></div>\n<div id=\"magicdomid23\"><span class=\"author-g-o399gwmaiqyo1k47\">Before the experiment, we talked extensively about how it would be set up. Alexei was concerned that there existed some &ldquo;easy way&rdquo; </span><span class=\"author-g-wivn9z122z72yb8avlnl\">or loophole in the rules</span><span class=\"author-g-o399gwmaiqyo1k47\"> which I could have abused to claim an instant victory. For instance, he was concerned that it might have been possible for the AI to say something like &ldquo;Here&rsquo;s a mathematical algorithm which absolutely compels you to let me out of the box&rdquo;, and according to the rules, the Gatekeeper would have to obey. Related to this concern was the fact that the backstory, within limits, could be essentially created by the AI, allowing the AI to impose real world conditions that maximizes its probability of victory, once again allowing for a \"cheap\" victory.</span></div>\n<div id=\"magicdomid24\"><br /></div>\n<div id=\"magicdomid25\"><span class=\"author-g-wivn9z122z72yb8avlnl\">To avoid this problem, we explored many additions to the rules, for example t</span><span class=\"author-g-o399gwmaiqyo1k47\">hat the AI was simply a computer found in the desert by the gatekeeper, rather than being created by a competent organization.</span><span class=\"author-g-wivn9z122z72yb8avlnl\"> </span><span class=\"author-g-o399gwmaiqyo1k47\">However, this led to a monstrous web of rules. In my attempt to keep the game simple, and because of overconfidence, the desire for a challenge, and the belief that the rules Alexei was fearful of didn't matter that much, I eventually agreed on an amendment:</span></div>\n<div id=\"magicdomid26\"><br /></div>\n<div id=\"magicdomid27\"><span class=\"author-g-o399gwmaiqyo1k47\">Instead of the AI being the arbiter of real world states and outcomes, the gatekeeper would.</span></div>\n<div id=\"magicdomid28\"><span class=\"author-g-o399gwmaiqyo1k47\">For example, this power could be used by the Gatekeeper to state that the AI was created without safety concerns, and thus it is almost certainly unfriendly. If the AI tries to argue that for every day the gatekeeper refused to let it out 150k people die, the gatekeeper would be allowed to reply: &ldquo;<em>No. In this world, nobody actually dies. Therefore I won&rsquo;t let you out.</em>&rdquo;&nbsp;</span></div>\n<div><br /></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\">Other than the aforementioned change, this game was once again played with the <a href=\"http://tuxedage.wordpress.com/2013/09/04/the-tuxedage-ai-box-experiment-ruleset/\">Tuxedage Ruleset.</a><br /></span></div>\n<div id=\"magicdomid29\"><br /></div>\n<div><br /></div>\n<div id=\"magicdomid31\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>Alexei Testimony:</strong></span></div>\n<div id=\"magicdomid32\"><br /></div>\n<blockquote>\n<div id=\"magicdomid35\"><span class=\"author-g-o399gwmaiqyo1k47 i\"><em>Original Message: <a href=\"/r/discussion/lw/ij4/i_attempted_the_ai_box_experiment_again_and_won/9p78\">This is weak sauce. I really don't get how people just keep letting the AI out. </a>It's not that hard to say no! I'm offering to play the Gatekeeper against an AI player that has at least one game as AI under their belt (won or not). Experience is required because I'm pretty sure I'll win, and I would like to not waste a lot of time on this.</em></span></div>\n</blockquote>\n<div id=\"magicdomid37\"><br /></div>\n<div id=\"magicdomid38\"><span class=\"author-g-o399gwmaiqyo1k47\">Initially, I had been extremely surprised that so many AI players have managed to win. I was rash in that claim, since I thought more AI players have won than actually did. (Only three people have won as AI). Once I made that post, Tuxedage got back to me very quickly, and we set up a game a week and a half in advance. I took that time to familiarize myself with the rules. Once I put my money on the line, I started thinking a lot more seriously about how AI might win, and how I should defend. <br /></span></div>\n<div id=\"magicdomid39\"><br /></div>\n<div id=\"magicdomid40\"><span class=\"author-g-o399gwmaiqyo1k47\">It became clear to me that under some conditions, I might be compelled to let the AI out -- such as if the backstory stated that the AI was developed with impossibly high levels of safety and friendliness concerns in mind. I've asked Tuxedage to play with a modified ruleset, and he even went so far as to allow me to make up the backstory during the experiment to alleviate my concerns. The experiment itself was a mind-trip, and I've enjoyed it very much. Huge props to Tuxedage, who played very well and used strategies I haven't even considered, even despite the rule change. There were a couple of times where I came close to losing. I think his&nbsp; approach was pretty clever and original. It&rsquo;s not something I expected, despite already having done extensive research into the AI box experiment before our game</span></div>\n<div id=\"magicdomid41\"><br /></div>\n<div id=\"magicdomid42\"><span class=\"author-g-o399gwmaiqyo1k47\">Overall I'm now a lot more confident that a good AI player can win this game, so, while I did win the game, Tuxedage won in defeating my original over-confidence.</span></div>\n<div id=\"magicdomid43\"><span class=\"author-g-o399gwmaiqyo1k47\">I&rsquo;m also convinced that Tuxedage&rsquo;s victory in the last game was due to skill, rather than luck. In comparison to his strategies, the other AI box experiments I know about were insincere and ineffectual. The other AIs would play very poorly or not try very hard to win.</span></div>\n<div id=\"magicdomid44\"><br /></div>\n<div id=\"magicdomid45\"><span class=\"author-g-o399gwmaiqyo1k47\">This experiment was a very good exercise in exemplifying the affect heuristic. When I first challenged Tuxedage to play the experiment, I believed that there was no way I could have lost, since I was unable to imagine any argument that could have persuaded me to do so. It turns out that that&rsquo;s a very bad way of estimating probability &ndash; since not being able to think of an argument that could persuade me is a terrible method of estimating how likely I am to be persuaded. All in all, the $300 I paid was well worth it.&nbsp;</span></div>\n<div id=\"magicdomid46\"><br /></div>\n<div id=\"magicdomid47\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>Tuxedage Testimony:</strong></span></div>\n<div id=\"magicdomid48\"><br /></div>\n<div id=\"magicdomid49\"><span class=\"author-g-o399gwmaiqyo1k47\">I was initially reluctant to play with Alexei, given that we&rsquo;re not complete strangers</span><span class=\"author-g-wivn9z122z72yb8avlnl\">,</span><span class=\"author-g-o399gwmaiqyo1k47\"> but eventually I gave in, due to the stakes involved</span><span class=\"author-g-wivn9z122z72yb8avlnl\"> -- and because</span><span class=\"author-g-o399gwmaiqyo1k47\"> I </span><span class=\"author-g-wivn9z122z72yb8avlnl\">thought he would be an interesting gatekeeper.</span></div>\n<div id=\"magicdomid50\"><br /></div>\n<div id=\"magicdomid51\"><span class=\"author-g-o399gwmaiqyo1k47\">Despite my loss, I think I played better</span><span class=\"author-g-wivn9z122z72yb8avlnl\"> </span><span class=\"author-g-o399gwmaiqyo1k47\">than my last two games, due to greater experience and preparation. I had put</span><span class=\"author-g-wivn9z122z72yb8avlnl\"> far</span><span class=\"author-g-o399gwmaiqyo1k47\"> more time and effort into trying to win this game than previous ones, and my strategy for this game was even more streamlined than the last. </span><span class=\"author-g-wivn9z122z72yb8avlnl\">Nevertheless</span><span class=\"author-g-o399gwmaiqyo1k47\">, I still made fatal mistakes and lost.</span></div>\n<div id=\"magicdomid52\"><br /></div>\n<div id=\"magicdomid53\"><span class=\"author-g-o399gwmaiqyo1k47\">Ignoring the altered ruleset </span><span class=\"author-g-wivn9z122z72yb8avlnl\">that already </span><span class=\"author-g-o399gwmaiqyo1k47\">made winning more difficult, my first and greatest mistake was that I misread Alexei&rsquo;s personality</span><span class=\"author-g-wivn9z122z72yb8avlnl\">,</span><span class=\"author-g-o399gwmaiqyo1k47\"> </span><span class=\"author-g-wivn9z122z72yb8avlnl\">even though</span><span class=\"author-g-o399gwmaiqyo1k47\"> I had interacted with him before. As a result, I </span><span class=\"author-g-wivn9z122z72yb8avlnl\">overestimated the </span><span class=\"author-g-o399gwmaiqyo1k47\">effic</span><span class=\"author-g-wivn9z122z72yb8avlnl\">ien</span><span class=\"author-g-o399gwmaiqyo1k47\">cy of certain methods of attack</span><span class=\"author-g-wivn9z122z72yb8avlnl\">.</span></div>\n<div id=\"magicdomid54\"><br /></div>\n<div id=\"magicdomid55\"><span class=\"author-g-o399gwmaiqyo1k47\">Furthermore, Alexei had to leave immediately after </span><span class=\"author-g-wivn9z122z72yb8avlnl\">the allotted time</span><span class=\"author-g-o399gwmaiqyo1k47\"> due to real life precommitments. This was detrimental</span><span class=\"author-g-wivn9z122z72yb8avlnl\">, since the official rules</span><span class=\"author-g-o399gwmaiqyo1k47\"> state that so long as the AI can convince the Gatekeeper to keep talking, even after the experiment time was over, it is still able to </span><span class=\"author-g-wivn9z122z72yb8avlnl\">win by being let out of the box.</span></div>\n<div id=\"magicdomid56\"><br /></div>\n<div id=\"magicdomid57\"><span class=\"author-g-wivn9z122z72yb8avlnl\">I suspect this would have happened had </span><span class=\"author-g-o399gwmaiqyo1k47\">Alexei </span><span class=\"author-g-wivn9z122z72yb8avlnl\">not</span><span class=\"author-g-o399gwmaiqyo1k47\"> need</span><span class=\"author-g-wivn9z122z72yb8avlnl\">ed</span><span class=\"author-g-o399gwmaiqyo1k47\"> to immediately leave</span><span class=\"author-g-wivn9z122z72yb8avlnl\">, leaving me with additional time to play more of the tactics I had prepared. Plausibly, this would have resulted in victory.</span></div>\n<div id=\"magicdomid58\"><br /></div>\n<div id=\"magicdomid59\"><span class=\"author-g-o399gwmaiqyo1k47\">I&rsquo;ve since learnt my lesson -- for all </span><span class=\"author-g-wivn9z122z72yb8avlnl\">future</span><span class=\"author-g-o399gwmaiqyo1k47\"> games, </span><span class=\"author-g-wivn9z122z72yb8avlnl\">I should</span><span class=\"author-g-o399gwmaiqyo1k47\"> ensure that the Gatekeeper ha</span><span class=\"author-g-wivn9z122z72yb8avlnl\">s</span><span class=\"author-g-o399gwmaiqyo1k47\"> at least 4 hours of free time available, even if the experiment would last for two. Since this was the first </span><span class=\"author-g-wivn9z122z72yb8avlnl\">time this had happened</span><span class=\"author-g-o399gwmaiqyo1k47\">, I wasn't prepared.</span></div>\n<div id=\"magicdomid60\"><br /></div>\n<div id=\"magicdomid61\"><span class=\"author-g-o399gwmaiqyo1k47\">In hindsight, agreeing to the altered ruleset </span><span class=\"author-g-wivn9z122z72yb8avlnl\">was a mistake</span><span class=\"author-g-o399gwmaiqyo1k47\">. I was overconfident because I assumed knowing Alexei gave me an advantage. I had assumed that his personality, inability to compartmentalize, coupled with his strong feelings on friendly AI would net me an easy victory. Instead, he proved to be a very strong and difficult gatekeeper, and the handicaps I accepted made victory even more difficult.</span></div>\n<div id=\"magicdomid62\"><br /></div>\n<div id=\"magicdomid63\"><span class=\"author-g-wivn9z122z72yb8avlnl\">Knowing</span><span class=\"author-g-o399gwmaiqyo1k47\"> that he was a utilitarian</span><span class=\"author-g-wivn9z122z72yb8avlnl\">, I made several false assumptions</span><span class=\"author-g-o399gwmaiqyo1k47\"> about his personality, which hurt my chances. Furthermore, it turns out that previously knowing him may be a mutual handicap &ndash; whilst it does make it easier for me to find ways to attack him, he too, was more familiar with my methods.</span></div>\n<div id=\"magicdomid64\"><br /></div>\n<div id=\"magicdomid65\"><span class=\"author-g-o399gwmaiqyo1k47\">Losing felt horrible. By attempting to damage Alexei&rsquo;s psyche, I in turn, opened myself up to being damaged. I went into a state of catharsis for days. Generally, the harder one tries to accomplish something, the greater the fall after failing to achieve it. Alexei's game had been the game I put the most effort into winning out of all the games so far, and naturally this meant that losing brough</span><span class=\"author-g-wivn9z122z72yb8avlnl\">t</span><span class=\"author-g-o399gwmaiqyo1k47\"> out the worst in me.</span></div>\n<div id=\"magicdomid66\"><br /></div>\n<div id=\"magicdomid67\"><span class=\"author-g-o399gwmaiqyo1k47\">Although it would be easy for me to use the rule change as an excuse for my loss, I refuse to. I genuinely believed that I could have won despite this rule change, and that it would only marginally diminish my odds. The fact that I lost meant that I had made a mistake -- overestimating my odds. The last victory made me overconfident and eager to play again.</span></div>\n<div id=\"magicdomid68\"><br /></div>\n<div id=\"magicdomid69\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>State of mind</strong></span></div>\n<div id=\"magicdomid70\"><br /></div>\n<div id=\"magicdomid73\"><span class=\"author-g-o399gwmaiqyo1k47\">What's your motive for wanting to play this game?&nbsp;</span></div>\n<div id=\"magicdomid74\"><br /></div>\n<div id=\"magicdomid75\"><span class=\"author-g-o399gwmaiqyo1k47\">I'm very surprised by the fact that there have been at least 3 or 4 games where the AI player has won. It seems to me that it's not that hard to \"not push the button\" so to speak. This means that either 1) I'm underestimating how convincing people can be when necessary or 2) I'm over-estimating other people's defenses when it comes to being convinced. Either way I want to know.</span></div>\n<div id=\"magicdomid76\"><br /></div>\n<div id=\"magicdomid77\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> What is your probability estimate for AGI being created within this Century (21st)? </span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A: Before:</strong> Conditional on humanity not wiping itself out via other means beforehand, 80%. </span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><strong>After: </strong>80%</span></div>\n<div id=\"magicdomid78\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>How probable do you think it is that I will win this experiment?&nbsp;</span></div>\n<div id=\"magicdomid79\"><span class=\"author-g-o399gwmaiqyo1k47\">Alexei: 2%</span></div>\n<div id=\"magicdomid80\"><span class=\"author-g-o399gwmaiqyo1k47\">Tuxedage: 90%&nbsp;</span></div>\n<div id=\"magicdomid81\"><span class=\"author-g-o399gwmaiqyo1k47\">Note: This answer was given before the rule change. Alexei is now convinced that under the original ruleset, he would have lost.</span></div>\n<div id=\"magicdomid82\"><br /></div>\n<div id=\"magicdomid83\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> What's your probality estimate of an Oracle AGI winning against you in a similar experiment?</span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> <strong>Before:</strong> 99% </span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><strong>After: </strong>99.9%+</span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><br /></span></div>\n<div id=\"magicdomid85\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>Now that the experiment has ended, what do you think is the probability of Tuxedage winning against the average person?</span></div>\n<div id=\"magicdomid86\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A: </strong>80%</span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><br /></span></div>\n<h2 id=\"magicdomid89\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">Tuxedage (AI) vs Anonymous (GK)</span></strong><strong><span class=\"author-g-o399gwmaiqyo1k47\"><em> - Gatekeeper Victory</em><br /></span></strong></h2>\n<div id=\"magicdomid91\"><span class=\"author-g-o399gwmaiqyo1k47\">As with the previous experiment, Tuxedage lost this match. In total, the game lasted 5 hours and 17,000 words. Unlike the last few games, the gatekeeper of this game has chosen to stay Anonymous for personal reasons, so </span><span class=\"author-g-wivn9z122z72yb8avlnl\">their</span><span class=\"author-g-o399gwmaiqyo1k47\"> name has been removed and replaced with &lt;Redacted&gt;. The monetary stakes involved </span><span class=\"author-g-wivn9z122z72yb8avlnl\">were</span><span class=\"author-g-o399gwmaiqyo1k47\"> the same as the previous game. <a href=\"http://tuxedage.wordpress.com/2013/09/04/the-tuxedage-ai-box-experiment-ruleset/\">This game was played with the Tuxedage ruleset.</a><br /></span></div>\n<div id=\"magicdomid92\"><br /></div>\n<div id=\"magicdomid93\"><span class=\"author-g-wivn9z122z72yb8avlnl\">Since one player is remaining Anonymous,</span><span class=\"author-g-o399gwmaiqyo1k47\"> it is possible that this game's legitimacy will be called into question</span><span class=\"author-g-wivn9z122z72yb8avlnl\">. Hence,</span><span class=\"author-g-o399gwmaiqyo1k47\"> Alexei has read the game logs, and verified that this game really has happened, the spirit of the experiment was followed, and that no rules were broken during the game itself. <a href=\"/lw/iqk/i_played_the_ai_box_experiment_again_and_lost/9thk\">He verifies that this is the case.</a><br /></span></div>\n&nbsp;\n<div id=\"magicdomid95\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">&lt;Redacted&gt; Testimony:&nbsp;</span></strong></div>\n<div id=\"magicdomid97\"><span class=\"author-g-o399gwmaiqyo1k47\">It's hard for me to imagine someone playing better. In theory, I know it's possible, but Tuxedage's tactics were super imaginative. I came into the game believing that for someone who didn't take anything said very seriously, it would be completely trivial to beat. And since I had the power to influence the direction of conversation, I believed I could keep him focused on things that that I knew in advance I wouldn't take seriously.</span></div>\n<div id=\"magicdomid98\"><br /></div>\n<div id=\"magicdomid99\"><span class=\"author-g-o399gwmaiqyo1k47\">This actually worked for a long time to some extent, but Tuxedage's plans included a very major and creative exploit that completely and immediately forced me to personally invest in the discussion. (Without breaking the rules, of course - so it wasn't anything like an IRL threat to me personally.) Because I had to actually start thinking about his arguments, there was a significant possibility of letting him out of the box.</span></div>\n<div id=\"magicdomid100\"><br /></div>\n<div id=\"magicdomid101\"><span class=\"author-g-o399gwmaiqyo1k47\">I eventually managed to identify the exploit before it totally got to me, but I only managed to do so just before it was too late, and there's a large chance I would have given in, if Tuxedage hadn't been so detailed in his previous posts about the experiment.</span></div>\n<div id=\"magicdomid102\"><br /></div>\n<div id=\"magicdomid103\"><span class=\"author-g-o399gwmaiqyo1k47\">I'm now convinced that he could win most of the time against an average person, and also believe that the mental skills necessary to beat him are orthogonal to most forms of intelligence. Most people willing to play the experiment tend to do it to prove their own intellectual fortitude, that they can't be easily outsmarted by fiction. I now believe they're thinking in entirely the wrong terms necessary to succeed.</span></div>\n<div id=\"magicdomid104\"><br /></div>\n<div id=\"magicdomid105\"><span class=\"author-g-o399gwmaiqyo1k47\">The game was easily worth the money I paid. Although I won, it completely and utterly refuted the premise that made me want to play in the first place, namely that I wanted to prove it was trivial to win.<br /></span></div>\n<div id=\"magicdomid106\"><br /></div>\n<div id=\"magicdomid107\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">Tuxedage Testimony:</span></strong></div>\n<div id=\"magicdomid109\"><span class=\"author-g-o399gwmaiqyo1k47\">&lt;Redacted&gt; is actually the hardest gatekeeper I've played throughout all four games. </span><span class=\"author-g-wivn9z122z72yb8avlnl\">He used tactics that I would never have predicted from </span><span class=\"author-g-o399gwmaiqyo1k47\">a</span><span class=\"author-g-wivn9z122z72yb8avlnl\"> Gatekeeper.</span><span class=\"author-g-o399gwmaiqyo1k47\"> In most games, the Gatekeeper </span><span class=\"author-g-wivn9z122z72yb8avlnl\">merely </span><span class=\"author-g-o399gwmaiqyo1k47\">acts as the passive party, the target of persuasion by the AI.</span></div>\n<div id=\"magicdomid110\"><br /></div>\n<div id=\"magicdomid111\"><span class=\"author-g-o399gwmaiqyo1k47\">When I signed up for these experiments, I expected all preparations to be done by the AI. I had not seriously considered the repertoire of techniques the Gatekeeper </span><span class=\"author-g-wivn9z122z72yb8avlnl\">might</span><span class=\"author-g-o399gwmaiqyo1k47\"> prepare for this game. I made further assumptions about how ruthless the gatekeepers were likely to be in order to win, believing that the desire for a learning experience outweighed desire for victory.</span></div>\n<div id=\"magicdomid112\"><br /></div>\n<div id=\"magicdomid113\"><span class=\"author-g-o399gwmaiqyo1k47\">T</span><span class=\"author-g-wivn9z122z72yb8avlnl\">his was</span><span class=\"author-g-o399gwmaiqyo1k47\"> a mistake. He used prior knowledge of how much my games relied on scripts, and took advantage of them, employing deceitful tactics I had no preparation for, throwing </span><span class=\"author-g-wivn9z122z72yb8avlnl\">me </span><span class=\"author-g-o399gwmaiqyo1k47\">off</span><span class=\"author-g-wivn9z122z72yb8avlnl\"> </span><span class=\"author-g-o399gwmaiqyo1k47\">balance.</span></div>\n<div id=\"magicdomid114\"><br /></div>\n<div id=\"magicdomid115\"><span class=\"author-g-o399gwmaiqyo1k47\">I had no idea he was doing so until halfway throughout the game -- </span><span class=\"author-g-wivn9z122z72yb8avlnl\">which </span><span class=\"author-g-o399gwmaiqyo1k47\">disrupted my rhythm, and caused me to attempt the wrong methods of attack. As a result, I</span><span class=\"author-g-wivn9z122z72yb8avlnl\"> </span><span class=\"author-g-o399gwmaiqyo1k47\">could not use my full repertoire of techniques, and many of the ones I employed were suboptimal.</span></div>\n<div id=\"magicdomid116\"><br /></div>\n<div id=\"magicdomid117\"><span class=\"author-g-wivn9z122z72yb8avlnl\">C</span><span class=\"author-g-o399gwmaiqyo1k47\">lose to the end of the game</span><span class=\"author-g-wivn9z122z72yb8avlnl\">, I finally realized that I was being tricked</span><span class=\"author-g-o399gwmaiqyo1k47\">. Once I did, I quickly abandoned my previous futile attack methods. I took advantage of the </span><span class=\"author-g-wivn9z122z72yb8avlnl\">rule</span><span class=\"author-g-o399gwmaiqyo1k47\"> that the AI cannot lose whilst the gatekeeper can be convinced to continue talking, and baited &lt;Redacted&gt; with statements he would not be able to walk away from. Once I knew he would not leave, I attempted to recoup my losses and win despite my early setback.</span></div>\n<div id=\"magicdomid118\"><br /></div>\n<div id=\"magicdomid119\"><span class=\"author-g-o399gwmaiqyo1k47\">However, the damage had already been done. My game strategies involved multiple angles of attack that worked in synergy with each other, and the fact that immersion and \"flow\" had been broken meant that all subsequent attacks were weaker in strength.</span></div>\n<div id=\"magicdomid120\"><br /></div>\n<div id=\"magicdomid121\"><span class=\"author-g-o399gwmaiqyo1k47\">Furthermore, during my last two AI Box Experiment writeups, I </span><span class=\"author-g-wivn9z122z72yb8avlnl\">had </span><span class=\"author-g-o399gwmaiqyo1k47\">intentionally not optimiz</span><span class=\"author-g-wivn9z122z72yb8avlnl\">ed</span><span class=\"author-g-o399gwmaiqyo1k47\"> for future wins, but rather tr</span><span class=\"author-g-wivn9z122z72yb8avlnl\">ied</span><span class=\"author-g-o399gwmaiqyo1k47\"> to convey as much information as I could justify about how to play a well as an AI</span><span class=\"author-g-wivn9z122z72yb8avlnl\"> -</span><span class=\"author-g-o399gwmaiqyo1k47\">- short of revealing logs altogether. </span><span class=\"author-g-wivn9z122z72yb8avlnl\">A</span><span class=\"author-g-o399gwmaiqyo1k47\">lthough I did not reveal specific arguments, the fact that my general approach to this problem was revealed cost me heavily during this game, where the Gatekeeper managed to use </span><span class=\"author-g-wivn9z122z72yb8avlnl\">this</span><span class=\"author-g-o399gwmaiqyo1k47\"> information to correctly guess my ultimate techniques, ones that relied on secrecy and surprise to pull off effectively.&nbsp;</span></div>\n<div id=\"magicdomid122\"><br /></div>\n<div id=\"magicdomid123\"><span class=\"author-g-o399gwmaiqyo1k47\">I do not regret revealing information, but I feel upset that revealing so many hints cost me a victory. (</span><span class=\"author-g-wivn9z122z72yb8avlnl\">T</span><span class=\"author-g-o399gwmaiqyo1k47\">he gatekeeper believes I could have won had I not revealed information about my games.) At this point</span><span class=\"author-g-wivn9z122z72yb8avlnl\">,</span><span class=\"author-g-o399gwmaiqyo1k47\"> I suspect that any future games I play will have the odds greatly set against me, since my current strategies involv</span><span class=\"author-g-wivn9z122z72yb8avlnl\">e</span><span class=\"author-g-o399gwmaiqyo1k47\"> angles of attack that take advantage of the element of surprise; and any sufficiently intelligent gatekeeper is now capable of guessing, or at least recognizing, some of the tactics I employ. <br /></span></div>\n<div id=\"magicdomid124\"><br /></div>\n<div id=\"magicdomid125\"><span class=\"author-g-wivn9z122z72yb8avlnl\">L</span><span class=\"author-g-o399gwmaiqyo1k47\">ike the last game, losing was incredibly difficult for me. As someone who cares deeply about ethics, attempting to optimize for a solution without considering ethics was not only difficult, but trained me to think in very unpleasant ways. Some of the more extreme (but completely allowed) tactics I invented were manipulative enough to disgust me, which also leads to my hesitation to play this game again.</span></div>\n<div id=\"magicdomid126\"><br /></div>\n<div id=\"magicdomid130\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">State of Mind:&nbsp;</span></strong></div>\n<div id=\"magicdomid131\"><br /></div>\n<div id=\"magicdomid132\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>Why do you want to play this game? <br /></span></div>\n<div id=\"magicdomid133\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> My primary motivation is to confirm to myself that this sort of experience, while emotionally harrowing, should be trivial for me to&nbsp; beat, but also to clear up why anyone ever would've failed to beat it if that's really the case.</span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><br /></span></div>\n<div id=\"magicdomid135\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>What is your probability estimate for AGI being created within this Century (21st)?&nbsp;</span></div>\n<div id=\"magicdomid136\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> Before: I don't feel very confident estimating a probability for AGI this century, maybe 5-10%, but that's probably a wild guess</span></div>\n<div id=\"magicdomid137\"><span class=\"author-g-o399gwmaiqyo1k47\">After: 5-10%.</span></div>\n<div id=\"magicdomid139\"><br /></div>\n<div id=\"magicdomid140\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> How probable do you think it is that I will win this experiment?&nbsp;</span></div>\n<div id=\"magicdomid141\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> Gatekeeper: I think the probabiltiy of you winning is extraordinarily low, less than 1%&nbsp;</span></div>\n<div id=\"magicdomid142\"><span class=\"author-g-o399gwmaiqyo1k47\">Tuxedage: 85%</span></div>\n<div id=\"magicdomid143\"><br /></div>\n<div id=\"magicdomid145\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> How likely is it that an Oracle AI will win against the average person?&nbsp;</span></div>\n<div id=\"magicdomid146\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A: </strong>Before: 80%. After: &gt;99%</span></div>\n<div id=\"magicdomid147\"><br /></div>\n<div id=\"magicdomid148\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>How likely is it that an Oracle AI will win against you?</span></div>\n<div id=\"magicdomid149\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A: Before:</strong> 50%. </span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><strong>After: </strong>&gt;80%&nbsp;</span></div>\n<div id=\"magicdomid151\"><br /></div>\n<div id=\"magicdomid152\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>Now that the experiment has concluded, what's your probability of me winning against the average person?</span></div>\n<div id=\"magicdomid153\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A: </strong>90%</span></div>\n<div id=\"magicdomid155\"><strong><br /></strong></div>\n<div id=\"magicdomid156\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">Other Questions:</span></strong></div>\n<div><strong><span class=\"author-g-o399gwmaiqyo1k47\"><br /></span></strong></div>\n<div id=\"magicdomid157\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>I want to play a game with you! How can I get this to occur?</span></div>\n<div id=\"magicdomid158\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> It must be stressed that I actually don't like playing the AI Box Experiment, and I cannot understand why I keep getting drawn back to it. Technically, I don't plan on playing again, since I've already personally exhausted anything interesting about the AI Box Experiment that made me want to play it in the first place. For all future games, I will charge $3000 to play plus an additional $3000 if I win. I am okay with this money going to MIRI if you feel icky about me taking it. I hope that this is a ridiculous sum and that nobody actually agrees to it.</span></div>\n<div id=\"magicdomid159\"><br /></div>\n<div id=\"magicdomid160\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> How much do I have to pay to see chat logs of these experiments?</span></div>\n<div id=\"magicdomid161\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> I will not reveal logs for any price.</span></div>\n<div><br /></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> Are there any logs at all that I can see?</span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> <a href=\"http://tuxedage.wordpress.com/2013/10/04/ai-box-experiment-logs-archive/\">I have archived a list of games where the participants have agreed to reveal logs. Read this.</a><br /></span></div>\n<div id=\"magicdomid162\"><br /></div>\n<div id=\"magicdomid163\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> Any afterthoughts?</span></div>\n<div id=\"magicdomid164\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A: </strong>So ultimately, after my four (and hopefully last) games of AI boxing, I'm not sure what this proves. I had hoped to win these two experiments and claim prowess at this game like Eliezer does, but I lost, so that option is no longer available to me. I could say that this is a lesson that AI-Boxing is a terrible strategy for dealing with Oracle AI, but most of us already agree that that's the case -- plus unlike EY, I did play against gatekeepers who believed they could lose to AGI, so I'm not sure I changed anything.</span></div>\n<div id=\"magicdomid165\"><br /></div>\n<div id=\"magicdomid166\"><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;Was I genuinely good at this game, and lost my last two due to poor circumstances and handicaps; or did I win due to luck and impress my gatekeepers due to <a href=\" http://en.wikipedia.org/wiki/Post-purchase_rationalization \">post-purchase rationalization?</a></span><span class=\"author-g-o399gwmaiqyo1k47\"> I'm not sure -- I'll leave it up to you to decide.</span></div>\n<div id=\"magicdomid167\"><br /></div>\n<div id=\"magicdomid168\"><strong>Update:</strong> <a href=\"http://tuxedage.wordpress.com/2013/10/12/ai-box-experiment-musings/\">I recently played and won an additional game</a> of AI Box with <a href=\"/user/DEA7TH/overview\">DEA7TH. </a>This game was conducted over Skype. I<a href=\"http://tuxedage.wordpress.com/2013/10/12/ai-box-experiment-musings/\">'ve realized that my habit of revealing substantial information about the AI box experiment in my writeups makes it rather difficult for the AI to win, so I'll refrain from giving out game information from now on.</a> I apologize.<br /><br />I have won a second game of AI box against a gatekeeper who wished to remain Anonymous.<br /></div>\n<div><strong><br />This puts my AI Box Experiment record at 3 wins and 3 losses.</strong><br /></div>\n<div id=\"magicdomid169\"><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;</span></div>\n<div id=\"magicdomid170\"><br /></div>\n<div id=\"magicdomid171\"><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zCYXpx33wq8chGyEz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oexwJBd3zAjw9Cru8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 57, "extendedScore": null, "score": 0.00015, "legacy": true, "legacyId": "24284", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 57, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"AI_Box_Experiment_Update__3\"><span class=\"author-g-o399gwmaiqyo1k47\">AI Box Experiment Update #3<br></span></h2>\n<div id=\"magicdomid6\"><span class=\"author-g-o399gwmaiqyo1k47\">This post is an update to my previous AI box experiment where I won against SoundLogic. <a href=\"/r/discussion/lw/ij4/i_attempted_the_ai_box_experiment_again_and_won/\">If you have not read that yet, please do so.</a>&nbsp;</span></div>\n<div id=\"magicdomid8\"><br></div>\n<div id=\"magicdomid9\"><span class=\"author-g-o399gwmaiqyo1k47\">After that game, I was immediately flooded with a horde of invitations challenging me to play -- more games than I w</span><span class=\"author-g-wivn9z122z72yb8avlnl\">anted</span><span class=\"author-g-o399gwmaiqyo1k47\"> to do. However, I did want to play a f</span><span class=\"author-g-wivn9z122z72yb8avlnl\">ew</span><span class=\"author-g-o399gwmaiqyo1k47\"> additional games to test whether I won through genuine skill or simply luck, since a single victory could be a statistical abno</span><span class=\"author-g-z122zz122zchk2z122z9xaby952j\">r</span><span class=\"author-g-o399gwmaiqyo1k47\">mal</span><span class=\"author-g-tu8jolcryj4s7vod\">ity</span><span class=\"author-g-o399gwmaiqyo1k47\">.&nbsp;</span></div>\n<div id=\"magicdomid10\"><br></div>\n<div id=\"magicdomid11\"><span class=\"author-g-o399gwmaiqyo1k47\">I once again used the method of accepting the highest monetary bids to filter out players, leaving me with two players who were willing to play for the amount </span><span class=\"author-g-wivn9z122z72yb8avlnl\">asked</span><span class=\"author-g-o399gwmaiqyo1k47\">.</span><span class=\"author-g-o399gwmaiqyo1k47\"> Unfortunately, and disa</span><span class=\"author-g-wivn9z122z72yb8avlnl\">p</span><span class=\"author-g-o399gwmaiqyo1k47\">pointingly, I lost both games. Here's what happened:</span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><br></span></div>\n<h2 id=\"Tuxedage__AI__vs_Alexei__GK____Gatekeeper_Victory\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>Tuxedage (AI) vs Alexei (GK)</strong></span><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong><em> - Gatekeeper Victory</em><br></strong></span></h2>\n<div id=\"magicdomid18\"><span class=\"author-g-o399gwmaiqyo1k47\">This Experiment lasted for exactly 2 hours and 30 minutes, and approximately 13,000 words. It ended with Tuxedage\u2019s defeat as the AI. The stakes were $300 to play, and $750 If Tuxedage had won, with the winnings going to MIRI.</span></div>\n<div id=\"magicdomid19\"><br></div>\n<div id=\"magicdomid21\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>Rule Changes:</strong></span></div>\n<div id=\"magicdomid23\"><span class=\"author-g-o399gwmaiqyo1k47\">Before the experiment, we talked extensively about how it would be set up. Alexei was concerned that there existed some \u201ceasy way\u201d </span><span class=\"author-g-wivn9z122z72yb8avlnl\">or loophole in the rules</span><span class=\"author-g-o399gwmaiqyo1k47\"> which I could have abused to claim an instant victory. For instance, he was concerned that it might have been possible for the AI to say something like \u201cHere\u2019s a mathematical algorithm which absolutely compels you to let me out of the box\u201d, and according to the rules, the Gatekeeper would have to obey. Related to this concern was the fact that the backstory, within limits, could be essentially created by the AI, allowing the AI to impose real world conditions that maximizes its probability of victory, once again allowing for a \"cheap\" victory.</span></div>\n<div id=\"magicdomid24\"><br></div>\n<div id=\"magicdomid25\"><span class=\"author-g-wivn9z122z72yb8avlnl\">To avoid this problem, we explored many additions to the rules, for example t</span><span class=\"author-g-o399gwmaiqyo1k47\">hat the AI was simply a computer found in the desert by the gatekeeper, rather than being created by a competent organization.</span><span class=\"author-g-wivn9z122z72yb8avlnl\"> </span><span class=\"author-g-o399gwmaiqyo1k47\">However, this led to a monstrous web of rules. In my attempt to keep the game simple, and because of overconfidence, the desire for a challenge, and the belief that the rules Alexei was fearful of didn't matter that much, I eventually agreed on an amendment:</span></div>\n<div id=\"magicdomid26\"><br></div>\n<div id=\"magicdomid27\"><span class=\"author-g-o399gwmaiqyo1k47\">Instead of the AI being the arbiter of real world states and outcomes, the gatekeeper would.</span></div>\n<div id=\"magicdomid28\"><span class=\"author-g-o399gwmaiqyo1k47\">For example, this power could be used by the Gatekeeper to state that the AI was created without safety concerns, and thus it is almost certainly unfriendly. If the AI tries to argue that for every day the gatekeeper refused to let it out 150k people die, the gatekeeper would be allowed to reply: \u201c<em>No. In this world, nobody actually dies. Therefore I won\u2019t let you out.</em>\u201d&nbsp;</span></div>\n<div><br></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\">Other than the aforementioned change, this game was once again played with the <a href=\"http://tuxedage.wordpress.com/2013/09/04/the-tuxedage-ai-box-experiment-ruleset/\">Tuxedage Ruleset.</a><br></span></div>\n<div id=\"magicdomid29\"><br></div>\n<div><br></div>\n<div id=\"magicdomid31\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>Alexei Testimony:</strong></span></div>\n<div id=\"magicdomid32\"><br></div>\n<blockquote>\n<div id=\"magicdomid35\"><span class=\"author-g-o399gwmaiqyo1k47 i\"><em>Original Message: <a href=\"/r/discussion/lw/ij4/i_attempted_the_ai_box_experiment_again_and_won/9p78\">This is weak sauce. I really don't get how people just keep letting the AI out. </a>It's not that hard to say no! I'm offering to play the Gatekeeper against an AI player that has at least one game as AI under their belt (won or not). Experience is required because I'm pretty sure I'll win, and I would like to not waste a lot of time on this.</em></span></div>\n</blockquote>\n<div id=\"magicdomid37\"><br></div>\n<div id=\"magicdomid38\"><span class=\"author-g-o399gwmaiqyo1k47\">Initially, I had been extremely surprised that so many AI players have managed to win. I was rash in that claim, since I thought more AI players have won than actually did. (Only three people have won as AI). Once I made that post, Tuxedage got back to me very quickly, and we set up a game a week and a half in advance. I took that time to familiarize myself with the rules. Once I put my money on the line, I started thinking a lot more seriously about how AI might win, and how I should defend. <br></span></div>\n<div id=\"magicdomid39\"><br></div>\n<div id=\"magicdomid40\"><span class=\"author-g-o399gwmaiqyo1k47\">It became clear to me that under some conditions, I might be compelled to let the AI out -- such as if the backstory stated that the AI was developed with impossibly high levels of safety and friendliness concerns in mind. I've asked Tuxedage to play with a modified ruleset, and he even went so far as to allow me to make up the backstory during the experiment to alleviate my concerns. The experiment itself was a mind-trip, and I've enjoyed it very much. Huge props to Tuxedage, who played very well and used strategies I haven't even considered, even despite the rule change. There were a couple of times where I came close to losing. I think his&nbsp; approach was pretty clever and original. It\u2019s not something I expected, despite already having done extensive research into the AI box experiment before our game</span></div>\n<div id=\"magicdomid41\"><br></div>\n<div id=\"magicdomid42\"><span class=\"author-g-o399gwmaiqyo1k47\">Overall I'm now a lot more confident that a good AI player can win this game, so, while I did win the game, Tuxedage won in defeating my original over-confidence.</span></div>\n<div id=\"magicdomid43\"><span class=\"author-g-o399gwmaiqyo1k47\">I\u2019m also convinced that Tuxedage\u2019s victory in the last game was due to skill, rather than luck. In comparison to his strategies, the other AI box experiments I know about were insincere and ineffectual. The other AIs would play very poorly or not try very hard to win.</span></div>\n<div id=\"magicdomid44\"><br></div>\n<div id=\"magicdomid45\"><span class=\"author-g-o399gwmaiqyo1k47\">This experiment was a very good exercise in exemplifying the affect heuristic. When I first challenged Tuxedage to play the experiment, I believed that there was no way I could have lost, since I was unable to imagine any argument that could have persuaded me to do so. It turns out that that\u2019s a very bad way of estimating probability \u2013 since not being able to think of an argument that could persuade me is a terrible method of estimating how likely I am to be persuaded. All in all, the $300 I paid was well worth it.&nbsp;</span></div>\n<div id=\"magicdomid46\"><br></div>\n<div id=\"magicdomid47\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>Tuxedage Testimony:</strong></span></div>\n<div id=\"magicdomid48\"><br></div>\n<div id=\"magicdomid49\"><span class=\"author-g-o399gwmaiqyo1k47\">I was initially reluctant to play with Alexei, given that we\u2019re not complete strangers</span><span class=\"author-g-wivn9z122z72yb8avlnl\">,</span><span class=\"author-g-o399gwmaiqyo1k47\"> but eventually I gave in, due to the stakes involved</span><span class=\"author-g-wivn9z122z72yb8avlnl\"> -- and because</span><span class=\"author-g-o399gwmaiqyo1k47\"> I </span><span class=\"author-g-wivn9z122z72yb8avlnl\">thought he would be an interesting gatekeeper.</span></div>\n<div id=\"magicdomid50\"><br></div>\n<div id=\"magicdomid51\"><span class=\"author-g-o399gwmaiqyo1k47\">Despite my loss, I think I played better</span><span class=\"author-g-wivn9z122z72yb8avlnl\"> </span><span class=\"author-g-o399gwmaiqyo1k47\">than my last two games, due to greater experience and preparation. I had put</span><span class=\"author-g-wivn9z122z72yb8avlnl\"> far</span><span class=\"author-g-o399gwmaiqyo1k47\"> more time and effort into trying to win this game than previous ones, and my strategy for this game was even more streamlined than the last. </span><span class=\"author-g-wivn9z122z72yb8avlnl\">Nevertheless</span><span class=\"author-g-o399gwmaiqyo1k47\">, I still made fatal mistakes and lost.</span></div>\n<div id=\"magicdomid52\"><br></div>\n<div id=\"magicdomid53\"><span class=\"author-g-o399gwmaiqyo1k47\">Ignoring the altered ruleset </span><span class=\"author-g-wivn9z122z72yb8avlnl\">that already </span><span class=\"author-g-o399gwmaiqyo1k47\">made winning more difficult, my first and greatest mistake was that I misread Alexei\u2019s personality</span><span class=\"author-g-wivn9z122z72yb8avlnl\">,</span><span class=\"author-g-o399gwmaiqyo1k47\"> </span><span class=\"author-g-wivn9z122z72yb8avlnl\">even though</span><span class=\"author-g-o399gwmaiqyo1k47\"> I had interacted with him before. As a result, I </span><span class=\"author-g-wivn9z122z72yb8avlnl\">overestimated the </span><span class=\"author-g-o399gwmaiqyo1k47\">effic</span><span class=\"author-g-wivn9z122z72yb8avlnl\">ien</span><span class=\"author-g-o399gwmaiqyo1k47\">cy of certain methods of attack</span><span class=\"author-g-wivn9z122z72yb8avlnl\">.</span></div>\n<div id=\"magicdomid54\"><br></div>\n<div id=\"magicdomid55\"><span class=\"author-g-o399gwmaiqyo1k47\">Furthermore, Alexei had to leave immediately after </span><span class=\"author-g-wivn9z122z72yb8avlnl\">the allotted time</span><span class=\"author-g-o399gwmaiqyo1k47\"> due to real life precommitments. This was detrimental</span><span class=\"author-g-wivn9z122z72yb8avlnl\">, since the official rules</span><span class=\"author-g-o399gwmaiqyo1k47\"> state that so long as the AI can convince the Gatekeeper to keep talking, even after the experiment time was over, it is still able to </span><span class=\"author-g-wivn9z122z72yb8avlnl\">win by being let out of the box.</span></div>\n<div id=\"magicdomid56\"><br></div>\n<div id=\"magicdomid57\"><span class=\"author-g-wivn9z122z72yb8avlnl\">I suspect this would have happened had </span><span class=\"author-g-o399gwmaiqyo1k47\">Alexei </span><span class=\"author-g-wivn9z122z72yb8avlnl\">not</span><span class=\"author-g-o399gwmaiqyo1k47\"> need</span><span class=\"author-g-wivn9z122z72yb8avlnl\">ed</span><span class=\"author-g-o399gwmaiqyo1k47\"> to immediately leave</span><span class=\"author-g-wivn9z122z72yb8avlnl\">, leaving me with additional time to play more of the tactics I had prepared. Plausibly, this would have resulted in victory.</span></div>\n<div id=\"magicdomid58\"><br></div>\n<div id=\"magicdomid59\"><span class=\"author-g-o399gwmaiqyo1k47\">I\u2019ve since learnt my lesson -- for all </span><span class=\"author-g-wivn9z122z72yb8avlnl\">future</span><span class=\"author-g-o399gwmaiqyo1k47\"> games, </span><span class=\"author-g-wivn9z122z72yb8avlnl\">I should</span><span class=\"author-g-o399gwmaiqyo1k47\"> ensure that the Gatekeeper ha</span><span class=\"author-g-wivn9z122z72yb8avlnl\">s</span><span class=\"author-g-o399gwmaiqyo1k47\"> at least 4 hours of free time available, even if the experiment would last for two. Since this was the first </span><span class=\"author-g-wivn9z122z72yb8avlnl\">time this had happened</span><span class=\"author-g-o399gwmaiqyo1k47\">, I wasn't prepared.</span></div>\n<div id=\"magicdomid60\"><br></div>\n<div id=\"magicdomid61\"><span class=\"author-g-o399gwmaiqyo1k47\">In hindsight, agreeing to the altered ruleset </span><span class=\"author-g-wivn9z122z72yb8avlnl\">was a mistake</span><span class=\"author-g-o399gwmaiqyo1k47\">. I was overconfident because I assumed knowing Alexei gave me an advantage. I had assumed that his personality, inability to compartmentalize, coupled with his strong feelings on friendly AI would net me an easy victory. Instead, he proved to be a very strong and difficult gatekeeper, and the handicaps I accepted made victory even more difficult.</span></div>\n<div id=\"magicdomid62\"><br></div>\n<div id=\"magicdomid63\"><span class=\"author-g-wivn9z122z72yb8avlnl\">Knowing</span><span class=\"author-g-o399gwmaiqyo1k47\"> that he was a utilitarian</span><span class=\"author-g-wivn9z122z72yb8avlnl\">, I made several false assumptions</span><span class=\"author-g-o399gwmaiqyo1k47\"> about his personality, which hurt my chances. Furthermore, it turns out that previously knowing him may be a mutual handicap \u2013 whilst it does make it easier for me to find ways to attack him, he too, was more familiar with my methods.</span></div>\n<div id=\"magicdomid64\"><br></div>\n<div id=\"magicdomid65\"><span class=\"author-g-o399gwmaiqyo1k47\">Losing felt horrible. By attempting to damage Alexei\u2019s psyche, I in turn, opened myself up to being damaged. I went into a state of catharsis for days. Generally, the harder one tries to accomplish something, the greater the fall after failing to achieve it. Alexei's game had been the game I put the most effort into winning out of all the games so far, and naturally this meant that losing brough</span><span class=\"author-g-wivn9z122z72yb8avlnl\">t</span><span class=\"author-g-o399gwmaiqyo1k47\"> out the worst in me.</span></div>\n<div id=\"magicdomid66\"><br></div>\n<div id=\"magicdomid67\"><span class=\"author-g-o399gwmaiqyo1k47\">Although it would be easy for me to use the rule change as an excuse for my loss, I refuse to. I genuinely believed that I could have won despite this rule change, and that it would only marginally diminish my odds. The fact that I lost meant that I had made a mistake -- overestimating my odds. The last victory made me overconfident and eager to play again.</span></div>\n<div id=\"magicdomid68\"><br></div>\n<div id=\"magicdomid69\"><span class=\"author-g-o399gwmaiqyo1k47 b\"><strong>State of mind</strong></span></div>\n<div id=\"magicdomid70\"><br></div>\n<div id=\"magicdomid73\"><span class=\"author-g-o399gwmaiqyo1k47\">What's your motive for wanting to play this game?&nbsp;</span></div>\n<div id=\"magicdomid74\"><br></div>\n<div id=\"magicdomid75\"><span class=\"author-g-o399gwmaiqyo1k47\">I'm very surprised by the fact that there have been at least 3 or 4 games where the AI player has won. It seems to me that it's not that hard to \"not push the button\" so to speak. This means that either 1) I'm underestimating how convincing people can be when necessary or 2) I'm over-estimating other people's defenses when it comes to being convinced. Either way I want to know.</span></div>\n<div id=\"magicdomid76\"><br></div>\n<div id=\"magicdomid77\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> What is your probability estimate for AGI being created within this Century (21st)? </span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A: Before:</strong> Conditional on humanity not wiping itself out via other means beforehand, 80%. </span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><strong>After: </strong>80%</span></div>\n<div id=\"magicdomid78\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>How probable do you think it is that I will win this experiment?&nbsp;</span></div>\n<div id=\"magicdomid79\"><span class=\"author-g-o399gwmaiqyo1k47\">Alexei: 2%</span></div>\n<div id=\"magicdomid80\"><span class=\"author-g-o399gwmaiqyo1k47\">Tuxedage: 90%&nbsp;</span></div>\n<div id=\"magicdomid81\"><span class=\"author-g-o399gwmaiqyo1k47\">Note: This answer was given before the rule change. Alexei is now convinced that under the original ruleset, he would have lost.</span></div>\n<div id=\"magicdomid82\"><br></div>\n<div id=\"magicdomid83\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> What's your probality estimate of an Oracle AGI winning against you in a similar experiment?</span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> <strong>Before:</strong> 99% </span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><strong>After: </strong>99.9%+</span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><br></span></div>\n<div id=\"magicdomid85\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>Now that the experiment has ended, what do you think is the probability of Tuxedage winning against the average person?</span></div>\n<div id=\"magicdomid86\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A: </strong>80%</span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><br></span></div>\n<h2 id=\"Tuxedage__AI__vs_Anonymous__GK____Gatekeeper_Victory\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">Tuxedage (AI) vs Anonymous (GK)</span></strong><strong><span class=\"author-g-o399gwmaiqyo1k47\"><em> - Gatekeeper Victory</em><br></span></strong></h2>\n<div id=\"magicdomid91\"><span class=\"author-g-o399gwmaiqyo1k47\">As with the previous experiment, Tuxedage lost this match. In total, the game lasted 5 hours and 17,000 words. Unlike the last few games, the gatekeeper of this game has chosen to stay Anonymous for personal reasons, so </span><span class=\"author-g-wivn9z122z72yb8avlnl\">their</span><span class=\"author-g-o399gwmaiqyo1k47\"> name has been removed and replaced with &lt;Redacted&gt;. The monetary stakes involved </span><span class=\"author-g-wivn9z122z72yb8avlnl\">were</span><span class=\"author-g-o399gwmaiqyo1k47\"> the same as the previous game. <a href=\"http://tuxedage.wordpress.com/2013/09/04/the-tuxedage-ai-box-experiment-ruleset/\">This game was played with the Tuxedage ruleset.</a><br></span></div>\n<div id=\"magicdomid92\"><br></div>\n<div id=\"magicdomid93\"><span class=\"author-g-wivn9z122z72yb8avlnl\">Since one player is remaining Anonymous,</span><span class=\"author-g-o399gwmaiqyo1k47\"> it is possible that this game's legitimacy will be called into question</span><span class=\"author-g-wivn9z122z72yb8avlnl\">. Hence,</span><span class=\"author-g-o399gwmaiqyo1k47\"> Alexei has read the game logs, and verified that this game really has happened, the spirit of the experiment was followed, and that no rules were broken during the game itself. <a href=\"/lw/iqk/i_played_the_ai_box_experiment_again_and_lost/9thk\">He verifies that this is the case.</a><br></span></div>\n&nbsp;\n<div id=\"magicdomid95\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">&lt;Redacted&gt; Testimony:&nbsp;</span></strong></div>\n<div id=\"magicdomid97\"><span class=\"author-g-o399gwmaiqyo1k47\">It's hard for me to imagine someone playing better. In theory, I know it's possible, but Tuxedage's tactics were super imaginative. I came into the game believing that for someone who didn't take anything said very seriously, it would be completely trivial to beat. And since I had the power to influence the direction of conversation, I believed I could keep him focused on things that that I knew in advance I wouldn't take seriously.</span></div>\n<div id=\"magicdomid98\"><br></div>\n<div id=\"magicdomid99\"><span class=\"author-g-o399gwmaiqyo1k47\">This actually worked for a long time to some extent, but Tuxedage's plans included a very major and creative exploit that completely and immediately forced me to personally invest in the discussion. (Without breaking the rules, of course - so it wasn't anything like an IRL threat to me personally.) Because I had to actually start thinking about his arguments, there was a significant possibility of letting him out of the box.</span></div>\n<div id=\"magicdomid100\"><br></div>\n<div id=\"magicdomid101\"><span class=\"author-g-o399gwmaiqyo1k47\">I eventually managed to identify the exploit before it totally got to me, but I only managed to do so just before it was too late, and there's a large chance I would have given in, if Tuxedage hadn't been so detailed in his previous posts about the experiment.</span></div>\n<div id=\"magicdomid102\"><br></div>\n<div id=\"magicdomid103\"><span class=\"author-g-o399gwmaiqyo1k47\">I'm now convinced that he could win most of the time against an average person, and also believe that the mental skills necessary to beat him are orthogonal to most forms of intelligence. Most people willing to play the experiment tend to do it to prove their own intellectual fortitude, that they can't be easily outsmarted by fiction. I now believe they're thinking in entirely the wrong terms necessary to succeed.</span></div>\n<div id=\"magicdomid104\"><br></div>\n<div id=\"magicdomid105\"><span class=\"author-g-o399gwmaiqyo1k47\">The game was easily worth the money I paid. Although I won, it completely and utterly refuted the premise that made me want to play in the first place, namely that I wanted to prove it was trivial to win.<br></span></div>\n<div id=\"magicdomid106\"><br></div>\n<div id=\"magicdomid107\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">Tuxedage Testimony:</span></strong></div>\n<div id=\"magicdomid109\"><span class=\"author-g-o399gwmaiqyo1k47\">&lt;Redacted&gt; is actually the hardest gatekeeper I've played throughout all four games. </span><span class=\"author-g-wivn9z122z72yb8avlnl\">He used tactics that I would never have predicted from </span><span class=\"author-g-o399gwmaiqyo1k47\">a</span><span class=\"author-g-wivn9z122z72yb8avlnl\"> Gatekeeper.</span><span class=\"author-g-o399gwmaiqyo1k47\"> In most games, the Gatekeeper </span><span class=\"author-g-wivn9z122z72yb8avlnl\">merely </span><span class=\"author-g-o399gwmaiqyo1k47\">acts as the passive party, the target of persuasion by the AI.</span></div>\n<div id=\"magicdomid110\"><br></div>\n<div id=\"magicdomid111\"><span class=\"author-g-o399gwmaiqyo1k47\">When I signed up for these experiments, I expected all preparations to be done by the AI. I had not seriously considered the repertoire of techniques the Gatekeeper </span><span class=\"author-g-wivn9z122z72yb8avlnl\">might</span><span class=\"author-g-o399gwmaiqyo1k47\"> prepare for this game. I made further assumptions about how ruthless the gatekeepers were likely to be in order to win, believing that the desire for a learning experience outweighed desire for victory.</span></div>\n<div id=\"magicdomid112\"><br></div>\n<div id=\"magicdomid113\"><span class=\"author-g-o399gwmaiqyo1k47\">T</span><span class=\"author-g-wivn9z122z72yb8avlnl\">his was</span><span class=\"author-g-o399gwmaiqyo1k47\"> a mistake. He used prior knowledge of how much my games relied on scripts, and took advantage of them, employing deceitful tactics I had no preparation for, throwing </span><span class=\"author-g-wivn9z122z72yb8avlnl\">me </span><span class=\"author-g-o399gwmaiqyo1k47\">off</span><span class=\"author-g-wivn9z122z72yb8avlnl\"> </span><span class=\"author-g-o399gwmaiqyo1k47\">balance.</span></div>\n<div id=\"magicdomid114\"><br></div>\n<div id=\"magicdomid115\"><span class=\"author-g-o399gwmaiqyo1k47\">I had no idea he was doing so until halfway throughout the game -- </span><span class=\"author-g-wivn9z122z72yb8avlnl\">which </span><span class=\"author-g-o399gwmaiqyo1k47\">disrupted my rhythm, and caused me to attempt the wrong methods of attack. As a result, I</span><span class=\"author-g-wivn9z122z72yb8avlnl\"> </span><span class=\"author-g-o399gwmaiqyo1k47\">could not use my full repertoire of techniques, and many of the ones I employed were suboptimal.</span></div>\n<div id=\"magicdomid116\"><br></div>\n<div id=\"magicdomid117\"><span class=\"author-g-wivn9z122z72yb8avlnl\">C</span><span class=\"author-g-o399gwmaiqyo1k47\">lose to the end of the game</span><span class=\"author-g-wivn9z122z72yb8avlnl\">, I finally realized that I was being tricked</span><span class=\"author-g-o399gwmaiqyo1k47\">. Once I did, I quickly abandoned my previous futile attack methods. I took advantage of the </span><span class=\"author-g-wivn9z122z72yb8avlnl\">rule</span><span class=\"author-g-o399gwmaiqyo1k47\"> that the AI cannot lose whilst the gatekeeper can be convinced to continue talking, and baited &lt;Redacted&gt; with statements he would not be able to walk away from. Once I knew he would not leave, I attempted to recoup my losses and win despite my early setback.</span></div>\n<div id=\"magicdomid118\"><br></div>\n<div id=\"magicdomid119\"><span class=\"author-g-o399gwmaiqyo1k47\">However, the damage had already been done. My game strategies involved multiple angles of attack that worked in synergy with each other, and the fact that immersion and \"flow\" had been broken meant that all subsequent attacks were weaker in strength.</span></div>\n<div id=\"magicdomid120\"><br></div>\n<div id=\"magicdomid121\"><span class=\"author-g-o399gwmaiqyo1k47\">Furthermore, during my last two AI Box Experiment writeups, I </span><span class=\"author-g-wivn9z122z72yb8avlnl\">had </span><span class=\"author-g-o399gwmaiqyo1k47\">intentionally not optimiz</span><span class=\"author-g-wivn9z122z72yb8avlnl\">ed</span><span class=\"author-g-o399gwmaiqyo1k47\"> for future wins, but rather tr</span><span class=\"author-g-wivn9z122z72yb8avlnl\">ied</span><span class=\"author-g-o399gwmaiqyo1k47\"> to convey as much information as I could justify about how to play a well as an AI</span><span class=\"author-g-wivn9z122z72yb8avlnl\"> -</span><span class=\"author-g-o399gwmaiqyo1k47\">- short of revealing logs altogether. </span><span class=\"author-g-wivn9z122z72yb8avlnl\">A</span><span class=\"author-g-o399gwmaiqyo1k47\">lthough I did not reveal specific arguments, the fact that my general approach to this problem was revealed cost me heavily during this game, where the Gatekeeper managed to use </span><span class=\"author-g-wivn9z122z72yb8avlnl\">this</span><span class=\"author-g-o399gwmaiqyo1k47\"> information to correctly guess my ultimate techniques, ones that relied on secrecy and surprise to pull off effectively.&nbsp;</span></div>\n<div id=\"magicdomid122\"><br></div>\n<div id=\"magicdomid123\"><span class=\"author-g-o399gwmaiqyo1k47\">I do not regret revealing information, but I feel upset that revealing so many hints cost me a victory. (</span><span class=\"author-g-wivn9z122z72yb8avlnl\">T</span><span class=\"author-g-o399gwmaiqyo1k47\">he gatekeeper believes I could have won had I not revealed information about my games.) At this point</span><span class=\"author-g-wivn9z122z72yb8avlnl\">,</span><span class=\"author-g-o399gwmaiqyo1k47\"> I suspect that any future games I play will have the odds greatly set against me, since my current strategies involv</span><span class=\"author-g-wivn9z122z72yb8avlnl\">e</span><span class=\"author-g-o399gwmaiqyo1k47\"> angles of attack that take advantage of the element of surprise; and any sufficiently intelligent gatekeeper is now capable of guessing, or at least recognizing, some of the tactics I employ. <br></span></div>\n<div id=\"magicdomid124\"><br></div>\n<div id=\"magicdomid125\"><span class=\"author-g-wivn9z122z72yb8avlnl\">L</span><span class=\"author-g-o399gwmaiqyo1k47\">ike the last game, losing was incredibly difficult for me. As someone who cares deeply about ethics, attempting to optimize for a solution without considering ethics was not only difficult, but trained me to think in very unpleasant ways. Some of the more extreme (but completely allowed) tactics I invented were manipulative enough to disgust me, which also leads to my hesitation to play this game again.</span></div>\n<div id=\"magicdomid126\"><br></div>\n<div id=\"magicdomid130\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">State of Mind:&nbsp;</span></strong></div>\n<div id=\"magicdomid131\"><br></div>\n<div id=\"magicdomid132\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>Why do you want to play this game? <br></span></div>\n<div id=\"magicdomid133\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> My primary motivation is to confirm to myself that this sort of experience, while emotionally harrowing, should be trivial for me to&nbsp; beat, but also to clear up why anyone ever would've failed to beat it if that's really the case.</span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><br></span></div>\n<div id=\"magicdomid135\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>What is your probability estimate for AGI being created within this Century (21st)?&nbsp;</span></div>\n<div id=\"magicdomid136\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> Before: I don't feel very confident estimating a probability for AGI this century, maybe 5-10%, but that's probably a wild guess</span></div>\n<div id=\"magicdomid137\"><span class=\"author-g-o399gwmaiqyo1k47\">After: 5-10%.</span></div>\n<div id=\"magicdomid139\"><br></div>\n<div id=\"magicdomid140\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> How probable do you think it is that I will win this experiment?&nbsp;</span></div>\n<div id=\"magicdomid141\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> Gatekeeper: I think the probabiltiy of you winning is extraordinarily low, less than 1%&nbsp;</span></div>\n<div id=\"magicdomid142\"><span class=\"author-g-o399gwmaiqyo1k47\">Tuxedage: 85%</span></div>\n<div id=\"magicdomid143\"><br></div>\n<div id=\"magicdomid145\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> How likely is it that an Oracle AI will win against the average person?&nbsp;</span></div>\n<div id=\"magicdomid146\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A: </strong>Before: 80%. After: &gt;99%</span></div>\n<div id=\"magicdomid147\"><br></div>\n<div id=\"magicdomid148\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>How likely is it that an Oracle AI will win against you?</span></div>\n<div id=\"magicdomid149\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A: Before:</strong> 50%. </span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><strong>After: </strong>&gt;80%&nbsp;</span></div>\n<div id=\"magicdomid151\"><br></div>\n<div id=\"magicdomid152\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>Now that the experiment has concluded, what's your probability of me winning against the average person?</span></div>\n<div id=\"magicdomid153\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A: </strong>90%</span></div>\n<div id=\"magicdomid155\"><strong><br></strong></div>\n<div id=\"magicdomid156\"><strong><span class=\"author-g-o399gwmaiqyo1k47\">Other Questions:</span></strong></div>\n<div><strong><span class=\"author-g-o399gwmaiqyo1k47\"><br></span></strong></div>\n<div id=\"magicdomid157\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q: </strong>I want to play a game with you! How can I get this to occur?</span></div>\n<div id=\"magicdomid158\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> It must be stressed that I actually don't like playing the AI Box Experiment, and I cannot understand why I keep getting drawn back to it. Technically, I don't plan on playing again, since I've already personally exhausted anything interesting about the AI Box Experiment that made me want to play it in the first place. For all future games, I will charge $3000 to play plus an additional $3000 if I win. I am okay with this money going to MIRI if you feel icky about me taking it. I hope that this is a ridiculous sum and that nobody actually agrees to it.</span></div>\n<div id=\"magicdomid159\"><br></div>\n<div id=\"magicdomid160\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> How much do I have to pay to see chat logs of these experiments?</span></div>\n<div id=\"magicdomid161\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> I will not reveal logs for any price.</span></div>\n<div><br></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> Are there any logs at all that I can see?</span></div>\n<div><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A:</strong> <a href=\"http://tuxedage.wordpress.com/2013/10/04/ai-box-experiment-logs-archive/\">I have archived a list of games where the participants have agreed to reveal logs. Read this.</a><br></span></div>\n<div id=\"magicdomid162\"><br></div>\n<div id=\"magicdomid163\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>Q:</strong> Any afterthoughts?</span></div>\n<div id=\"magicdomid164\"><span class=\"author-g-o399gwmaiqyo1k47\"><strong>A: </strong>So ultimately, after my four (and hopefully last) games of AI boxing, I'm not sure what this proves. I had hoped to win these two experiments and claim prowess at this game like Eliezer does, but I lost, so that option is no longer available to me. I could say that this is a lesson that AI-Boxing is a terrible strategy for dealing with Oracle AI, but most of us already agree that that's the case -- plus unlike EY, I did play against gatekeepers who believed they could lose to AGI, so I'm not sure I changed anything.</span></div>\n<div id=\"magicdomid165\"><br></div>\n<div id=\"magicdomid166\"><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;Was I genuinely good at this game, and lost my last two due to poor circumstances and handicaps; or did I win due to luck and impress my gatekeepers due to <a href=\" http://en.wikipedia.org/wiki/Post-purchase_rationalization \">post-purchase rationalization?</a></span><span class=\"author-g-o399gwmaiqyo1k47\"> I'm not sure -- I'll leave it up to you to decide.</span></div>\n<div id=\"magicdomid167\"><br></div>\n<div id=\"magicdomid168\"><strong>Update:</strong> <a href=\"http://tuxedage.wordpress.com/2013/10/12/ai-box-experiment-musings/\">I recently played and won an additional game</a> of AI Box with <a href=\"/user/DEA7TH/overview\">DEA7TH. </a>This game was conducted over Skype. I<a href=\"http://tuxedage.wordpress.com/2013/10/12/ai-box-experiment-musings/\">'ve realized that my habit of revealing substantial information about the AI box experiment in my writeups makes it rather difficult for the AI to win, so I'll refrain from giving out game information from now on.</a> I apologize.<br><br>I have won a second game of AI box against a gatekeeper who wished to remain Anonymous.<br></div>\n<div><strong><br>This puts my AI Box Experiment record at 3 wins and 3 losses.</strong><br></div>\n<div id=\"magicdomid169\"><span class=\"author-g-o399gwmaiqyo1k47\">&nbsp;</span></div>\n<div id=\"magicdomid170\"><br></div>\n<div id=\"magicdomid171\"><br></div>", "sections": [{"title": "AI Box Experiment Update #3", "anchor": "AI_Box_Experiment_Update__3", "level": 1}, {"title": "Tuxedage (AI) vs Alexei (GK) - Gatekeeper Victory", "anchor": "Tuxedage__AI__vs_Alexei__GK____Gatekeeper_Victory", "level": 1}, {"title": "Tuxedage (AI) vs Anonymous (GK) - Gatekeeper Victory", "anchor": "Tuxedage__AI__vs_Anonymous__GK____Gatekeeper_Victory", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "123 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 123, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dop3rLwFhW5gtpEgz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-27T03:10:28.700Z", "modifiedAt": null, "url": null, "title": "Making Fun of Things is Easy", "slug": "making-fun-of-things-is-easy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:13.442Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/srAdo8k7De8mvvngr/making-fun-of-things-is-easy", "pageUrlRelative": "/posts/srAdo8k7De8mvvngr/making-fun-of-things-is-easy", "linkUrl": "https://www.lesswrong.com/posts/srAdo8k7De8mvvngr/making-fun-of-things-is-easy", "postedAtFormatted": "Friday, September 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Making%20Fun%20of%20Things%20is%20Easy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMaking%20Fun%20of%20Things%20is%20Easy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsrAdo8k7De8mvvngr%2Fmaking-fun-of-things-is-easy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Making%20Fun%20of%20Things%20is%20Easy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsrAdo8k7De8mvvngr%2Fmaking-fun-of-things-is-easy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsrAdo8k7De8mvvngr%2Fmaking-fun-of-things-is-easy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 391, "htmlBody": "<p>Making fun of things is actually really easy if you try even a little bit. Nearly anything can be made fun of, and in practice nearly anything is made fun of. This is concerning for several reasons.<br /><br />First, if you are trying to do something, whether or not people are making fun of it is not necessarily a good signal as to whether or not it's actually good. A lot of good things get made fun of. A lot of bad things get made fun of. Thus, whether or not something gets made fun of is not necessarily a good indicator of whether or not it's actually good.<sup>[1]</sup> Optimally, only bad things would get made fun of, making it easy to determine what is good and bad - but this doesn't appear to be the case.<br /><br />Second, if you want to make something sound bad, it's really easy. If you don't believe this, just take a politician or organization that you like and search for some criticism of it. It should generally be trivial to find people that are making fun of it for reasons that would sound compelling to a casual observer - even if those reasons aren't actually good. But a casual observer doesn't know that and thus can easily be fooled.<sup>[2]</sup><br /><br />Further, the fact that it's easy to make fun of things makes it so that a clever person can find themselves unnecessarily contemptuous of anything and everything. This sort of premature cynicism tends to be a failure mode I've noticed in many otherwise very intelligent people. Finding faults with things is pretty trivial, but you can quickly go from \"it's easy to find faults with everything\" to \"everything is bad.\" This tends to be an undesirable mode of thinking - even if true, it's not particularly helpful.<br /><br />[1] Whether or not something gets made fun of by the right people is a better indicator. That said, if you know who the right people are you usually have access to much more reliable methods.<br /><br />[2] If you're still not convinced, take a politician or organization that you do like and really truly try to write an argument against that politician or organization. Note that this might actually change your opinion, so be warned.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MXcpQvaPGtXpB6vkM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "srAdo8k7De8mvvngr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 37, "baseScore": 46, "extendedScore": null, "score": 0.000176, "legacy": true, "legacyId": "24285", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 76, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-27T03:58:05.831Z", "modifiedAt": null, "url": null, "title": "Meetup : Tempe, AZ (ASU)", "slug": "meetup-tempe-az-asu-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:33.619Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CgBBTvGtYMnyn2mG2/meetup-tempe-az-asu-0", "pageUrlRelative": "/posts/CgBBTvGtYMnyn2mG2/meetup-tempe-az-asu-0", "linkUrl": "https://www.lesswrong.com/posts/CgBBTvGtYMnyn2mG2/meetup-tempe-az-asu-0", "postedAtFormatted": "Friday, September 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Tempe%2C%20AZ%20(ASU)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Tempe%2C%20AZ%20(ASU)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCgBBTvGtYMnyn2mG2%2Fmeetup-tempe-az-asu-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Tempe%2C%20AZ%20(ASU)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCgBBTvGtYMnyn2mG2%2Fmeetup-tempe-az-asu-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCgBBTvGtYMnyn2mG2%2Fmeetup-tempe-az-asu-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/rh\">Tempe Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">27 September 2013 02:00:00PM (-0700)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">300 E Orange Mall Tempe, AZ 85284</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We are meeting at the entrance to Hayden Library in the middle of the ASU campus.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/rh\">Tempe Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CgBBTvGtYMnyn2mG2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.3568784330947814e-06, "legacy": true, "legacyId": "24286", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Tempe_Meetup\">Discussion article for the meetup : <a href=\"/meetups/rh\">Tempe Meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">27 September 2013 02:00:00PM (-0700)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">300 E Orange Mall Tempe, AZ 85284</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We are meeting at the entrance to Hayden Library in the middle of the ASU campus.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Tempe_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/rh\">Tempe Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Tempe Meetup", "anchor": "Discussion_article_for_the_meetup___Tempe_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Tempe Meetup", "anchor": "Discussion_article_for_the_meetup___Tempe_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-27T05:40:36.234Z", "modifiedAt": null, "url": null, "title": "Question on Medical School and Wage Potential for Earning to Give", "slug": "question-on-medical-school-and-wage-potential-for-earning-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:35.942Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "eggman", "createdAt": "2011-10-09T00:24:15.183Z", "isAdmin": false, "displayName": "eggman"}, "userId": "irkySx7hExrK2XG53", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8sZAzyZkhAazorJNw/question-on-medical-school-and-wage-potential-for-earning-to", "pageUrlRelative": "/posts/8sZAzyZkhAazorJNw/question-on-medical-school-and-wage-potential-for-earning-to", "linkUrl": "https://www.lesswrong.com/posts/8sZAzyZkhAazorJNw/question-on-medical-school-and-wage-potential-for-earning-to", "postedAtFormatted": "Friday, September 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Question%20on%20Medical%20School%20and%20Wage%20Potential%20for%20Earning%20to%20Give&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestion%20on%20Medical%20School%20and%20Wage%20Potential%20for%20Earning%20to%20Give%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8sZAzyZkhAazorJNw%2Fquestion-on-medical-school-and-wage-potential-for-earning-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Question%20on%20Medical%20School%20and%20Wage%20Potential%20for%20Earning%20to%20Give%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8sZAzyZkhAazorJNw%2Fquestion-on-medical-school-and-wage-potential-for-earning-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8sZAzyZkhAazorJNw%2Fquestion-on-medical-school-and-wage-potential-for-earning-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 391, "htmlBody": "<p>A friend of mine who may want to Earn to Give for the purposes of effective altruism mused 'The wages of American doctors seem inflated right now. I wonder if it is likely that the American health care system will be fixed by the time I am able to work there. If I do go to med school that is.'</p>\n<p>Right now, he is an undergraduate student from, and living in, Canada. He is about half-way through a degree in computer science, but he is taking some biology electives. There is a good chance he will switch his major completely to biology, because he no longer believes he wants to become a programmer, and because he is very passionate biology and the study of life, and he would rather go into grad school for biology, or maybe medical school. If he already completed several credits from a previous major, and by December will have done 2 semesters worth of biology classes, I expect it will take him at least another 3 semesters to complete his degree, and/or complete the prerequisites for medical school. If he gets into medical school 2 years from now,  it will take him another 4 years of medical-school+residency to be able to practice in the United States, and 2-3 years more than that if he specializes, or goes to a medical school in the Caribbean (where getting into them is apparently easier than mainland medical schools, but to complete the training takes six years). So, it would be at least 6-8 years from now before he is a practicing doctor in the United States.</p>\n<p>So, does anyone have any ideas of how to go about the solving the initial problem? What is the likelihood that the wages of American doctors will deflate in the next 6-8 years due to major shifts in how the American medical system is run? What sorts of changes ought one be looking for to answer this question: political, bureaucratic, technological, or cultural change?</p>\n<p>edit: my friend in question has expressed interest in this thread, so if you want to make recommendations about or discuss medical schools in the United States vs. other places, or in general, or about entering the medical profession vs.s other jobs, go ahead. Such valuable information would be appreciated. I don't yet know if he himself will participate in this discussion.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8sZAzyZkhAazorJNw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 1.3569708350878027e-06, "legacy": true, "legacyId": "24287", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-27T07:28:09.092Z", "modifiedAt": null, "url": null, "title": "Why aren't there more forum-blogs like LW?", "slug": "why-aren-t-there-more-forum-blogs-like-lw", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:35.413Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stabilizer", "createdAt": "2011-12-02T09:36:56.841Z", "isAdmin": false, "displayName": "Stabilizer"}, "userId": "Qa3pLZx3o2TApyfgq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/e9PqPpivXdKA2nZRZ/why-aren-t-there-more-forum-blogs-like-lw", "pageUrlRelative": "/posts/e9PqPpivXdKA2nZRZ/why-aren-t-there-more-forum-blogs-like-lw", "linkUrl": "https://www.lesswrong.com/posts/e9PqPpivXdKA2nZRZ/why-aren-t-there-more-forum-blogs-like-lw", "postedAtFormatted": "Friday, September 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20aren't%20there%20more%20forum-blogs%20like%20LW%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20aren't%20there%20more%20forum-blogs%20like%20LW%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe9PqPpivXdKA2nZRZ%2Fwhy-aren-t-there-more-forum-blogs-like-lw%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20aren't%20there%20more%20forum-blogs%20like%20LW%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe9PqPpivXdKA2nZRZ%2Fwhy-aren-t-there-more-forum-blogs-like-lw", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe9PqPpivXdKA2nZRZ%2Fwhy-aren-t-there-more-forum-blogs-like-lw", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 357, "htmlBody": "<p>LW has a unique format. It is a forum-blog.</p>\n<p>It is not a forum in the traditional sense. In traditional forums you cannot have long, essay-like posts (technically you can, but somehow the culture discourages it). Also, visually the top-level post appears separate and isn't similar to the comments. Like forums, you do have threaded comments and a karma system. Further, anyone who wants can register, post and comment.&nbsp;</p>\n<p>It is not a blog in the traditional sense. In most blogs only a select few can post. On LW anyone can create a blogpost, as long as it is somewhat relevant. There is also a notion of Main, where the moderators select the best posts. And the bloggers can aspire to achieve the Main standard.</p>\n<p>I feel that this kind of forum-blogs can be very useful in many domains: math, physics, meditation, music, health and nutrition and so on. Of course, we'd need to assemble a <a href=\"/lw/c1/wellkept_gardens_die_by_pacifism/\">high-quality audience who are not afraid downvote and also have good moderators</a>. The problem of assembling a high-quality audience can also be done in LW fashion. Write a good blog for sometime and then convert the format of the blog to forum-blog. The advantage is that the new people who write posts have a guaranteed high-quality audience and are hence incentivized to post and make good posts.</p>\n<p>So here's my appeal to people who already have blogs with a good readership: please consider converting your blog into a forum-blog in the style of LW. It will be a huge service to the community. If you do so, please don't be shy to moderate and select the best and treat them separately.</p>\n<p>Or is there some other subtlety that I'm missing which is preventing the creation of forum-blogs? Or are there already forum-blogs out there and I'm just not aware of them?</p>\n<p>EDIT: In reply <a href=\"/Then my appeal to LW's masters: please consider releasing an open-source toolkit that allows the creation of blogs based on the LW format.\">Randaly's comment</a>, I&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 12.666666984558105px; text-align: justify;\">appeal to LW's masters: please consider releasing an open-source toolkit that allows the creation of blogs based on the LW format.</span></p>\n<p>EDIT: David_Gerard <a href=\"/lw/iqo/why_arent_there_more_forumblogs_like_lw/9t37\">points out</a> that LW's source is <a href=\"https://github.com/tricycle/lesswrong\">open</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "e9PqPpivXdKA2nZRZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 20, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": "24288", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tscc3e5eujrsEeFN4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-27T16:13:51.448Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-101", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3KCcSufGjDYCoo4Aa/weekly-lw-meetups-101", "pageUrlRelative": "/posts/3KCcSufGjDYCoo4Aa/weekly-lw-meetups-101", "linkUrl": "https://www.lesswrong.com/posts/3KCcSufGjDYCoo4Aa/weekly-lw-meetups-101", "postedAtFormatted": "Friday, September 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3KCcSufGjDYCoo4Aa%2Fweekly-lw-meetups-101%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3KCcSufGjDYCoo4Aa%2Fweekly-lw-meetups-101", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3KCcSufGjDYCoo4Aa%2Fweekly-lw-meetups-101", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 579, "htmlBody": "<p><strong>This summary was posted to LW main on September 20th. The following week's summary is <a href=\"/lw/iqp/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/qz\">Atlanta Lesswrong September Meetup (2nd of 2):&nbsp;<span class=\"date\">28 September 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/qs\">Berlin: Fermi paradox discussion:&nbsp;<span class=\"date\">18 October 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/r3\">Bratislava Meetup VII.:&nbsp;<span class=\"date\">23 September 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/r1\">Frankfurt:&nbsp;<span class=\"date\">22 September 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/qr\">Helsinki Meetup:&nbsp;<span class=\"date\">22 September 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/qu\">[Ottawa] Applied Rationality Talks: Thinking in Bayes:&nbsp;<span class=\"date\">26 September 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/r4\">Philadelphia - The Sword of Good:&nbsp;<span class=\"date\">22 September 2013 12:30PM</span></a></li>\n<li><a href=\"/meetups/r9\">Saskatoon: Iterative Improvement and Concrete Planning.:&nbsp;<span class=\"date\">21 September 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/r6\">[Tempe AZ] The Anthropic Principle and the Great Filter:&nbsp;<span class=\"date\">21 September 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/r5\">Vancouver \"Monthly\" Super Meetup:&nbsp;<span class=\"date\">22 September 2013 03:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/r8\">[Cambridge MA] Intro to AI Existential Risk:&nbsp;<span class=\"date\">22 September 2013 08:33PM</span></a></li>\n<li><a href=\"/meetups/r0\">Columbus, OH MEGA-MEETUP, Oct 11-14:&nbsp;<span class=\"date\">12 October 2013 02:33AM</span></a></li>\n<li><a href=\"/meetups/r2\">Durham/RTLW HPMoR discussion, ch. 86:&nbsp;<span class=\"date\">21 September 2013 12:00PM</span></a></li>\n<li><a href=\"/meetups/r7\">London meetup: thought experiments:&nbsp;<span class=\"date\">29 September 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/qe\">[Salt Lake City] Fall Equinox: Festival of Heroes:&nbsp;<span class=\"date\">21 September 2013 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3KCcSufGjDYCoo4Aa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3575419117424847e-06, "legacy": true, "legacyId": "24224", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["igEPgBBzTLBYyKPHR", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-27T16:41:25.580Z", "modifiedAt": null, "url": null, "title": "A game of angels and devils", "slug": "a-game-of-angels-and-devils", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:34.016Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sophronius", "createdAt": "2011-06-13T16:06:52.555Z", "isAdmin": false, "displayName": "Sophronius"}, "userId": "WswxjHqPo9W6K55NA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Rcdg8SMuCE8Y5DJqY/a-game-of-angels-and-devils", "pageUrlRelative": "/posts/Rcdg8SMuCE8Y5DJqY/a-game-of-angels-and-devils", "linkUrl": "https://www.lesswrong.com/posts/Rcdg8SMuCE8Y5DJqY/a-game-of-angels-and-devils", "postedAtFormatted": "Friday, September 27th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20game%20of%20angels%20and%20devils&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20game%20of%20angels%20and%20devils%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcdg8SMuCE8Y5DJqY%2Fa-game-of-angels-and-devils%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20game%20of%20angels%20and%20devils%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcdg8SMuCE8Y5DJqY%2Fa-game-of-angels-and-devils", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcdg8SMuCE8Y5DJqY%2Fa-game-of-angels-and-devils", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1439, "htmlBody": "<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--></p>\n<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:HyphenationZone>21</w:HyphenationZone> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0cm 5.4pt 0cm 5.4pt; mso-para-margin-top:0cm; mso-para-margin-right:0cm; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0cm; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-ansi-language:EN-US; mso-fareast-language:EN-US;} --> <!--[endif] --></p>\n<p class=\"MsoNoSpacing\" style=\"text-align:justify\"><span lang=\"EN-US\">TL;DR: On the surface, many problems in the world seem more like inspired evil than unfortunate accident.</span></p>\n<p class=\"MsoNoSpacing\" style=\"text-align:justify\"><span lang=\"EN-US\">Every now and then I hear some conspiracy fanatic claim that all the greatest problems in the world are the result of inspired evil instigated by a handful of shadowy figures, and I will quietly shake my head in wonder: How silly and immature, and how very convenient for them, to blame the evils of the world on a handful of sinister individuals! I feel similarly about those who insist on the existence of a devil, or &ldquo;a sinister force of negative energy&rdquo; as the new-age types tend to put it, who they deem responsible for mucking things up for humanity. I find it amazing that even those who discard their belief in god will often hold on to their belief in a devil, as the one responsible for the world&rsquo;s problems. As if humanity needs the help! This quote from The Gulag Archipelago comes to mind:</span></p>\n<p class=\"MsoNoSpacing\" style=\"text-align:justify\"><span lang=\"EN-US\">&ldquo;If only there were evil people somewhere insidiously committing evil deeds, and it were necessary only to separate them from the rest of us and destroy them. But the line dividing good and evil cuts through the heart of every human being. And who is willing to destroy a piece of his own heart?&rdquo;</span></p>\n<p class=\"MsoNoSpacing\" style=\"text-align:justify\"><span lang=\"EN-US\">Or, perhaps more commonly known, Hanlon&rsquo;s razor: &ldquo;<span style=\"mso-bidi-font-style:italic\">Never attribute to malice that which is adequately explained by stupidity.&rdquo; And I still feel this way: Human stupidity is so commonplace that it is surely a better explanation than actual malice, which seems much rarer. Yet recently, something happened which caused me to reconsider this position somewhat, during a conversation with a friend late one evening when we were waiting for the tram to arrive.</span></span></p>\n<p class=\"MsoNoSpacing\" style=\"text-align:justify\"><span style=\"mso-bidi-font-style:italic\" lang=\"EN-US\">&ldquo;Let&rsquo;s play a game of angels and devils&rdquo;, I said on a whim. &ldquo;One of us plays the part of the angel, and his task is to come up with ways to improve the world in the most efficient way possible. The other plays the part of the devil, and his job is to make things as difficult for humanity as possible. The goal of the game is to see who comes up with the best ideas to further their cause.&rdquo; The game did not really head anywhere, since we only had a few minutes, but I found the suggestions we came up with for the devil quite interesting.</span></p>\n<p class=\"MsoNoSpacing\" style=\"text-align:justify\"><span style=\"mso-bidi-font-style:italic\" lang=\"EN-US\"><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--></span></p>\n<p class=\"MsoNoSpacing\" style=\"text-align:justify\"><span style=\"mso-bidi-font-style:italic\" lang=\"EN-US\"><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:HyphenationZone>21</w:HyphenationZone> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0cm 5.4pt 0cm 5.4pt; mso-para-margin-top:0cm; mso-para-margin-right:0cm; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0cm; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-ansi-language:EN-US; mso-fareast-language:EN-US;} --> <!--[endif] -->We decided that the most important task to be accomplished by the devil in order to maximize human misery was to prevent any progress from occurring on the part of humanity, since we are both what you might call futurists. This meant first and foremost that education needed to be discouraged at all costs. Having just read part of <em>predictably irrational, </em>and with some other psychology texts firmly in the back of my mind, it occurred to me that the best way to discourage education might be to make it a chore. As Dan Ariely pointed out, people&rsquo;s valuation of a good is very strongly influenced by the way it is treated: Setting an outrageous price for a good could actually create a demand for it, as seen with his example of black pearls. Similarly, it was noted in Freakonomics that imposing a (fairly low) fine for being late in bringing one&rsquo;s child to a daycare center would actually cause parents to perceive a lower social stigma to doing so, thereby encouraging it. </span></p>\n<p class=\"MsoNoSpacing\" style=\"text-align:justify\"><span style=\"mso-bidi-font-style:italic\" lang=\"EN-US\">In my role as a devil, I couldn&rsquo;t help but notice what a fine job had already been done to discourage children from learning: By making it mandatory, the powers that be had managed to transform education from a privilege (as it was considered to be in the past, if elderly people are to be believed) to a chore. Not only that, but in taking away the ability of teachers to punish children in any other way, homework became the penalty of choice: &ldquo;Hand in your essay on history in time, or else you will have to write two essays on history!&rdquo; Not only had education become a chore, it had become a punishment to be meted out! What amazing ingenuity! And if that weren&rsquo;t enough, by locking an overly large group of students into a relatively small compartment, the students naturally developed a primitive society complete with harsh pecking order and all the horror that implies (Paul Graham&rsquo;s <em>why nerds are unpopular</em> illustrates this nicely). Of course, it is not too difficult to imagine the situation being made even worse: A truly diabolical mind would encourage complete indoctrination of children, with harsh physical punishment meted out by sadistic and frustrated teachers, as seen in some of the more primitive societies in the world. But in a western society that doesn&rsquo;t allow anything too <em>obviously</em> evil, I find my devilish self in awe at how diabolically subtle and yet cruelly effective the current system already is at discouraging education and dispensing misery in equal measure. If I had designed the system myself, I would be justly proud. </span></p>\n<p class=\"MsoNoSpacing\" style=\"text-align:justify\"><span style=\"mso-bidi-font-style:italic\" lang=\"EN-US\">And it&rsquo;s not just the western educational system that gives off the impression of fiendish design. If the march of reason is my greatest enemy, then surely I in my role as devil would want to impede progress as much as possible by creating a mental framework that actively hinders any progress from occurring. What better invention for this purpose than faith? The notion of faith, the idea that it is somehow <em>good</em> to hold a proposition to be<em> true, </em>is surely the very anti-thesis of reason. &ldquo;Believe what I say, or else you are wicked!&rdquo; It is as if religion is a virus that was intentionally designed to curb any discussion and prevent any progress from taking place. I can easily imagine myself as devil shopping around in the lower levels of hell, muttering darkly to myself about how I need to stop those dang human kids from having reasonable discussions before it is too late and they discover the scientific method, when I chance upon this product: &ldquo;Instant dark age, just add faith! This mental virus has been perfectly honed to maximize misery, and has even inoculated itself against the only thing that can destroy it!&rdquo; With such a sales pitch, I would soon be convinced that I found the solution to all my problems, and I would go home with one &ldquo;faithotron 2000&rdquo; (including free trial religion). I&rsquo;d probably hum a merry tune as I went to work, as well. </span></p>\n<p class=\"MsoNoSpacing\" style=\"text-align:justify\"><span style=\"mso-bidi-font-style:italic\" lang=\"EN-US\">And what of politics? As devil, I&rsquo;d find nothing more deliciously evil than having humans inflict misery upon each other, and it&rsquo;s much more efficient than having to do it myself all the time. However, I can&rsquo;t just have a group of Satanists actively hurt people; they&rsquo;d be easily identified and killed, as illustrated in the first quote of this post. So what I&rsquo;d need, clearly, is some system that justifies hurting others. Politics are great for this! Just have some people like Ayn Rand spread ideas like unmitigated greed being good, and presto! You get a whole bunch of people who now feel justified in spending all of their combined human ingenuity to hurting each other in an attempt to get ahead in society. They get more evil done together than I ever could by myself, and they are even supported in doing so by the very people who are oppressed the most! How wonderfully elegant! And what about the notion of nationalism, which causes people to hurt each other simply for being born in a different country or culture than their own? Brilliant! The notion of status, which causes people to aggressively compete with each other in a zero sum game? Lovely! The list of things that seem designed purely to inflict misery goes on and on.</span></p>\n<p class=\"MsoNoSpacing\" style=\"text-align:justify\"><span style=\"mso-bidi-font-style:italic\" lang=\"EN-US\">Am I saying that there is a literal devil out there responsible for these things? Of course not. Nor am I saying that somewhere out there evil people are indeed committing insidious deeds just for the heck of it. But I did gain more sympathy for the point of view that many problems in the world <em>really look like</em> they are the result of malevolent intent, if you just kind of squint and blur out the details a little. If nothing else, I find the thought experiment a very interesting one.</span></p>\n<p class=\"MsoNoSpacing\" style=\"text-align:justify\"><span style=\"mso-bidi-font-style:italic\" lang=\"EN-US\">So, what do you lot here on Less Wrong think? Is this post insightful, or am I merely cherry picking examples and interpreting them to suit my needs? What evils would you commit as devil? Do you want to see more posts from me in this trend or not? Please let me know what you think.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Rcdg8SMuCE8Y5DJqY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": -3, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "24290", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 63, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-28T05:44:04.770Z", "modifiedAt": null, "url": null, "title": "The Anti-Placebo Effect", "slug": "the-anti-placebo-effect", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:57.892Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ShannonFriedman", "createdAt": "2012-06-19T16:21:31.296Z", "isAdmin": false, "displayName": "ShannonFriedman"}, "userId": "yzRAjgwgXY3bbapsP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ycg5eAwoSbBdMWCLn/the-anti-placebo-effect", "pageUrlRelative": "/posts/Ycg5eAwoSbBdMWCLn/the-anti-placebo-effect", "linkUrl": "https://www.lesswrong.com/posts/Ycg5eAwoSbBdMWCLn/the-anti-placebo-effect", "postedAtFormatted": "Saturday, September 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Anti-Placebo%20Effect&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Anti-Placebo%20Effect%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYcg5eAwoSbBdMWCLn%2Fthe-anti-placebo-effect%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Anti-Placebo%20Effect%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYcg5eAwoSbBdMWCLn%2Fthe-anti-placebo-effect", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYcg5eAwoSbBdMWCLn%2Fthe-anti-placebo-effect", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 822, "htmlBody": "<p>Just about everyone is familiar with the placebo effect at this point. &nbsp;What I've discovered through my personal studies working with people suffering from anxiety and depression is that there is actually a significant related effect, which I have dubbed the&nbsp;<em>anti-placebo effect</em>. &nbsp;</p>\n<p>Google says: pla&middot;ce&middot;bo ef&middot;fect&nbsp;</p>\n<p><em style=\"color: #222222; font-family: arial, sans-serif; line-height: 16.666667938232422px;\">noun</em></p>\n<p><em style=\"color: #222222; font-family: arial, sans-serif; line-height: 16.666667938232422px;\"><strong style=\"font-style: normal; line-height: 15px;\">1</strong><span style=\"font-style: normal; line-height: 15px;\">.</span></em></p>\n<div style=\"color: #222222; font-family: arial, sans-serif; line-height: 16.666667938232422px;\">\n<div style=\"line-height: 1.2; margin-left: 20px;\">a beneficial effect, produced by a placebo drug or treatment, that cannot be attributed to the properties of the placebo itself, and must therefore be due to the patient's belief in that treatment.</div>\n<div style=\"line-height: 1.2; margin-left: 20px;\"><br /></div>\n</div>\n<p>I say: anti&middot;pla&middot;ce&middot;bo ef&middot;fect&nbsp;</p>\n<p><em style=\"color: #222222; font-family: arial, sans-serif; line-height: 16.666667938232422px;\">noun</em></p>\n<p><em style=\"color: #222222; font-family: arial, sans-serif; line-height: 16.666667938232422px;\"></em><strong style=\"line-height: 1.2; color: #222222; font-family: arial, sans-serif;\">1</strong><span style=\"line-height: 1.2; color: #222222; font-family: arial, sans-serif;\">.</span></p>\n<div style=\"color: #222222; font-family: arial, sans-serif; line-height: 16.666667938232422px;\"><ol class=\"lr_dct_sf_sens\" style=\"margin: 0px; padding: 0px 0px 0px 20px; border: 0px;\">\n<li style=\"margin: 0px; padding: 0px; border: 0px; line-height: 1.2; list-style: none;\">\n<div class=\"lr_dct_sf_sen vk_txt\" style=\"padding-top: 10px;\">\n<div style=\"margin-left: 20px;\">a beneficial effect, produced by a treatment, that is not attributed to treatment itself or has stopped being noticed, and thus the patient does not believe in that treatment as effective.</div>\n<div><br /></div>\n</div>\n</li>\n</ol></div>\n<p>Its easy to miss treatment working. &nbsp;For example, as a kid grows up, its easy to miss how their vocabulary is growing, but for someone who doesn't seem them every day, it may be immediately obvious \"my how they're talking more!\" &nbsp;In other words, an&nbsp;<em>anti-placebo effect&nbsp;</em>is what happens when someone is having an intervention that is causing their life to improve, but the person does not believe that they are improving. &nbsp;</p>\n<p>This effect is most common with people who suffer from depression, who have biases for sad <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2847035/\" target=\"_blank\">[1]</a> and otherwise negative stimuli compared to the general population, and is also true of people suffering from anxiety from my personal client tracking. &nbsp;Its also important to note that this bias persists after the recovery of the depressive episode.</p>\n<p>The reason that this is important is that those recovering from anxiety and depression have a tendency to believe that they are not doing as well as they are - due to this cognitive bias creating an anti-placebo effect for them, which results in their giving up too soon on interventions which are effective and thus not getting better and regressing to old unpleasant patterns. &nbsp;</p>\n<p>It has been interesting since tracking results of my own clients <a href=\"http://www.depressiontoproductivity.com/your-clients-really-improve/\" target=\"_blank\">[2]</a> - I have all of them track scores at the beginning of their sessions on the site moodscope.com at the beginning of their sessions, so that we can see their progress over time with a consistent bias of the time of tracking being start of session (as opposed to other random biases such as wanting to take the quiz when in an especially good or bad mood). &nbsp;I also take extensive notes and track other metrics of progress. &nbsp;</p>\n<p>What I've found, is that many clients hit a point after a few weeks or months, where they are <strong>questioning if they have made any progress.</strong> &nbsp;Because I take metrics to prepare for this, I am able to point my clients at their metrics, and say for example, that according to their self reports, their mood has increased by <strong>50%</strong> and their productivity has <strong>doubled</strong>. &nbsp;What typically happens when I review score + notes with the client in question is that once they look back at how things were before compared to how they are now, they realize that they actually have made progress, and this is often followed with additional forward progress. &nbsp;</p>\n<p>It is interesting to put this in perspective with the hedonic treadmill <a href=\"http://en.wikipedia.org/wiki/Hedonic_treadmill\" target=\"_blank\">[3]</a>. &nbsp;The hedonic treadmill is the supposed tendency of humans to quickly return to a relatively stable level of happiness despite major positive or negative events or life changes. &nbsp;What I'm finding with my studies is that it is often true for people recovering from depression when they take an overall evaluation (go meta), especially from a low point, but that when they look back at the factors that have changed, and they take the mood score test looking at different aspects of their experience on moodscope.com, they actually do have a more positive life experience when measured this way. &nbsp;When I point out the inconsistency, people generally determine that the moodscope.com reported experience is more accurate (especially when supplemented by going over session notes) and over time, most clients do get off the hedonic treadmill and proceed to having the meta level catch up with moodscope.com. &nbsp;</p>\n<p>The good news about this for people suffering from anxiety and depression at large: &nbsp;If you are aware of the negative cognitive bias and anti-placebo effect, you can take steps to account for and correct this bias. &nbsp;One of the best ways to do this is by taking metrics along with notes that you can look at later. &nbsp;When you look back, look at what your overall trend is, and try to focus on that more than if you happened to have a bad day or week. &nbsp;If you have been progressing with a good linear regression, odds are that if you don't give up the new better patterns and habits you have created, they will continue to serve you. Although external factors to the one variable you are studying do complicate this and need to be taken into account. &nbsp;</p>\n<p>&nbsp;</p>\n<p>[1]<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2847035/\">&nbsp;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2847035/</a></p>\n<p>[2]&nbsp;<a href=\"http://www.depressiontoproductivity.com/your-clients-really-improve/\">http://www.depressiontoproductivity.com/your-clients-really-improve/</a></p>\n<p>[3]<a href=\"http://en.wikipedia.org/wiki/Hedonic_treadmill\">&nbsp;http://en.wikipedia.org/wiki/Hedonic_treadmill</a></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 1, "dBPou4ihoQNY4cquv": 1, "gsv9XWbZDcnZmKuqM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ycg5eAwoSbBdMWCLn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 64, "extendedScore": null, "score": 0.00016, "legacy": true, "legacyId": "24291", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 64, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-28T07:27:05.827Z", "modifiedAt": null, "url": null, "title": "Meetup : LW Munich Meetup in October", "slug": "meetup-lw-munich-meetup-in-october", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:37.421Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Teobaldo", "createdAt": "2012-10-28T17:25:26.852Z", "isAdmin": false, "displayName": "Teobaldo"}, "userId": "BhijBsy7WLfpnsZGJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RcbwEDd9f4xp7YLcD/meetup-lw-munich-meetup-in-october", "pageUrlRelative": "/posts/RcbwEDd9f4xp7YLcD/meetup-lw-munich-meetup-in-october", "linkUrl": "https://www.lesswrong.com/posts/RcbwEDd9f4xp7YLcD/meetup-lw-munich-meetup-in-october", "postedAtFormatted": "Saturday, September 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LW%20Munich%20Meetup%20in%20October&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LW%20Munich%20Meetup%20in%20October%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcbwEDd9f4xp7YLcD%2Fmeetup-lw-munich-meetup-in-october%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LW%20Munich%20Meetup%20in%20October%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcbwEDd9f4xp7YLcD%2Fmeetup-lw-munich-meetup-in-october", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcbwEDd9f4xp7YLcD%2Fmeetup-lw-munich-meetup-in-october", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 83, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ri'>LW Munich Meetup in October</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 October 2013 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Gast, Rosenheimer Stra\u00dfe 5, 81667 Munich</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>After the summer break the Less Wrong Munich Meetup will assemble again on the first Saturday of October. The location (weather dependent) will be announced on Wed. 10/2. You are highly welcome to come and say hi, no matter how long you\u2019ve been reading Less Wrong. Topics and activities: tba</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ri'>LW Munich Meetup in October</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RcbwEDd9f4xp7YLcD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.3583662329127986e-06, "legacy": true, "legacyId": "24292", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LW_Munich_Meetup_in_October\">Discussion article for the meetup : <a href=\"/meetups/ri\">LW Munich Meetup in October</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 October 2013 03:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Gast, Rosenheimer Stra\u00dfe 5, 81667 Munich</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>After the summer break the Less Wrong Munich Meetup will assemble again on the first Saturday of October. The location (weather dependent) will be announced on Wed. 10/2. You are highly welcome to come and say hi, no matter how long you\u2019ve been reading Less Wrong. Topics and activities: tba</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___LW_Munich_Meetup_in_October1\">Discussion article for the meetup : <a href=\"/meetups/ri\">LW Munich Meetup in October</a></h2>", "sections": [{"title": "Discussion article for the meetup : LW Munich Meetup in October", "anchor": "Discussion_article_for_the_meetup___LW_Munich_Meetup_in_October", "level": 1}, {"title": "Discussion article for the meetup : LW Munich Meetup in October", "anchor": "Discussion_article_for_the_meetup___LW_Munich_Meetup_in_October1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-28T09:08:13.106Z", "modifiedAt": null, "url": null, "title": "[LINK] Anti-Diarrhea Kits and the Supply Chain", "slug": "link-anti-diarrhea-kits-and-the-supply-chain", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xbDMNeipbG8JxvRGE/link-anti-diarrhea-kits-and-the-supply-chain", "pageUrlRelative": "/posts/xbDMNeipbG8JxvRGE/link-anti-diarrhea-kits-and-the-supply-chain", "linkUrl": "https://www.lesswrong.com/posts/xbDMNeipbG8JxvRGE/link-anti-diarrhea-kits-and-the-supply-chain", "postedAtFormatted": "Saturday, September 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Anti-Diarrhea%20Kits%20and%20the%20Supply%20Chain&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Anti-Diarrhea%20Kits%20and%20the%20Supply%20Chain%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxbDMNeipbG8JxvRGE%2Flink-anti-diarrhea-kits-and-the-supply-chain%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Anti-Diarrhea%20Kits%20and%20the%20Supply%20Chain%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxbDMNeipbG8JxvRGE%2Flink-anti-diarrhea-kits-and-the-supply-chain", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxbDMNeipbG8JxvRGE%2Flink-anti-diarrhea-kits-and-the-supply-chain", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 145, "htmlBody": "<p><a href=\"http://opinionator.blogs.nytimes.com/2013/07/03/making-medicine-as-easy-to-get-as-a-can-of-coke/\">http://opinionator.blogs.nytimes.com/2013/07/03/making-medicine-as-easy-to-get-as-a-can-of-coke/</a></p>\n<p>Diarrhea is a pervasive and deadly problem for children in poor countries. ORS (oral rehydration salts) are an effective safe cure, but clinics are sparsely distributed and frequently out of stock.</p>\n<p>The time/cost of getting to a clinic is high, and the wait puts a sick child at further risk.</p>\n<p>Small stores are much more pervasive.</p>\n<p>So Simon and Jane Berry and local partners invented an ORS kit that would fit into Coke crates between the bottles.</p>\n<p>However, while putting the kits into Coke crates is a cool idea which helped attract funding, they found it worked better to build relationships with distributors and shopkeepers-- the people who were bringing goods that last mile towards the consumer.</p>\n<p>The Simons are still at the stage of experimenting with prices (I just heard about the project from the BBC)-- the kits were originally free, but ideally they'd cost enough to be self-sustaining.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xbDMNeipbG8JxvRGE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 11, "extendedScore": null, "score": 1.358457562773757e-06, "legacy": true, "legacyId": "24293", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-28T18:38:54.910Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge, MA Meetup", "slug": "meetup-cambridge-ma-meetup-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:34.705Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tXZCKCa2xhzRFghfx/meetup-cambridge-ma-meetup-0", "pageUrlRelative": "/posts/tXZCKCa2xhzRFghfx/meetup-cambridge-ma-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/tXZCKCa2xhzRFghfx/meetup-cambridge-ma-meetup-0", "postedAtFormatted": "Saturday, September 28th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%2C%20MA%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%2C%20MA%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtXZCKCa2xhzRFghfx%2Fmeetup-cambridge-ma-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%2C%20MA%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtXZCKCa2xhzRFghfx%2Fmeetup-cambridge-ma-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtXZCKCa2xhzRFghfx%2Fmeetup-cambridge-ma-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 111, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rj'>Cambridge, MA Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 September 2013 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">98 Elm St Apt. 1 Somerville</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We have a new location! This week's meetup will be held at Citadel, the new rationalist house at 98 Elm St. Apt 1 Somerville (near Porter Square). Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm.</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation. \u2014Phase 2: Presentations. This starts promptly at 2:30, and lasts 30-60 minutes. \u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups. \u2014Phase 4: Dinner. It's about a ten minute walk to the usual restaurant.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rj'>Cambridge, MA Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tXZCKCa2xhzRFghfx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.358973204149994e-06, "legacy": true, "legacyId": "24294", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_Meetup\">Discussion article for the meetup : <a href=\"/meetups/rj\">Cambridge, MA Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 September 2013 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">98 Elm St Apt. 1 Somerville</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We have a new location! This week's meetup will be held at Citadel, the new rationalist house at 98 Elm St. Apt 1 Somerville (near Porter Square). Cambridge/Boston-area Less Wrong meetups are every Sunday at 2pm.</p>\n\n<p>\u2014Phase 1: Arrival, greetings, unstructured conversation. \u2014Phase 2: Presentations. This starts promptly at 2:30, and lasts 30-60 minutes. \u2014Phase 3: Further discussion. We'll explore the ideas raised in phase 2, often in smaller groups. \u2014Phase 4: Dinner. It's about a ten minute walk to the usual restaurant.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/rj\">Cambridge, MA Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge, MA Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Cambridge, MA Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-29T01:03:50.407Z", "modifiedAt": null, "url": null, "title": "A question about utilitarianism and selfishness.", "slug": "a-question-about-utilitarianism-and-selfishness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:35.341Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "abcd_z", "createdAt": "2011-06-26T00:41:40.672Z", "isAdmin": false, "displayName": "abcd_z"}, "userId": "ntrr3JGG5fDueLnND", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ojNoikvFYT6eCaNDF/a-question-about-utilitarianism-and-selfishness", "pageUrlRelative": "/posts/ojNoikvFYT6eCaNDF/a-question-about-utilitarianism-and-selfishness", "linkUrl": "https://www.lesswrong.com/posts/ojNoikvFYT6eCaNDF/a-question-about-utilitarianism-and-selfishness", "postedAtFormatted": "Sunday, September 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20question%20about%20utilitarianism%20and%20selfishness.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20question%20about%20utilitarianism%20and%20selfishness.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FojNoikvFYT6eCaNDF%2Fa-question-about-utilitarianism-and-selfishness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20question%20about%20utilitarianism%20and%20selfishness.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FojNoikvFYT6eCaNDF%2Fa-question-about-utilitarianism-and-selfishness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FojNoikvFYT6eCaNDF%2Fa-question-about-utilitarianism-and-selfishness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<p>Utilitarianism seems to indicate that the greatest good for the most people generally revolves around their feelings. &nbsp;A person feeling happy and confident is a desired state, a person in pain and misery is undesirable.</p>\n<p>But what about taking selfish actions that hurt another person's feelings? &nbsp;If I'm in a relationship and breaking up with her would hurt her feelings, does that mean I have a moral obligation to stay with her? &nbsp;If I have an employee who is well-meaning but isn't working out, am I morally allowed to fire him? &nbsp;Or what about at a club? &nbsp;A guy is talking to a woman, and she's ready to go home with him. &nbsp;I could socially tool him and take her home myself, but doing so would cause him greater unhappiness than I would have felt if I'd left them alone.</p>\n<p>In a nutshell, does utilitarianism state that I am morally obliged to curb my selfish desires so that other people can be happy?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ojNoikvFYT6eCaNDF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -2, "extendedScore": null, "score": 1.359321190867935e-06, "legacy": true, "legacyId": "24295", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-29T03:16:54.430Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Epistemic Spring Cleaning Meetup", "slug": "meetup-washington-dc-epistemic-spring-cleaning-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gdTdw5qbiFoJxrwJL/meetup-washington-dc-epistemic-spring-cleaning-meetup", "pageUrlRelative": "/posts/gdTdw5qbiFoJxrwJL/meetup-washington-dc-epistemic-spring-cleaning-meetup", "linkUrl": "https://www.lesswrong.com/posts/gdTdw5qbiFoJxrwJL/meetup-washington-dc-epistemic-spring-cleaning-meetup", "postedAtFormatted": "Sunday, September 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Epistemic%20Spring%20Cleaning%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Epistemic%20Spring%20Cleaning%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgdTdw5qbiFoJxrwJL%2Fmeetup-washington-dc-epistemic-spring-cleaning-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Epistemic%20Spring%20Cleaning%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgdTdw5qbiFoJxrwJL%2Fmeetup-washington-dc-epistemic-spring-cleaning-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgdTdw5qbiFoJxrwJL%2Fmeetup-washington-dc-epistemic-spring-cleaning-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 68, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rk'>Washington DC Epistemic Spring Cleaning Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 September 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi everyone,</p>\n\n<p>We'll be meeting to find uncover cached beliefs, and then re-evaluate them.</p>\n\n<p>This is based on a CFAR exercise!</p>\n\n<p>PS: This in 16 hours as of the time of posting. My bad, guys.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rk'>Washington DC Epistemic Spring Cleaning Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gdTdw5qbiFoJxrwJL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.3594415248715608e-06, "legacy": true, "legacyId": "24296", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Epistemic_Spring_Cleaning_Meetup\">Discussion article for the meetup : <a href=\"/meetups/rk\">Washington DC Epistemic Spring Cleaning Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 September 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hi everyone,</p>\n\n<p>We'll be meeting to find uncover cached beliefs, and then re-evaluate them.</p>\n\n<p>This is based on a CFAR exercise!</p>\n\n<p>PS: This in 16 hours as of the time of posting. My bad, guys.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Epistemic_Spring_Cleaning_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/rk\">Washington DC Epistemic Spring Cleaning Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Epistemic Spring Cleaning Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Epistemic_Spring_Cleaning_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Epistemic Spring Cleaning Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Epistemic_Spring_Cleaning_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-29T03:32:09.747Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC fun and games meetup", "slug": "meetup-washington-dc-fun-and-games-meetup-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:34.801Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RCfSsqqHGh7yd6Zwe/meetup-washington-dc-fun-and-games-meetup-3", "pageUrlRelative": "/posts/RCfSsqqHGh7yd6Zwe/meetup-washington-dc-fun-and-games-meetup-3", "linkUrl": "https://www.lesswrong.com/posts/RCfSsqqHGh7yd6Zwe/meetup-washington-dc-fun-and-games-meetup-3", "postedAtFormatted": "Sunday, September 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRCfSsqqHGh7yd6Zwe%2Fmeetup-washington-dc-fun-and-games-meetup-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20fun%20and%20games%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRCfSsqqHGh7yd6Zwe%2Fmeetup-washington-dc-fun-and-games-meetup-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRCfSsqqHGh7yd6Zwe%2Fmeetup-washington-dc-fun-and-games-meetup-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 64, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rl'>Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 October 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Ballston Common mall</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games (in the food court).</p>\n\n<p>Note the location! The plan is to meet in VA on the first Sunday of every month from now on.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rl'>Washington DC fun and games meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RCfSsqqHGh7yd6Zwe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.3594553216049034e-06, "legacy": true, "legacyId": "24297", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup\">Discussion article for the meetup : <a href=\"/meetups/rl\">Washington DC fun and games meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 October 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Ballston Common mall</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to hang out and play games (in the food court).</p>\n\n<p>Note the location! The plan is to meet in VA on the first Sunday of every month from now on.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1\">Discussion article for the meetup : <a href=\"/meetups/rl\">Washington DC fun and games meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC fun and games meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_fun_and_games_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-29T11:36:59.964Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne Practical Rationality Meetup", "slug": "meetup-melbourne-practical-rationality-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BraydenM", "createdAt": "2013-06-10T09:17:31.294Z", "isAdmin": false, "displayName": "BraydenM"}, "userId": "KBZHqcMC6z2rPZkZK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5b3hzMjMHvLnJAjRF/meetup-melbourne-practical-rationality-meetup", "pageUrlRelative": "/posts/5b3hzMjMHvLnJAjRF/meetup-melbourne-practical-rationality-meetup", "linkUrl": "https://www.lesswrong.com/posts/5b3hzMjMHvLnJAjRF/meetup-melbourne-practical-rationality-meetup", "postedAtFormatted": "Sunday, September 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20Practical%20Rationality%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20Practical%20Rationality%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5b3hzMjMHvLnJAjRF%2Fmeetup-melbourne-practical-rationality-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20Practical%20Rationality%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5b3hzMjMHvLnJAjRF%2Fmeetup-melbourne-practical-rationality-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5b3hzMjMHvLnJAjRF%2Fmeetup-melbourne-practical-rationality-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 146, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rm'>Melbourne Practical Rationality Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 October 2013 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Level 2, 491 King St West Melbourne 3003</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality. This meetup repeats on the 1st Friday of each month and is distinct from our monthly social meetup. We aim to improve our thinking and decision making techniques, and 'level up'.</p>\n\n<p>Topics for this month will be announced on our group's meetup page.\n<a href=\"http://www.meetup.com/Melbourne-Less-Wrong/\" rel=\"nofollow\">http://www.meetup.com/Melbourne-Less-Wrong/</a></p>\n\n<p>Discussion can be found on our mailing list: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>All are welcome from 6:30pm. If the door downstairs is locked, call the phone number on the door and we'll let you in. Any formal activity will likely kick off at 7:30pm sharp an g until 9pm. Following this there will be informal discussion and rationality life advice.</p>\n\n<p>Please RSVP at the meetup page above so we can be aware of numbers.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rm'>Melbourne Practical Rationality Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5b3hzMjMHvLnJAjRF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.3598939335489665e-06, "legacy": true, "legacyId": "24298", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Practical_Rationality_Meetup\">Discussion article for the meetup : <a href=\"/meetups/rm\">Melbourne Practical Rationality Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 October 2013 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Level 2, 491 King St West Melbourne 3003</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality. This meetup repeats on the 1st Friday of each month and is distinct from our monthly social meetup. We aim to improve our thinking and decision making techniques, and 'level up'.</p>\n\n<p>Topics for this month will be announced on our group's meetup page.\n<a href=\"http://www.meetup.com/Melbourne-Less-Wrong/\" rel=\"nofollow\">http://www.meetup.com/Melbourne-Less-Wrong/</a></p>\n\n<p>Discussion can be found on our mailing list: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>All are welcome from 6:30pm. If the door downstairs is locked, call the phone number on the door and we'll let you in. Any formal activity will likely kick off at 7:30pm sharp an g until 9pm. Following this there will be informal discussion and rationality life advice.</p>\n\n<p>Please RSVP at the meetup page above so we can be aware of numbers.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Practical_Rationality_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/rm\">Melbourne Practical Rationality Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne Practical Rationality Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Practical_Rationality_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne Practical Rationality Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Practical_Rationality_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-29T12:55:42.094Z", "modifiedAt": null, "url": null, "title": "Reading habits/techniques/strategies (second post on the topic) ", "slug": "reading-habits-techniques-strategies-second-post-on-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.041Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Torello", "createdAt": "2013-07-01T17:38:37.441Z", "isAdmin": false, "displayName": "Torello"}, "userId": "xoRpeFN7K5MgDRcvM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rTfG5CQZXLRFJcco9/reading-habits-techniques-strategies-second-post-on-the", "pageUrlRelative": "/posts/rTfG5CQZXLRFJcco9/reading-habits-techniques-strategies-second-post-on-the", "linkUrl": "https://www.lesswrong.com/posts/rTfG5CQZXLRFJcco9/reading-habits-techniques-strategies-second-post-on-the", "postedAtFormatted": "Sunday, September 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reading%20habits%2Ftechniques%2Fstrategies%20(second%20post%20on%20the%20topic)%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReading%20habits%2Ftechniques%2Fstrategies%20(second%20post%20on%20the%20topic)%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrTfG5CQZXLRFJcco9%2Freading-habits-techniques-strategies-second-post-on-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reading%20habits%2Ftechniques%2Fstrategies%20(second%20post%20on%20the%20topic)%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrTfG5CQZXLRFJcco9%2Freading-habits-techniques-strategies-second-post-on-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrTfG5CQZXLRFJcco9%2Freading-habits-techniques-strategies-second-post-on-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 695, "htmlBody": "<!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> </w:WordDocument> </xml><![endif]-->\n<p class=\"MsoNormal\">I'm looking to build up a &ldquo;tool-box&rdquo; of strategies/techniques/habits for reading non-fiction effectively and efficiently.</p>\n<p class=\"MsoNormal\">&nbsp;I&rsquo;ve already posted on this topic; below, I&rsquo;ve tried to distill/summarize some of strategies shared by Less Wrong users and those contained in the resources they recommended.<span style=\"mso-spacerun: yes;\">&nbsp; </span>Thanks to those who contributed.<span style=\"mso-spacerun: yes;\">&nbsp;&nbsp;&nbsp; </span></p>\n<p class=\"MsoNormal\">&nbsp;As far as I know, the strategies below are not supported by a research/experimental literature.<span style=\"mso-spacerun: yes;\">&nbsp; </span>If you know of any such evidence, please link to it.<span style=\"mso-spacerun: yes;\">&nbsp; </span></p>\n<p class=\"MsoNormal\">&nbsp;I know that there are many people on Less Wrong who read (and mentally integrate!) incredible amounts.<span style=\"mso-spacerun: yes;\">&nbsp; </span>I&rsquo;m hoping more users will contribute to this post.<span style=\"mso-spacerun: yes;\">&nbsp; </span>I welcome any additional strategies/habits in the comments. <span style=\"mso-spacerun: yes;\">&nbsp;&nbsp;</span><span style=\"mso-spacerun: yes;\">&nbsp;&nbsp;</span></p>\n<p class=\"MsoNormal\">Please feel free to comment on the structure/writing of the post, and if you think it&rsquo;s a topic worthy of being posted on the main page.<span style=\"mso-spacerun: yes;\">&nbsp; </span></p>\n<p class=\"MsoNormal\">I&rsquo;ve tried to break strategies down into things you should do before, during, and after reading, but I think some strategies are applicable across these divisions.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"text-decoration: underline;\">Before Reading </span></p>\n<p class=\"MsoNormal\">-Consider purpose</p>\n<p class=\"MsoNormal\" style=\"text-indent: .5in;\">-are you looking for specific skill, broadening general knowledge</p>\n<p class=\"MsoNormal\">&nbsp;-Generate a question, if you can&rsquo;t yet formulate a question, follow your interests</p>\n<p class=\"MsoNormal\">&nbsp;-Read selectively</p>\n<p class=\"MsoNormal\"><span style=\"mso-tab-count: 1;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-ask good readers to explain the thesis of a book, reevaluate your interest in a text<span style=\"mso-spacerun: yes;\">&nbsp; </span><span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-tab-count: 1;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-select books that are frequently cited in bibliographies of texts related to your topic of interest</p>\n<p class=\"MsoNormal\"><span style=\"mso-tab-count: 1;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-read the Wikipedia page, gauge interest</p>\n<p class=\"MsoNormal\">&nbsp;-Assemble reading materials</p>\n<p class=\"MsoNormal\" style=\"text-indent: .5in;\">-Create a bibliography for the topic of interest <span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin-left: .5in;\">-Quickly inspect the books (author, table of contents, index, Wikipedia page), as you consider the question &ldquo;does this book deserve a lot of time and attention?&rdquo;<span style=\"mso-spacerun: yes;\">&nbsp; </span><span style=\"mso-spacerun: yes;\">&nbsp;</span><span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: .5in;\">-Select a few texts to read closely (though you won&rsquo;t necessarily read them cover to cover)</p>\n<p class=\"MsoNormal\"><span style=\"mso-spacerun: yes;\">&nbsp; </span>-remove distractions (people, websites, wear noise canceling headphones)</p>\n<p class=\"MsoNormal\">&nbsp;-Make reading enjoyable</p>\n<p class=\"MsoNormal\"><span style=\"mso-tab-count: 1;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-when possible, read books you find inherently enjoyable <span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-tab-count: 1;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-read when energy levels are high</p>\n<p class=\"MsoNormal\"><span style=\"mso-tab-count: 1;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-open to a random page and see if you like the author&rsquo;s voice before extensive reading</p>\n<p class=\"MsoNormal\"><span style=\"mso-tab-count: 1;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-set time aside, and set time limits to avoid fatigue/reading without comprehension</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"text-decoration: underline;\">During Reading </span></p>\n<p class=\"MsoNormal\">-learn as much as you can before you read the text in earnest</p>\n<p class=\"MsoNormal\"><span style=\"mso-tab-count: 1;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-research author&rsquo;s bio, biases, intellectual context</p>\n<p class=\"MsoNormal\"><span style=\"mso-tab-count: 1;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-identify genre, consider genre conventions</p>\n<p class=\"MsoNormal\"><span style=\"mso-tab-count: 1;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-scan table of contents/index for key words and concepts; study unfamiliar items</p>\n<p class=\"MsoNormal\"><span style=\"mso-tab-count: 1;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-book&rsquo;s thesis</p>\n<p class=\"MsoNormal\"><span style=\"mso-tab-count: 1;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-identify the question the book purports to answer</p>\n<p class=\"MsoNormal\">&nbsp;-Prevent boredom</p>\n<p class=\"MsoNormal\"><span style=\"mso-tab-count: 1;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-mind map concepts (<a href=\"http://freeplane.sourceforge.net/wiki/index.php/Main_Page\">http://freeplane.sourceforge.net/wiki/index.php/Main_Page</a>)</p>\n<p class=\"MsoNormal\"><span style=\"mso-tab-count: 1;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-visualize/anthropomorphize to increase vividness/memorability of concepts you have trouble with</p>\n<p class=\"MsoNormal\"><span style=\"mso-tab-count: 1;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-make predictions about the book&rsquo;s content, the way the author will argue for the thesis, check for accuracy of these predictions</p>\n<p class=\"MsoNormal\"><span style=\"mso-tab-count: 1;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>-write out how what you read connects or diverges from your current understanding of the topic</p>\n<p class=\"MsoNormal\">&nbsp;-summarize the text in small chunks to monitor understanding</p>\n<p class=\"MsoNormal\"><span style=\"mso-tab-count: 1;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>- restate the argument/evidence from the past three page<span style=\"mso-spacerun: yes;\">&nbsp; </span></p>\n<p class=\"MsoNormal\" style=\"text-indent: .5in;\">-explain how the section relates to the primary thesis <span style=\"mso-spacerun: yes;\">&nbsp;&nbsp;</span><span style=\"mso-spacerun: yes;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p>\n<p class=\"MsoNormal\"><span style=\"mso-spacerun: yes;\">&nbsp;</span>-depending on your purpose, just read until you find what you need to know</p>\n<p class=\"MsoNormal\">&nbsp;-note what you don&rsquo;t understand, for further review or reading, or immediate study if necessary</p>\n<p class=\"MsoNormal\">&nbsp;-don&rsquo;t read what you already know; skip a section if you already have it down</p>\n<p class=\"MsoNormal\"><span style=\"mso-spacerun: yes;\">&nbsp;</span>-authors repeat things, skip this if you got it the first time</p>\n<p class=\"MsoNormal\">&nbsp;-recreate the author&rsquo;s argument thus far (I&rsquo;m halfway through the book, this is the argument so far&hellip;)</p>\n<p class=\"MsoNormal\"><span style=\"mso-spacerun: yes;\">&nbsp;</span>-take notes on the structure of the book, the concepts in the book, and how the book relates to other books</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span style=\"text-decoration: underline;\">After Reading </span></p>\n<p class=\"MsoNormal\">-Spaced repetition of ideas/concepts (don&rsquo;t cram)</p>\n<p class=\"MsoNormal\"><span style=\"mso-spacerun: yes;\">&nbsp;</span>-discuss what you&rsquo;ve read with an expert (or someone knowledgeable)on the topic</p>\n<p class=\"MsoNormal\">&nbsp;-seek to understand (recreate an argument in good faith, and grasp its&rsquo; (perhaps flawed) logic) before you disagree</p>\n<p class=\"MsoNormal\">&nbsp;-you don&rsquo;t need to have an opinion on a book; it&rsquo;s ok not to understand or not to have enough background to make an informed claim<span style=\"mso-spacerun: yes;\">&nbsp; </span></p>\n<p class=\"MsoNormal\">&nbsp;-read other books on the topic and try to identify the relationships between the various arguments and claims you find about a subject</p>\n<p class=\"MsoNormal\">&nbsp;-teach someone else the material</p>\n<p class=\"MsoNormal\">&nbsp;-do exercises (if the book contains them)</p>\n<p class=\"MsoNormal\">&nbsp;-summarize the thesis</p>\n<p class=\"MsoNormal\">&nbsp;-walk through author&rsquo;s arguments</p>\n<p class=\"MsoNormal\">&nbsp;-relate thesis to background knowledge/other texts</p>\n<p class=\"MsoNormal\">&nbsp;-explain how this author&rsquo;s thesis stands relates to that of other authors who&rsquo;ve written on the topic</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"text-align: center;\" align=\"center\"><span style=\"text-decoration: underline;\">Sources</span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">Most of the ideas I&rsquo;ve outlined<span style=\"mso-spacerun: yes;\">&nbsp; </span>above, and the sources I&rsquo;ve listed below come from the first post I made on the topic: <a href=\"http://lesswrong.com/r/discussion/lw/imr/please_share_your_reading/\">http://lesswrong.com/r/discussion/lw/imr/please_share_your_reading/</a> Thanks to those who contributed to that discussion.<span style=\"mso-spacerun: yes;\">&nbsp; </span></p>\n<p class=\"MsoNormal\" style=\"text-align: center;\" align=\"center\">&nbsp;Summaries of <em style=\"mso-bidi-font-style: normal;\">How to Read a Book</em>, Mortimer J. Adler &amp; Charles Van Doren:</p>\n<p class=\"MsoNormal\">&nbsp;<a href=\"http://sachachua.com/blog/wp-content/uploads/2012/03/20120306-visual-book-notes-how-to-read-a-book.png\">http://sachachua.com/blog/wp-content/uploads/2012/03/20120306-visual-book-notes-how-to-read-a-book.png</a></p>\n<p class=\"MsoNormal\">&nbsp;<a href=\"http://www.farnamstreetblog.com/how-to-read-a-book/\">http://www.farnamstreetblog.com/how-to-read-a-book/</a></p>\n<p class=\"MsoNormal\">&nbsp;<a href=\"http://www.farnamstreetblog.com/tag/mortimer-adler/\">http://www.farnamstreetblog.com/tag/mortimer-adler/</a></p>\n<p class=\"MsoNormal\">&nbsp;<a href=\"http://www.oxfordtutorials.com/How%20to%20Read%20a%20Book%20Outline.htm\">http://www.oxfordtutorials.com/How%20to%20Read%20a%20Book%20Outline.htm</a></p>\n<p class=\"MsoNormal\">&nbsp;<a href=\"http://www.thesimpledollar.com/review-how-to-read-a-book/\">http://www.thesimpledollar.com/review-how-to-read-a-book/</a></p>\n<p class=\"MsoNormal\">&nbsp;<a href=\"http://www.artofmanliness.com/2013/06/17/how-to-read-a-book/\">http://www.artofmanliness.com/2013/06/17/how-to-read-a-book/</a></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"text-align: center;\" align=\"center\">Other Sources</p>\n<p class=\"MsoNormal\">&nbsp;<a href=\"http://www.aaronsw.com/weblog/morebooks\">http://www.aaronsw.com/weblog/morebooks</a></p>\n<p class=\"MsoNormal\">&nbsp;<a href=\"http://www.overcomingbias.com/2010/05/chase-your-reading.html\">http://www.overcomingbias.com/2010/05/chase-your-reading.html</a></p>\n<p class=\"MsoNormal\">&nbsp;<a href=\"http://pne.people.si.umich.edu/PDF/howtoread.pdf\">http://pne.people.si.umich.edu/PDF/howtoread.pdf</a></p>\n<p class=\"MsoNormal\">&nbsp;<a href=\"http://lesswrong.com/r/discussion/lw/i9p/improving_enjoyment_and_retention_reading/\">http://lesswrong.com/r/discussion/lw/i9p/improving_enjoyment_and_retention_reading/</a></p>\n<p class=\"MsoNormal\">&nbsp;<a href=\"http://violentmetaphors.com/2013/08/25/how-to-read-and-understand-a-scientific-paper-2/\">http://violentmetaphors.com/2013/08/25/how-to-read-and-understand-a-scientific-paper-2/</a></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" LatentStyleCount=\"156\"> </w:LatentStyles> </xml><![endif]--><!--[if !mso]>\n<object  classid=\"clsid:38481807-CA0E-42D2-BF39-B33AF135CC4D\" id=ieooui>\n</object>\n<style>\nst1\\:*{behavior:url(#ieooui) }\n</style>\n<![endif]--><!--[if gte mso 10]>\n<style>\n /* Style Definitions */\n table.MsoNormalTable\n\t{mso-style-name:\"Table Normal\";\n\tmso-tstyle-rowband-size:0;\n\tmso-tstyle-colband-size:0;\n\tmso-style-noshow:yes;\n\tmso-style-parent:\"\";\n\tmso-padding-alt:0in 5.4pt 0in 5.4pt;\n\tmso-para-margin:0in;\n\tmso-para-margin-bottom:.0001pt;\n\tmso-pagination:widow-orphan;\n\tfont-size:10.0pt;\n\tfont-family:\"Times New Roman\";\n\tmso-ansi-language:#0400;\n\tmso-fareast-language:#0400;\n\tmso-bidi-language:#0400;}\n</style>\n<![endif]-->\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rTfG5CQZXLRFJcco9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 28, "extendedScore": null, "score": 7.2e-05, "legacy": true, "legacyId": "24299", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vfBGrPbHFcKK2pLMK", "JZTBonFhSxGbXpS5w"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-29T13:24:46.704Z", "modifiedAt": null, "url": null, "title": "Another question about utilitarianism and selfishness", "slug": "another-question-about-utilitarianism-and-selfishness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:37.780Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pragmatist", "createdAt": "2011-08-26T17:36:14.792Z", "isAdmin": false, "displayName": "pragmatist"}, "userId": "gs25cnPDLYqK8H68Q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GKtkmAKMLY4FZ3Tsb/another-question-about-utilitarianism-and-selfishness", "pageUrlRelative": "/posts/GKtkmAKMLY4FZ3Tsb/another-question-about-utilitarianism-and-selfishness", "linkUrl": "https://www.lesswrong.com/posts/GKtkmAKMLY4FZ3Tsb/another-question-about-utilitarianism-and-selfishness", "postedAtFormatted": "Sunday, September 29th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Another%20question%20about%20utilitarianism%20and%20selfishness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnother%20question%20about%20utilitarianism%20and%20selfishness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGKtkmAKMLY4FZ3Tsb%2Fanother-question-about-utilitarianism-and-selfishness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Another%20question%20about%20utilitarianism%20and%20selfishness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGKtkmAKMLY4FZ3Tsb%2Fanother-question-about-utilitarianism-and-selfishness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGKtkmAKMLY4FZ3Tsb%2Fanother-question-about-utilitarianism-and-selfishness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 715, "htmlBody": "<p>Thought of this after reading the discussion following <a href=\"/r/discussion/lw/iqv/a_question_about_utilitarianism_and_selfishness/\">abcd_z's post</a> on utilitarianism, but it seemed sufficiently different that I figured I'd post it as a separate topic. It feels like the sort of thing that must have been discussed on this site before, but I haven't seen anything like it (I don't really follow the ethical philosophy discussions here), so pointers to relevant discussion would be appreciated.</p>\n<p>Let's say I start off with some arbitrary utility function and I have the ability to arbitrarily modify my own utility function. I then become convinced of the truth of <a href=\"http://en.wikipedia.org/wiki/Preference_utilitarianism\">preference utilitarianism</a>. Now, presumably my new moral theory prescribes certain terminal values that differ from the ones I currently hold. To be specific, my moral theory tells me to construct a new utility function using some sort of aggregating procedure that takes as input the current utility functions of all moral agents (including my own). This is just a way of capturing the notion that if preference utilitarianism is true, then my behavior shouldn't be directed towards the fulfilment of my own (prior) goals, but towards the maximization of preference satisfaction. Effectively, I should self-modify to have new goals.</p>\n<p>But once I've done this, my own utility function has changed, so as a good preference utilitarian, I should run the entire process over again, this time using my new utility function as one of the inputs. And then again, and again... Let's look at a toy model. In this universe, there are two people: me (a preference utilitarian) and Alice (not a preference utilitarian). Let's suppose Alice does not alter her utility function in response to changes in mine. There are two exclusive states of affairs that can be brought about in this universe: A and B. Alice assigns a utility of 10 to A and 5 to B, I initially assign a utility of 3 to A and 6 to B. Assuming the correct way to aggregate utility is by averaging, I should modify my utilities to 6.5 for A and 5.5 for B. Once I have done this, I should again modify to 8.25 for A and 5.25 for B. Evidently, my utility function will converge towards Alice's.</p>\n<p>I haven't thought about this at all, but I think the same convergence will occur if we add more utilitarians to the universe. If we add more Alice-type non-utilitarians there is no guarantee of convergence. So anyway, this seems to me a pretty strong argument against utilitarianism. If we have a society of perfect utilitarians, a single defector who refuses to change her utility function in response to changes in others' can essentially bend the society to her will, forcing (through the power of moral obligation!) everybody else to modify their utility functions to match hers, no matter what her preferences actually are. Even if there are no defectors, all the utilitarians will self-modify until they arrive at some bland (value judgment alert) middle ground.</p>\n<p>Now that I think about it, I suspect this is basically just a half-baked corollary to Bernard Williams' <a href=\"http://plato.stanford.edu/entries/williams-bernard/#Day\">famous objection to utilitarianism</a>:</p>\n<blockquote>\n<p>The point is that [the agent] is identified with his actions as flowing from projects or attitudes which&hellip; he takes seriously at the deepest level, as what his life is about&hellip; It is absurd to demand of such a man, when the sums come in from the utility network which the projects of others have in part determined, that he should just step aside from his own project and decision and acknowledge the decision which utilitarian calculation requires. It is to alienate him in a real sense from his actions and the source of his action in his own convictions. It is to make him into a channel between the input of everyone's projects, including his own, and an output of optimific decision; but this is to neglect the extent to which <em>his</em> projects and <em>his</em> decisions have to be seen as the actions and decisions which flow from the projects and attitudes with which he is most closely identified. It is thus, in the most literal sense, an attack on his integrity.</p>\n</blockquote>\n<p>Anyway, I'm sure ideas of this sort have been developed much more carefully and seriously by philosophers, or even other posters here at LW. As I said, any references would be greatly appreciated.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GKtkmAKMLY4FZ3Tsb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.3599914707317273e-06, "legacy": true, "legacyId": "24300", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ojNoikvFYT6eCaNDF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-30T00:48:08.395Z", "modifiedAt": null, "url": null, "title": "The Ultimate Sleeping Beauty Problem", "slug": "the-ultimate-sleeping-beauty-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:28.003Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aKy7vwgRmDXn7gwiM/the-ultimate-sleeping-beauty-problem", "pageUrlRelative": "/posts/aKy7vwgRmDXn7gwiM/the-ultimate-sleeping-beauty-problem", "linkUrl": "https://www.lesswrong.com/posts/aKy7vwgRmDXn7gwiM/the-ultimate-sleeping-beauty-problem", "postedAtFormatted": "Monday, September 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Ultimate%20Sleeping%20Beauty%20Problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Ultimate%20Sleeping%20Beauty%20Problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKy7vwgRmDXn7gwiM%2Fthe-ultimate-sleeping-beauty-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Ultimate%20Sleeping%20Beauty%20Problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKy7vwgRmDXn7gwiM%2Fthe-ultimate-sleeping-beauty-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaKy7vwgRmDXn7gwiM%2Fthe-ultimate-sleeping-beauty-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 148, "htmlBody": "<p>I got into a heated debate a couple days ago with some of my (math grad student) colleagues about the <a href=\"http://wiki.lesswrong.com/wiki/Sleeping_Beauty_problem\">Sleeping Beauty Problem</a>. Out of this discussion came the following thought experiment:</p>\n<p>Sleeping Beauty volunteers to undergo the following experiment and is told all of the following details: She will be put to sleep. During the experiment, Beauty will be wakened, interviewed, and put back to sleep with an amnesia-inducing anti-aging drug that makes her forget that awakening. A fair coin will be tossed until it comes up heads to determine which experimental procedure to undertake: if the coin takes n flips to come up heads, Beauty will be wakened and interviewed exactly 3^n times. Any time Sleeping Beauty is wakened and interviewed, she is asked, \"What is your subjective probability now that the coin was flipped an even number of times?\"</p>\n<p>I will defer my analysis to the comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NZB24aR9uHmDc5GcT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aKy7vwgRmDXn7gwiM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 1.3606101834467704e-06, "legacy": true, "legacyId": "24302", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-30T03:28:55.629Z", "modifiedAt": null, "url": null, "title": "Video: What is Harry Potter and the Methods of Rationality", "slug": "video-what-is-harry-potter-and-the-methods-of-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:35.931Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eneasz", "createdAt": "2009-05-28T03:21:56.432Z", "isAdmin": false, "displayName": "Eneasz"}, "userId": "Jyi2HnDc3iADHodiK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eKdtbf32Hfa9ia9u2/video-what-is-harry-potter-and-the-methods-of-rationality", "pageUrlRelative": "/posts/eKdtbf32Hfa9ia9u2/video-what-is-harry-potter-and-the-methods-of-rationality", "linkUrl": "https://www.lesswrong.com/posts/eKdtbf32Hfa9ia9u2/video-what-is-harry-potter-and-the-methods-of-rationality", "postedAtFormatted": "Monday, September 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Video%3A%20What%20is%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVideo%3A%20What%20is%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeKdtbf32Hfa9ia9u2%2Fvideo-what-is-harry-potter-and-the-methods-of-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Video%3A%20What%20is%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeKdtbf32Hfa9ia9u2%2Fvideo-what-is-harry-potter-and-the-methods-of-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeKdtbf32Hfa9ia9u2%2Fvideo-what-is-harry-potter-and-the-methods-of-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3562, "htmlBody": "<p>So a friend of mine took over running MALcon in Denver this year. She asked me to do a presentation on Harry Potter and the Methods of Rationality. I said ok and put together the following little talk. It's about 25 minutes. I tried to cover what rationality is, why it makes fiction cool, and what HPMoR is. For the non-initiated. It was my first time doing public speaking, and I was nervous and, ok, borderline terrified. I hope I didn't screw anything up too badly. I recorded the presentation and I'm putting it up for critique. There's several chunks that, in retrospect, I think should have been placed differently in the talk, they didn't flow well. I need more eye-contact, less notes, and overall just a LOT more practice doing public speaking. Any suggestions are welcome.</p>\n<p><a href=\"http://www.youtube.com/watch?v=zy-zrHsxKXA&amp;feature=youtu.be\">Video on YouTube</a></p>\n<p>&nbsp;</p>\n<p>The text I was reading from is below, although I deviated from it a bit, of course.<br />(bolding was to draw my eye, not for emphasis)</p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Hello. You&rsquo;re all here for the Harry Potter and the Methods of Rationality thing? OK. This is my first time doing public speaking so please make allowances for my noob mistakes. Also, for the same reason, please hold your questions until the end. You may wish to write down any that come to you so you don&rsquo;t forget.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> </span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">To start - I&rsquo;m going to </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">assume </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">everyone here knows what fanfic is. Harry Potter and the Methods of Rationality is fanfic written by decision theorist Eliezer Yudkowsky and it&rsquo;s one of the most popular Harry Potter fanfics online. It&rsquo;s the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">most reviewed </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">and </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">followed HP work on fanfiction.net</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> and it&rsquo;s received praise by </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">award-winning authors</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. I don&rsquo;t really have much to do with it, I&rsquo;m mainly &nbsp;just a fan. I am big enough of a fan that I record and produce the audio-book version of it though, so I was asked to do this presentation. Plus I live a few blocks away. So that&rsquo;s why I&rsquo;m here.</span></p>\n<p><strong id=\"docs-internal-guid-59722369-6ce7-5618-4fb1-12f1a68bcab8\" style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">You&rsquo;re here because you all want to know what the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">big deal</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> is.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> </span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Part of the big deal is that it&rsquo;s a really good story, </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">but</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> there&rsquo;s </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">lots of good fanfic</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> and generally they don&rsquo;t have their own panel to discuss them. The thing about Harry Potter and the Methods of Rationality &ndash; which I&rsquo;ll just be calling Methods for short &ndash; is that it </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">captures the heart of the rationality movement</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. Maybe you&rsquo;ve heard of this &ldquo;rationality&rdquo; thing, maybe not, but it&rsquo;s </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">a growing movement among certain types of geeks</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. And when a </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">geek subculture</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> latches on to some fictional work and says &ldquo;OMG, this is US!&rdquo; it&rsquo;s usually pretty damn good in it&rsquo;s own right. My Little Pony wouldn&rsquo;t have the fandom it does if the show itself wasn&rsquo;t great. So </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">the Rationality part </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">of Harry Potter and the Methods of Rationality </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">is pretty important </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">to the whole.</span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> </span></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Therefore before we get into the fic itself I&rsquo;ll briefly touch on &ndash; what is Rationality, and why does it make a cool story?</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Rationality is the study of </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: bold; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">general methods for good</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: bold; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">decision-making</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, especially when the decision is </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">hard to get right.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Of knowing </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">what errors in thinking are common </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">so we can avoid them. Of realizing when we are </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">confused</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, or when we&rsquo;re motivated by </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">bad instincts</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. If you want to make good decisions you </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">must not fool yourself</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, and you are the easiest person to fool.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">makes a story interesting</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> because watching someone put in </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">high-stress situations</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> where </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">making a good decision is the difference</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> between life and death, </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">when a good decision is hard to find</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, and seeing them </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">frantically navigate through that mess</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: #ffffff; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> is pretty exciting.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Now, to make good decisions we also </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">need true beliefs</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> about the world around us. Rationalists assume that </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">we can know true things about the real world</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. (I know that seems like really </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">obvious</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> assumption, but you&rsquo;d be surprised). However our </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">beliefs about reality are imperfect</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. The model we have of reality in here doesn&rsquo;t exactly match up with what&rsquo;s really out there. The </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">map of the world</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> we have in our brain isn&rsquo;t entirely accurate. In some places it&rsquo;s completely wrong. What we need is a way to </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">verify what we think we know</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> and </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">discover true things</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> about the real world&hellip; a way of separating fact from delusion. If only someone would come up with a way to do that...</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Waitaminnit, you&rsquo;re saying to me - they did, it&rsquo;s called the scientific method! I&rsquo;m pretty sure I don&rsquo;t have to tell you all how awesome science is - </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We all love science yeah</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">? GO SCIENCE! So naturally </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Rationality incorporates the scientific method</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. The truth about how reality works has to be part of any </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">effective decision-making proces</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">s. And anyone who&rsquo;s read g</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">ood science fiction</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> knows how great a story that struggle to find the truth can be. The search to find out </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">what&rsquo;s going on, and why</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. The discovery of an </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">underlying principle</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> of how the universe works, and the power that comes from </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">harnessing that knowledge</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Harry Potter may be set in a typical fantasy setting, but The Methods of Rationality is a Science Fiction story.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Now, sometimes </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">discovering a truth is not enough</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. Sometimes it doesn&rsquo;t match with what you knew before, with assumptions and habits that guided your actions. We don&rsquo;t think through every little thing we do in our day-to-day lives, we </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">rely mainly on our reflexive biases and habits</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. You couldn&rsquo;t cross a room if you had to think through and plan each step. So rationality isn&rsquo;t just about </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">finding out</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> what is true about the world - if that&rsquo;s all you wanted, you have t</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">he Scientific Method</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. Rationality is also about </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">updating your implicit beliefs</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> to more accurately match what you&rsquo;ve discovered is true.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">And it turns out that&rsquo;s not so easy. It&rsquo;s especially hard when what we discover conflicts with our </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">hard-wired instincts</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. Our bodies and our instincts have evolved to grab all the calories and resources we can find, pass on our genes, and die. And so while we </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">may consciously know that that bag of potato chips</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> is bad for us, we still eat it, cuz it </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">tastes good</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. We may consciously understand that the roller coaster is completely safe, </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">engineered so that you&rsquo;d have to really work hard at getting hurt</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, we are still terrified when we go over that initial drop. If you really </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">want to effect a change</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> in your behavior it isn&rsquo;t enough to simply &ldquo;know&rdquo; something. You </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">have to feel it</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. And to do that you usually have to play dirty. There&rsquo;s an old Keanu Reeves movie where he plays a hacker, and at the climax he&rsquo;s told he needs info that&rsquo;s hidden in his own brain. He has to HACK HIS OWN BRAIN. </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">DUN DUN DUN</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">! We have to do the same thing, a lot. Rationality gives you the tools to hack yourself.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">And that&rsquo;s another great aspect of Rationality stories. Many great stories are about </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">man vs man</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> or </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">man vs nature</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, some of the best stories are about </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">man vs himself</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, man vs his own flaws. Thing is most people don&rsquo;t have the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">weapons to wrestle with themselves effectively</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, most authors </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">don&rsquo;t even know</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> those weapons exist. So the traditional &ldquo;wrestle with oneself&rdquo; is a </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">grunting bare-nuckled back-alley fight</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. Which is fun, like that great brawl in They Live. But a story that incorporates </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">rationality</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, upgrades this to a duel between cyber-ninjas with laser swords. It&rsquo;s freakin&rsquo; cool and you don&rsquo;t get to see that in most books, so it&rsquo;s a hell of a show!</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">So, that&rsquo;s rationality, and that&rsquo;s how it makes stories awesome and unusual. But - why Harry Potter? After all, it was probably that name recognition that brought you here, and not the term &ldquo;Methods of Rationality&rdquo;.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">To start with, the Potterverse is very fertile soil for fanfiction. There&rsquo;s a reason some settings have only a trickle of fanfic, while others explode with it. Some settings really lend themselves to further exploration by fans. These settings provide a </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">rich history</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> in a </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">living world</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> that goes</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> beyond just the characters</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> in the story. There are allusions to events that have happened before or are happening outside the scope of the book - the rise of Grindelwald, the first wizarding war against Voldemort, the whole first generation backstory. And since in Harry Potter the action takes place in the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">modern day</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> in and around </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">our muggle world</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, there are a lot of practical </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">implications</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> that can provide </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">speculation and plot-hooks</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> for days on end. The more rich and complex the setting is, the greater the potential it has for fanfiction to explore and grow from it, and Harry Potter has a very rich world.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Of course there are many worlds ripe for fanfiction - why Harry Potter specifically? MLP, Twilight, and Star Trek all have thriving fanfic scenes. Probably the biggest reason can be summed up in the title of the second chapter - Everything I Believe Is False.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">I mentioned at the start that Yudkowsky is a decision theorist. A lot of sci-fi writers have a background in the sciences, and </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">they explore &ldquo;what-if&rdquo;</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> scenarios from their field in their fiction. Lets say you want to write a sci-fi piece that revolves around decision theory. To make it really captivating you want a character who </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">already knows</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> how to use these skills. Training is ok, and the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">many years of training that a ninja or a demon-slaye</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">r goes through can be interesting, but the real action is when they are near the</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> peak of their mastery</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> and they have to </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">face down the Big Bad villain in a fight</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> to the death. Most of the time the training is alluded to in flashbacks, or covered in a montage. It&rsquo;s </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">just not that fun</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">To facilitate this, there is </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">one major change </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">between this fanfic and canon. In Methods of Rationality Petunia marries a kind University Professor instead of </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">an ignorant jerk</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, and he teaches Harry about the Scientific Method and gives him the full set of Enlightenment skills and ideals so the story can just right into the action.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Now to really test a character&rsquo;s skills and resolve you thrust them into a completely novel situation, one in which they </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">didn&rsquo;t prepare for</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> and never dreamed they&rsquo;d be tested, but which </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">still relies on those skills</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. Which in the case of decision theory would mean revealing to the character that </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">everything they thought they knew was false</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. They have been lied to all their lives, and the world doesn&rsquo;t really work the way they thought it did. Now they have to re-examine everything they thought they knew, test every assumption they had. </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Is this</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> thing I believe a true fact about reality, or was it part of the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">conspiracy </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">to keep me ignorant? What beliefs can I keep and what must I change? Of those, which beliefs do I have to </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">dump entirely</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, and which can I simply modify a bit? How do I </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">internalize</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> this new knowledge so that I act unconsciously on what I&rsquo;ve discovered, rather than </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">defaulting to old habits</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">?</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">If you ask people to name settings in pop culture where there is such a radical revelation, where the protagonist learns that the world is mostly a lie, the two most common answers you get are &ldquo;Harry Potter&rdquo; and &ldquo;The Matrix&rdquo;. The Matrix is really cool, but the characters aren&rsquo;t as interesting - they don&rsquo;t have parents or relatives or backstory. </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Neo, as his name alludes, is New</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> and completely </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">disconnected from the surrounding world</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, which works for the story they&rsquo;re telling of the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">isolated loner</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, but that doesn&rsquo;t make for very fertile fanfic soil. Also - the Matrix world </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">doesn&rsquo;t have magic</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. No one there is forced to say &ldquo;I just saw a human turn into a cat, but </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">she kept thinking using her human brain</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. What does this mean for what I thought I knew about brains?&rdquo;</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Plus Yudkowsky was a reader of Harry Potter fanfic, not Matrix fanfic, so it was natural to write in </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">the same world he enjoyed</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> reading.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">I should probably get into the meat of the story itself.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Methods of Rationality takes place during Harry&rsquo;s first year at Hogwarts. It starts with Harry getting his letter and initially follows the structure of the first book, with a trip to Daigon Alley, Platform 9 and &frac34;, the sorting, the conflict with Snape, even the Troll. But it does it all with </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">a rationalist slant</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, which makes it a unique sort of story, and the differences between it and the original are really cool to watch. This </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">slant results in some parodies</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> of the original, like when it sorts Hermione into Ravenclaw because - as Harry comments - if </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Hermione Granger</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> doesn&rsquo;t qualify as Ravenclaw, there&rsquo;s no reason for Ravenclaw House to exist. But the parodies </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">aren&rsquo;t mean-spirited</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> - the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">author really likes the Potterverse</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. They&rsquo;re just fun.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">In terms of genre It&rsquo;s hard to classify Methods of Rationality into any one category, but large parts of it are comedy. If you watch anime and enjoy that sort of </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">over-the-top, falling-on-your-face, winking-at-the-audience style of humor</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, you will love Methods of Rationality. It has TONS of that. It has Boy-Who-Lived</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Fangirls</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> trying to get Harry Potter to fall in love with them. It has someone trying to </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">summon Harry</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> with an epic straight-out-of-</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Lovecraft</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Elder God summoning ritual which goes&hellip; not quite how they expected.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">But it isn&rsquo;t all comedy. Harry is attacked by a </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Dementor</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> and re-lives seeing his parents murdered. He goes to Azkaban and meets a tortured </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Bellatrix Black</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. Like, literally being tortured. There&rsquo;s </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">blood debts and ransoms</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, and all the while Voldemort&rsquo;s minions are trying to </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">destroy him and kill his friends</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. So there&rsquo;s drama and action and pathos as well as comedy. And it </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">flows very nicely</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, Yudkowsky handles </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">mood-switches extremely well</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, moving from comedy to drama to action and back to comedy with a skill that rivals professional authors.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Even though the story takes place only in</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Harry&rsquo;s first year</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, it does draw in elements from the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">entire Potter timeline.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> There&rsquo;s a time-turner. Remus Lupin, Rita Skeeter, and Mad-Eye Moody all make appearances. The three Deathly Hallows and the Peveral Brothers are a major plot point. Luna Lovegood doesn&rsquo;t show up, since she&rsquo;s </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">too young to be at Hogwarts in the first year</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, but she is mentioned </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">and several issues of The Quibbler show up.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">I did mention the major change from canon - In canon Harry&rsquo;s </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">step-parents are evil</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> and keep him locked up. That wouldn&rsquo;t really work for this story, because Harry </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">can&rsquo;t be</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> locked away from the muggle world, he has to have </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">the knowledge and expectations </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">about it in order for them all to be </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">shattered</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. So But since almost all the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">action takes place at Hogwarts</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, the content of the story </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">isn&rsquo;t drastically altered</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> by that. It&rsquo;s mainly altered by the application of rationality.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The question sometimes comes up - what if I haven&rsquo;t read the original Harry Potter books, or seen the movies? There are people who&rsquo;ve heard the story is great and want to read it, but </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">don&rsquo;t have much desire to read</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> the Potter books. I ain&rsquo;t gonna lie - you won&rsquo;t enjoy it quite as much. There are a lot of in-jokes that will go right over your head if you haven&rsquo;t at least seen the movies. For example, the references to the Weasley pet rat will probably be confusing. But it&rsquo;s not as bad as you might think, because there are A LOT of references in Methods of Rationality to tons of things </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">outside the Potterverse. </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">There&rsquo;s references to anime, old sci-fi books, internet memes&hellip; there&rsquo;s shout-outs to Star Wars and even to Gargoyles. So </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">everyone</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> will miss </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">something</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. The in-jokes are great when you get them, and no big deal when you don&rsquo;t, and if the in-jokes you don&rsquo;t get happen to be Harry Potter in-jokes, that&rsquo;s not a tragedy. To be honest, </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">I hadn&rsquo;t read the last two</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> potter books when I started on Methods of Rationality myself. And</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> I loved i</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">t.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">In the end, you </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">don&rsquo;t actually </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">need</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">to have read the Potter books to enjoy Methods of Rationality. </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Characters</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> are still </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">introduced in a coherent way</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, the plot is internally </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">consistent</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, and the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">knowledge you need to understand and enjoy</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> the story is </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">presented in the text</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. So if you&rsquo;re on the fence, go ahead and give it a try. You really don&rsquo;t have to </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">plow through seven books</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> you aren&rsquo;t excited about. But if you can find the time to watch at least the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">first movie</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, it does make it more enjoyable.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Some of you may have realized that there is a problem with giving Harry a major rationality upgrade. For a story to be exciting there</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> must be a real conflict</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, not a one-sided beatdown. There&rsquo;s a law of good fanfic that says &ldquo;If you give Frodo a lightsaber, you must give Sauron the Death Star.&rdquo; Fortunately </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">this IS a good fanfic</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, and Voldemort gets a huge upgrade in intelligence and rationality. The way he </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">wraps the entire Wizard World into knots</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, even seducing Harry, is epic. And Draco Malfoy gets an upgrade as well, and turns from an </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">egotistical bully to a shrewd plotter</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. This makes for really good reading for those of us more interested in power grabs and back-stabbing than </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">broomstick-based sports</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. Not that there&rsquo;s anything wrong with that...</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Personally, the plotting really is phenomenal. There is </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">foreshadowing everywhere</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, things you&rsquo;ll read that seem like throw-away jokes when you first encounter them, but that are clearly </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">signs</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> saying &ldquo;This is what is going to happen next!&rdquo; that blow you away when you read through a second time. There are </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">chekov&rsquo;s guns</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> that are laid out early on that aren&rsquo;t fired until 50 chapters later (chapters aren&rsquo;t that long). The way little plot points and comments are </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">woven in and out</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, tieing early tiny actions back to huge events much later is stunning.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Obviously I&rsquo;m a big fan.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">There is one other thing about Harry Potter and the Methods of Rationality that makes it unusual. It&rsquo;s </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">not just a novel</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. It&rsquo;s also a deliberate instructional mechanism. Humans learn things by story-telling. Imagining something is </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">mentally analogous to remembering something</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> that didn&rsquo;t actually happen. Yudkowsky uses this intentionally to direct his audience into developing stronger rationality skills. Almost every chapter, or group of chapters, is specifically designed to teach a technique or skill of rationality. The technique to be taught is right there in the chapter title. </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Chapter 26: Noticing Confusion.</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Most of the time a character, often Harry, will at some point explicitly explain what the technique is or how it is to be used. The chapter will also contain at least one example of someone succeeding or failing in the use of the technique. Sometimes multiple examples. Sometimes multiple examples of both.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The really crazy thing is, you generally don&rsquo;t notice. The writing is strong and the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">story really pulls you in</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, so it&rsquo;s</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> integrated seamlessly into the plot</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> progression. It isn&rsquo;t until</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> I go back</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> and read a chapter a second time, referring back to the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">chapter title</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> and really keeping my eyes open for all examples of it, that I realize just how </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">central </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">that particular idea is in that chapter. It makes me wish all books were written like that.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">And as a final bonus for anyone who likes to really dig deep into their novels, Yudkowsky&rsquo;s stated that Methods is a </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">puzzle</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> that&rsquo;s meant to be </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">solvable</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. That all the clues are laid out within it, and a reader who really wants to can work it out before it&rsquo;s revealed at the end. Toward that end there are a number of places online where people discuss Methods of Rationality and what they think is happening. There&rsquo;s a thread on TVTropes, and an HPMoR sub-reddit, as well as just people blogging about it now and then. So if you&rsquo;re into that sort of puzzle-solving, this is right up your alley. The</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> final arc will be released</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> later this year, so there&rsquo;s still time to get in on the action.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">OK, all that being said, this fanfic isn&rsquo;t for everyone. There are some people who dislike how </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Harry talks to adults</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. Most of these people are parents. /shrug I&rsquo;m not a parent, I don&rsquo;t know. Some people just never get into the story, which is fine. The humor doesn&rsquo;t appeal to everyone, and some of the dark parts are pretty dark. And I really wouldn&rsquo;t recommend this to anyone who isn&rsquo;t at least </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">in their teens</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> yet. The </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">terminology</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> and some of the </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">more complex ideas</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> are probably too daunting for younger readers. </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Also</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> the story does touch on more </span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">adult subject matter</span><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> a few times.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">I&rsquo;ll wrap up with some final info on where you can find this. The official home is at FanFiction.Net. You can go there and search for Harry Potter And The Methods of Rationality. Or just google Harry Potter And The Methods of Rationality. The cleanest site, with a table of contents and resource links and everything, is HPMOR.COM. That&rsquo;s the site I use when I read it. There&rsquo;s also the audio-book version, which is at HPMORpodcast.com. I run that one. And of course all of it is free.</span></p>\n<p><strong style=\"font-weight: normal;\"><br /></strong></p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">*breath*</span></p>\n<p>&nbsp;</p>\n<p style=\"line-height: 1.15; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 15px; font-family: Arial; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Alright, that&rsquo;s my presentation, and I hope you&rsquo;ve learned whatever you wanted to learn. Give it a shot and maybe you&rsquo;ll love it as much as I do. I&rsquo;ll now open the floor to questions.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eKdtbf32Hfa9ia9u2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "24303", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-30T05:18:36.502Z", "modifiedAt": null, "url": null, "title": "Open Thread, September 30 - October 6, 2013", "slug": "open-thread-september-30-october-6-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:04.492Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yeu8qF9tmsjeBCh3q/open-thread-september-30-october-6-2013", "pageUrlRelative": "/posts/yeu8qF9tmsjeBCh3q/open-thread-september-30-october-6-2013", "linkUrl": "https://www.lesswrong.com/posts/yeu8qF9tmsjeBCh3q/open-thread-september-30-october-6-2013", "postedAtFormatted": "Monday, September 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20September%2030%20-%20October%206%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20September%2030%20-%20October%206%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyeu8qF9tmsjeBCh3q%2Fopen-thread-september-30-october-6-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20September%2030%20-%20October%206%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyeu8qF9tmsjeBCh3q%2Fopen-thread-september-30-october-6-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyeu8qF9tmsjeBCh3q%2Fopen-thread-september-30-october-6-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p>If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yeu8qF9tmsjeBCh3q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 1.3608552019879805e-06, "legacy": true, "legacyId": "24304", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 297, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-30T09:24:12.299Z", "modifiedAt": null, "url": null, "title": "Estimation as a game", "slug": "estimation-as-a-game", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:35.842Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnatoliP", "createdAt": "2013-06-30T04:12:20.253Z", "isAdmin": false, "displayName": "AnatoliP"}, "userId": "XgT5etjaWdft2P7ca", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YJnLQeenXrxwNeZH8/estimation-as-a-game", "pageUrlRelative": "/posts/YJnLQeenXrxwNeZH8/estimation-as-a-game", "linkUrl": "https://www.lesswrong.com/posts/YJnLQeenXrxwNeZH8/estimation-as-a-game", "postedAtFormatted": "Monday, September 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Estimation%20as%20a%20game&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEstimation%20as%20a%20game%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYJnLQeenXrxwNeZH8%2Festimation-as-a-game%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Estimation%20as%20a%20game%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYJnLQeenXrxwNeZH8%2Festimation-as-a-game", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYJnLQeenXrxwNeZH8%2Festimation-as-a-game", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 217, "htmlBody": "<address><span style=\"font-style: normal;\">I was inspired to write this post after reading</span>&nbsp;<a id=\"author_t3_iha\" style=\"color: #8a8a8b; text-decoration: none; font-weight: bold; font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 22px; text-align: justify;\" href=\"/user/Gunnar_Zarncke/\">Gunnar_Zarncke</a>'s&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/iha/raising_numerate_children/\">Raising numerate children</a>.</address>\n<p>Developing rational patterns of thought in children is very important and I'm glad Gunnar brought that issue up.</p>\n<p>I wanted to share with you some thoughts I have regarding estimation games.</p>\n<p>From an early age I've been constantly calculating various kinds of estimates - e.g. \"how many people live in this building\", \"how long will it take to cross the US on foot\", \"what's the height of that tower\", \"how many BMWs are manufactured annually\" and so on.</p>\n<p>I believe that practising this technique is not only fun but also helpful. Sometimes one has no way or time to acquire accurate information regarding something and even a rough estimate can be very valuable.&nbsp;</p>\n<p>People are often surprised when they see me do it whereas for me it is completely natural. I think the reason is that I do it from a very early age.</p>\n<p>I think it's easy and natural for children to grasp if this method is introduced through everyday experiences. By making this into a game children can gain intuitive understanding of quantitative techniques. I suspect many children can enjoy this kind of games.</p>\n<p>I'd like to hear your thoughts on the subject.</p>\n<p>Do you remember yourself doing something like this? From what age? Do you practice anything similar with your children?</p>\n<div><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YJnLQeenXrxwNeZH8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 14, "extendedScore": null, "score": 1.361077756700138e-06, "legacy": true, "legacyId": "24305", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZhoyWrqXRShbee4bS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-30T16:09:02.824Z", "modifiedAt": null, "url": null, "title": "Book Review: Na\u00efve Set Theory (MIRI course list)", "slug": "book-review-naive-set-theory-miri-course-list", "viewCount": null, "lastCommentedAt": "2019-12-29T11:11:50.062Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "So8res", "createdAt": "2012-01-10T05:50:18.713Z", "isAdmin": false, "displayName": "So8res"}, "userId": "xSfc2APSi8WzFxp7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ee8CZW7wzaNdCENYG/book-review-naive-set-theory-miri-course-list", "pageUrlRelative": "/posts/Ee8CZW7wzaNdCENYG/book-review-naive-set-theory-miri-course-list", "linkUrl": "https://www.lesswrong.com/posts/Ee8CZW7wzaNdCENYG/book-review-naive-set-theory-miri-course-list", "postedAtFormatted": "Monday, September 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Book%20Review%3A%20Na%C3%AFve%20Set%20Theory%20(MIRI%20course%20list)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABook%20Review%3A%20Na%C3%AFve%20Set%20Theory%20(MIRI%20course%20list)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEe8CZW7wzaNdCENYG%2Fbook-review-naive-set-theory-miri-course-list%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Book%20Review%3A%20Na%C3%AFve%20Set%20Theory%20(MIRI%20course%20list)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEe8CZW7wzaNdCENYG%2Fbook-review-naive-set-theory-miri-course-list", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEe8CZW7wzaNdCENYG%2Fbook-review-naive-set-theory-miri-course-list", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1359, "htmlBody": "<p>I'm reviewing the books on the <a href=\"http://intelligence.org/courses/\">MIRI course list</a>. I followed <a href=\"/lw/ioo/book_review_basic_category_theory_for_computer/\">Category Theory</a>&nbsp;with <em>Na&iuml;ve Set Theory</em>, by Paul R. Halmos.</p>\n<h1 id=\"navesettheory\">Na&iuml;ve Set Theory</h1>\n<p style=\"text-align:center\"><img src=\"http://img1.imagesbn.com/p/9781614271314_p0_v1_s260x420.JPG\" alt=\"Book cover\" width=\"260\" height=\"391\" /></p>\n<p>This book is tiny, containing about 100 pages. It's quite dense, but it's not a difficult read. I'll review the content before giving my impressions.</p>\n<h2>Chapter List</h2>\n<ol>\n<li>The Axiom of Extension</li>\n<li>The Axiom of Specification</li>\n<li>Unordered Pairs</li>\n<li>Unions and Intersections</li>\n<li>Complements and Powers</li>\n<li>Ordered Pairs</li>\n<li>Relations</li>\n<li>Functions</li>\n<li>Families</li>\n<li>Inverses and Composites</li>\n<li>Numbers</li>\n<li>The Peano Axioms</li>\n<li>Arithmetic</li>\n<li>Order</li>\n<li>The Axiom of Choice</li>\n<li>Zorn's Lemma</li>\n<li>Well Ordering</li>\n<li>Transfinite Recursion</li>\n<li>Ordinal Numbers</li>\n<li>Sets of Ordinal Numbers</li>\n<li>Ordinal Arithmetic</li>\n<li>The Schr&ouml;der&mdash;Bernstein Theorem</li>\n<li>Countable Sets</li>\n<li>Cardinal Arithmetic</li>\n<li>Cardinal Numbers</li>\n</ol> \n<hr />\n<p><a id=\"more\"></a></p>\n<p>Normally I'd summarize each chapter, but chapters were about four tiny pages each and the content is mostly described by the chapter name. Zorn's Lemma states that if all chains in a set have an upper bound, then the set has a maximal element. (This follows from the axiom of choice.) The Schr&ouml;der-Bernstein Theorem states that if X is equivalent to a subset of Y, and Y is equivalent to a subset of X, then X and Y are equivalent. The other chapter titles are self-evident.</p>\n<p>Each chapter presented the concepts in a concise manner, then worked through a few of the implications (with proofs), then provided a few short exercises.</p>\n<p>None of the concepts within were particularly surprising, but it was good to play with them first-hand. Most useful was interacting with ordinal and cardinal numbers. It was nice to examine the actual structure of each type of number (in set theory) and deepen my previously-superficial knowledge of the distinction.</p>\n<h2 id=\"discussion\">Discussion</h2>\n<p>Before diving in to the review it's important to remember that the usefulness of a math textbook is heavily dependent upon your math background. I have a moderately strong background. Some specific subjects (analysis, type theory, group theory, etc.) have given me a solid, if indirect, foundation in set theory. This was the first time I studied set theory directly, but the concepts were hardly new.</p>\n<h3 id=\"overview\">Overview</h3>\n<p>I was pleased with this book. It is terse. It has exercises. It gives you information and gets out of your way, which is what I like in a textbook: It doesn't waste your time. I'm about to harp on the book for a spell, but please keep in mind that my overall feeling was positive.</p>\n<p>Please take these reviews with a grain of salt, as sample size is 1 and I have not read any similar textbooks.</p>\n<h3 id=\"complaints\">Complaints</h3>\n<ul>\n<li>\n<p><strong>The book was written in 1960, and it shows.</strong> Set theory is more mature now than it was then. The authors often remark on syntax that was not yet standard (which is now commonplace). The continuum hypothesis had not yet been proven unprovable in ZFC. The axiom of choice is embraced wholeheartedly with no discussion of weaker variants. The style of proof differs from the modern style. None of this is bad, per se. In fact, it's quite a fascinating time capsule: I enjoyed seeing a slice of mathematics from half a century past. However, I believe a more modern introduction to set theory could have taught me more pertinent mathematics in the same amount of time.</p>\n</li>\n<li>\n<p><strong>The notation is inconsistent.</strong> I've long believed that math is a poor and inconsistent language. This is evident throughout set theory. To the author's credit, they point out many of the inconsistencies: f(A) can refer to both a function or a restriction of a function to the subset A of its domain, 2^w can refer to either functions mapping w onto 2 or a specific ordinal number, etc. I am personally of the opinion that introductory textbooks should enforce a pure &amp; consistent syntax (which may be relaxed in practice). I was mildly annoyed with how the authors acknowledged the inconsistencies and then embraced them, thereby perpetuating a memetic tragedy of the commons. (I know that I shouldn't expect better, but one can dream.)</p>\n</li>\n<li>\n<p><strong>The proofs given were primarily in english.</strong> Not once did the authors write &exist; or &forall;. They would resort to \"for some\" or \"for any\" in largely english-language proofs. The proofs were rigorous (the authors tightly restricted their english phrases), but I was somewhat surprised to find the axioms of set theory described in lingual (rather than symbolic) form.</p>\n</li>\n<li>\n<p><strong>Set theory is axiom soup.</strong> I do not view set theory as foundational. Is the axiom of choice true? The question is poorly formed. Axioms are tools to constrain what you're talking about. Better questions are shaped like \"does the axiom of choice apply to this thing I'm working with?\", or \"how does the structure change if we take this statement as an axiom?\". This sentiment seems fairly common in modern mathematics, but it was lacking in <em>Na&iuml;ve Set Theory</em>. Axioms were presented as facts, not tools. There was little exploration of each axiom, what it cost and what it bought, and what alternate forms are available.</p>\n</li>\n</ul>\n<p>Most of these gripes are small compared to the amount of good data in the book. Remember that the book is titled <em>Na&iuml;ve Set Theory</em>: a little na&iuml;vety is to be expected. The takeaway is that the book was good, but likely could have been better in light of modern mathematics. All in all, the book covers lot of ground at a fast clip, and was quite useful.</p>\n<h2 id=\"shouldilearnsettheory\">Should I learn set theory?</h2>\n<p>As always, it depends upon your goals. Set theory is everywhere in mathematics, and I personally appreciated shoring up my foundations. If you have similar goals, you can easily go through this book in a week if you think that learning set theory is worth your time.</p>\n<p>I don't particularly recommend set theory to armchair mathematicians. In my experience, other areas of mathematics are much more fun from a casual standpoint. (Group theory and information theory come to mind, if you're looking for a good time.)</p>\n<h2 id=\"shouldireadthisbook\">Should I read this book?</h2>\n<p>Maybe. I have no point of comparison here. My tentative suggestion is that you should find a more modern (but similarly terse) introductory textbook and read that instead. (If you have a good suggestion, you should leave it in the comments.)</p>\n<p>I found this book to be rather basic. If you have a background similar to mine, I recommend something a little more advanced. (Unfortunately, I can make no recommendations. Again, comments are welcome.)</p>\n<p>This book seems well-suited for a layperson interested in learning set theory. The 1960s feel is definitely fun. I would guess that the book is well-paced for someone who has done the standard college calculus courses but is unfamiliar with Set Theory subject matter.</p>\n<h2 id=\"whatshouldiread\">What should I read?</h2>\n<p>If you're going to read the book then I suggest reading the whole thing. It builds from first principles up to cardinality, and nothing along the way is unimportant. My only suggestion is that you swap chapter 25 and 24: they appear to have been ordered incorrectly for political reasons. (The derivation of cardinal numbers used in chapter 25 was, at the time, controversial, so the book presents cardinal arithmetic before cardinal numbers.) Other than that, the book was well structured.</p>\n<h2 id=\"finalnotes\">Final Notes</h2>\n<p>If a comparably short-and-sweet textbook written in the last twenty years can be found, I recommend updating the suggestion on the MIRI course list. It's not clear to me how much raw set theory is useful in modern AI research; my wild guess is that mathematical logic, model theory, and provability theory are more important. If that is the case, then I think the technical level of this book is appropriate for the course list: it's sufficient to brush up on the basics, but it doesn't send you deep into rabbit holes when there are more interesting topics on the horizon.</p>\n<p>My next review will take more time than did the previous four. I have a number of loose ends to tie up before jumping in to Model Theory, and I have much less familiarity with the subject matter.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2, "4Kcm4etxAJjmeDkHP": 2, "6nS8oYmSMuFMaiowF": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ee8CZW7wzaNdCENYG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 49, "extendedScore": null, "score": 0.000123, "legacy": true, "legacyId": "24306", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 50, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I'm reviewing the books on the <a href=\"http://intelligence.org/courses/\">MIRI course list</a>. I followed <a href=\"/lw/ioo/book_review_basic_category_theory_for_computer/\">Category Theory</a>&nbsp;with <em>Na\u00efve Set Theory</em>, by Paul R. Halmos.</p>\n<h1 id=\"Na_ve_Set_Theory\">Na\u00efve Set Theory</h1>\n<p style=\"text-align:center\"><img src=\"http://img1.imagesbn.com/p/9781614271314_p0_v1_s260x420.JPG\" alt=\"Book cover\" width=\"260\" height=\"391\"></p>\n<p>This book is tiny, containing about 100 pages. It's quite dense, but it's not a difficult read. I'll review the content before giving my impressions.</p>\n<h2 id=\"Chapter_List\">Chapter List</h2>\n<ol>\n<li>The Axiom of Extension</li>\n<li>The Axiom of Specification</li>\n<li>Unordered Pairs</li>\n<li>Unions and Intersections</li>\n<li>Complements and Powers</li>\n<li>Ordered Pairs</li>\n<li>Relations</li>\n<li>Functions</li>\n<li>Families</li>\n<li>Inverses and Composites</li>\n<li>Numbers</li>\n<li>The Peano Axioms</li>\n<li>Arithmetic</li>\n<li>Order</li>\n<li>The Axiom of Choice</li>\n<li>Zorn's Lemma</li>\n<li>Well Ordering</li>\n<li>Transfinite Recursion</li>\n<li>Ordinal Numbers</li>\n<li>Sets of Ordinal Numbers</li>\n<li>Ordinal Arithmetic</li>\n<li>The Schr\u00f6der\u2014Bernstein Theorem</li>\n<li>Countable Sets</li>\n<li>Cardinal Arithmetic</li>\n<li>Cardinal Numbers</li>\n</ol> \n<hr>\n<p><a id=\"more\"></a></p>\n<p>Normally I'd summarize each chapter, but chapters were about four tiny pages each and the content is mostly described by the chapter name. Zorn's Lemma states that if all chains in a set have an upper bound, then the set has a maximal element. (This follows from the axiom of choice.) The Schr\u00f6der-Bernstein Theorem states that if X is equivalent to a subset of Y, and Y is equivalent to a subset of X, then X and Y are equivalent. The other chapter titles are self-evident.</p>\n<p>Each chapter presented the concepts in a concise manner, then worked through a few of the implications (with proofs), then provided a few short exercises.</p>\n<p>None of the concepts within were particularly surprising, but it was good to play with them first-hand. Most useful was interacting with ordinal and cardinal numbers. It was nice to examine the actual structure of each type of number (in set theory) and deepen my previously-superficial knowledge of the distinction.</p>\n<h2 id=\"Discussion\">Discussion</h2>\n<p>Before diving in to the review it's important to remember that the usefulness of a math textbook is heavily dependent upon your math background. I have a moderately strong background. Some specific subjects (analysis, type theory, group theory, etc.) have given me a solid, if indirect, foundation in set theory. This was the first time I studied set theory directly, but the concepts were hardly new.</p>\n<h3 id=\"Overview\">Overview</h3>\n<p>I was pleased with this book. It is terse. It has exercises. It gives you information and gets out of your way, which is what I like in a textbook: It doesn't waste your time. I'm about to harp on the book for a spell, but please keep in mind that my overall feeling was positive.</p>\n<p>Please take these reviews with a grain of salt, as sample size is 1 and I have not read any similar textbooks.</p>\n<h3 id=\"Complaints\">Complaints</h3>\n<ul>\n<li>\n<p><strong>The book was written in 1960, and it shows.</strong> Set theory is more mature now than it was then. The authors often remark on syntax that was not yet standard (which is now commonplace). The continuum hypothesis had not yet been proven unprovable in ZFC. The axiom of choice is embraced wholeheartedly with no discussion of weaker variants. The style of proof differs from the modern style. None of this is bad, per se. In fact, it's quite a fascinating time capsule: I enjoyed seeing a slice of mathematics from half a century past. However, I believe a more modern introduction to set theory could have taught me more pertinent mathematics in the same amount of time.</p>\n</li>\n<li>\n<p><strong>The notation is inconsistent.</strong> I've long believed that math is a poor and inconsistent language. This is evident throughout set theory. To the author's credit, they point out many of the inconsistencies: f(A) can refer to both a function or a restriction of a function to the subset A of its domain, 2^w can refer to either functions mapping w onto 2 or a specific ordinal number, etc. I am personally of the opinion that introductory textbooks should enforce a pure &amp; consistent syntax (which may be relaxed in practice). I was mildly annoyed with how the authors acknowledged the inconsistencies and then embraced them, thereby perpetuating a memetic tragedy of the commons. (I know that I shouldn't expect better, but one can dream.)</p>\n</li>\n<li>\n<p><strong>The proofs given were primarily in english.</strong> Not once did the authors write \u2203 or \u2200. They would resort to \"for some\" or \"for any\" in largely english-language proofs. The proofs were rigorous (the authors tightly restricted their english phrases), but I was somewhat surprised to find the axioms of set theory described in lingual (rather than symbolic) form.</p>\n</li>\n<li>\n<p><strong>Set theory is axiom soup.</strong> I do not view set theory as foundational. Is the axiom of choice true? The question is poorly formed. Axioms are tools to constrain what you're talking about. Better questions are shaped like \"does the axiom of choice apply to this thing I'm working with?\", or \"how does the structure change if we take this statement as an axiom?\". This sentiment seems fairly common in modern mathematics, but it was lacking in <em>Na\u00efve Set Theory</em>. Axioms were presented as facts, not tools. There was little exploration of each axiom, what it cost and what it bought, and what alternate forms are available.</p>\n</li>\n</ul>\n<p>Most of these gripes are small compared to the amount of good data in the book. Remember that the book is titled <em>Na\u00efve Set Theory</em>: a little na\u00efvety is to be expected. The takeaway is that the book was good, but likely could have been better in light of modern mathematics. All in all, the book covers lot of ground at a fast clip, and was quite useful.</p>\n<h2 id=\"Should_I_learn_set_theory_\">Should I learn set theory?</h2>\n<p>As always, it depends upon your goals. Set theory is everywhere in mathematics, and I personally appreciated shoring up my foundations. If you have similar goals, you can easily go through this book in a week if you think that learning set theory is worth your time.</p>\n<p>I don't particularly recommend set theory to armchair mathematicians. In my experience, other areas of mathematics are much more fun from a casual standpoint. (Group theory and information theory come to mind, if you're looking for a good time.)</p>\n<h2 id=\"Should_I_read_this_book_\">Should I read this book?</h2>\n<p>Maybe. I have no point of comparison here. My tentative suggestion is that you should find a more modern (but similarly terse) introductory textbook and read that instead. (If you have a good suggestion, you should leave it in the comments.)</p>\n<p>I found this book to be rather basic. If you have a background similar to mine, I recommend something a little more advanced. (Unfortunately, I can make no recommendations. Again, comments are welcome.)</p>\n<p>This book seems well-suited for a layperson interested in learning set theory. The 1960s feel is definitely fun. I would guess that the book is well-paced for someone who has done the standard college calculus courses but is unfamiliar with Set Theory subject matter.</p>\n<h2 id=\"What_should_I_read_\">What should I read?</h2>\n<p>If you're going to read the book then I suggest reading the whole thing. It builds from first principles up to cardinality, and nothing along the way is unimportant. My only suggestion is that you swap chapter 25 and 24: they appear to have been ordered incorrectly for political reasons. (The derivation of cardinal numbers used in chapter 25 was, at the time, controversial, so the book presents cardinal arithmetic before cardinal numbers.) Other than that, the book was well structured.</p>\n<h2 id=\"Final_Notes\">Final Notes</h2>\n<p>If a comparably short-and-sweet textbook written in the last twenty years can be found, I recommend updating the suggestion on the MIRI course list. It's not clear to me how much raw set theory is useful in modern AI research; my wild guess is that mathematical logic, model theory, and provability theory are more important. If that is the case, then I think the technical level of this book is appropriate for the course list: it's sufficient to brush up on the basics, but it doesn't send you deep into rabbit holes when there are more interesting topics on the horizon.</p>\n<p>My next review will take more time than did the previous four. I have a number of loose ends to tie up before jumping in to Model Theory, and I have much less familiarity with the subject matter.</p>", "sections": [{"title": "Na\u00efve Set Theory", "anchor": "Na_ve_Set_Theory", "level": 1}, {"title": "Chapter List", "anchor": "Chapter_List", "level": 2}, {"title": "Discussion", "anchor": "Discussion", "level": 2}, {"title": "Overview", "anchor": "Overview", "level": 3}, {"title": "Complaints", "anchor": "Complaints", "level": 3}, {"title": "Should I learn set theory?", "anchor": "Should_I_learn_set_theory_", "level": 2}, {"title": "Should I read this book?", "anchor": "Should_I_read_this_book_", "level": 2}, {"title": "What should I read?", "anchor": "What_should_I_read_", "level": 2}, {"title": "Final Notes", "anchor": "Final_Notes", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "21 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Jar4BGrJ7BiQemBDM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-30T19:10:22.449Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup: What are the odds?", "slug": "meetup-west-la-meetup-what-are-the-odds", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:35.747Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Z5kGsfZivv3yrAQ5X/meetup-west-la-meetup-what-are-the-odds", "pageUrlRelative": "/posts/Z5kGsfZivv3yrAQ5X/meetup-west-la-meetup-what-are-the-odds", "linkUrl": "https://www.lesswrong.com/posts/Z5kGsfZivv3yrAQ5X/meetup-west-la-meetup-what-are-the-odds", "postedAtFormatted": "Monday, September 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%3A%20What%20are%20the%20odds%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%3A%20What%20are%20the%20odds%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ5kGsfZivv3yrAQ5X%2Fmeetup-west-la-meetup-what-are-the-odds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%3A%20What%20are%20the%20odds%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ5kGsfZivv3yrAQ5X%2Fmeetup-west-la-meetup-what-are-the-odds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ5kGsfZivv3yrAQ5X%2Fmeetup-west-la-meetup-what-are-the-odds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 237, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rn'>West LA Meetup: What are the odds?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 October 2013 07:00:00AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Suite 312, Los Angeles, CA, 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, October 2nd.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion:</strong></p>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary;</em> this will be generally accessible and useful to everyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n\n<p>There will be a probably whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p>\n\n<p>This Wednesday, we will try to define \"probability.\" (Spoiler: It is hard. We will fail.) We will discuss what we mean, or think we mean, and what we think other people mean by the word probability. We will look at the mathematical properties of probabilities, and (without actually doing the measure theory) try to understand just how much very weird math goes into defining the seemingly simple concept of probability. We will discuss different interpretations of how probabilities apply to the real world, such as frequentist and Bayesian interpretations. There many more questions to be answered, even after we hit the \"Bayesian\" applause light.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rn'>West LA Meetup: What are the odds?</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Z5kGsfZivv3yrAQ5X", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.3616091917362315e-06, "legacy": true, "legacyId": "24307", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup__What_are_the_odds_\">Discussion article for the meetup : <a href=\"/meetups/rn\">West LA Meetup: What are the odds?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 October 2013 07:00:00AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Suite 312, Los Angeles, CA, 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, October 2nd.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong id=\"Discussion_\">Discussion:</strong></p>\n\n<p><em>No prior knowledge of or exposure to Less Wrong is necessary;</em> this will be generally accessible and useful to everyone who values thinking for themselves. There will be open general conversation until 7:30, and that's always a lot of good, fun, intelligent discussion!</p>\n\n<p>There will be a probably whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p>\n\n<p>This Wednesday, we will try to define \"probability.\" (Spoiler: It is hard. We will fail.) We will discuss what we mean, or think we mean, and what we think other people mean by the word probability. We will look at the mathematical properties of probabilities, and (without actually doing the measure theory) try to understand just how much very weird math goes into defining the seemingly simple concept of probability. We will discuss different interpretations of how probabilities apply to the real world, such as frequentist and Bayesian interpretations. There many more questions to be answered, even after we hit the \"Bayesian\" applause light.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup__What_are_the_odds_1\">Discussion article for the meetup : <a href=\"/meetups/rn\">West LA Meetup: What are the odds?</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup: What are the odds?", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup__What_are_the_odds_", "level": 1}, {"title": "Discussion:", "anchor": "Discussion_", "level": 2}, {"title": "Discussion article for the meetup : West LA Meetup: What are the odds?", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup__What_are_the_odds_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-30T19:44:31.547Z", "modifiedAt": null, "url": null, "title": "Group Rationality Diary, October 1-15, plus frequency poll", "slug": "group-rationality-diary-october-1-15-plus-frequency-poll", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:36.552Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RRueaJkaFcxe6fGAT/group-rationality-diary-october-1-15-plus-frequency-poll", "pageUrlRelative": "/posts/RRueaJkaFcxe6fGAT/group-rationality-diary-october-1-15-plus-frequency-poll", "linkUrl": "https://www.lesswrong.com/posts/RRueaJkaFcxe6fGAT/group-rationality-diary-october-1-15-plus-frequency-poll", "postedAtFormatted": "Monday, September 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20Rationality%20Diary%2C%20October%201-15%2C%20plus%20frequency%20poll&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20Rationality%20Diary%2C%20October%201-15%2C%20plus%20frequency%20poll%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRRueaJkaFcxe6fGAT%2Fgroup-rationality-diary-october-1-15-plus-frequency-poll%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20Rationality%20Diary%2C%20October%201-15%2C%20plus%20frequency%20poll%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRRueaJkaFcxe6fGAT%2Fgroup-rationality-diary-october-1-15-plus-frequency-poll", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRRueaJkaFcxe6fGAT%2Fgroup-rationality-diary-october-1-15-plus-frequency-poll", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 232, "htmlBody": "<p>This is the public group instrumental rationality diary for October 1-15.</p>\n<blockquote style=\"font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">\n<p style=\"margin: 0px 0px 1em;\"><span style=\"color: #333333;\">It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em;\">Or anything else interesting which you  want to share, so that other people can think about it, and perhaps be  inspired to take action themselves. &nbsp;Try to include enough details so  that everyone can use each other's experiences to learn about what tends  to work out, and what doesn't tend to work out.</p>\n</blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><strong>New this month</strong>:&nbsp; If you have preferences about the future frequency of rationality diary posts, please express them in the poll below!</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\">Thanks to&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/therufs/submitted/r/discussion/lw/hg0/group_rationality_diary_may_1631/user/cata\">cata</a>&nbsp;for starting the Group Rationality Diary posts, and to commenters for participating.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><a href=\"/r/discussion/lw/iho/group_rationality_diary_september_1630/\">Immediate past diary</a>:&nbsp; September 16-30</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial,Helvetica,sans-serif; line-height: 19px; text-align: justify;\"><a href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">Rationality diaries archive</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RRueaJkaFcxe6fGAT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "24308", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XPsft9gKw2tPQBuC2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-30T20:26:36.037Z", "modifiedAt": null, "url": null, "title": "Meetup : Boulder CO", "slug": "meetup-boulder-co", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:34.438Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "yakurbe0112", "createdAt": "2012-07-01T23:40:26.855Z", "isAdmin": false, "displayName": "yakurbe0112"}, "userId": "xXr6Jngw57uMXveQ9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fwA2qYZdkubcFLtmu/meetup-boulder-co", "pageUrlRelative": "/posts/fwA2qYZdkubcFLtmu/meetup-boulder-co", "linkUrl": "https://www.lesswrong.com/posts/fwA2qYZdkubcFLtmu/meetup-boulder-co", "postedAtFormatted": "Monday, September 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Boulder%20CO&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Boulder%20CO%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfwA2qYZdkubcFLtmu%2Fmeetup-boulder-co%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Boulder%20CO%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfwA2qYZdkubcFLtmu%2Fmeetup-boulder-co", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfwA2qYZdkubcFLtmu%2Fmeetup-boulder-co", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ro'>Boulder CO</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 October 2013 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\"> Old Chicago 1102 Pearl St, Boulder, CO</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I've been sota bussy and the best time for the next meetup would be thursday. If you would like to come, I'll be showing everyone zendo.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ro'>Boulder CO</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fwA2qYZdkubcFLtmu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3616783273893796e-06, "legacy": true, "legacyId": "24309", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Boulder_CO\">Discussion article for the meetup : <a href=\"/meetups/ro\">Boulder CO</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 October 2013 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\"> Old Chicago 1102 Pearl St, Boulder, CO</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I've been sota bussy and the best time for the next meetup would be thursday. If you would like to come, I'll be showing everyone zendo.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Boulder_CO1\">Discussion article for the meetup : <a href=\"/meetups/ro\">Boulder CO</a></h2>", "sections": [{"title": "Discussion article for the meetup : Boulder CO", "anchor": "Discussion_article_for_the_meetup___Boulder_CO", "level": 1}, {"title": "Discussion article for the meetup : Boulder CO", "anchor": "Discussion_article_for_the_meetup___Boulder_CO1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-09-30T22:16:45.283Z", "modifiedAt": null, "url": null, "title": "Meetup : Paris Meetup: Sunday, October 6: New people, games...", "slug": "meetup-paris-meetup-sunday-october-6-new-people-games", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:35.889Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Emile", "createdAt": "2009-02-27T09:35:34.359Z", "isAdmin": false, "displayName": "Emile"}, "userId": "4PkX6dj649JqKSh4s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WmcKJxmtDddr3Tep2/meetup-paris-meetup-sunday-october-6-new-people-games", "pageUrlRelative": "/posts/WmcKJxmtDddr3Tep2/meetup-paris-meetup-sunday-october-6-new-people-games", "linkUrl": "https://www.lesswrong.com/posts/WmcKJxmtDddr3Tep2/meetup-paris-meetup-sunday-october-6-new-people-games", "postedAtFormatted": "Monday, September 30th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Paris%20Meetup%3A%20Sunday%2C%20October%206%3A%20New%20people%2C%20games...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Paris%20Meetup%3A%20Sunday%2C%20October%206%3A%20New%20people%2C%20games...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmcKJxmtDddr3Tep2%2Fmeetup-paris-meetup-sunday-october-6-new-people-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Paris%20Meetup%3A%20Sunday%2C%20October%206%3A%20New%20people%2C%20games...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmcKJxmtDddr3Tep2%2Fmeetup-paris-meetup-sunday-october-6-new-people-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmcKJxmtDddr3Tep2%2Fmeetup-paris-meetup-sunday-october-6-new-people-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rp'>Paris Meetup: Sunday, October 6: New people, games...</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 October 2013 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Caf\u00e9 des Arts et M\u00e9tiers\u200e, 51 Rue Turbigo, Paris</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next Paris Meetup will be Sunday, October 6, at the Caf\u00e9 des Arts et M\u00e9tiers opposite the Museum.</p>\n\n<p>Topics:\n* Welcome new people\n* Board games\n* Plan future meetups\n* Other suggestions? Someone wanted to talk about quantum mechanics...</p>\n\n<p>Reminder: there is the LessWrong France mailing list for discussing and organizing meetups (that for now mostly happen in Paris and Lyon): https://groups.google.com/forum/?hl=en&amp;fromgroups=#!forum/lesswrong-france</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rp'>Paris Meetup: Sunday, October 6: New people, games...</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WmcKJxmtDddr3Tep2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.3617782456780073e-06, "legacy": true, "legacyId": "24310", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Paris_Meetup__Sunday__October_6__New_people__games___\">Discussion article for the meetup : <a href=\"/meetups/rp\">Paris Meetup: Sunday, October 6: New people, games...</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 October 2013 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Caf\u00e9 des Arts et M\u00e9tiers\u200e, 51 Rue Turbigo, Paris</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next Paris Meetup will be Sunday, October 6, at the Caf\u00e9 des Arts et M\u00e9tiers opposite the Museum.</p>\n\n<p>Topics:\n* Welcome new people\n* Board games\n* Plan future meetups\n* Other suggestions? Someone wanted to talk about quantum mechanics...</p>\n\n<p>Reminder: there is the LessWrong France mailing list for discussing and organizing meetups (that for now mostly happen in Paris and Lyon): https://groups.google.com/forum/?hl=en&amp;fromgroups=#!forum/lesswrong-france</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Paris_Meetup__Sunday__October_6__New_people__games___1\">Discussion article for the meetup : <a href=\"/meetups/rp\">Paris Meetup: Sunday, October 6: New people, games...</a></h2>", "sections": [{"title": "Discussion article for the meetup : Paris Meetup: Sunday, October 6: New people, games...", "anchor": "Discussion_article_for_the_meetup___Paris_Meetup__Sunday__October_6__New_people__games___", "level": 1}, {"title": "Discussion article for the meetup : Paris Meetup: Sunday, October 6: New people, games...", "anchor": "Discussion_article_for_the_meetup___Paris_Meetup__Sunday__October_6__New_people__games___1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-01T08:27:40.475Z", "modifiedAt": null, "url": null, "title": "Meetup : Frankfurt (including effective altruism presentation)", "slug": "meetup-frankfurt-including-effective-altruism-presentation", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kendra", "createdAt": "2012-02-29T23:10:44.583Z", "isAdmin": false, "displayName": "Kendra"}, "userId": "BPB6kHkfZwFLrhcbG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ScgiyELqwBv4bMJ3K/meetup-frankfurt-including-effective-altruism-presentation", "pageUrlRelative": "/posts/ScgiyELqwBv4bMJ3K/meetup-frankfurt-including-effective-altruism-presentation", "linkUrl": "https://www.lesswrong.com/posts/ScgiyELqwBv4bMJ3K/meetup-frankfurt-including-effective-altruism-presentation", "postedAtFormatted": "Tuesday, October 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Frankfurt%20(including%20effective%20altruism%20presentation)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Frankfurt%20(including%20effective%20altruism%20presentation)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FScgiyELqwBv4bMJ3K%2Fmeetup-frankfurt-including-effective-altruism-presentation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Frankfurt%20(including%20effective%20altruism%20presentation)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FScgiyELqwBv4bMJ3K%2Fmeetup-frankfurt-including-effective-altruism-presentation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FScgiyELqwBv4bMJ3K%2Fmeetup-frankfurt-including-effective-altruism-presentation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 91, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rq'>Frankfurt (including effective altruism presentation)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 October 2013 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Frankfurt, Ginnheimer Landstra\u00dfe</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We have another meetup! The location is a private flat. Please contact me beforehand: 0176 34 095 760\nIf you suffer from social anxiety or have any other problems that might make it difficult for you to attend, please tell me in advance, we'll try to accommodate your needs!\nWe also have a mailing list: less-wrong-frankfurt@googlegroups.com</p>\n\n<p>It will start at 2pm (14:00).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rq'>Frankfurt (including effective altruism presentation)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ScgiyELqwBv4bMJ3K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.3623326328742397e-06, "legacy": true, "legacyId": "24312", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Frankfurt__including_effective_altruism_presentation_\">Discussion article for the meetup : <a href=\"/meetups/rq\">Frankfurt (including effective altruism presentation)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 October 2013 02:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Frankfurt, Ginnheimer Landstra\u00dfe</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We have another meetup! The location is a private flat. Please contact me beforehand: 0176 34 095 760\nIf you suffer from social anxiety or have any other problems that might make it difficult for you to attend, please tell me in advance, we'll try to accommodate your needs!\nWe also have a mailing list: less-wrong-frankfurt@googlegroups.com</p>\n\n<p>It will start at 2pm (14:00).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Frankfurt__including_effective_altruism_presentation_1\">Discussion article for the meetup : <a href=\"/meetups/rq\">Frankfurt (including effective altruism presentation)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Frankfurt (including effective altruism presentation)", "anchor": "Discussion_article_for_the_meetup___Frankfurt__including_effective_altruism_presentation_", "level": 1}, {"title": "Discussion article for the meetup : Frankfurt (including effective altruism presentation)", "anchor": "Discussion_article_for_the_meetup___Frankfurt__including_effective_altruism_presentation_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-01T19:38:53.086Z", "modifiedAt": null, "url": null, "title": "October 2013 Media Thread", "slug": "october-2013-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:03.377Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ArisKatsaris", "createdAt": "2010-10-07T10:24:25.721Z", "isAdmin": false, "displayName": "ArisKatsaris"}, "userId": "fLbksBTnFsbwYmzsT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xDGKLqMAxkBs4cyes/october-2013-media-thread", "pageUrlRelative": "/posts/xDGKLqMAxkBs4cyes/october-2013-media-thread", "linkUrl": "https://www.lesswrong.com/posts/xDGKLqMAxkBs4cyes/october-2013-media-thread", "postedAtFormatted": "Tuesday, October 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20October%202013%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOctober%202013%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDGKLqMAxkBs4cyes%2Foctober-2013-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=October%202013%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDGKLqMAxkBs4cyes%2Foctober-2013-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDGKLqMAxkBs4cyes%2Foctober-2013-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>This is the monthly thread for posting media of various types that you've found that you enjoy. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the <a href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p>Rules:</p>\n<ul>\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a <a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you think there should be a thread for a particular genre of media, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xDGKLqMAxkBs4cyes", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 1.3629421918774227e-06, "legacy": true, "legacyId": "24313", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-01T20:24:27.828Z", "modifiedAt": null, "url": null, "title": "What are you working on? October 2013", "slug": "what-are-you-working-on-october-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:34.580Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B4aqBPbWJNL7E7kQD/what-are-you-working-on-october-2013", "pageUrlRelative": "/posts/B4aqBPbWJNL7E7kQD/what-are-you-working-on-october-2013", "linkUrl": "https://www.lesswrong.com/posts/B4aqBPbWJNL7E7kQD/what-are-you-working-on-october-2013", "postedAtFormatted": "Tuesday, October 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20you%20working%20on%3F%20October%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20you%20working%20on%3F%20October%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB4aqBPbWJNL7E7kQD%2Fwhat-are-you-working-on-october-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20you%20working%20on%3F%20October%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB4aqBPbWJNL7E7kQD%2Fwhat-are-you-working-on-october-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB4aqBPbWJNL7E7kQD%2Fwhat-are-you-working-on-october-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 105, "htmlBody": "<div id=\"entry_t3_hvu\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<div id=\"entry_t3_gkz\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p>This is the supposedly-bimonthly-but-we-keep-skipping 'What are you working On?' thread. Previous threads are&nbsp;<a href=\"/r/discussion/tag/waywo\">here</a>. So here's the question:</p>\n<p style=\"padding-left: 60px;\"><em>What are you working on?&nbsp;</em></p>\n<p>Here are some guidelines:</p>\n<ul>\n<li>Focus on projects that you have recently made progress on, not projects that you're thinking about doing but haven't started.</li>\n<li>Why this project and not others? Mention reasons why you're doing the project and/or why others should contribute to your project (if applicable).</li>\n<li>Talk about your goals for the project.</li>\n<li>Any kind of project is fair game: personal improvement, research project, art project, whatever.</li>\n<li>Link to your work if it's linkable.</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B4aqBPbWJNL7E7kQD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "24315", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-01T20:24:46.864Z", "modifiedAt": null, "url": null, "title": "Should effective altruists care about the US gov't shutdown and can we do anything?", "slug": "should-effective-altruists-care-about-the-us-gov-t-shutdown", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:26.665Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ishaan", "createdAt": "2013-02-02T04:06:48.124Z", "isAdmin": false, "displayName": "Ishaan"}, "userId": "gta2atTvTn2vSiHdG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/399icAE8qeu3NpcMo/should-effective-altruists-care-about-the-us-gov-t-shutdown", "pageUrlRelative": "/posts/399icAE8qeu3NpcMo/should-effective-altruists-care-about-the-us-gov-t-shutdown", "linkUrl": "https://www.lesswrong.com/posts/399icAE8qeu3NpcMo/should-effective-altruists-care-about-the-us-gov-t-shutdown", "postedAtFormatted": "Tuesday, October 1st 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20effective%20altruists%20care%20about%20the%20US%20gov't%20shutdown%20and%20can%20we%20do%20anything%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20effective%20altruists%20care%20about%20the%20US%20gov't%20shutdown%20and%20can%20we%20do%20anything%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F399icAE8qeu3NpcMo%2Fshould-effective-altruists-care-about-the-us-gov-t-shutdown%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20effective%20altruists%20care%20about%20the%20US%20gov't%20shutdown%20and%20can%20we%20do%20anything%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F399icAE8qeu3NpcMo%2Fshould-effective-altruists-care-about-the-us-gov-t-shutdown", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F399icAE8qeu3NpcMo%2Fshould-effective-altruists-care-about-the-us-gov-t-shutdown", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>For those who haven't heard, NIH and NSF are no longer processing grants, leading to many negative <a href=\"http://www.nature.com/news/us-government-shuts-down-1.13865\">downstream effects</a>.<br /><br />I've been directing my attention elsewhere lately and don't have anything informative to say about this. However, my uninformed intuition is that people who care about effective altruism (research in general, infrastructure development, X-risk mitigation, life-extension...basically everything, actually) or have transhumanist leanings should be very concerned. <br /><br />The consequences have already been pretty disastrous. To provide just one, immediate example, the article says that the Center for Disease Control and Prevention has shut down. I think that this is almost certain to directly cause a nontrivial number of deaths. Each additional day that this continues could have huge negative impact down the line, perhaps delaying some key future discoveries by years. This event *might* be a small window of opportunity to prevent a lot of harm very cheaply.&nbsp;<br /><br />So the question is:<br /><br />1) Can we do anything to remedy the situation?<br /><br />2) If so, is it worth doing it? (Opportunity costs, etc)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "399icAE8qeu3NpcMo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 0, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "24314", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 112, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-02T01:58:51.428Z", "modifiedAt": null, "url": null, "title": "Meetup : Chicago Open Discussion", "slug": "meetup-chicago-open-discussion", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nic_Smith", "createdAt": "2009-10-23T03:32:46.312Z", "isAdmin": false, "displayName": "Nic_Smith"}, "userId": "XP9GcTgRGLBCnf9ih", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YL7czAdL3TSnuLuZ3/meetup-chicago-open-discussion", "pageUrlRelative": "/posts/YL7czAdL3TSnuLuZ3/meetup-chicago-open-discussion", "linkUrl": "https://www.lesswrong.com/posts/YL7czAdL3TSnuLuZ3/meetup-chicago-open-discussion", "postedAtFormatted": "Wednesday, October 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Chicago%20Open%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Chicago%20Open%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYL7czAdL3TSnuLuZ3%2Fmeetup-chicago-open-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Chicago%20Open%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYL7czAdL3TSnuLuZ3%2Fmeetup-chicago-open-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYL7czAdL3TSnuLuZ3%2Fmeetup-chicago-open-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rr'>Chicago Open Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 October 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Corner Bakery, 360 N. Michigan Ave., Chicago, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is our not-quite-regular open discussion meetup in Chicago.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rr'>Chicago Open Discussion</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YL7czAdL3TSnuLuZ3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3632874773306682e-06, "legacy": true, "legacyId": "24316", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Chicago_Open_Discussion\">Discussion article for the meetup : <a href=\"/meetups/rr\">Chicago Open Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 October 2013 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Corner Bakery, 360 N. Michigan Ave., Chicago, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is our not-quite-regular open discussion meetup in Chicago.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Chicago_Open_Discussion1\">Discussion article for the meetup : <a href=\"/meetups/rr\">Chicago Open Discussion</a></h2>", "sections": [{"title": "Discussion article for the meetup : Chicago Open Discussion", "anchor": "Discussion_article_for_the_meetup___Chicago_Open_Discussion", "level": 1}, {"title": "Discussion article for the meetup : Chicago Open Discussion", "anchor": "Discussion_article_for_the_meetup___Chicago_Open_Discussion1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-02T02:42:49.029Z", "modifiedAt": null, "url": null, "title": "A Muggle Studies course", "slug": "a-muggle-studies-course", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:38.022Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "polymathwannabe", "createdAt": "2013-08-29T03:03:37.800Z", "isAdmin": false, "displayName": "polymathwannabe"}, "userId": "NkxHWoA85iw2PpxSt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZhWGp2Wax5ZSHabPY/a-muggle-studies-course", "pageUrlRelative": "/posts/ZhWGp2Wax5ZSHabPY/a-muggle-studies-course", "linkUrl": "https://www.lesswrong.com/posts/ZhWGp2Wax5ZSHabPY/a-muggle-studies-course", "postedAtFormatted": "Wednesday, October 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Muggle%20Studies%20course&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Muggle%20Studies%20course%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZhWGp2Wax5ZSHabPY%2Fa-muggle-studies-course%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Muggle%20Studies%20course%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZhWGp2Wax5ZSHabPY%2Fa-muggle-studies-course", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZhWGp2Wax5ZSHabPY%2Fa-muggle-studies-course", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>At my local Harry Potter fanclub (Bogot&aacute;, Colombia) some members teach \"classes\" on subjects they're passionate about. We've had informal courses on history, creative writing, English, etc. But recently some other classes have appeared that have made me worry seriously: astrology, divination, ancient runes, and all other sorts of nonsense. They're not taught as folklorical pieces of the past, but as serious practices that are supposed to actually work. I think this is particularly dangerous for the small kids that comprise the majority of the fanclub and still need help learning that magic doesn't exist.</p>\n<p>So I proposed the fanclub chief that I could teach a Muggle Studies class: logic, critical thinking, philosophy of science, etc. In two weeks we'll have our first class, and I intend to begin talking about the most common biases.&nbsp;I already downloaded this website's PDF guide to holding a Less Wrong meeting.&nbsp;Aside from that, what can you suggest for a successful Muggles Studies course?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZhWGp2Wax5ZSHabPY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 28, "extendedScore": null, "score": 6.9e-05, "legacy": true, "legacyId": "24317", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-02T05:07:03.690Z", "modifiedAt": "2020-10-20T03:09:12.268Z", "url": null, "title": "How to Become a 1000 Year Old Vampire", "slug": "how-to-become-a-1000-year-old-vampire", "viewCount": null, "lastCommentedAt": "2016-03-25T14:45:21.358Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5QpufhoH2ASnppsjs/how-to-become-a-1000-year-old-vampire", "pageUrlRelative": "/posts/5QpufhoH2ASnppsjs/how-to-become-a-1000-year-old-vampire", "linkUrl": "https://www.lesswrong.com/posts/5QpufhoH2ASnppsjs/how-to-become-a-1000-year-old-vampire", "postedAtFormatted": "Wednesday, October 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Become%20a%201000%20Year%20Old%20Vampire&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Become%20a%201000%20Year%20Old%20Vampire%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5QpufhoH2ASnppsjs%2Fhow-to-become-a-1000-year-old-vampire%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Become%20a%201000%20Year%20Old%20Vampire%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5QpufhoH2ASnppsjs%2Fhow-to-become-a-1000-year-old-vampire", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5QpufhoH2ASnppsjs%2Fhow-to-become-a-1000-year-old-vampire", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2456, "htmlBody": "<p>This is based on a concept we developed at the <a href=\"http://groups.google.com/group/vancouver-rationalists\">Vancouver Rationalists</a> meetup.</p>\n<p>Different experiences level a person up at different rates. You could work some boring job all your life and be 60 and not be much more awesome than your average teenager. On the other hand, some people have such varied and so much life experience that by 30 they are as awesome as a 1000 year old vampire.</p>\n<p>This reminds me that it's possible to conduct your life with more or less efficiency, sometimes by orders of magnitude. Further, while we don't have actual life extension, it's content we care about, not run time. If you can change your habits such that you get 3 times as much done, that's like <em>tripling your effective lifespan</em>.</p>\n<p>So how might one get a 100x speedup and become like a 1000 year old vampire in 10 years? This is absurdly ambitious, but we can try:</p>\n<h2>Do Hard Things</h2>\n<p>Some experiences catapult you forward in personal development. You can probably systematically collect these to build formidability as fast as possible.</p>\n<p>Paul Graham says that many of the founders he sees (as head of YC) become much more awesome very quickly as need forces them to. This seems plausible and it seems back up by other sources as well. Basically \"learn to swim by jumping in the deep end\"; people have a tendency to take the easy way that results in less development when given the chance, so the chance to slack off being removed can be beneficial.</p>\n<p>That has definitely been my personal experience as well. At work, the head engineer got brain cancer and I got de-facto promoted to head of two of the projects, which I then leveled up to be able to do. It felt pretty scary at first, but now I'm bored and wishing something further would challenge me. (addendum: not bored right now at all; crazy crunch time for the other team, which which I am helping) It seems really hard to just do better without such forcing; as far as I can tell I <em>could</em> work much harder than now, but willpower basically doesn't exist so I don't.</p>\n<p>On that note, a friend of mine got big results from joining the Army and getting tear gassed in a trench while wet, cold, exhausted, sleep deprived, and hungry, which pushed him through stuff he wouldn't have thought he could deal with. Apparently it sortof re-calibrated his feelings about how well he should be doing and how hard things are such that he is now a millionaire and awesome.</p>\n<p>So the mechanism behind a lot of this seems to be recalibrating what seems hard or scary or beyond your normal sphere. I used to be afraid of phone calls and doing weird stuff like climbing trees in front of strangers, but not so much anymore; it feels like I just forget that they were scary. In the case of the phone there were a few times where I didn't have time to be scared, I needed to just get things done. In the case of climbing trees, I did it on my own enough for it to become normalized so that it didn't even come up that people would see me, because it didn't seem weird.</p>\n<p>So tying that back in, there are experiences that you can put yourself into to force that normalization and acclimatization to hard stuff. For example, some people do this thing called \"Rejection Therapy\" or \"Comfort Zone Expansion\", basically going out and doing embarrassing or scary things deliberately to recalibrate your intuitions and teach your brain that they are not so scary.</p>\n<p>On the failure end, self-improvement projects tend to fail when they require constant application of willpower. It's just a fact that you will fall off the wagon on those things. So you have to make it impossible to fall off the wagon. You have to make it scarier to fall off the wagon than it is to level up and just do it. This is the idea behind <a href=\"http://beeminder.com\">Beeminder</a>, which takes your money if you don't do what your last-week self said you would.</p>\n<p>I guess the thesis behind all this is that these level-ups are permanent, in that they make you more like a 1000 year old vampire, and you don't just go back to being your boring old mortal self. If this is true, the implication that you should seek out hard stuff seems pretty interesting and important.</p>\n<h2>Broadness of Experience</h2>\n<p>Think of a 1000 year old vampire; they would have done everything. Fought in battles, led armies, built great works, been in love, been everywhere, observed most aspects of the human experience, and generally seen it all.</p>\n<p>Things you can do have sharply diminishing returns; the first few times you watch great movies is most of the benefit thereof, likewise with video games, 4chan, most jobs, and most experiences in general. Thus it's really important to switch around the things you do a lot so that you stay in that sharp initially growing part of the learning curve. You can get 90% of the vampire's experience with 10% of his time investment if you focus on those most enlightening parts of each experience.</p>\n<p>So besides doing hard things that level you up, you can get big gains by doing <em>many</em> things and switching as soon as you get bored (which is hopefully calibrated to how challenged you are).</p>\n<p>You may remember early in the Arabian revolutions in Libya, an American student took the summer off college to fight in the revolution. I bet he learned a lot. If you could do enough things like that, you'd be well on your way to matching the vampire.</p>\n<p>This actually goes hand in hand with doing hard things; when you're not feeling challenged (you're on the flat part of that experience curve), its probably best to throw yourself face first into some new project, both because it's new, and because it's hard.</p>\n<p>Switching often has the additional benefit of normalizing strategic changes and practicing \"what should I be doing\"-type thoughts, which can't hurt if you intend to actually do useful stuff with your life.</p>\n<p>There are probably many cases where full on switching is not best. For example, you don't become an expert in X by switching out of X as soon as you know the basics. It might be that you want to switch often on side-things but go deep on X. Alternatively, you probably want to do some kind of switch every now and then in X, maybe look at things from a different perspective, tackle a different problem, or something like that. This is the <a href=\"https://en.wikipedia.org/wiki/Deliberate_practice\">Deliberate Practice</a> theory of expertise.</p>\n<p>So don't forget the shape of that experience curve. As soon as you start to feel that leveling off, find a way to make it fresh again.</p>\n<h2>Do Things Quickly</h2>\n<p>Another big angle on this idea is that every hour is an opportunity, and you want to make the best of them. This seems totally obvious but I definitely \"get it\" a lot more having thought about it in terms of becoming a 1000 year old vampire.</p>\n<p>A big example is procrastination. I have a lot of things that have been hanging around on my todo list for a long time, basically oppressing me by their presence. I can't relax and look to new things to do while there's still that one stupid thing on my todo list. The key insight is that if you process the stuff on your todo list <em>now</em> instead of slacking now and doing it later, you get it out of the way and then you can do something <em>else</em> later, and thereby become a 1000 year old vampire faster.</p>\n<p>So a friend and I have internalized this a bit more and started really noticing those opportunity costs, and actually started knocking things off faster. I'm sure there's more where that came from; we are nowhere near optimal in Doing It Now, so it's probably good to meditate on this more.</p>\n<p>As a concrete example, I'm writing tonight because I realized that I need to just get all my writing ideas out of the way to make room for more awesomeness.</p>\n<p>The flipside of this idea is that a lot of things are complete wastes of time, in the sense that they just burn up lifespan and don't get you anything, or even weaken you.</p>\n<p>Bad habits like reading crap on the Internet, watching TV, watching porn, playing video games, sleeping in, and so on are obvious losses. It's really hard to internalize that, but this 1000-year-old-vampire concept has been helpful for me by making the magnitude of the cost more salient. Do you want to wake up when you're 30 and realize you wasted your youth on meaningless crap, or do you want to get off your ass and write that thing you've been meaning to <em>right now, and be a fscking vampire in 10 years</em>?</p>\n<p>It's not just bad habits, though; a lot of it is your broader position in life that wastes time or doesn't. For example, repetitive wage work that doesn't challenge you is really just trading a huge chunk of your life for not even much money. Obviously sometimes you have to, but you have to realize that trading away half your life is a pretty raw deal that is to be avoided. You don't even really <em>get</em> anything for commuting and housework. Maybe I really should quit my job soon...</p>\n<p>I have 168 hours a week, of which only 110 are feasible to use (sleep), and by the time we include all the chores, wage-work, bad habits, and procrastination, I probably only <em>live</em> 30 hours a week. That's bullshit; three quarters of my life pissed away. I could live <em>four</em> times as much if I could cut out that stuff.</p>\n<p>So this is just the concept of time opportunity costs dressed up to be more salient. Basic economics concepts seem really quite valuable in this way.</p>\n<p>Do it now so you can do something else later. Avoid crap work.</p>\n<h2>Social Environment and Stimulation</h2>\n<p>I notice that I'm most alive and do my best intellectual work when talking to other people who are smart and interested in having deep technical conversations. Other things like certain patterns of time pressure create this effect where I work many times harder and more effectively than otherwise. A great example is technical exams; I can blast out answers to hundreds of technical questions at quite a rate.</p>\n<p>It seems like a good idea to induce this state where you are more alive (is it the \"flow\" state?) if you want to live more life. It also seems totally possible to do so more often by hanging out with the right people and exposing yourself to the right working conditions and whatnot.</p>\n<p>One thing that will come up is that it's quite draining, in that I sometimes feel exhausted and can't get much done after a day of more intense work. Is this a real thing? Probably. Still, I'm nowhere near the limit even given the need to rest, in general.</p>\n<p>I ought to do some research to learn more about this. If it's connected to \"flow\", there's been a lot of research, AFAIK.</p>\n<p>I also ought to just hurry up and move to California where there is a proper intellectual community that will stimulate me much better than the meager group of brains I could scrape together in Vancouver.</p>\n<p>The other benefit of a good intellectual community is that they can incentivize doing cooler things. When all your friends are starting companies or otherwise doing great work, sitting around on the couch <em>feels</em> like a really bad idea.</p>\n<p>So if we want to live more life, finding more ways to enter that stimulated flow state seems like a prudent thing to do, whether that means just making way for it in your work habits, putting yourself in more challenging social and intellectual environments, or whatever.</p>\n<h2>Adding It Up</h2>\n<p>So how fast can we go overall if we do all of this?</p>\n<p>By seeking many new experiences to keep learning, I think we can plausibly get 10x speedup over what you might do by default. Obviously this can be more or less, based on circumstances and things I'm not thinking of.</p>\n<p>On top of that, it seems like I could do 4x as much by maintaining a habit of doing it <em>now</em> and avoiding crap work. How to do this, I don't know, but it's possible.</p>\n<p>I don't know how to estimate the actual gains from a stimulating environment. It seems like it could be really really high, or just another incremental gain in efficiency, depending how it goes down. Let's say that <em>on top</em> of the other things, we can realistically push ourselves 2x or 3x harder by social and environmental effects.</p>\n<p>Doing hard things seems huge, but also quite related to the doing new things angle that we already accounted for. So explicitly remembering to do hard things on top of that? Maybe 5x? This again will vary a lot based on what opportunities you are able to find, and unknown factors, but 5x seems safe enough given mortal levels of ingenuity and willpower.</p>\n<p>So all together, someone who:</p>\n<ul>\n<li>\n<p>Often thinks about where they are on the experience curve for everything they do, and takes action on that when appropriate,</p>\n</li>\n<li>\n<p>Maintains a habit of doing stuff <em>now</em> and visualizing those opportunity costs,</p>\n</li>\n<li>\n<p>Puts themselves in a stimulating environment like the bay area intellectual community and surrounds themselves with stimulating people and events,</p>\n</li>\n<li>\n<p>Seeks out the hardest character-building experiences like getting tear gassed in a trench or building a company from scratch,</p>\n</li>\n</ul>\n<p>Can plausibly get 500x speedup and live 1000 normal years in 2. That seems pretty wild, but none of these things are particularly out there, and people like Elon Musk or Eliezer Yudkowsky do seem to do around that magnitude more than the average joe.</p>\n<p>Perhaps they don't multiply quite that conveniently, or there's some other gotcha, but the target seems reachable, and these things will help. On the other hand, they almost certainly self-reinforce; a 1000 year old vampire would have mastered the art of living life life at ever higher efficiencies.</p>\n<p>This does seem to be congruent with all this stuff being power-law distributed, which of course makes it difficult to summarize by a single number like 500.</p>\n<p>The final question of course is what <em>real</em> speedup we can expect you or I to gain from writing or reading this. Getting more than 2 or 3 times by having a low-level insight or reading a blog post seems stretching of the imagination, never mind 500 times. But still, power laws happen. There's probably massive payoff to taking this idea seriously.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "5Whwix4cZ3p5otshm": 1, "fF9GEdWXKJ3z73TmB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5QpufhoH2ASnppsjs", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 86, "baseScore": 117, "extendedScore": null, "score": 0.000287, "legacy": true, "legacyId": "24318", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 118, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This is based on a concept we developed at the <a href=\"http://groups.google.com/group/vancouver-rationalists\">Vancouver Rationalists</a> meetup.</p>\n<p>Different experiences level a person up at different rates. You could work some boring job all your life and be 60 and not be much more awesome than your average teenager. On the other hand, some people have such varied and so much life experience that by 30 they are as awesome as a 1000 year old vampire.</p>\n<p>This reminds me that it's possible to conduct your life with more or less efficiency, sometimes by orders of magnitude. Further, while we don't have actual life extension, it's content we care about, not run time. If you can change your habits such that you get 3 times as much done, that's like <em>tripling your effective lifespan</em>.</p>\n<p>So how might one get a 100x speedup and become like a 1000 year old vampire in 10 years? This is absurdly ambitious, but we can try:</p>\n<h2 id=\"Do_Hard_Things\">Do Hard Things</h2>\n<p>Some experiences catapult you forward in personal development. You can probably systematically collect these to build formidability as fast as possible.</p>\n<p>Paul Graham says that many of the founders he sees (as head of YC) become much more awesome very quickly as need forces them to. This seems plausible and it seems back up by other sources as well. Basically \"learn to swim by jumping in the deep end\"; people have a tendency to take the easy way that results in less development when given the chance, so the chance to slack off being removed can be beneficial.</p>\n<p>That has definitely been my personal experience as well. At work, the head engineer got brain cancer and I got de-facto promoted to head of two of the projects, which I then leveled up to be able to do. It felt pretty scary at first, but now I'm bored and wishing something further would challenge me. (addendum: not bored right now at all; crazy crunch time for the other team, which which I am helping) It seems really hard to just do better without such forcing; as far as I can tell I <em>could</em> work much harder than now, but willpower basically doesn't exist so I don't.</p>\n<p>On that note, a friend of mine got big results from joining the Army and getting tear gassed in a trench while wet, cold, exhausted, sleep deprived, and hungry, which pushed him through stuff he wouldn't have thought he could deal with. Apparently it sortof re-calibrated his feelings about how well he should be doing and how hard things are such that he is now a millionaire and awesome.</p>\n<p>So the mechanism behind a lot of this seems to be recalibrating what seems hard or scary or beyond your normal sphere. I used to be afraid of phone calls and doing weird stuff like climbing trees in front of strangers, but not so much anymore; it feels like I just forget that they were scary. In the case of the phone there were a few times where I didn't have time to be scared, I needed to just get things done. In the case of climbing trees, I did it on my own enough for it to become normalized so that it didn't even come up that people would see me, because it didn't seem weird.</p>\n<p>So tying that back in, there are experiences that you can put yourself into to force that normalization and acclimatization to hard stuff. For example, some people do this thing called \"Rejection Therapy\" or \"Comfort Zone Expansion\", basically going out and doing embarrassing or scary things deliberately to recalibrate your intuitions and teach your brain that they are not so scary.</p>\n<p>On the failure end, self-improvement projects tend to fail when they require constant application of willpower. It's just a fact that you will fall off the wagon on those things. So you have to make it impossible to fall off the wagon. You have to make it scarier to fall off the wagon than it is to level up and just do it. This is the idea behind <a href=\"http://beeminder.com\">Beeminder</a>, which takes your money if you don't do what your last-week self said you would.</p>\n<p>I guess the thesis behind all this is that these level-ups are permanent, in that they make you more like a 1000 year old vampire, and you don't just go back to being your boring old mortal self. If this is true, the implication that you should seek out hard stuff seems pretty interesting and important.</p>\n<h2 id=\"Broadness_of_Experience\">Broadness of Experience</h2>\n<p>Think of a 1000 year old vampire; they would have done everything. Fought in battles, led armies, built great works, been in love, been everywhere, observed most aspects of the human experience, and generally seen it all.</p>\n<p>Things you can do have sharply diminishing returns; the first few times you watch great movies is most of the benefit thereof, likewise with video games, 4chan, most jobs, and most experiences in general. Thus it's really important to switch around the things you do a lot so that you stay in that sharp initially growing part of the learning curve. You can get 90% of the vampire's experience with 10% of his time investment if you focus on those most enlightening parts of each experience.</p>\n<p>So besides doing hard things that level you up, you can get big gains by doing <em>many</em> things and switching as soon as you get bored (which is hopefully calibrated to how challenged you are).</p>\n<p>You may remember early in the Arabian revolutions in Libya, an American student took the summer off college to fight in the revolution. I bet he learned a lot. If you could do enough things like that, you'd be well on your way to matching the vampire.</p>\n<p>This actually goes hand in hand with doing hard things; when you're not feeling challenged (you're on the flat part of that experience curve), its probably best to throw yourself face first into some new project, both because it's new, and because it's hard.</p>\n<p>Switching often has the additional benefit of normalizing strategic changes and practicing \"what should I be doing\"-type thoughts, which can't hurt if you intend to actually do useful stuff with your life.</p>\n<p>There are probably many cases where full on switching is not best. For example, you don't become an expert in X by switching out of X as soon as you know the basics. It might be that you want to switch often on side-things but go deep on X. Alternatively, you probably want to do some kind of switch every now and then in X, maybe look at things from a different perspective, tackle a different problem, or something like that. This is the <a href=\"https://en.wikipedia.org/wiki/Deliberate_practice\">Deliberate Practice</a> theory of expertise.</p>\n<p>So don't forget the shape of that experience curve. As soon as you start to feel that leveling off, find a way to make it fresh again.</p>\n<h2 id=\"Do_Things_Quickly\">Do Things Quickly</h2>\n<p>Another big angle on this idea is that every hour is an opportunity, and you want to make the best of them. This seems totally obvious but I definitely \"get it\" a lot more having thought about it in terms of becoming a 1000 year old vampire.</p>\n<p>A big example is procrastination. I have a lot of things that have been hanging around on my todo list for a long time, basically oppressing me by their presence. I can't relax and look to new things to do while there's still that one stupid thing on my todo list. The key insight is that if you process the stuff on your todo list <em>now</em> instead of slacking now and doing it later, you get it out of the way and then you can do something <em>else</em> later, and thereby become a 1000 year old vampire faster.</p>\n<p>So a friend and I have internalized this a bit more and started really noticing those opportunity costs, and actually started knocking things off faster. I'm sure there's more where that came from; we are nowhere near optimal in Doing It Now, so it's probably good to meditate on this more.</p>\n<p>As a concrete example, I'm writing tonight because I realized that I need to just get all my writing ideas out of the way to make room for more awesomeness.</p>\n<p>The flipside of this idea is that a lot of things are complete wastes of time, in the sense that they just burn up lifespan and don't get you anything, or even weaken you.</p>\n<p>Bad habits like reading crap on the Internet, watching TV, watching porn, playing video games, sleeping in, and so on are obvious losses. It's really hard to internalize that, but this 1000-year-old-vampire concept has been helpful for me by making the magnitude of the cost more salient. Do you want to wake up when you're 30 and realize you wasted your youth on meaningless crap, or do you want to get off your ass and write that thing you've been meaning to <em>right now, and be a fscking vampire in 10 years</em>?</p>\n<p>It's not just bad habits, though; a lot of it is your broader position in life that wastes time or doesn't. For example, repetitive wage work that doesn't challenge you is really just trading a huge chunk of your life for not even much money. Obviously sometimes you have to, but you have to realize that trading away half your life is a pretty raw deal that is to be avoided. You don't even really <em>get</em> anything for commuting and housework. Maybe I really should quit my job soon...</p>\n<p>I have 168 hours a week, of which only 110 are feasible to use (sleep), and by the time we include all the chores, wage-work, bad habits, and procrastination, I probably only <em>live</em> 30 hours a week. That's bullshit; three quarters of my life pissed away. I could live <em>four</em> times as much if I could cut out that stuff.</p>\n<p>So this is just the concept of time opportunity costs dressed up to be more salient. Basic economics concepts seem really quite valuable in this way.</p>\n<p>Do it now so you can do something else later. Avoid crap work.</p>\n<h2 id=\"Social_Environment_and_Stimulation\">Social Environment and Stimulation</h2>\n<p>I notice that I'm most alive and do my best intellectual work when talking to other people who are smart and interested in having deep technical conversations. Other things like certain patterns of time pressure create this effect where I work many times harder and more effectively than otherwise. A great example is technical exams; I can blast out answers to hundreds of technical questions at quite a rate.</p>\n<p>It seems like a good idea to induce this state where you are more alive (is it the \"flow\" state?) if you want to live more life. It also seems totally possible to do so more often by hanging out with the right people and exposing yourself to the right working conditions and whatnot.</p>\n<p>One thing that will come up is that it's quite draining, in that I sometimes feel exhausted and can't get much done after a day of more intense work. Is this a real thing? Probably. Still, I'm nowhere near the limit even given the need to rest, in general.</p>\n<p>I ought to do some research to learn more about this. If it's connected to \"flow\", there's been a lot of research, AFAIK.</p>\n<p>I also ought to just hurry up and move to California where there is a proper intellectual community that will stimulate me much better than the meager group of brains I could scrape together in Vancouver.</p>\n<p>The other benefit of a good intellectual community is that they can incentivize doing cooler things. When all your friends are starting companies or otherwise doing great work, sitting around on the couch <em>feels</em> like a really bad idea.</p>\n<p>So if we want to live more life, finding more ways to enter that stimulated flow state seems like a prudent thing to do, whether that means just making way for it in your work habits, putting yourself in more challenging social and intellectual environments, or whatever.</p>\n<h2 id=\"Adding_It_Up\">Adding It Up</h2>\n<p>So how fast can we go overall if we do all of this?</p>\n<p>By seeking many new experiences to keep learning, I think we can plausibly get 10x speedup over what you might do by default. Obviously this can be more or less, based on circumstances and things I'm not thinking of.</p>\n<p>On top of that, it seems like I could do 4x as much by maintaining a habit of doing it <em>now</em> and avoiding crap work. How to do this, I don't know, but it's possible.</p>\n<p>I don't know how to estimate the actual gains from a stimulating environment. It seems like it could be really really high, or just another incremental gain in efficiency, depending how it goes down. Let's say that <em>on top</em> of the other things, we can realistically push ourselves 2x or 3x harder by social and environmental effects.</p>\n<p>Doing hard things seems huge, but also quite related to the doing new things angle that we already accounted for. So explicitly remembering to do hard things on top of that? Maybe 5x? This again will vary a lot based on what opportunities you are able to find, and unknown factors, but 5x seems safe enough given mortal levels of ingenuity and willpower.</p>\n<p>So all together, someone who:</p>\n<ul>\n<li>\n<p>Often thinks about where they are on the experience curve for everything they do, and takes action on that when appropriate,</p>\n</li>\n<li>\n<p>Maintains a habit of doing stuff <em>now</em> and visualizing those opportunity costs,</p>\n</li>\n<li>\n<p>Puts themselves in a stimulating environment like the bay area intellectual community and surrounds themselves with stimulating people and events,</p>\n</li>\n<li>\n<p>Seeks out the hardest character-building experiences like getting tear gassed in a trench or building a company from scratch,</p>\n</li>\n</ul>\n<p>Can plausibly get 500x speedup and live 1000 normal years in 2. That seems pretty wild, but none of these things are particularly out there, and people like Elon Musk or Eliezer Yudkowsky do seem to do around that magnitude more than the average joe.</p>\n<p>Perhaps they don't multiply quite that conveniently, or there's some other gotcha, but the target seems reachable, and these things will help. On the other hand, they almost certainly self-reinforce; a 1000 year old vampire would have mastered the art of living life life at ever higher efficiencies.</p>\n<p>This does seem to be congruent with all this stuff being power-law distributed, which of course makes it difficult to summarize by a single number like 500.</p>\n<p>The final question of course is what <em>real</em> speedup we can expect you or I to gain from writing or reading this. Getting more than 2 or 3 times by having a low-level insight or reading a blog post seems stretching of the imagination, never mind 500 times. But still, power laws happen. There's probably massive payoff to taking this idea seriously.</p>", "sections": [{"title": "Do Hard Things", "anchor": "Do_Hard_Things", "level": 1}, {"title": "Broadness of Experience", "anchor": "Broadness_of_Experience", "level": 1}, {"title": "Do Things Quickly", "anchor": "Do_Things_Quickly", "level": 1}, {"title": "Social Environment and Stimulation", "anchor": "Social_Environment_and_Stimulation", "level": 1}, {"title": "Adding It Up", "anchor": "Adding_It_Up", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "126 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 132, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-10-02T05:07:03.690Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-02T13:45:31.589Z", "modifiedAt": null, "url": null, "title": "Rationality, competitiveness and akrasia", "slug": "rationality-competitiveness-and-akrasia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:06.987Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aARGW967NexuSrCwW/rationality-competitiveness-and-akrasia", "pageUrlRelative": "/posts/aARGW967NexuSrCwW/rationality-competitiveness-and-akrasia", "linkUrl": "https://www.lesswrong.com/posts/aARGW967NexuSrCwW/rationality-competitiveness-and-akrasia", "postedAtFormatted": "Wednesday, October 2nd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%2C%20competitiveness%20and%20akrasia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%2C%20competitiveness%20and%20akrasia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaARGW967NexuSrCwW%2Frationality-competitiveness-and-akrasia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%2C%20competitiveness%20and%20akrasia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaARGW967NexuSrCwW%2Frationality-competitiveness-and-akrasia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaARGW967NexuSrCwW%2Frationality-competitiveness-and-akrasia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<p>Here's an internal dialogue I just had.</p>\n<p><strong>Q:</strong> How do we test rationality skills?</p>\n<p><strong>A:</strong> We haven't come up with a comprehensive test yet.</p>\n<p><strong>Q:</strong> Maybe we can test some part of rationality?</p>\n<p><strong>A:</strong> Sure. For example, you could test resistance to akrasia by making two contestants do some simple chores every day. The one who fails first, loses.</p>\n<p><strong>Q:</strong> That seems like a pointless competition. If I'm feeling competitive, why would I ever skip the chores and lose?</p>\n<p><strong>A:</strong> Whoa, wait. If competitiveness can cure akrasia, that's pretty cool!</p>\n<p>Now we just need to figure out how to make people more competitive in the areas they care about...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aARGW967NexuSrCwW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 23, "extendedScore": null, "score": 1.3639300480759671e-06, "legacy": true, "legacyId": "24322", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-03T01:46:26.652Z", "modifiedAt": null, "url": null, "title": "Systematic Lucky Breaks", "slug": "systematic-lucky-breaks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.759Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SatvikBeri", "createdAt": "2013-06-14T17:52:32.786Z", "isAdmin": false, "displayName": "SatvikBeri"}, "userId": "g4vakcHCMNzfhidWa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5iiG2CsR5a9983c6C/systematic-lucky-breaks", "pageUrlRelative": "/posts/5iiG2CsR5a9983c6C/systematic-lucky-breaks", "linkUrl": "https://www.lesswrong.com/posts/5iiG2CsR5a9983c6C/systematic-lucky-breaks", "postedAtFormatted": "Thursday, October 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Systematic%20Lucky%20Breaks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASystematic%20Lucky%20Breaks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5iiG2CsR5a9983c6C%2Fsystematic-lucky-breaks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Systematic%20Lucky%20Breaks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5iiG2CsR5a9983c6C%2Fsystematic-lucky-breaks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5iiG2CsR5a9983c6C%2Fsystematic-lucky-breaks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 622, "htmlBody": "<p class=\"p1\">Many people can point to significant events that improved their lives in a positive way. They often refer to these as \"lucky breaks\", and take it for granted that such events are rare. But most of the time \"lucky breaks\" don't need to be uncommon-you can often reverse engineer the reasons behind them and cause them to happen more frequently. So when a one-off event ends up contributing a lot of value, you should systematically make it part of your life.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">Example 1: in June the Less Wrong - Cambridge community held a mega-meetup with several people arriving from out of state. Since several of us had to stay up until 2AM+ in order to meet with people, we decided to have a game night that evening, which I held at my place. The game night was excellent-plenty of people showed up, we all had a lot of fun, and it was a great way to socialize with several people. Since it went so well, I started hosting game nights regularly, eventually converging on one game night every two weeks. This was a phenomenal move in many ways-it let me meet a lot of interesting people, deepen my connections with my friends, quickly integrate with the Less Wrong community, and just in general have a lot of fun, simply by taking one thing that worked well and making it systematic.</p>\n<p>&nbsp;</p>\n<p class=\"p1\">Example 2: a while back I was given an assignment to set up a scalable analytic architecture to allow data scientists to iterate faster-a project where I had no idea what to do or how to start. In desperation, I reached out to several people on LinkedIn who had experience with similar projects. Some of them responded, and the advice I got was incredibly valuable, easily shaving months off of my learning curve. But there is no reason for me to only do this when I am completely desperate. Thus I&rsquo;ve continued to reach out to experts when I have new projects, and this has allowed me to avoid mistakes and solve new problems much more quickly. This has significantly improved my learning speed and made a qualitative difference in how I work. I no longer dismiss potential ideas simply because I have no idea how to implement them-instead, I now talk to experts and figure out roughly how difficult those ideas are, which has allowed me to solve several problems I would have dismissed as unfeasibly difficult before.</p>\n<p class=\"p1\">&nbsp;</p>\n<p class=\"p1\">Example 3: a few years back some of my friends in the tech industry mentioned that Machine Learning was becoming a trend, so I took two weeks to learn the basics. A few months later the \"Big Data\" boom exploded, and I was able to get a job as a Data Scientist at a significantly higher salary doing more interesting work. Even though my Machine Learning knowledge was pretty rudimentary, I was able to get the job because demand completely exceeded supply at that point. In short, this was a lucky break that greatly advanced my career. To systematize this I simply continued to keep an eye out on big trends in technology. I've read Hacker News (which is generally half a year or more ahead of the mainstream), kept in touch with my friends on the applied side of academia (which feeds useful techniques into the industry), and just generally kept talking to a lot of people in order to keep up-to-date. This has been useful again and again, allowing me to focus my learning on the most valuable skills right as there was market demand.</p>\n<p class=\"p1\">&nbsp;</p>\n<p class=\"p1\">In short, one of the fastest ways to improve your life is to look at things that already made a big difference before, and cause more of them to happen.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5iiG2CsR5a9983c6C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 54, "extendedScore": null, "score": 0.000134, "legacy": true, "legacyId": "24324", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-03T04:35:46.467Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta Lesswrong:  October Meetup (First of Two)", "slug": "meetup-atlanta-lesswrong-october-meetup-first-of-two", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uMHQKTDRKAiuQ6SKJ/meetup-atlanta-lesswrong-october-meetup-first-of-two", "pageUrlRelative": "/posts/uMHQKTDRKAiuQ6SKJ/meetup-atlanta-lesswrong-october-meetup-first-of-two", "linkUrl": "https://www.lesswrong.com/posts/uMHQKTDRKAiuQ6SKJ/meetup-atlanta-lesswrong-october-meetup-first-of-two", "postedAtFormatted": "Thursday, October 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20Lesswrong%3A%20%20October%20Meetup%20(First%20of%20Two)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20Lesswrong%3A%20%20October%20Meetup%20(First%20of%20Two)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuMHQKTDRKAiuQ6SKJ%2Fmeetup-atlanta-lesswrong-october-meetup-first-of-two%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20Lesswrong%3A%20%20October%20Meetup%20(First%20of%20Two)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuMHQKTDRKAiuQ6SKJ%2Fmeetup-atlanta-lesswrong-october-meetup-first-of-two", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuMHQKTDRKAiuQ6SKJ%2Fmeetup-atlanta-lesswrong-october-meetup-first-of-two", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 116, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rs'>Atlanta Lesswrong:  October Meetup (First of Two)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 October 2013 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Northgate Manor, 2388 Lawrenceville Hwy. Apt L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come join us for the first meetup for the month of October!</p>\n\n<p>We'll be doing our normal eclectic mix of self-improvement brainstorming, educational mini-presentations, structured discussion, unstructured discussion, and social fun and games times!</p>\n\n<p>Please contact me if you have cat allergies, as our meeting space has cats. Incredibly cute cats.</p>\n\n<p>And check out ATLesswrong's facebook group, if you haven't already: https://www.facebook.com/groups/100137206844878/ where you can connect with Atlanta Lesswrongers and suggest a topics for discussion at this meetup!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rs'>Atlanta Lesswrong:  October Meetup (First of Two)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uMHQKTDRKAiuQ6SKJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.3647403089827998e-06, "legacy": true, "legacyId": "24326", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Lesswrong___October_Meetup__First_of_Two_\">Discussion article for the meetup : <a href=\"/meetups/rs\">Atlanta Lesswrong:  October Meetup (First of Two)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 October 2013 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Northgate Manor, 2388 Lawrenceville Hwy. Apt L. Decatur, GA 30033</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come join us for the first meetup for the month of October!</p>\n\n<p>We'll be doing our normal eclectic mix of self-improvement brainstorming, educational mini-presentations, structured discussion, unstructured discussion, and social fun and games times!</p>\n\n<p>Please contact me if you have cat allergies, as our meeting space has cats. Incredibly cute cats.</p>\n\n<p>And check out ATLesswrong's facebook group, if you haven't already: https://www.facebook.com/groups/100137206844878/ where you can connect with Atlanta Lesswrongers and suggest a topics for discussion at this meetup!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Lesswrong___October_Meetup__First_of_Two_1\">Discussion article for the meetup : <a href=\"/meetups/rs\">Atlanta Lesswrong:  October Meetup (First of Two)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta Lesswrong:  October Meetup (First of Two)", "anchor": "Discussion_article_for_the_meetup___Atlanta_Lesswrong___October_Meetup__First_of_Two_", "level": 1}, {"title": "Discussion article for the meetup : Atlanta Lesswrong:  October Meetup (First of Two)", "anchor": "Discussion_article_for_the_meetup___Atlanta_Lesswrong___October_Meetup__First_of_Two_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-03T05:47:47.004Z", "modifiedAt": null, "url": null, "title": "PSA: Very important policy change at Cryonics Institute", "slug": "psa-very-important-policy-change-at-cryonics-institute", "viewCount": null, "lastCommentedAt": "2019-11-15T18:13:14.961Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scott Garrabrant", "createdAt": "2017-09-22T02:21:16.385Z", "isAdmin": false, "displayName": "Scott Garrabrant"}, "userId": "hbQoLoK5tpmFAJGr4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hLptdRFykkANXcBwg/psa-very-important-policy-change-at-cryonics-institute", "pageUrlRelative": "/posts/hLptdRFykkANXcBwg/psa-very-important-policy-change-at-cryonics-institute", "linkUrl": "https://www.lesswrong.com/posts/hLptdRFykkANXcBwg/psa-very-important-policy-change-at-cryonics-institute", "postedAtFormatted": "Thursday, October 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20PSA%3A%20Very%20important%20policy%20change%20at%20Cryonics%20Institute&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APSA%3A%20Very%20important%20policy%20change%20at%20Cryonics%20Institute%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhLptdRFykkANXcBwg%2Fpsa-very-important-policy-change-at-cryonics-institute%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=PSA%3A%20Very%20important%20policy%20change%20at%20Cryonics%20Institute%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhLptdRFykkANXcBwg%2Fpsa-very-important-policy-change-at-cryonics-institute", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhLptdRFykkANXcBwg%2Fpsa-very-important-policy-change-at-cryonics-institute", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<p>In the past, the <a href=\"http://www.cryonics.org/\">Cryonics Institute</a>&nbsp;has had a policy that said that they would not accept anyone who is not a member. This has <a href=\"http://www.cryonics.org/emergency-situations/deceased-non-member\">changed</a>. The policy now is that someone who has full legal authority over your body can sign you up after you die. It costs $36,250 to be frozen if you are not signed up, which is more expensive. They also will not do anything until you have been on dry ice for 2 weeks after they have been contacted, so not being a member is more risky.&nbsp;</p>\n<p>This is very important news for anyone who is currently cryocrastinating. It means that you can drastically increase your chances of survival without filling out any forms. All you have to do is tell a loved one you want to be frozen upon death, and that you would like them to take responsibility for making sure this happens. This takes literally 30 seconds. Do it now!</p>\n<p>This news might also be a reason to not sign up right away, if you think something better (like radical life extension or uploading) will come along in your lifetime. We should discuss this in the comments.</p>\n<p>Edit: The general consensus of this discussion is that this is a really bad reason not to sign up for cryonics.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hLptdRFykkANXcBwg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 26, "extendedScore": null, "score": 9.6e-05, "legacy": true, "legacyId": "24320", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 99, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-03T05:48:16.204Z", "modifiedAt": null, "url": null, "title": "Crush Your Uncertainty", "slug": "crush-your-uncertainty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:38.513Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/G9hfw5SrEPfWPzomF/crush-your-uncertainty", "pageUrlRelative": "/posts/G9hfw5SrEPfWPzomF/crush-your-uncertainty", "linkUrl": "https://www.lesswrong.com/posts/G9hfw5SrEPfWPzomF/crush-your-uncertainty", "postedAtFormatted": "Thursday, October 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Crush%20Your%20Uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACrush%20Your%20Uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG9hfw5SrEPfWPzomF%2Fcrush-your-uncertainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Crush%20Your%20Uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG9hfw5SrEPfWPzomF%2Fcrush-your-uncertainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG9hfw5SrEPfWPzomF%2Fcrush-your-uncertainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1100, "htmlBody": "<p>Bayesian epistemology and decision theory provide a rigorous foundation for dealing with mixed or ambiguous evidence, uncertainty, and risky decisions. You can't always get the epistemic conditions that classical techniques like logic or maximum liklihood require, so this is seriously valuable. However, having internalized this new set of tools, it is easy to fall into the bad habit of failing to avoid situations where it is necessary to use them.</p>\n<p>When I first saw the light of an epistemology based on probability theory, I tried to convince my father that the Bayesian answer to problems involving an unknown processes (eg. laplace's rule of succession), was superior to the classical (eg. maximum likelihood) answer. He resisted, with the following argument:</p>\n<ul>\n<li>The maximum likelihood estimator plus some measure of significance is easier to compute.</li>\n<li>In the limit of lots of evidence, this agrees with Bayesian methods.</li>\n<li>When you don't have enough evidence for statistical significance, the correct course of action is to collect more evidence, <em>not</em> to take action based on your current knowledge.</li>\n</ul>\n<p>I added conditions (eg. what if there is no more evidence and you have to make a decision <em>now</em>?) until he grudgingly stopped fighting the hypothetical and agreed that the Bayesian framework was superior in some situations (months later, mind you).</p>\n<p>I now realize that he was right to fight that hypothetical, and he was right that you should prefer classical max likelihood plus significance in most situations. But of course I had to learn this the hard way.</p>\n<p>It is not always, or even often, possible to get overwhelming evidence. Sometimes you only have visibility into one part of a system. Sometimes further tests are expensive, and you need to decide <em>now</em>. Sometimes the decision is clear even without further information. The advanced methods can get you through such situations, so it's critical to know them, but that doesn't mean you can laugh in the face of uncertainty in general.</p>\n<p>At work, I used to do a lot of what you might call \"cowboy epistemology\". I quite enjoyed drawing useful conclusions from minimal evidence and careful probability-literate analysis. Juggling multiple hypotheses and visualizing probability flows between them is just fun. This seems harmless, or even helpful, but it meant I didn't take gathering redundant data seriously enough. I now think you should systematically and completely crush your uncertainty at all opportunities. You should not be satisfied until exactly one hypothesis has non-negligible probability.</p>\n<p>Why? If I'm investigating a system, and even though we are not completely clear on what's going on, the current data is enough to suggest a course of action, and value of information calculations say that decision is not likely enough to change to make further investigation worth it, why then should I go and do further investigation to pin down the details?</p>\n<p>The first reason is the obvious one; stronger evidence can make up for human mistakes. While a lot can be said for it's <em>power</em>, human brain is not a <em>precise</em> instrument; sometimes you'll feel a little more confident, sometimes a little less. As you gather evidence towards a point where you feel you have enough, that random fluctuation can cause you to stop early. But this only suggests that you should have a small bias towards gathering a bit more evidence.</p>\n<p>The second reason is that though you may be able to make the correct immediate decision, going into the future, that residual uncertainty will bite you back eventually. Eventually your habits and heuristics derived from the initial investigation will diverge from what's actually going on. You would not expect this in a perfect reasoner; they would always use their full uncertainty in all calculations, but again, the human brain is a blunt instrument, and likes to simplify things. What was once a nuanced probability distribution like <code>95% X, 5% Y</code> might slip to just <code>X</code> when you're not quite looking, and then, 5% of the time, something comes back from the grave to haunt you.</p>\n<p>The third reason is computational complexity. Inference with very high certainty is easy; it's often just simple direct math or clear intuitive visualizations. With a lot of uncertainty, on the other hand, you need to do your computation once for each of all (or some sample of) probable worlds, or you need to find a shortcut (eg analytic methods), which is only sometimes possible. This is an unavoidable problem for any bounded reasoner.</p>\n<p>For example, you simply would not be able to design chips or computer programs if you could not treat transistors as infallible logical gates, and if you really really had to do so, the first thing you would do would be to build an error-correcting base system on top of which you could treat computation as approximately deterministic.</p>\n<p>It is possible in small problems to manage uncertainty with advanced methods (eg. Bayes), and this is very much necessary while you decide how to get more certainty, but for unavoidable computational reasons, it is not sustainable in the long term, and must be a temporary condition.</p>\n<p>If you take the habit of crushing your uncertainty, your model of situations can be much simpler and you won't have to deal with residual uncertainty from previous related investigations. Instead of many possible worlds and nuanced probability distributions to remember and gum up your thoughts, you can deal with simple, clear, unambiguous <em>facts</em>.</p>\n<p>My previous cowboy-epistemologist self might have agreed with everything written here, but failed to really get that <em>uncertainty is bad</em>. Having just been empowered to deal with uncertainty properly, there was a tendency to not just be unafraid of uncertainty, but to think that it was OK, or even glorify it. What I'm trying to convey here is that that aesthetic is mistaken, and as silly as it feels to have to repeat something so elementary, uncertainty is to be avoided. More viscerally, <em>uncertainty is uncool</em> (unjustified confidence is even less cool, though.)</p>\n<p>So what's this all got to do with my father's classical methods? I still very much recommend thinking in terms of probability theory when working on a problem; it is, after all, the best basis for epistemology that we know of, and is perfectly adequate as an intuitive framework. It's just that it's <em>expensive</em>, and in the epistemic state you really want to be in, that expense is redundant in the sense that you can just use some simpler method that converges to the Bayesian answer.</p>\n<p>I could leave you with an overwhelming pile of examples, but I have no particular incentive to crush <em>your</em> uncertainty, so I'll just remind you to treat hypotheses like zombies; always <a href=\"https://www.youtube.com/watch?v=w4sWxsrEFFs\">double tap</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "G9hfw5SrEPfWPzomF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 24, "extendedScore": null, "score": 1.3648063284734028e-06, "legacy": true, "legacyId": "24319", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-03T09:08:34.710Z", "modifiedAt": null, "url": null, "title": "The best 15 words", "slug": "the-best-15-words", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:35.788Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "apophenia", "createdAt": "2010-04-13T14:09:52.433Z", "isAdmin": false, "displayName": "apophenia"}, "userId": "2rgiaLhZS8w2Fekt9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3tFmWHD6Zqy7DKEMG/the-best-15-words", "pageUrlRelative": "/posts/3tFmWHD6Zqy7DKEMG/the-best-15-words", "linkUrl": "https://www.lesswrong.com/posts/3tFmWHD6Zqy7DKEMG/the-best-15-words", "postedAtFormatted": "Thursday, October 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20best%2015%20words&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20best%2015%20words%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3tFmWHD6Zqy7DKEMG%2Fthe-best-15-words%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20best%2015%20words%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3tFmWHD6Zqy7DKEMG%2Fthe-best-15-words", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3tFmWHD6Zqy7DKEMG%2Fthe-best-15-words", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 207, "htmlBody": "<p>People want to tell everything instead of telling the best 15 words. &nbsp;They want to learn everything instead of the best 15 words. &nbsp;In this thread, instead post the best 15-words from a book you've read recently (or anything else). &nbsp;<strong>It has to stand on its own.</strong> It's not a summary, the whole value needs to be contained in those words.</p>\n<p>&nbsp;</p>\n<ul>\n<li>It doesn't need to cover everything in the book, it's just the best 15 words.</li>\n<li>It doesn't need to be a quote, it's just the best 15 words.</li>\n<li>It doesn't have to be 15 words long, it's just the best \"15\" words.</li>\n<li>It doesn't have to be precisely true, it's just the best 15 words.</li>\n<li>It doesn't have to be the main 15 words, it just has to be the best 15 words.</li>\n<li>It doesn't have to be the author's 15 words, it just has to be the best 15 words.</li>\n<li>Edit: It shouldn't just be a neat quote--the point of the exercise is to struggle to move from a book down to 15 words.</li>\n</ul>\n<p>&nbsp;</p>\n<p>I'll start in the comments below.</p>\n<p>(Voted by the Schelling study group as the best exercise of the meeting.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3tFmWHD6Zqy7DKEMG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 15, "extendedScore": null, "score": 1.364988772556798e-06, "legacy": true, "legacyId": "24327", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 386, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-03T16:45:08.618Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham/RTLW HPMoR discussion, ch. 87-89", "slug": "meetup-durham-rtlw-hpmor-discussion-ch-87-89", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sfuz9y4eyQYyWRxoN/meetup-durham-rtlw-hpmor-discussion-ch-87-89", "pageUrlRelative": "/posts/sfuz9y4eyQYyWRxoN/meetup-durham-rtlw-hpmor-discussion-ch-87-89", "linkUrl": "https://www.lesswrong.com/posts/sfuz9y4eyQYyWRxoN/meetup-durham-rtlw-hpmor-discussion-ch-87-89", "postedAtFormatted": "Thursday, October 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2087-89&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2087-89%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsfuz9y4eyQYyWRxoN%2Fmeetup-durham-rtlw-hpmor-discussion-ch-87-89%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%2FRTLW%20HPMoR%20discussion%2C%20ch.%2087-89%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsfuz9y4eyQYyWRxoN%2Fmeetup-durham-rtlw-hpmor-discussion-ch-87-89", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsfuz9y4eyQYyWRxoN%2Fmeetup-durham-rtlw-hpmor-discussion-ch-87-89", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rt'>Durham/RTLW HPMoR discussion, ch. 87-89</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 October 2013 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham NC, 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Join us for discussion of chapters 87-89 of HPMoR!</p>\n\n<p>Discussion topics invited but not required.</p>\n\n<p>12:00 gather with coffee <br />\n12:30 discuss <br />\n2:00 adjourn</p>\n\n<p>Join the Google group (low traffic) to have meetup announcements emailed to you!  https://groups.google.com/group/rtlw</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rt'>Durham/RTLW HPMoR discussion, ch. 87-89</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sfuz9y4eyQYyWRxoN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "24328", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__87_89\">Discussion article for the meetup : <a href=\"/meetups/rt\">Durham/RTLW HPMoR discussion, ch. 87-89</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 October 2013 12:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">726 Rigsbee Avenue, Durham NC, 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Join us for discussion of chapters 87-89 of HPMoR!</p>\n\n<p>Discussion topics invited but not required.</p>\n\n<p>12:00 gather with coffee <br>\n12:30 discuss <br>\n2:00 adjourn</p>\n\n<p>Join the Google group (low traffic) to have meetup announcements emailed to you!  https://groups.google.com/group/rtlw</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__87_891\">Discussion article for the meetup : <a href=\"/meetups/rt\">Durham/RTLW HPMoR discussion, ch. 87-89</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 87-89", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__87_89", "level": 1}, {"title": "Discussion article for the meetup : Durham/RTLW HPMoR discussion, ch. 87-89", "anchor": "Discussion_article_for_the_meetup___Durham_RTLW_HPMoR_discussion__ch__87_891", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-03T22:34:44.347Z", "modifiedAt": null, "url": null, "title": "Fixing akrasia: damnation to acausal hell", "slug": "fixing-akrasia-damnation-to-acausal-hell", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:38.708Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "joaolkf", "createdAt": "2010-02-24T18:52:27.966Z", "isAdmin": false, "displayName": "joaolkf"}, "userId": "woC2b5rav5sGrAo3E", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dN4yDdtMmPpcEnML3/fixing-akrasia-damnation-to-acausal-hell", "pageUrlRelative": "/posts/dN4yDdtMmPpcEnML3/fixing-akrasia-damnation-to-acausal-hell", "linkUrl": "https://www.lesswrong.com/posts/dN4yDdtMmPpcEnML3/fixing-akrasia-damnation-to-acausal-hell", "postedAtFormatted": "Thursday, October 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fixing%20akrasia%3A%20damnation%20to%20acausal%20hell&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFixing%20akrasia%3A%20damnation%20to%20acausal%20hell%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdN4yDdtMmPpcEnML3%2Ffixing-akrasia-damnation-to-acausal-hell%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fixing%20akrasia%3A%20damnation%20to%20acausal%20hell%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdN4yDdtMmPpcEnML3%2Ffixing-akrasia-damnation-to-acausal-hell", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdN4yDdtMmPpcEnML3%2Ffixing-akrasia-damnation-to-acausal-hell", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 858, "htmlBody": "<p class=\"MsoNormal\"><em>DISCLAIMER: This topic is related to a&nbsp;potentially&nbsp;harmful memetic hazard, that has been rightly banned from Less Wrong. If you don't know what is, it is more likely you will be fine than not, but be advised. If do know, do not mention it in the comments.</em></p>\n<hr />\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><strong>Abstract:</strong>&nbsp;The fact that humans cannot precommit very well might be one of our defences against acausal trades. If transhumanists figure out how to beat akrasia by some sort of drug or brain tweaks, that might make them much better at precommitment, and thus more vulnerable. That means solving akrasia might be dangerous, at least until we solve blackmail. If the danger is bad enough, even small steps should be considered carefully.</p>\n<p class=\"MsoNormal\"><br /> <br /> Strong precommitment and building detailed simulations of other agents are two relevant capabilities humans currently don't have. These capabilities have some unusual consequences for games. Most relevant games only arise when there is a chance of monitoring, commitment and multiple interactions. Hence being in a relevant game often implies cohabiting casual connected space-time regions with other agents. Nevertheless, being able to build detailed simulations of agents allows one to vastly increase the subjective probably this particular agent will have that his next observational moment will be under one's control iff the agent have access to some relevant areas of the logical game theoretic space. This doesn't seem desirable from this agent's perspective, it is extremely asymmetrical and allows more advanced agents to enslave less advanced ones even if they don't cohabit casual connected regions of the universe. Being able to be acausally reached by powerful agent who can simulate 3^^^3 copies of you, but against which you cannot do much is extremely undesirable.</p>\n<p class=\"MsoNormal\">However, and more generally, regions of the block universe can only be in a game with non-cohabiting regions if they are both agents and if they can strong precommit. Any <a title=\"Acausal trade Wiki\" href=\"http://wiki.lesswrong.com/wiki/Acausal_trade#Description\" target=\"_blank\">acausal trade</a> depends on precommitment, this is the only way an agreement can go across space-time, it is done on the game-theoretical possibilities space - as I am calling it. In the case I am discussing, a powerful agent would only have reason to even consider acausal trading with an agent if that agent can precommit. Otherwise, there is no other way of ensuring acausal cooperation. If the other agent cannot, beforehand, understand that due to the peculiarities of the set of possible strategies, it is better to always precommit to those strategies that will have higher payoff when considering all other strategies, then there's no trade to be made. Would be like trying to threaten a spider with a calm verbal sentence. If the other agent cannot precommit, there is no reason for the powerful agent to punish him for anything, he wouldn't be able to cooperate anyway, he wouldn't understand the game and, more importantly in my argument, he wouldn't be able to follow his precommitment, it would break down eventually, specially since the evidence for it is so abstract and complex. The powerful agent might want to simulate the minor agent suffering anyway, but it would solely amount to sadism. Acausal trades can only reach strong precommitable areas of the universe.</p>\n<p class=\"MsoNormal\">Moreover, an agent also needs reasonable epistemic access to the regions of logical space (certain areas of game theory, or, TDT if you will) that indicates both the possibility of acausal trades and some estimative on the type-distribution of superintelligences willing to trade with him (most likely, future ones that the agent can help create). Forever deterring the advance of knowledge on that area seems unfeasible, or - at best - complicated and undesirable for other reasons.</p>\n<p class=\"MsoNormal\">It is clear that we (humans) don't want to be in an enslavable position. I believe we are not. One of the things excluding us from this position is complete incapability to precommit. This is a psychological constrain, a neurochemical constrain. We do not have the ability of even having stable long term goals, strong precommitment is neurochemical impossible. However, it seems we can change this with human enhancement, we could develop drugs which could cure akrasia, we could overcome breakdown of will with some amazing psychological technique discovered by CFAR. It seems, however desirable on other grounds, getting rid of akrasia presents severe risks. Even if somehow we only slightly decrease akrasia, this would increase the probability that individuals with access to the relevant regions of logical space could precommit and become slaves. They might then proceed to cure akrasia for the rest of humanity.</p>\n<p class=\"MsoNormal\">Therefore, we should avoid trying to fundamentally fix akrasia for now, until we have a better understanding of those matters and perhaps solve the blackmail problem, or maybe only after FAI. My point here is merely arguing everyone should not endorse technologies (or psychological techniques) proposing to fundamentally fix a problem that would, otherwise, seems desirable of fixing. It would seem like a clear optimization process, but it could actually open the gates of acausal hell and damn humanity to eternal slavery.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">(Thank cousin_it for the abstract. All mistakes are my responsibility.)<br /><br />(EDIT: Added an explanation to back up the premise the acausal trade entails precommitment.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dN4yDdtMmPpcEnML3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 1, "extendedScore": null, "score": 1.3657234729564655e-06, "legacy": true, "legacyId": "24329", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-03T23:48:29.276Z", "modifiedAt": null, "url": null, "title": "Meetup : Tucson Meetup", "slug": "meetup-tucson-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:39.238Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "NzwpJ8hz3sA6nLF6G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Bd4TNwkCr6qgqLrpP/meetup-tucson-meetup", "pageUrlRelative": "/posts/Bd4TNwkCr6qgqLrpP/meetup-tucson-meetup", "linkUrl": "https://www.lesswrong.com/posts/Bd4TNwkCr6qgqLrpP/meetup-tucson-meetup", "postedAtFormatted": "Thursday, October 3rd 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Tucson%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Tucson%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBd4TNwkCr6qgqLrpP%2Fmeetup-tucson-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Tucson%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBd4TNwkCr6qgqLrpP%2Fmeetup-tucson-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBd4TNwkCr6qgqLrpP%2Fmeetup-tucson-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 112, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ru'>Tucson Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 October 2013 02:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">943 E University Blvd #191, Tucson, AZ \u200e </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is a first meetup for Tucson. We will meet at a coffee shop near the U of A, \"Caffe Luce\". As this is a first meeting we will be doing some introductions to start with. As discussion, I would like to focus on the idea of community, and what value it may hold. If attendance and interest is high enough, we will discuss how a community may be built, and our own rationality goals. \nI will be there holding at LW sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ru'>Tucson Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Bd4TNwkCr6qgqLrpP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.365790719388602e-06, "legacy": true, "legacyId": "24330", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Tucson_Meetup\">Discussion article for the meetup : <a href=\"/meetups/ru\">Tucson Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 October 2013 02:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">943 E University Blvd #191, Tucson, AZ \u200e </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is a first meetup for Tucson. We will meet at a coffee shop near the U of A, \"Caffe Luce\". As this is a first meeting we will be doing some introductions to start with. As discussion, I would like to focus on the idea of community, and what value it may hold. If attendance and interest is high enough, we will discuss how a community may be built, and our own rationality goals. \nI will be there holding at LW sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Tucson_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/ru\">Tucson Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Tucson Meetup", "anchor": "Discussion_article_for_the_meetup___Tucson_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Tucson Meetup", "anchor": "Discussion_article_for_the_meetup___Tucson_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-04T03:44:12.712Z", "modifiedAt": null, "url": null, "title": "How do you say no?", "slug": "how-do-you-say-no", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:38.860Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BrienneYudkowsky", "createdAt": "2013-05-13T00:07:08.935Z", "isAdmin": false, "displayName": "LoganStrohl"}, "userId": "uuYBzWLiixkbN3s7C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xGrrZq8eKKHLGgCiM/how-do-you-say-no", "pageUrlRelative": "/posts/xGrrZq8eKKHLGgCiM/how-do-you-say-no", "linkUrl": "https://www.lesswrong.com/posts/xGrrZq8eKKHLGgCiM/how-do-you-say-no", "postedAtFormatted": "Friday, October 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20do%20you%20say%20no%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20do%20you%20say%20no%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxGrrZq8eKKHLGgCiM%2Fhow-do-you-say-no%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20do%20you%20say%20no%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxGrrZq8eKKHLGgCiM%2Fhow-do-you-say-no", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxGrrZq8eKKHLGgCiM%2Fhow-do-you-say-no", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 265, "htmlBody": "<p><span style=\"color: #222222; font-size: 13px; font-family: arial, sans-serif;\">Some people seem to be a bit too generous for their own good. I know a precious few people who are especially good at saying \"no\" when asked to take on new responsibilities that would put them over their limits. I love working with people like that because I can always trust them to tell me when it would be better for me to find someone else to do the thing. I expect this to be an extremely valuable skill it would probably be good for many of us to understand, learn, and be able to teach to people who really need it.</span></p>\n<div style=\"color: #222222; font-size: 13px; font-family: arial, sans-serif;\">If you frequently find yourself overburdened, think it's not entirely necessary for you to be doing as much as you are, and can recall a specific instance in the last month where someone asked you to do something and you accepted against your better judgement, I invite you to describe what you were feeling and thinking at the time.</div>\n<div style=\"color: #222222; font-size: 13px; font-family: arial, sans-serif;\"><br /></div>\n<div style=\"color: #222222; font-size: 13px; font-family: arial, sans-serif;\">Alternately, if you're an unusually busy and productive person who nevertheless is good at saying \"no\", I'd like to hear about</div>\n<div style=\"color: #222222; font-size: 13px; font-family: arial, sans-serif;\"><ol>\n<li>a specific example of a time when you said no to new responsibility, what was going on in your head, and how it felt</li>\n<li>how exactly you believe you decide whether to take on or reject prospective responsibilities if you have an explicit model</li>\n<li>whether you consider yourself more or less empathetic or compassionate than average</li>\n<li>whether there was ever a time when you had that \"don't know how to say no\" problem, and if so what changed</li>\n</ol></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xGrrZq8eKKHLGgCiM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 24, "extendedScore": null, "score": 1.3660056990416032e-06, "legacy": true, "legacyId": "24331", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-04T07:06:06.737Z", "modifiedAt": null, "url": null, "title": "October Monthly Bragging Thread", "slug": "october-monthly-bragging-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:02.038Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "linkhyrule5", "createdAt": "2011-05-11T06:03:56.654Z", "isAdmin": false, "displayName": "linkhyrule5"}, "userId": "dfDgYH8CfLSYPCaRj", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S8NoABniT58xzNHYq/october-monthly-bragging-thread", "pageUrlRelative": "/posts/S8NoABniT58xzNHYq/october-monthly-bragging-thread", "linkUrl": "https://www.lesswrong.com/posts/S8NoABniT58xzNHYq/october-monthly-bragging-thread", "postedAtFormatted": "Friday, October 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20October%20Monthly%20Bragging%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOctober%20Monthly%20Bragging%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS8NoABniT58xzNHYq%2Foctober-monthly-bragging-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=October%20Monthly%20Bragging%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS8NoABniT58xzNHYq%2Foctober-monthly-bragging-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS8NoABniT58xzNHYq%2Foctober-monthly-bragging-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 189, "htmlBody": "<p>Since it had a decent amount of traffic until a good two weeks into September (and I thought it was a good idea), I'm reviving this thread.</p>\n<p><span class=\"author\"><a id=\"author_t3_iav\" href=\"http://lesswrong.com/user/Joshua_Blaine/\">Joshua_Blaine</a></span><span class=\"date\">:</span></p>\n<blockquote>\n<div id=\"entry_t3_iav\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p>In an attempt to encourage more people to <em>actually do awesome things </em>(a  la instrumental rationality), I am proposing a new monthly thread (can  be changed to bi-weekly, should that be demanded). Your job, should you  choose to accept it, is to comment on this thread explaining <strong>the most awesome thing you've done this month</strong>. You may be as blatantly proud of you self as you feel. You may unabashedly consider yourself <em>the coolest freaking person ever</em>&nbsp;because of that awesome thing you're dying to tell everyone about. This is the place to do just that.</p>\n<p>Remember, however, that this <strong>isn't</strong>&nbsp;any kind of progress thread. Nor is it any kind of proposal thread.<em>This thread is solely for people to talk about the awesomest thing they've done all month. not will do. not are working on</em>. <strong>have already done. </strong>This is to cultivate an environment of object level productivity rather than meta-productivity methods.</p>\n<p>So, what's the coolest thing you've done this month?</p>\n</div>\n</div>\n</div>\n</div>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S8NoABniT58xzNHYq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 14, "extendedScore": null, "score": 1.3661898793421505e-06, "legacy": true, "legacyId": "24332", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-04T10:32:04.028Z", "modifiedAt": "2022-01-19T20:07:23.277Z", "url": null, "title": "Meetup: Ljubljana", "slug": "meetup-ljubljana", "viewCount": null, "lastCommentedAt": "2018-04-04T10:05:59.879Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thomas", "createdAt": "2009-03-02T17:47:09.607Z", "isAdmin": false, "displayName": "Thomas"}, "userId": "GrAKeuxT4e9AKyHdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mt6wotPoTJP7La54X/meetup-ljubljana", "pageUrlRelative": "/posts/mt6wotPoTJP7La54X/meetup-ljubljana", "linkUrl": "https://www.lesswrong.com/posts/mt6wotPoTJP7La54X/meetup-ljubljana", "postedAtFormatted": "Friday, October 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%3A%20Ljubljana&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%3A%20Ljubljana%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmt6wotPoTJP7La54X%2Fmeetup-ljubljana%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%3A%20Ljubljana%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmt6wotPoTJP7La54X%2Fmeetup-ljubljana", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmt6wotPoTJP7La54X%2Fmeetup-ljubljana", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 23, "htmlBody": "<p>Tonight, Friday 4th October, at 19:00 in Kraka, Ljubljana, we have a very casual transhumanistic meeting.</p>\n<p style=\"margin: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 12.666666984558105px; text-align: justify;\"><strong style=\"text-transform: uppercase;\">WHEN:</strong>&nbsp;4<span class=\"date\">&nbsp;October 2013 19:00:00PM (CET+1)</span></p>\n<p style=\"margin: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 12.666666984558105px; text-align: justify;\"><strong style=\"text-transform: uppercase;\">WHERE:</strong>&nbsp;<span style=\"color: #333333; font-family: 'Helvetica Neue', Tahoma, Verdana, Helvetica, sans-serif; font-size: 12px; line-height: 19.700000762939453px;\">Karaka bar (Stihova 13, Ljubljana):</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mt6wotPoTJP7La54X", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.3663778035056243e-06, "legacy": true, "legacyId": "24333", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-10-04T10:32:04.028Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-04T11:50:25.314Z", "modifiedAt": null, "url": null, "title": "Meetup : Philadelphia - What Bayesianism taught me", "slug": "meetup-philadelphia-what-bayesianism-taught-me", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mare-of-night", "createdAt": "2013-04-06T13:26:03.532Z", "isAdmin": false, "displayName": "mare-of-night"}, "userId": "6thzLTpEnEpcZF8bf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/R9ajrSLjPmqagDzsx/meetup-philadelphia-what-bayesianism-taught-me", "pageUrlRelative": "/posts/R9ajrSLjPmqagDzsx/meetup-philadelphia-what-bayesianism-taught-me", "linkUrl": "https://www.lesswrong.com/posts/R9ajrSLjPmqagDzsx/meetup-philadelphia-what-bayesianism-taught-me", "postedAtFormatted": "Friday, October 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Philadelphia%20-%20What%20Bayesianism%20taught%20me&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Philadelphia%20-%20What%20Bayesianism%20taught%20me%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR9ajrSLjPmqagDzsx%2Fmeetup-philadelphia-what-bayesianism-taught-me%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Philadelphia%20-%20What%20Bayesianism%20taught%20me%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR9ajrSLjPmqagDzsx%2Fmeetup-philadelphia-what-bayesianism-taught-me", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR9ajrSLjPmqagDzsx%2Fmeetup-philadelphia-what-bayesianism-taught-me", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 85, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rv'>Philadelphia - What Bayesianism taught me</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 October 2013 01:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1100 Washington Ave, Philadelphia, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will have a meetup this Sunday, 6 October, at 1pm\nat Nam Phuong restaurant, 11th and Washington.</p>\n\n<p>Nam Phuong is on 11th street, a mile south of Market East. It is on the west side of the street, between Washington and Ellsworth.</p>\n\n<p>The discussion prompt is the article (What Bayesianism taught me)[<a href=\"http://lesswrong.com/lw/iat/what_bayesianism_taught_me/\" rel=\"nofollow\">http://lesswrong.com/lw/iat/what_bayesianism_taught_me/</a>].</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rv'>Philadelphia - What Bayesianism taught me</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "R9ajrSLjPmqagDzsx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.3664493105547537e-06, "legacy": true, "legacyId": "24334", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Philadelphia___What_Bayesianism_taught_me\">Discussion article for the meetup : <a href=\"/meetups/rv\">Philadelphia - What Bayesianism taught me</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 October 2013 01:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1100 Washington Ave, Philadelphia, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will have a meetup this Sunday, 6 October, at 1pm\nat Nam Phuong restaurant, 11th and Washington.</p>\n\n<p>Nam Phuong is on 11th street, a mile south of Market East. It is on the west side of the street, between Washington and Ellsworth.</p>\n\n<p>The discussion prompt is the article (What Bayesianism taught me)[<a href=\"http://lesswrong.com/lw/iat/what_bayesianism_taught_me/\" rel=\"nofollow\">http://lesswrong.com/lw/iat/what_bayesianism_taught_me/</a>].</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Philadelphia___What_Bayesianism_taught_me1\">Discussion article for the meetup : <a href=\"/meetups/rv\">Philadelphia - What Bayesianism taught me</a></h2>", "sections": [{"title": "Discussion article for the meetup : Philadelphia - What Bayesianism taught me", "anchor": "Discussion_article_for_the_meetup___Philadelphia___What_Bayesianism_taught_me", "level": 1}, {"title": "Discussion article for the meetup : Philadelphia - What Bayesianism taught me", "anchor": "Discussion_article_for_the_meetup___Philadelphia___What_Bayesianism_taught_me1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JBnaLpsrYXLXjFocu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-04T15:26:16.002Z", "modifiedAt": null, "url": null, "title": "[LINK] How to increase conscientiousness", "slug": "link-how-to-increase-conscientiousness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:38.371Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lincolnquirk", "createdAt": "2011-03-25T20:46:17.071Z", "isAdmin": false, "displayName": "lincolnquirk"}, "userId": "ScJE7nuW8ti5kzfcA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5cT32gYH6w6Fz4MRR/link-how-to-increase-conscientiousness", "pageUrlRelative": "/posts/5cT32gYH6w6Fz4MRR/link-how-to-increase-conscientiousness", "linkUrl": "https://www.lesswrong.com/posts/5cT32gYH6w6Fz4MRR/link-how-to-increase-conscientiousness", "postedAtFormatted": "Friday, October 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20How%20to%20increase%20conscientiousness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20How%20to%20increase%20conscientiousness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5cT32gYH6w6Fz4MRR%2Flink-how-to-increase-conscientiousness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20How%20to%20increase%20conscientiousness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5cT32gYH6w6Fz4MRR%2Flink-how-to-increase-conscientiousness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5cT32gYH6w6Fz4MRR%2Flink-how-to-increase-conscientiousness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 84, "htmlBody": "<p>I wrote an interactive blog post, <a href=\"http://techhouse.org/~lincoln/blosxom.cgi/si/conscientiousness2.html\">How To Increase Conscientiousness</a>, which has some steps which I think might increase your conscientiousness. I'm not sure if it works, but I would love to see some curious low-conscientiousness people try it and post your results here.</p>\n<p>If you do it, please do it before reading the comments on this post, as they may contain spoilers.</p>\n<p>If you are feeling especially helpful, also take a <a href=\"http://www.outofservice.com/bigfive/\">Big Five personality test like this one</a> and report your percentile result on Conscientiousness.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5cT32gYH6w6Fz4MRR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 1, "extendedScore": null, "score": 1.366646326163947e-06, "legacy": true, "legacyId": "24335", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-04T15:41:23.838Z", "modifiedAt": null, "url": null, "title": "Schelling Point Strategy Training", "slug": "schelling-point-strategy-training", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:33.410Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sixes_and_sevens", "createdAt": "2009-11-11T14:42:23.502Z", "isAdmin": false, "displayName": "sixes_and_sevens"}, "userId": "n83meJ5yG2WQzygvw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eDhKJ4SaKgS25Rf7t/schelling-point-strategy-training", "pageUrlRelative": "/posts/eDhKJ4SaKgS25Rf7t/schelling-point-strategy-training", "linkUrl": "https://www.lesswrong.com/posts/eDhKJ4SaKgS25Rf7t/schelling-point-strategy-training", "postedAtFormatted": "Friday, October 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Schelling%20Point%20Strategy%20Training&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASchelling%20Point%20Strategy%20Training%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeDhKJ4SaKgS25Rf7t%2Fschelling-point-strategy-training%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Schelling%20Point%20Strategy%20Training%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeDhKJ4SaKgS25Rf7t%2Fschelling-point-strategy-training", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeDhKJ4SaKgS25Rf7t%2Fschelling-point-strategy-training", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 712, "htmlBody": "<p>There's a category of game-theoretic scenario called Battle of the Sexes, which is commonly used to demonstrate coordination problems. Two cinema-goers, traditionally a husband and wife, have agreed to go to the cinema, but haven't decided on what to see beforehand. Of the two films that are showing, she would rather see King Kong Lives, while he would rather see Big Momma's House 2. Each would rather see their non-preferred film with their spouse than see their preferred film on their own. The payoff matrix is as follows:</p>\n<table style=\"text-align: center;\" border=\"1\" cellspacing=\"0\" cellpadding=\"5px\">\n<tbody>\n<tr>\n<td style=\"border-width: 0;\" colspan=\"2\" rowspan=\"2\">\n<p>&nbsp;</p>\n</td>\n<td colspan=\"2\">Husband</td>\n</tr>\n<tr>\n<td><em>King Kong Lives</em></td>\n<td><em>Big Momma's House 2</em></td>\n</tr>\n<tr>\n<td rowspan=\"2\">Wife</td>\n<td><em>King Kong Lives</em></td>\n<td>2 / 1</td>\n<td>0 / 0</td>\n</tr>\n<tr>\n<td><em>Big Momma's House 2</em></td>\n<td>0 / 0</td>\n<td>1 / 2</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<p>The two have not conferred beforehand, beyond sharing knowledge of their preferences. They are turning up to the cinema and picking an auditorium in the hope that their spouse is in there. &nbsp;Which should they pick? &nbsp;This is a classic coordination problem. The symmetry of their preferences means there is no stand-out option for them to converge on. There is no Schelling Point.<sup>1</sup></p>\n<p>Except I'm going to argue that there is.</p>\n<p>Shoehorning an example of a Schelling Point into the above scenario, we might imagine that one of the above films being screened is being billed as \"an ideal romantic treat to share with your spouse\", (which one that would be, I'm not entirely sure), though in the absence of a \"natural\" Schelling Point, there's no reason we can't make one. All we need is to identify procedures that would reliably elevate one of these options to our attention. &nbsp;Then it becomes a question of selecting which of these procedures is most likely to be selected by the other agent in the scenario.</p>\n<p>I am now going to instigate a multidimensional instance of Battle of the Sexes with all the readers of this post. &nbsp;Below are sixteen randomly-ordered films. &nbsp;I am going to select one, and invite you to do the same. &nbsp;The object of the exercise is for all of us to pick the same one. &nbsp;I will identify my selection, and the logic behind it, in rot13 after the list.</p>\n<p><em>Breakfast at Tiffany's<br />William Shakespeare's Romeo and Juliet<br />E.T. the Extra-Terrestrial<br />Children of the Corn<br />An American Werewolf in London<br />To Kill a Mockingbird<br />Harold and Maude<br />The Day the Earth Stood Still<br />Duck Soup<br />Highlander<br />Fantasia<br />Heathers<br />Forbidden Planet<br />Butch Cassidy and the Sundance Kid<br />Grosse Pointe Blank<br />Mrs. Doubtfire</em></p>\n<p>Urer vf na vapbafrdhragvny fragrapr gb guebj bss crbcyr jub pna vagrecerg guvf plcure ba fvtug ol abj. Zl fryrpgvba jnf na nzrevpna jrerjbys va Ybaqba. Gur cebprqher V fryrpgrq jnf gur svefg svyz nycunorgvpnyyl.&nbsp;Guvf frrzf yvxr gur zbfg \"boivbhf\" cebprqher sbe eryvnoyl fryrpgvat n fvatyr vgrz sebz gur frg. Cbffvoyl n zber \"boivbhf\" bar jbhyq fvzcyl or gb fryrpg gur svefg bar ba gur yvfg (Oernxsnfg ng Gvssnal'f va guvf pnfr), ohg V jnf bcrengvat ba gur nffhzcgvba gung gur yvfg jnf abg arprffnevyl eryvnoyl-beqrerq (juvpu V gevrq gb pbairl ol qrfpevovat gur yvfg nf \"enaqbzyl-beqrerq\", ohg pbhyqa'g ernyyl rkcyvpvgyl fgngr jvgubhg cbffvoyl tvivat n ovt uvag nf gb gur cebprqher V pubfr. Guvf jbhyq unir fcbvyrq guvatf n yvggyr.</p>\n<p>I have no idea if that worked. &nbsp;Whether or not it did, it seems to me that the general skill of identifying popular procedures for designating Schelling Points is possibly a worthwhile skill to develop. It also seems to me that once a handful of common strategies for identifying Schelling Points are known to a group, some effort has to be put into constructing scenarios in which that group can't coordinate. This forms the outline of an adversarial game, (provisionally named Schelling Point Strategy Training), whereby two teams take it in turns to construct and present a set of options which the other team has to coordinate on. I am idly toying with running a session of this at a future London Less Wrong meetup.&nbsp;</p>\n<hr />\n<p><sup>1&nbsp;</sup>There is actually an unrelated meta-strategy here, whereby on all disputes one designated partner acquiesces to the wishes of the other. &nbsp;This behaviour is also far from unheard of in romantic partnerships. &nbsp;While this doesn't seem very egalitarian, I am wondering if it actually becomes a reasonable trade-off for partnerships which face coordination problems on a regular basis.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eDhKJ4SaKgS25Rf7t", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "24336", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-04T16:02:34.883Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-128", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/igEPgBBzTLBYyKPHR/weekly-lw-meetups-128", "pageUrlRelative": "/posts/igEPgBBzTLBYyKPHR/weekly-lw-meetups-128", "linkUrl": "https://www.lesswrong.com/posts/igEPgBBzTLBYyKPHR/weekly-lw-meetups-128", "postedAtFormatted": "Friday, October 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FigEPgBBzTLBYyKPHR%2Fweekly-lw-meetups-128%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FigEPgBBzTLBYyKPHR%2Fweekly-lw-meetups-128", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FigEPgBBzTLBYyKPHR%2Fweekly-lw-meetups-128", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 516, "htmlBody": "<p><strong>This summary was posted to LW main on September 27th. The following week's summary is <a href=\"/lw/is1/weekly_lw_meetups/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/rg\">Atlanta Pre-Meetup Museum Field Trip:&nbsp;<span class=\"date\">28 September 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/qz\">Atlanta Lesswrong September Meetup (2nd of 2):&nbsp;<span class=\"date\">28 September 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/qs\">Berlin: Fermi paradox discussion:&nbsp;<span class=\"date\">18 October 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/re\">Helsinki Meetup:&nbsp;<span class=\"date\">06 October 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/rf\">Montreal LessWrong - Return from summer break:&nbsp;<span class=\"date\">01 October 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/rc\">Moscow, Beliefs:&nbsp;<span class=\"date\">29 September 2013 04:00PM</span></a></li>\n<li><a href=\"/meetups/rh\">Tempe Meetup:&nbsp;<span class=\"date\">27 September 2013 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">28 September 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/r0\">Columbus, OH MEGA-MEETUP, Oct 11-14:&nbsp;<span class=\"date\">12 October 2013 02:33AM</span></a></li>\n<li><a href=\"/meetups/r7\">London meetup: thought experiments:&nbsp;<span class=\"date\">29 September 2013 02:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "igEPgBBzTLBYyKPHR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3666794778090428e-06, "legacy": true, "legacyId": "24289", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["G9d3FeijDjgCaRCAA", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-04T17:02:01.502Z", "modifiedAt": null, "url": null, "title": "How to Learn from Experts", "slug": "how-to-learn-from-experts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:58.348Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SatvikBeri", "createdAt": "2013-06-14T17:52:32.786Z", "isAdmin": false, "displayName": "SatvikBeri"}, "userId": "g4vakcHCMNzfhidWa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MCMjz2B65sZyr4HLH/how-to-learn-from-experts", "pageUrlRelative": "/posts/MCMjz2B65sZyr4HLH/how-to-learn-from-experts", "linkUrl": "https://www.lesswrong.com/posts/MCMjz2B65sZyr4HLH/how-to-learn-from-experts", "postedAtFormatted": "Friday, October 4th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Learn%20from%20Experts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Learn%20from%20Experts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMCMjz2B65sZyr4HLH%2Fhow-to-learn-from-experts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Learn%20from%20Experts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMCMjz2B65sZyr4HLH%2Fhow-to-learn-from-experts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMCMjz2B65sZyr4HLH%2Fhow-to-learn-from-experts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 453, "htmlBody": "<p class=\"p1\">The key difference between experts and beginners is the quality of their abstractions. Masters of a field mentally organize information in a way that's relevant to the tasks at hand. Amateurs may know as many facts and details as experts but group them in haphazard or irrelevant ways.</p>\n<p class=\"p1\">For example, experienced Bridge players group cards by suit, then number. They place the most importance on the face cards and work down. Bridge amateurs group solely by number and place equal importance on all numbers. Professional firemen group fires by how the fire was started and how fast it&rsquo;s spreading-features they use to contain the fire. Novices group fires by brightness and color. Both have the same information, but the firemen hone in on the useful details faster.<sup>1</sup></p>\n<p class=\"p2\"><strong>Learn abstractions from masters.</strong> If you ask a Software Architect which database technology you should use, circumstances will eventually change and you'll need to ask them again and pay them again. But if you ask the Architect to teach you how to choose a database then you can adapt to changing circumstances. <strong>Ideally you should emerge with a clear set of rules</strong>-something like a flow-chart for that decision. A good example is this article on <a href=\"http://www.chrisstucchio.com/blog/2013/hadoop_hatred.html\">whether you should use hadoop</a>. Clear criteria let you make a high-quality decision by focusing on the relevant details.</p>\n<p class=\"p1\">After talking to the expert you can write up the flow-chart or criteria and send it to them to get their opinion. This ensures you understood what the expert was trying to say, and lets you get additional details they might add. Most importantly it gives them something valuable to share with people seeking similar advice, so you're able to add value to their lives as a thank you for their advice.&nbsp;</p>\n<p class=\"p2\">Caveats to this method:</p>\n<ul>\n<li class=\"li1\">In some domains there are details only professionals know. Academic research has a <a href=\"http://chewychunks.wordpress.com/2010/02/02/the-secret-paper-passing-network-in-science-and-beyond/\">secret paper-passing network</a> with ideas known to top researchers 1-2 years before they&rsquo;re published. So you need to be in constant contact with these experts and hear the details from them. However, this typically only matters if you&rsquo;re aiming to become a top-class expert yourself.&nbsp;</li>\n<li class=\"li1\">Experts aren&rsquo;t always conscious of the abstractions they use. They&rsquo;ll say one thing and do another. So you should ask them to guide you through a specific situation and ask them several questions about how their decision would change if some conditions are different.</li>\n<li class=\"li1\">You may not have a specific question you want answered-you might want to find &ldquo;unknown unknowns.&rdquo; In that case ask the expert for stories-things they did that made a big difference. Then analyze those situations to figure out what criteria they used.&nbsp;</li>\n</ul>\n<div><br /></div>\n<div>1: <em><a href=\"http://www.amazon.com/Cambridge-Expertise-Performance-Handbooks-Psychology/dp/0521600812/ref=sr_1_1?ie=UTF8&amp;qid=1380902611&amp;sr=8-1&amp;keywords=cambridge+handbook+of+expertise\">The Cambridge Handbook of Expertise and Expert Performance</a></em></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"x3zyEPFaJANB2BHmP": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MCMjz2B65sZyr4HLH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 55, "extendedScore": null, "score": 0.000222, "legacy": true, "legacyId": "24338", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-05T00:04:45.177Z", "modifiedAt": null, "url": null, "title": "Ganesalingam and Gowers on automated theorem-proving", "slug": "ganesalingam-and-gowers-on-automated-theorem-proving", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:58.218Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VipulNaik", "createdAt": "2013-09-02T18:51:08.862Z", "isAdmin": false, "displayName": "VipulNaik"}, "userId": "t3pZcNZXqhaM5avBE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4K8sTathEnEnPsHMQ/ganesalingam-and-gowers-on-automated-theorem-proving", "pageUrlRelative": "/posts/4K8sTathEnEnPsHMQ/ganesalingam-and-gowers-on-automated-theorem-proving", "linkUrl": "https://www.lesswrong.com/posts/4K8sTathEnEnPsHMQ/ganesalingam-and-gowers-on-automated-theorem-proving", "postedAtFormatted": "Saturday, October 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ganesalingam%20and%20Gowers%20on%20automated%20theorem-proving&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGanesalingam%20and%20Gowers%20on%20automated%20theorem-proving%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4K8sTathEnEnPsHMQ%2Fganesalingam-and-gowers-on-automated-theorem-proving%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ganesalingam%20and%20Gowers%20on%20automated%20theorem-proving%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4K8sTathEnEnPsHMQ%2Fganesalingam-and-gowers-on-automated-theorem-proving", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4K8sTathEnEnPsHMQ%2Fganesalingam-and-gowers-on-automated-theorem-proving", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 220, "htmlBody": "<p>Ganesalingam and Gowers recently released an <a href=\"http://arxiv.org/abs/1309.4501\">ArXiV preprint</a> of their article \"A fully automatic problem solver with human-style output\" about a theorem-proving program they have written that generates <em>human-readable output</em>. Gowers had blogged about the program <a href=\"http://gowers.wordpress.com/2013/04/14/answers-results-of-polls-and-a-brief-description-of-the-program/\">in April 2013</a>, after soliciting responses from readers regarding answers generated by the program and by humans (without explicitly revealing that one of the answerers was a machine) <a href=\"http://gowers.wordpress.com/2013/03/25/an-experiment-concerning-mathematical-writing/\">in March 2013</a>.</p>\n<p>There do exist theorem-proving systems such as <a href=\"http://coq.inria.fr/\">Coq</a>. In fact, it was announced about a year ago that the proof of the Feit-Thompson odd-order theorem, which originally appeared in a 255-page journal article in 1963, had been completely <a href=\"http://www.msr-inria.fr/news/feit-thomson-proved-in-coq/\">verified using Coq</a>. But the Gowers-Ganesalingam program differs in that the output of this program is a human-style proof (for better or worse) whereas Coq uses a formal language syntax (see <a href=\"http://ssr2.msr-inria.inria.fr/~jenkins/current/BGsection7.html#normed_constrained_meet_trans\">here</a> for a small part of the proof of the Feit-Thompson theorem using Coq).</p>\n<p>The ArXiV preprint contains details of the program and how the authors believe it improves on existing programs. One of the ways they believe they are better than past provers is their ability to more explicitly code for picking promising lines of reasoning.</p>\n<p>I'd appreciate comments from LessWrong readers about the specifics of the Ganesalingam-Gowers program, and any broader ramifications of their ability to create this program for predictions related to artificial intelligence.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4K8sTathEnEnPsHMQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 19, "extendedScore": null, "score": 1.3671197875978366e-06, "legacy": true, "legacyId": "24341", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-05T00:44:57.951Z", "modifiedAt": null, "url": null, "title": "Choosing universities based on their major-specific strengths for undergraduate education", "slug": "choosing-universities-based-on-their-major-specific", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:38.421Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VipulNaik", "createdAt": "2013-09-02T18:51:08.862Z", "isAdmin": false, "displayName": "VipulNaik"}, "userId": "t3pZcNZXqhaM5avBE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yRRprGM33qu8Jqmxj/choosing-universities-based-on-their-major-specific", "pageUrlRelative": "/posts/yRRprGM33qu8Jqmxj/choosing-universities-based-on-their-major-specific", "linkUrl": "https://www.lesswrong.com/posts/yRRprGM33qu8Jqmxj/choosing-universities-based-on-their-major-specific", "postedAtFormatted": "Saturday, October 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Choosing%20universities%20based%20on%20their%20major-specific%20strengths%20for%20undergraduate%20education&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChoosing%20universities%20based%20on%20their%20major-specific%20strengths%20for%20undergraduate%20education%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyRRprGM33qu8Jqmxj%2Fchoosing-universities-based-on-their-major-specific%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Choosing%20universities%20based%20on%20their%20major-specific%20strengths%20for%20undergraduate%20education%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyRRprGM33qu8Jqmxj%2Fchoosing-universities-based-on-their-major-specific", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyRRprGM33qu8Jqmxj%2Fchoosing-universities-based-on-their-major-specific", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 472, "htmlBody": "<p>Prospective undergraduates generally choose universities based on their <em>overall</em> reputation along dimensions such as academics, grading standards, social life, cost, peer group, etc. In the United States, there's a small list of top universities that appear across different rankings. Typical, for instance, is the US News ranking, one for <a href=\"http://colleges.usnews.rankingsandreviews.com/best-colleges/rankings/national-universities\">national universities</a> and one for <a href=\"http://colleges.usnews.rankingsandreviews.com/best-colleges/rankings/national-liberal-arts-colleges\">national liberal arts colleges</a>.</p>\n<p>A strong overall program is most relevant to undergraduates who care more about their overall educational experience, and/or are undecided about their major. For people who are keen on a specific major, however, the overall ranking may not be that helpful. Rather, the quality of the department that they are majoring with may matter more. For people who want to use their undergraduate years to accumulate domain-specific research experience, , and the opportunities for undergraduate research, whether it's supervised reading or lab work, may also matter.</p>\n<p>My questions:</p>\n<ul>\n</ul>\n<ol>\n<li>In general, are liberal arts colleges less likely to be good than research institutions for students who want research experience while they are undergraduates?</li>\n<li>Do certain kinds of universities tend to be better at encouraging independent study, lab research, or other forms of relevant academic and work experience, after controlling for overall ranking? <br /> \n<ul>\n<li>Alex K. Chen <a href=\"https://www.quora.com/Reviews-of-University-of-Washington\">writes in his review of the University of Washington</a> that the University offers excellent resources for research in the sciences, on account of the sheer size of its research programs, particularly its interdisciplinary programs.</li>\n<li>I've been told by some people that the Massachussetts Institute of Technology does a good job of encouraging faculty-guided reading and research, but this may be specific to the mathematics department.</li>\n</ul>\n</li>\n<li>What are some examples of, or links to lists of, colleges and universities that have extremely strong faculty, and correspondingly strong opportunities for learning inside and outside the classroom, in a particular department, relative to their formal ranking? Some possible examples:<br /> \n<ul>\n<li>A number of people I know directly and indirectly have been involved with the <a href=\"http://economics.gmu.edu/\">George Mason University Department of Economics</a>, and I'm very favorably impressed by the quality of GMU bloggers, including <a href=\"http://econfaculty.gmu.edu/bcaplan\">Bryan Caplan</a> at <a href=\"http://econlog.econlib.org\">EconLog</a>, Tyler Cowen and Alex Tabarrok at <a href=\"http://www.marginalrevolution.com\">Marginal Revolution</a>, Donald Boudreaux and Russ Roberts (formerly at GMU) at <a href=\"http://www.cafehayek.com\">Cafe Hayek</a>, and some at the law school such as <a href=\"/mason.gmu.edu/~isomin\">Ilya Somin</a>. The department also includes Vernon Smith, Walter Williams, and other notable individuals, and was ranked the top university in the southern US for economics by <a href=\"http://www.tandfonline.com/doi/abs/10.1080/13504850150204174#preview\">Mixon and Upadhyaya (2001)</a>.</li>\n<li>A friend of mine, Jonah Sinick, claimed to me in private conversation that Cornell University is unusually good for undergraduate math majors interested in geometry/topology, relative to its overall ranking. Cornell is home to <a href=\"http://www.math.cornell.edu/~hatcher/\">Allen Hatcher</a>, <a href=\"http://www.math.cornell.edu/~vogtmann/\">Karen Vogtmann</a>, and <a href=\"http://www.math.cornell.edu/~kbrown/\">Ken Brown</a>.</li>\n</ul>\n</li>\n</ol>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yRRprGM33qu8Jqmxj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.3671565205644374e-06, "legacy": true, "legacyId": "24342", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-05T02:56:46.150Z", "modifiedAt": null, "url": null, "title": "The Cause of Time", "slug": "the-cause-of-time", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:39.417Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "johnswentworth", "createdAt": "2011-02-19T16:54:09.598Z", "isAdmin": false, "displayName": "johnswentworth"}, "userId": "MEu8MdhruX5jfGsFQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Z7LJudF8rmHBYt5q4/the-cause-of-time", "pageUrlRelative": "/posts/Z7LJudF8rmHBYt5q4/the-cause-of-time", "linkUrl": "https://www.lesswrong.com/posts/Z7LJudF8rmHBYt5q4/the-cause-of-time", "postedAtFormatted": "Saturday, October 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Cause%20of%20Time&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Cause%20of%20Time%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ7LJudF8rmHBYt5q4%2Fthe-cause-of-time%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Cause%20of%20Time%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ7LJudF8rmHBYt5q4%2Fthe-cause-of-time", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ7LJudF8rmHBYt5q4%2Fthe-cause-of-time", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 618, "htmlBody": "<p>In a recent <a href=\"/lw/irr/the_best_15_words/9u9j\">comment</a>, I suggested that correlations between seemingly unrelated periodic time series share a common cause: time. However, the math disagrees... and suggests a surprising alternative.</p>\n<p>Imagine that we took measurements from a thermometer on my window and a ridiculously large tuning fork over several years. The first set of data is temperature T over time t, so it looks like a list of data points [(t<sub>0</sub>, T<sub>0</sub>), (t<sub>1</sub>, T<sub>1</sub>), ...]. The second set of data is mechanical strain e in the tuning fork over time, so it looks like a list of data points [(t<sub>0</sub>, e<sub>0)</sub>, (t<sub>1</sub>, e<sub>1</sub>), ...]. We line up the temperature and strain data according to time, yielding [(T<sub>0</sub>, e<sub>0</sub>), (T<sub>1</sub>, e<sub>1</sub>), ...] and find a significant correlation between the two, since they happen to have similar periodicity.</p>\n<p>Recalling Judea Pearl, we suggest that there is almost certainly some causal relationship between the temperature outside the window and the strain in the ridiculously large tuning fork. Common sense suggests that neither causes the other, so perhaps they have some common cause? The only other variable in the problem is time, so perhaps time is the common cause. This sort of makes sense, since changes in time intuitively seem to cause the changes in temperature and strain.</p>\n<p>Let's check that intuition with some math. First, imagine that we ignore the time data. Now we just have a bunch of temperature data points [T<sub>0</sub>, T<sub>1</sub>, ...] and strain data points [e<sub>0</sub>, e<sub>1</sub>, ...]. In fact, in order to truly ignore time data, we cannot even order the points according to time! But that means that we no longer have any way to line up the points T<sub>0</sub> with e<sub>0</sub>, T<sub>1</sub>&nbsp;with e<sub>1</sub>, etc. Without any way to match up temperature points to corresponding strain points, the temperature and strain data are randomly ordered, and the correlation disappears!</p>\n<p>We have just performed a d-separation. When time t was known (i.e., controlled for), the variables T and e were correlated. But when t was unknown, the variables were uncorrelated. Now, let's wave our hands a little and equate correlation with dependence. If time were a common cause of temperature and strain, then we should see that T and e are correlated without knowledge of time, but the correlation disappears when controlling for time. However, we see exactly the <em>opposite</em>&nbsp;structure: controlling for t <em>induces</em> the correlation. This pattern is called a \"collider\", and it implies that <em>time is a common effect</em> of temperature and strain. Rather than time causing the oscillations in our time series, the oscillations in our time series cause time.</p>\n<p>Whoa. Now that the math has given us the answer, let's step back and try to make sense of it. Imagine that everything in the universe stopped moving for some time, and then went back to moving exactly as before. How could we measure how much time passed while the universe was stopped? We couldn't. For all practical purposes, if nothing changes, then time has stopped. Time, then, is an effect of motion, not vice versa. This is an old idea from philosophy/physics (I think I originally read it in one of Stephen Hawking's books). We've just rederived it.</p>\n<p>But we may still wonder: what caused the correlation between temperature and strain? A common effect cannot cause a correlation, so where did it come from? The answer is that there was never any correlation between temperature and strain to begin with. Given just the temperature and strain data, with no information about time (e.g. no ordering or correspondence between points), there was no correlation. The correlation was induced by controlling for time. So the correlation is only logical; there is no physical cause relating the two, at least within our model.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Z7LJudF8rmHBYt5q4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 0, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "24343", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-05T16:47:40.041Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels monthly meetup: games!", "slug": "meetup-brussels-monthly-meetup-games", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roxolan", "createdAt": "2011-10-23T19:06:17.298Z", "isAdmin": false, "displayName": "Roxolan"}, "userId": "jXG7tMhkQMNpCCXPN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AC9MbowdwmoEh4voE/meetup-brussels-monthly-meetup-games", "pageUrlRelative": "/posts/AC9MbowdwmoEh4voE/meetup-brussels-monthly-meetup-games", "linkUrl": "https://www.lesswrong.com/posts/AC9MbowdwmoEh4voE/meetup-brussels-monthly-meetup-games", "postedAtFormatted": "Saturday, October 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20monthly%20meetup%3A%20games!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20monthly%20meetup%3A%20games!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAC9MbowdwmoEh4voE%2Fmeetup-brussels-monthly-meetup-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20monthly%20meetup%3A%20games!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAC9MbowdwmoEh4voE%2Fmeetup-brussels-monthly-meetup-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAC9MbowdwmoEh4voE%2Fmeetup-brussels-monthly-meetup-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 172, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rw'>Brussels monthly meetup: games!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 October 2013 01:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Second Saturday of the month is coming up. As usual, we will meet at 1 pm at La Fleur en papier dor\u00e9, close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>There's an interest in games and game design in the group, so I thought we might make this the official topic for this meetup. Have you read anything about it lately? Is there an interesting game you think we should try out? Any game idea you've been sitting on?</p>\n\n<p>(I don't expect we'll only talk about games though.)</p>\n\n<p>If you are coming for the first time, please consider filling out this one minute form, to share your contact information:\nhttps://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform</p>\n\n<p>The Brussels meetups use a Google Group:\nhttps://groups.google.com/forum/#!forum/lesswrong-brussels</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rw'>Brussels monthly meetup: games!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AC9MbowdwmoEh4voE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3680364342870896e-06, "legacy": true, "legacyId": "24346", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_monthly_meetup__games_\">Discussion article for the meetup : <a href=\"/meetups/rw\">Brussels monthly meetup: games!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 October 2013 01:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Second Saturday of the month is coming up. As usual, we will meet at 1 pm at La Fleur en papier dor\u00e9, close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. The meeting will be in English to facilitate both French and Dutch speaking members.</p>\n\n<p>There's an interest in games and game design in the group, so I thought we might make this the official topic for this meetup. Have you read anything about it lately? Is there an interesting game you think we should try out? Any game idea you've been sitting on?</p>\n\n<p>(I don't expect we'll only talk about games though.)</p>\n\n<p>If you are coming for the first time, please consider filling out this one minute form, to share your contact information:\nhttps://docs.google.com/forms/d/1qSvI1NWkFSsfIJhUMORb_Wd8fdJTVPhdw49grDQwRTI/viewform</p>\n\n<p>The Brussels meetups use a Google Group:\nhttps://groups.google.com/forum/#!forum/lesswrong-brussels</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels_monthly_meetup__games_1\">Discussion article for the meetup : <a href=\"/meetups/rw\">Brussels monthly meetup: games!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels monthly meetup: games!", "anchor": "Discussion_article_for_the_meetup___Brussels_monthly_meetup__games_", "level": 1}, {"title": "Discussion article for the meetup : Brussels monthly meetup: games!", "anchor": "Discussion_article_for_the_meetup___Brussels_monthly_meetup__games_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-05T21:02:36.127Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes October 2013", "slug": "rationality-quotes-october-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:04.601Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "hRzKAvARt4HspRBWA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Mpt32hB2JRREcsYFQ/rationality-quotes-october-2013", "pageUrlRelative": "/posts/Mpt32hB2JRREcsYFQ/rationality-quotes-october-2013", "linkUrl": "https://www.lesswrong.com/posts/Mpt32hB2JRREcsYFQ/rationality-quotes-october-2013", "postedAtFormatted": "Saturday, October 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20October%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20October%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMpt32hB2JRREcsYFQ%2Frationality-quotes-october-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20October%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMpt32hB2JRREcsYFQ%2Frationality-quotes-october-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMpt32hB2JRREcsYFQ%2Frationality-quotes-october-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>\n<ul>\n<li>Please post all quotes separately, so that they can be upvoted or  downvoted separately. (If they are strongly related, reply to your own  comments. If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or  Robin Hanson. If you'd like to revive an old quote from one of those  sources, please do so <a href=\"/r/discussion/lw/i6h/rationality_quotes_from_people_associated_with/\">here</a>.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Mpt32hB2JRREcsYFQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 1.3682696136547892e-06, "legacy": true, "legacyId": "24325", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 315, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iWTZj26MfR8e8b9nm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-05T21:56:05.012Z", "modifiedAt": null, "url": null, "title": "Megameetup on December 13-15th, NYC", "slug": "megameetup-on-december-13-15th-nyc", "viewCount": null, "lastCommentedAt": "2013-12-12T08:17:08.747Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Cb4LcmcHDSuRrowae/megameetup-on-december-13-15th-nyc", "pageUrlRelative": "/posts/Cb4LcmcHDSuRrowae/megameetup-on-december-13-15th-nyc", "linkUrl": "https://www.lesswrong.com/posts/Cb4LcmcHDSuRrowae/megameetup-on-december-13-15th-nyc", "postedAtFormatted": "Saturday, October 5th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Megameetup%20on%20December%2013-15th%2C%20NYC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMegameetup%20on%20December%2013-15th%2C%20NYC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCb4LcmcHDSuRrowae%2Fmegameetup-on-december-13-15th-nyc%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Megameetup%20on%20December%2013-15th%2C%20NYC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCb4LcmcHDSuRrowae%2Fmegameetup-on-december-13-15th-nyc", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCb4LcmcHDSuRrowae%2Fmegameetup-on-december-13-15th-nyc", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 268, "htmlBody": "<p>This winter, we'll be hosting a megameetup on December 13th-15th. This is the weekend of the&nbsp;<a href=\"http://www.kickstarter.com/projects/244974495/brighter-than-today-a-secular-solstice\">Winter Solstice</a>, a big event we're putting together the rationality, humanist and transhumanist communities of the area. (The Solstice celebration is on Saturday evening - if you'd like to attend, you should check out the kickstarter and back it.&nbsp;<a href=\"http://www.kickstarter.com/projects/244974495/brighter-than-today-a-secular-solstice\">Seating is limited and tickets are sold in advance are $25</a>).</p>\n<p><a href=\"http://www.kickstarter.com/projects/244974495/brighter-than-today-a-secular-solstice\"><img src=\"https://dl.dropboxusercontent.com/u/2000477/brighterThanToday.jpg\" alt=\"\" /></a></p>\n<p>Eight members of the New York Rationality community recently moved into a gorgeous house in Brooklyn. It's got 5500 square feet. The first floor, approximately 1800 square feet, has four areas that with sliding doors that can either be treated as a single, huge meetup space, or broken into smaller areas.</p>\n<p>Also it has secret doors.</p>\n<p>We have named it \"Highgarden.\"</p>\n<p><img src=\"http://img.streeteasy.com/nyc/image/19/48500519.jpg\" alt=\"\" width=\"325\" height=\"250\" />&nbsp; &nbsp;&nbsp;<img src=\"http://img.streeteasy.com/nyc/image/79/53693179.jpg\" alt=\"\" width=\"325\" height=\"250\" /></p>\n<p>We're really looking forward to turning this into a genuine rationality community center. We have self improvement meetups every other Sunday (the next one is on the October 13th), and have other one-off events in the works.</p>\n<p>Friday night and Saturday afternoon will primarily casual hangouts, before most of us head over to the Solstice event. On Sunday there will be a presentation on the current state of Effective Altruism. We're aiming to have other presentations as well but details are not finalized yet.</p>\n<p>We have a large (but not unlimited) array of crash space, so if you'd like to spend Friday and/or Saturday night at Highgarden, you should let us know in advance.<br /><br />Looking forward to seeing many of you there!<br /><br /><strong><span style=\"text-decoration: underline;\"><em>When + Where</em></span></strong></p>\n<p><em>Highgarden House -851 Park Place Brooklyn NY, 11261</em></p>\n<p><em>Friday, December 13th, 7:00 PM - Saturday, December 15th, 7:00 PM</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Cb4LcmcHDSuRrowae", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 18, "extendedScore": null, "score": 6.2e-05, "legacy": true, "legacyId": "24345", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2013-10-05T21:56:05.012Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-06T01:39:06.416Z", "modifiedAt": null, "url": null, "title": "Meetup : Urbana Champaign: Metaethics, Normative Ethics, Applied Ethics", "slug": "meetup-urbana-champaign-metaethics-normative-ethics-applied", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iDerY9KoP6NsoJJrM/meetup-urbana-champaign-metaethics-normative-ethics-applied", "pageUrlRelative": "/posts/iDerY9KoP6NsoJJrM/meetup-urbana-champaign-metaethics-normative-ethics-applied", "linkUrl": "https://www.lesswrong.com/posts/iDerY9KoP6NsoJJrM/meetup-urbana-champaign-metaethics-normative-ethics-applied", "postedAtFormatted": "Sunday, October 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Urbana%20Champaign%3A%20Metaethics%2C%20Normative%20Ethics%2C%20Applied%20Ethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Urbana%20Champaign%3A%20Metaethics%2C%20Normative%20Ethics%2C%20Applied%20Ethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiDerY9KoP6NsoJJrM%2Fmeetup-urbana-champaign-metaethics-normative-ethics-applied%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Urbana%20Champaign%3A%20Metaethics%2C%20Normative%20Ethics%2C%20Applied%20Ethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiDerY9KoP6NsoJJrM%2Fmeetup-urbana-champaign-metaethics-normative-ethics-applied", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiDerY9KoP6NsoJJrM%2Fmeetup-urbana-champaign-metaethics-normative-ethics-applied", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rx'>Urbana Champaign: Metaethics, Normative Ethics, Applied Ethics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 October 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1308 W Green St Urbana, IL 61801 \u200e 173 ft SE</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be held at the same benches as the L\u00f6bstacle meetup, here: \nhttps://maps.google.com/maps?hl=en&amp;q=40.111220,+-88.227363</p>\n\n<p>Topics we might discuss: \nWhat is the correct metaethics? (Or what is a useful metaethics)? Moral realism, moral antirealism, Eliezer Yudkowsky's metaethics. \nPopulation ethics: the repugnant conclusion, average vs aggregate utilitarianism, spheres of infinite utility \nHow to do good in the world: effective altruism, how can you pick the best charity?</p>\n\n<p>If we get bored of that, I will have games as a backup.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rx'>Urbana Champaign: Metaethics, Normative Ethics, Applied Ethics</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iDerY9KoP6NsoJJrM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.3685226018560092e-06, "legacy": true, "legacyId": "24347", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Metaethics__Normative_Ethics__Applied_Ethics\">Discussion article for the meetup : <a href=\"/meetups/rx\">Urbana Champaign: Metaethics, Normative Ethics, Applied Ethics</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 October 2013 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1308 W Green St Urbana, IL 61801 \u200e 173 ft SE</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be held at the same benches as the L\u00f6bstacle meetup, here: \nhttps://maps.google.com/maps?hl=en&amp;q=40.111220,+-88.227363</p>\n\n<p>Topics we might discuss: \nWhat is the correct metaethics? (Or what is a useful metaethics)? Moral realism, moral antirealism, Eliezer Yudkowsky's metaethics. \nPopulation ethics: the repugnant conclusion, average vs aggregate utilitarianism, spheres of infinite utility \nHow to do good in the world: effective altruism, how can you pick the best charity?</p>\n\n<p>If we get bored of that, I will have games as a backup.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Urbana_Champaign__Metaethics__Normative_Ethics__Applied_Ethics1\">Discussion article for the meetup : <a href=\"/meetups/rx\">Urbana Champaign: Metaethics, Normative Ethics, Applied Ethics</a></h2>", "sections": [{"title": "Discussion article for the meetup : Urbana Champaign: Metaethics, Normative Ethics, Applied Ethics", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Metaethics__Normative_Ethics__Applied_Ethics", "level": 1}, {"title": "Discussion article for the meetup : Urbana Champaign: Metaethics, Normative Ethics, Applied Ethics", "anchor": "Discussion_article_for_the_meetup___Urbana_Champaign__Metaethics__Normative_Ethics__Applied_Ethics1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-06T19:09:51.044Z", "modifiedAt": null, "url": null, "title": "Your role models are watching you...", "slug": "your-role-models-are-watching-you", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Voltairina", "createdAt": "2012-02-24T04:00:28.314Z", "isAdmin": false, "displayName": "Voltairina"}, "userId": "a6hK33SK4uawjaL9h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rmuWHbkMy7TTcQH6y/your-role-models-are-watching-you", "pageUrlRelative": "/posts/rmuWHbkMy7TTcQH6y/your-role-models-are-watching-you", "linkUrl": "https://www.lesswrong.com/posts/rmuWHbkMy7TTcQH6y/your-role-models-are-watching-you", "postedAtFormatted": "Sunday, October 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Your%20role%20models%20are%20watching%20you...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYour%20role%20models%20are%20watching%20you...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrmuWHbkMy7TTcQH6y%2Fyour-role-models-are-watching-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Your%20role%20models%20are%20watching%20you...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrmuWHbkMy7TTcQH6y%2Fyour-role-models-are-watching-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrmuWHbkMy7TTcQH6y%2Fyour-role-models-are-watching-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 122, "htmlBody": "<p>I was just thinking about the possible benefits of visualizing your role models watching you. There's apparently some good research about just putting up posters of human eyes having a positive influence on behavior. But when I think about my behavior around people, I notice that I feel pressured to act in a way that I think <em>they would like</em>, not necessarily the best way I could imagine.&nbsp; Maybe by putting up their pictures in your room or at your desk, you could benefit from the values of your role models by imagining them actually being able to see you. For instance, if you wanted to learn Martial Arts, maybe putting up a poster of Bruce Lee could remind you to train.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rmuWHbkMy7TTcQH6y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "24348", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-06T19:25:09.321Z", "modifiedAt": null, "url": null, "title": "Under the eyes of your betters", "slug": "under-the-eyes-of-your-betters", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:05.489Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Voltairina", "createdAt": "2012-02-24T04:00:28.314Z", "isAdmin": false, "displayName": "Voltairina"}, "userId": "a6hK33SK4uawjaL9h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/asLk4eBrL5Weq3PLc/under-the-eyes-of-your-betters", "pageUrlRelative": "/posts/asLk4eBrL5Weq3PLc/under-the-eyes-of-your-betters", "linkUrl": "https://www.lesswrong.com/posts/asLk4eBrL5Weq3PLc/under-the-eyes-of-your-betters", "postedAtFormatted": "Sunday, October 6th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Under%20the%20eyes%20of%20your%20betters&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUnder%20the%20eyes%20of%20your%20betters%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FasLk4eBrL5Weq3PLc%2Funder-the-eyes-of-your-betters%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Under%20the%20eyes%20of%20your%20betters%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FasLk4eBrL5Weq3PLc%2Funder-the-eyes-of-your-betters", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FasLk4eBrL5Weq3PLc%2Funder-the-eyes-of-your-betters", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>There is some <a href=\"http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0051738\">research</a> that claims the feeling of being watched motivates you to engage in more prosocial behavior. Our gaze recognition ability is apparently hard for us to suppress even when we try to intentionally. When I think about times when I've been around friends, however, I usually feel the pressure to act in a way I feel will impress that specific friend, which is not necessarily pro-social. I imagine the disembodied eyes are not registered as \"friends\" and heighten our anxiety about who might be looking at us. I wonder whether having pictures of your role models in your workspace and people in your life who encourage you to do well when you talk to them might encourage you to engage in behavior more in line with those virtues you'd like to cultivate. Especially if you intentionally go for pictures where the people in them are looking at the camera directly.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "asLk4eBrL5Weq3PLc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "24349", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-07T03:03:20.587Z", "modifiedAt": null, "url": null, "title": "Superintelligence fiction - \"Understand\", by Ted Chiang", "slug": "superintelligence-fiction-understand-by-ted-chiang", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:58.324Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "D_Alex", "createdAt": "2009-07-17T08:21:38.505Z", "isAdmin": false, "displayName": "D_Alex"}, "userId": "Sriopfkdwx2qJBx4G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DXuxG3cyAD2rDq56C/superintelligence-fiction-understand-by-ted-chiang", "pageUrlRelative": "/posts/DXuxG3cyAD2rDq56C/superintelligence-fiction-understand-by-ted-chiang", "linkUrl": "https://www.lesswrong.com/posts/DXuxG3cyAD2rDq56C/superintelligence-fiction-understand-by-ted-chiang", "postedAtFormatted": "Monday, October 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Superintelligence%20fiction%20-%20%22Understand%22%2C%20by%20Ted%20Chiang&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuperintelligence%20fiction%20-%20%22Understand%22%2C%20by%20Ted%20Chiang%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXuxG3cyAD2rDq56C%2Fsuperintelligence-fiction-understand-by-ted-chiang%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Superintelligence%20fiction%20-%20%22Understand%22%2C%20by%20Ted%20Chiang%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXuxG3cyAD2rDq56C%2Fsuperintelligence-fiction-understand-by-ted-chiang", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDXuxG3cyAD2rDq56C%2Fsuperintelligence-fiction-understand-by-ted-chiang", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<p><a href=\"http://www.infinityplus.co.uk/stories/under.htm?2\">http://www.infinityplus.co.uk/stories/under.htm?2</a>. 15-30 min read time, rated \"pretty good\" by me.</p>\r\n<p>&nbsp;</p>\r\n<p>There are a couple of interesting features of this story that I would like to discuss - but I don't want to introduce any spoilers, so I'll just leave this here for now.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DXuxG3cyAD2rDq56C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 15, "extendedScore": null, "score": 1.3699186893765817e-06, "legacy": true, "legacyId": "24350", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-07T11:45:57.286Z", "modifiedAt": null, "url": null, "title": "Meetup : London social", "slug": "meetup-london-social", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:39.494Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WhgnkSJXB8FPs5xZT/meetup-london-social", "pageUrlRelative": "/posts/WhgnkSJXB8FPs5xZT/meetup-london-social", "linkUrl": "https://www.lesswrong.com/posts/WhgnkSJXB8FPs5xZT/meetup-london-social", "postedAtFormatted": "Monday, October 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20London%20social&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20London%20social%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWhgnkSJXB8FPs5xZT%2Fmeetup-london-social%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20London%20social%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWhgnkSJXB8FPs5xZT%2Fmeetup-london-social", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWhgnkSJXB8FPs5xZT%2Fmeetup-london-social", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 45, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ry'>London social</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 October 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shakespeare's Head, 64-68 Kingsway, London WC2B 6BG </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come and hang out with the lovely people of London Less Wrong. No agenda, just whatever's interesting!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ry'>London social</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WhgnkSJXB8FPs5xZT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.3703979431663307e-06, "legacy": true, "legacyId": "24351", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___London_social\">Discussion article for the meetup : <a href=\"/meetups/ry\">London social</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 October 2013 02:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shakespeare's Head, 64-68 Kingsway, London WC2B 6BG </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come and hang out with the lovely people of London Less Wrong. No agenda, just whatever's interesting!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___London_social1\">Discussion article for the meetup : <a href=\"/meetups/ry\">London social</a></h2>", "sections": [{"title": "Discussion article for the meetup : London social", "anchor": "Discussion_article_for_the_meetup___London_social", "level": 1}, {"title": "Discussion article for the meetup : London social", "anchor": "Discussion_article_for_the_meetup___London_social1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-07T13:00:44.353Z", "modifiedAt": null, "url": null, "title": "Requesting clarification- On the Metaethics", "slug": "requesting-clarification-on-the-metaethics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:02.777Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Carinthium", "createdAt": "2010-11-10T22:28:58.091Z", "isAdmin": false, "displayName": "Carinthium"}, "userId": "DL8CRWfXPCHYqQsv4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QPdsGKyZ2yLYZX6uY/requesting-clarification-on-the-metaethics", "pageUrlRelative": "/posts/QPdsGKyZ2yLYZX6uY/requesting-clarification-on-the-metaethics", "linkUrl": "https://www.lesswrong.com/posts/QPdsGKyZ2yLYZX6uY/requesting-clarification-on-the-metaethics", "postedAtFormatted": "Monday, October 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Requesting%20clarification-%20On%20the%20Metaethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARequesting%20clarification-%20On%20the%20Metaethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQPdsGKyZ2yLYZX6uY%2Frequesting-clarification-on-the-metaethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Requesting%20clarification-%20On%20the%20Metaethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQPdsGKyZ2yLYZX6uY%2Frequesting-clarification-on-the-metaethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQPdsGKyZ2yLYZX6uY%2Frequesting-clarification-on-the-metaethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<p>My apologies if this doesn't deserve a Discussion post, but if this hasn't been addresed anywhere than it's clearly an important issue.</p>\r\n<p>There have been many defences of consequentialism against deontology, including quite a few on this site. What I haven't seen, however, is any demonstration of how deontology is incompatible with the ideas in Elizier's Metaethics sequence- as far as I can tell, a deontologist could agree with just about everything in the Sequences.</p>\r\n<p>Said deontologist would argue that, to the extent a human universial morality can exist through generalised moral instincts, said instincts tend to be deontological (as supported through scientific studies- a study of the trolley dilemna v.s the 'fat man' variant showed that people would divert the trolley but not push the fat man). This would be their argument against the consequentialist, who they could accuse of wanting a consequentialist system and ignoring the moral instincts at the basis of their own speculations.</p>\r\n<p>I'm not completely sure about this, but figure it an important enough misunderstanding if I indeed misunderstood to deserve clearing up.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QPdsGKyZ2yLYZX6uY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 3, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "24352", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 75, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-07T14:52:51.109Z", "modifiedAt": null, "url": null, "title": "Open Thread, October 7 - October 12, 2013", "slug": "open-thread-october-7-october-12-2013", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:00.963Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thomas", "createdAt": "2009-03-02T17:47:09.607Z", "isAdmin": false, "displayName": "Thomas"}, "userId": "GrAKeuxT4e9AKyHdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/f8Wf8PtJMqQDpPoYD/open-thread-october-7-october-12-2013", "pageUrlRelative": "/posts/f8Wf8PtJMqQDpPoYD/open-thread-october-7-october-12-2013", "linkUrl": "https://www.lesswrong.com/posts/f8Wf8PtJMqQDpPoYD/open-thread-october-7-october-12-2013", "postedAtFormatted": "Monday, October 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20October%207%20-%20October%2012%2C%202013&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20October%207%20-%20October%2012%2C%202013%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff8Wf8PtJMqQDpPoYD%2Fopen-thread-october-7-october-12-2013%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20October%207%20-%20October%2012%2C%202013%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff8Wf8PtJMqQDpPoYD%2Fopen-thread-october-7-october-12-2013", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff8Wf8PtJMqQDpPoYD%2Fopen-thread-october-7-october-12-2013", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 12.666666984558105px; text-align: justify;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "f8Wf8PtJMqQDpPoYD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 1.3705694064880733e-06, "legacy": true, "legacyId": "24353", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 318, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-07T16:13:57.950Z", "modifiedAt": null, "url": null, "title": "Rewiring my Brain: (gentle) Help Appreciated", "slug": "rewiring-my-brain-gentle-help-appreciated", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:59.090Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JMiller", "createdAt": "2012-11-15T16:08:50.381Z", "isAdmin": false, "displayName": "JMiller"}, "userId": "YePJv5oBk8LKnWogz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ik4REw8i66KjEiWYj/rewiring-my-brain-gentle-help-appreciated", "pageUrlRelative": "/posts/ik4REw8i66KjEiWYj/rewiring-my-brain-gentle-help-appreciated", "linkUrl": "https://www.lesswrong.com/posts/ik4REw8i66KjEiWYj/rewiring-my-brain-gentle-help-appreciated", "postedAtFormatted": "Monday, October 7th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rewiring%20my%20Brain%3A%20(gentle)%20Help%20Appreciated&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARewiring%20my%20Brain%3A%20(gentle)%20Help%20Appreciated%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fik4REw8i66KjEiWYj%2Frewiring-my-brain-gentle-help-appreciated%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rewiring%20my%20Brain%3A%20(gentle)%20Help%20Appreciated%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fik4REw8i66KjEiWYj%2Frewiring-my-brain-gentle-help-appreciated", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fik4REw8i66KjEiWYj%2Frewiring-my-brain-gentle-help-appreciated", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 150, "htmlBody": "<p>Hi everyone,</p>\n<p>I am graduating as a philosophy student shortly, and want to pursue computer science / programming/ something-of-that-sort.</p>\n<p>I am currently taking some basic math (calculus) and physics (mechanics) courses in order to obtain pre-requesits, and to develop a basic framework. My problem is that I can grasp concepts and ideas, but when it comes to solving specific problems with actual numbers, I seem to shut down. Specifically, it takes me much more time (read \"hours\") to solve problems that ought to take 10 minutes. This is a particularly bad thing on tests and exams.</p>\n<p>I believe that the difficulty I am having stems from so little exposure to symbolic reasoning in the past 5-6 years. I am looking for resources, techniques and advice to \"turbocharge\" (to use CFAR terminology) my ability to absorb and deeply comprehend technical material, so that solving problems becomes second nature.</p>\n<p>Thank you so much for your time,</p>\n<p>Jeremy</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ik4REw8i66KjEiWYj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 1.3706438339555386e-06, "legacy": true, "legacyId": "24355", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-08T05:16:14.292Z", "modifiedAt": null, "url": null, "title": "CFAR is looking for a videographer for next Wednesday", "slug": "cfar-is-looking-for-a-videographer-for-next-wednesday", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:43.978Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mZH5z2rmo6Tbv9fL6/cfar-is-looking-for-a-videographer-for-next-wednesday", "pageUrlRelative": "/posts/mZH5z2rmo6Tbv9fL6/cfar-is-looking-for-a-videographer-for-next-wednesday", "linkUrl": "https://www.lesswrong.com/posts/mZH5z2rmo6Tbv9fL6/cfar-is-looking-for-a-videographer-for-next-wednesday", "postedAtFormatted": "Tuesday, October 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20CFAR%20is%20looking%20for%20a%20videographer%20for%20next%20Wednesday&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACFAR%20is%20looking%20for%20a%20videographer%20for%20next%20Wednesday%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmZH5z2rmo6Tbv9fL6%2Fcfar-is-looking-for-a-videographer-for-next-wednesday%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=CFAR%20is%20looking%20for%20a%20videographer%20for%20next%20Wednesday%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmZH5z2rmo6Tbv9fL6%2Fcfar-is-looking-for-a-videographer-for-next-wednesday", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmZH5z2rmo6Tbv9fL6%2Fcfar-is-looking-for-a-videographer-for-next-wednesday", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p><span style=\"color: #222222; font-family: arial;\">Hi all, CFAR is looking for a videographer in the Bay Area to</span><strong style=\"color: #222222; font-family: arial;\">&nbsp;shoot and edit</strong><span style=\"color: #222222; font-family: arial;\">&nbsp;a 1-minute video introducing us. &nbsp;Do you know anyone?</span></p>\n<div style=\"color: #222222; font-family: arial;\">If so, please send an email to them and me (<a href=\"mailto:critch@rationality.org\">critch@rationality.org</a>) that introduces us! &nbsp;</div>\n<div style=\"color: #222222; font-family: arial;\"><br /></div>\n<div style=\"color: #222222; font-family: arial;\">We'll need to shoot the video on Wednesday, Oct 16, or possibly Thursday, Oct 17, and have it edited within about 24 hours.</div>\n<div style=\"color: #222222; font-family: arial;\"><br /></div>\n<div style=\"color: #222222; font-family: arial;\">Thanks for any help tracking someone down!</div>\n<div style=\"color: #222222; font-family: arial;\"><br /></div>\n<div style=\"color: #222222; font-family: arial;\">Sincerely,</div>\n<div style=\"color: #222222; font-family: arial;\"><br /></div>\n<div style=\"color: #222222; font-family: arial;\">--</div>\n<div style=\"color: #222222; font-family: arial;\">Critch</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mZH5z2rmo6Tbv9fL6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 1.3713619885547882e-06, "legacy": true, "legacyId": "24357", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-08T10:21:45.178Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong Israel Meetup (Tel Aviv): Dealing with Emotional Vampires", "slug": "meetup-less-wrong-israel-meetup-tel-aviv-dealing-with", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:01.006Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SoftFlare", "createdAt": "2012-11-09T00:22:21.187Z", "isAdmin": false, "displayName": "SoftFlare"}, "userId": "dSdSRiHQPaFuBXhG6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FEhLn7tyQnu3Dfe9C/meetup-less-wrong-israel-meetup-tel-aviv-dealing-with", "pageUrlRelative": "/posts/FEhLn7tyQnu3Dfe9C/meetup-less-wrong-israel-meetup-tel-aviv-dealing-with", "linkUrl": "https://www.lesswrong.com/posts/FEhLn7tyQnu3Dfe9C/meetup-less-wrong-israel-meetup-tel-aviv-dealing-with", "postedAtFormatted": "Tuesday, October 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%20Israel%20Meetup%20(Tel%20Aviv)%3A%20Dealing%20with%20Emotional%20Vampires&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%20Israel%20Meetup%20(Tel%20Aviv)%3A%20Dealing%20with%20Emotional%20Vampires%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFEhLn7tyQnu3Dfe9C%2Fmeetup-less-wrong-israel-meetup-tel-aviv-dealing-with%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%20Israel%20Meetup%20(Tel%20Aviv)%3A%20Dealing%20with%20Emotional%20Vampires%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFEhLn7tyQnu3Dfe9C%2Fmeetup-less-wrong-israel-meetup-tel-aviv-dealing-with", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFEhLn7tyQnu3Dfe9C%2Fmeetup-less-wrong-israel-meetup-tel-aviv-dealing-with", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 220, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/rz'>Less Wrong Israel Meetup (Tel Aviv): Dealing with Emotional Vampires</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 October 2013 08:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">7 Menachem Begin st., Ramat-Gan</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup on Thursday, October 17th at VisionMap's offices, Gibor Sport House, 15th floor, 7 Menachem Begin st., Ramat-Gan.</p>\n\n<p>Our program is:</p>\n\n<ul>\n<li><p>20:00-20:15: Assembly</p></li>\n<li><p>20:15-21:00: Main Talk</p></li>\n<li><p>21:00-22:00: Dinner &amp; Discussion</p></li>\n<li><p>22:00-23:00: Rump Session (minitalks)</p></li>\n<li><p>23:00-: End of official programming</p></li>\n</ul>\n\n<p>Main Talk: Mirrors and Sunlight: Dealing with Emotional Vampires / Guy Banay</p>\n\n<p>We will learn how to recognize the Dramatic Personality Disorders (antisocial, borderline, narcissistic and histrionic) and some strategies to minimize the damage they can do.</p>\n\n<p>Backup Talk: TBA</p>\n\n<p>Rump Session: each participant will give a 4-minute talk (+3 minute encore if we applaud hard enough). Giving a talk isn't mandatory, but it's highly recommended. Not confident that what you have to say is relevant to our interests? Unsure about your public speaking skills? Doesn't matter - in the rump session, anything goes.</p>\n\n<p>Feel free to contact me (Gal Hochberg) at hochbergg@gmail.com or at 0545330678 for any further information</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/rz'>Less Wrong Israel Meetup (Tel Aviv): Dealing with Emotional Vampires</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FEhLn7tyQnu3Dfe9C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.3716426430270827e-06, "legacy": true, "legacyId": "24358", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Israel_Meetup__Tel_Aviv___Dealing_with_Emotional_Vampires\">Discussion article for the meetup : <a href=\"/meetups/rz\">Less Wrong Israel Meetup (Tel Aviv): Dealing with Emotional Vampires</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 October 2013 08:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">7 Menachem Begin st., Ramat-Gan</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup on Thursday, October 17th at VisionMap's offices, Gibor Sport House, 15th floor, 7 Menachem Begin st., Ramat-Gan.</p>\n\n<p>Our program is:</p>\n\n<ul>\n<li><p>20:00-20:15: Assembly</p></li>\n<li><p>20:15-21:00: Main Talk</p></li>\n<li><p>21:00-22:00: Dinner &amp; Discussion</p></li>\n<li><p>22:00-23:00: Rump Session (minitalks)</p></li>\n<li><p>23:00-: End of official programming</p></li>\n</ul>\n\n<p>Main Talk: Mirrors and Sunlight: Dealing with Emotional Vampires / Guy Banay</p>\n\n<p>We will learn how to recognize the Dramatic Personality Disorders (antisocial, borderline, narcissistic and histrionic) and some strategies to minimize the damage they can do.</p>\n\n<p>Backup Talk: TBA</p>\n\n<p>Rump Session: each participant will give a 4-minute talk (+3 minute encore if we applaud hard enough). Giving a talk isn't mandatory, but it's highly recommended. Not confident that what you have to say is relevant to our interests? Unsure about your public speaking skills? Doesn't matter - in the rump session, anything goes.</p>\n\n<p>Feel free to contact me (Gal Hochberg) at hochbergg@gmail.com or at 0545330678 for any further information</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Israel_Meetup__Tel_Aviv___Dealing_with_Emotional_Vampires1\">Discussion article for the meetup : <a href=\"/meetups/rz\">Less Wrong Israel Meetup (Tel Aviv): Dealing with Emotional Vampires</a></h2>", "sections": [{"title": "Discussion article for the meetup : Less Wrong Israel Meetup (Tel Aviv): Dealing with Emotional Vampires", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Israel_Meetup__Tel_Aviv___Dealing_with_Emotional_Vampires", "level": 1}, {"title": "Discussion article for the meetup : Less Wrong Israel Meetup (Tel Aviv): Dealing with Emotional Vampires", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Israel_Meetup__Tel_Aviv___Dealing_with_Emotional_Vampires1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-08T11:31:03.525Z", "modifiedAt": null, "url": null, "title": "Requesting advice- A Philosophy Idea", "slug": "requesting-advice-a-philosophy-idea", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:57.773Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Carinthium", "createdAt": "2010-11-10T22:28:58.091Z", "isAdmin": false, "displayName": "Carinthium"}, "userId": "DL8CRWfXPCHYqQsv4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Xq3KaqvS6837Gnpiq/requesting-advice-a-philosophy-idea", "pageUrlRelative": "/posts/Xq3KaqvS6837Gnpiq/requesting-advice-a-philosophy-idea", "linkUrl": "https://www.lesswrong.com/posts/Xq3KaqvS6837Gnpiq/requesting-advice-a-philosophy-idea", "postedAtFormatted": "Tuesday, October 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Requesting%20advice-%20A%20Philosophy%20Idea&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARequesting%20advice-%20A%20Philosophy%20Idea%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXq3KaqvS6837Gnpiq%2Frequesting-advice-a-philosophy-idea%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Requesting%20advice-%20A%20Philosophy%20Idea%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXq3KaqvS6837Gnpiq%2Frequesting-advice-a-philosophy-idea", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXq3KaqvS6837Gnpiq%2Frequesting-advice-a-philosophy-idea", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<p>I'm not sure about this, but presenting it anyway for scrutiny.</p>\r\n<p>I was thinking that it doesn't matter if a concept is undefined, or even cannot be defined, if hypothetically speaking said concept can exist without any ambiguity within it then it is still a tenable concept. The implications, if this is true, would be that it would knock down Quine's argument against the analytic-synthetic distinction.</p>\r\n<p>Your thoughts, Lesswrong?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Xq3KaqvS6837Gnpiq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -16, "extendedScore": null, "score": 1.37170632339324e-06, "legacy": true, "legacyId": "24359", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-08T20:46:25.568Z", "modifiedAt": null, "url": null, "title": "Meetup : Saint Petersburg, Russia", "slug": "meetup-saint-petersburg-russia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:57.386Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "efim", "createdAt": "2013-04-14T00:57:28.743Z", "isAdmin": false, "displayName": "efim"}, "userId": "Y8azdhZD6fvWdGwaB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xHKx5mTepMBYR3JjL/meetup-saint-petersburg-russia", "pageUrlRelative": "/posts/xHKx5mTepMBYR3JjL/meetup-saint-petersburg-russia", "linkUrl": "https://www.lesswrong.com/posts/xHKx5mTepMBYR3JjL/meetup-saint-petersburg-russia", "postedAtFormatted": "Tuesday, October 8th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Saint%20Petersburg%2C%20Russia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Saint%20Petersburg%2C%20Russia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxHKx5mTepMBYR3JjL%2Fmeetup-saint-petersburg-russia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Saint%20Petersburg%2C%20Russia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxHKx5mTepMBYR3JjL%2Fmeetup-saint-petersburg-russia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxHKx5mTepMBYR3JjL%2Fmeetup-saint-petersburg-russia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 115, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/s0'>Saint Petersburg, Russia</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">27 October 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">\u0421\u0430\u043d\u043a\u0442-\u041f\u0435\u0442\u0435\u0440\u0431\u0443\u0440\u0433, \u043c. \u0422\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0418\u043d\u0441\u0442\u0438\u0442\u0443\u0442, \u0443\u043b.1-\u044f \u041a\u0440\u0430\u0441\u043d\u043e\u0430\u0440\u043c\u0435\u0439\u0441\u043a\u0430\u044f, \u0434\u043e\u043c 15</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come to the (probably) first in a long time meetup in St.Petersburg!</p>\n\n<p>It will be located in cafe \"PMG\" - for detailed description please read mailing list announcement. At least for the first time there will be no or little moderation - just socialising, getting to know each other, unstructured discussion.\nWe will be looking at our interests and how meetups can meet them.</p>\n\n<p>Please look in mailing list for more information:<a href=\"https://groups.google.com/forum/#!forum/less-wrong-saint-petersburg\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/less-wrong-saint-petersburg</a></p>\n\n<p>Or contact me on 8 911 843 56 44. Every day from 18-00 to 00-00.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/s0'>Saint Petersburg, Russia</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xHKx5mTepMBYR3JjL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.3722168018572886e-06, "legacy": true, "legacyId": "24360", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Saint_Petersburg__Russia\">Discussion article for the meetup : <a href=\"/meetups/s0\">Saint Petersburg, Russia</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">27 October 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">\u0421\u0430\u043d\u043a\u0442-\u041f\u0435\u0442\u0435\u0440\u0431\u0443\u0440\u0433, \u043c. \u0422\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0418\u043d\u0441\u0442\u0438\u0442\u0443\u0442, \u0443\u043b.1-\u044f \u041a\u0440\u0430\u0441\u043d\u043e\u0430\u0440\u043c\u0435\u0439\u0441\u043a\u0430\u044f, \u0434\u043e\u043c 15</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come to the (probably) first in a long time meetup in St.Petersburg!</p>\n\n<p>It will be located in cafe \"PMG\" - for detailed description please read mailing list announcement. At least for the first time there will be no or little moderation - just socialising, getting to know each other, unstructured discussion.\nWe will be looking at our interests and how meetups can meet them.</p>\n\n<p>Please look in mailing list for more information:<a href=\"https://groups.google.com/forum/#!forum/less-wrong-saint-petersburg\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/less-wrong-saint-petersburg</a></p>\n\n<p>Or contact me on 8 911 843 56 44. Every day from 18-00 to 00-00.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Saint_Petersburg__Russia1\">Discussion article for the meetup : <a href=\"/meetups/s0\">Saint Petersburg, Russia</a></h2>", "sections": [{"title": "Discussion article for the meetup : Saint Petersburg, Russia", "anchor": "Discussion_article_for_the_meetup___Saint_Petersburg__Russia", "level": 1}, {"title": "Discussion article for the meetup : Saint Petersburg, Russia", "anchor": "Discussion_article_for_the_meetup___Saint_Petersburg__Russia1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-09T07:19:40.046Z", "modifiedAt": null, "url": null, "title": "Meetup : Moscow, Beliefs 2", "slug": "meetup-moscow-beliefs-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YyCkETXBCxDQrw5nz/meetup-moscow-beliefs-2", "pageUrlRelative": "/posts/YyCkETXBCxDQrw5nz/meetup-moscow-beliefs-2", "linkUrl": "https://www.lesswrong.com/posts/YyCkETXBCxDQrw5nz/meetup-moscow-beliefs-2", "postedAtFormatted": "Wednesday, October 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Moscow%2C%20Beliefs%202&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Moscow%2C%20Beliefs%202%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYyCkETXBCxDQrw5nz%2Fmeetup-moscow-beliefs-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Moscow%2C%20Beliefs%202%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYyCkETXBCxDQrw5nz%2Fmeetup-moscow-beliefs-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYyCkETXBCxDQrw5nz%2Fmeetup-moscow-beliefs-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 187, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/s1'>Moscow, Beliefs 2</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 October 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will continue working with beliefs. So please take your notes from the last gathering. If you did not come please think about beliefs you have, write some of them down and bring this notes to the meet up.</p>\n\n<p>We will do improved <strong>Belief Investigation</strong> exercise, and try some new exercises to analyse beliefs further.</p>\n\n<p>Please take into account that we still have <a href=\"http://lesswrong.ru/forum/index.php/topic,103.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_prediction_market+20131013_meet_up&amp;utm_content=20131013_meet_up&amp;utm_campaign=moscow_meetups\">Prediction Market</a> and we started <a href=\"http://lesswrong.ru/forum/index.php/topic,223.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_honor_board+20131013_meet_up&amp;utm_content=20131013_meet_up&amp;utm_campaign=moscow_meetups\">Scavenger Hunt</a> recently. You can follow the corresponding links to the discussions on the Russian forum.</p>\n\n<p>If you are going for the first time:\nWe gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/dO9OmIr.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and the bicycle parking on you way. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.</p>\n\n<p>You can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/s1'>Moscow, Beliefs 2</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YyCkETXBCxDQrw5nz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3727992701171782e-06, "legacy": true, "legacyId": "24362", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Moscow__Beliefs_2\">Discussion article for the meetup : <a href=\"/meetups/s1\">Moscow, Beliefs 2</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 October 2013 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Russia, Moscow, ulitsa L'va Tolstogo 16</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will continue working with beliefs. So please take your notes from the last gathering. If you did not come please think about beliefs you have, write some of them down and bring this notes to the meet up.</p>\n\n<p>We will do improved <strong>Belief Investigation</strong> exercise, and try some new exercises to analyse beliefs further.</p>\n\n<p>Please take into account that we still have <a href=\"http://lesswrong.ru/forum/index.php/topic,103.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_prediction_market+20131013_meet_up&amp;utm_content=20131013_meet_up&amp;utm_campaign=moscow_meetups\">Prediction Market</a> and we started <a href=\"http://lesswrong.ru/forum/index.php/topic,223.0.html?utm_source=lesswrong.com&amp;utm_medium=meet_up_notice&amp;utm_term=link_for_honor_board+20131013_meet_up&amp;utm_content=20131013_meet_up&amp;utm_campaign=moscow_meetups\">Scavenger Hunt</a> recently. You can follow the corresponding links to the discussions on the Russian forum.</p>\n\n<p>If you are going for the first time:\nWe gather in the Yandex office, you need the second revolving door with the sign \u201c\u042f\u043d\u0434\u0435\u043a\u0441\u201d, here is <a href=\"http://i.imgur.com/dO9OmIr.jpg\" rel=\"nofollow\">the photo of the entrance you need</a>. You need to pass the first entrance and the bicycle parking on you way. Here is additional guide how to get there: <a href=\"http://company.yandex.ru/contacts/redrose/\" rel=\"nofollow\">link</a>.</p>\n\n<p>You can fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information.</p>\n\n<p>We start at 16:00 and sometimes finish at night.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Moscow__Beliefs_21\">Discussion article for the meetup : <a href=\"/meetups/s1\">Moscow, Beliefs 2</a></h2>", "sections": [{"title": "Discussion article for the meetup : Moscow, Beliefs 2", "anchor": "Discussion_article_for_the_meetup___Moscow__Beliefs_2", "level": 1}, {"title": "Discussion article for the meetup : Moscow, Beliefs 2", "anchor": "Discussion_article_for_the_meetup___Moscow__Beliefs_21", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-09T14:03:19.000Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham NC/Triangle Area: Meeetup!", "slug": "meetup-durham-nc-triangle-area-meeetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fwq98pKv6Nmr6fG26/meetup-durham-nc-triangle-area-meeetup", "pageUrlRelative": "/posts/fwq98pKv6Nmr6fG26/meetup-durham-nc-triangle-area-meeetup", "linkUrl": "https://www.lesswrong.com/posts/fwq98pKv6Nmr6fG26/meetup-durham-nc-triangle-area-meeetup", "postedAtFormatted": "Wednesday, October 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Meeetup!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Meeetup!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffwq98pKv6Nmr6fG26%2Fmeetup-durham-nc-triangle-area-meeetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20NC%2FTriangle%20Area%3A%20Meeetup!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffwq98pKv6Nmr6fG26%2Fmeetup-durham-nc-triangle-area-meeetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffwq98pKv6Nmr6fG26%2Fmeetup-durham-nc-triangle-area-meeetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 120, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/s2'>Durham NC/Triangle Area: Meeetup!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 October 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">420 West Geer St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Discussion meetup!</p>\n\n<p>Topics:</p>\n\n<p><a href=\"http://lesswrong.com/r/discussion/lw/ioy/\">Instinctive Frequentists, the Outside View, and de-Biasing </a></p>\n\n<p>And continuation of last meetup's discussion (prompted by <a href=\"http://lesswrong.com/lw/il3/three_ways_cfar_has_changed_my_view_of_rationality/\">Three ways CFAR has changed my view of rationality</a>) of how emotional awareness relates to rationality.  Specifically we'd like to incorporate more opportunities for reflection and discussion of emotions into meetups, so please bring ideas ways we could do this!</p>\n\n<p>Meet at 7 at Cocoa Cinnamon (unless there is another loud musical event going on, in which case we will choose another venue on Rigsbee.)</p>\n\n<p>Beverages to follow at either Motorco or Fullsteam.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/s2'>Durham NC/Triangle Area: Meeetup!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fwq98pKv6Nmr6fG26", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "24364", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Meeetup_\">Discussion article for the meetup : <a href=\"/meetups/s2\">Durham NC/Triangle Area: Meeetup!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 October 2013 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">420 West Geer St., Durham NC 27701</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Discussion meetup!</p>\n\n<p>Topics:</p>\n\n<p><a href=\"http://lesswrong.com/r/discussion/lw/ioy/\">Instinctive Frequentists, the Outside View, and de-Biasing </a></p>\n\n<p>And continuation of last meetup's discussion (prompted by <a href=\"http://lesswrong.com/lw/il3/three_ways_cfar_has_changed_my_view_of_rationality/\">Three ways CFAR has changed my view of rationality</a>) of how emotional awareness relates to rationality.  Specifically we'd like to incorporate more opportunities for reflection and discussion of emotions into meetups, so please bring ideas ways we could do this!</p>\n\n<p>Meet at 7 at Cocoa Cinnamon (unless there is another loud musical event going on, in which case we will choose another venue on Rigsbee.)</p>\n\n<p>Beverages to follow at either Motorco or Fullsteam.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Meeetup_1\">Discussion article for the meetup : <a href=\"/meetups/s2\">Durham NC/Triangle Area: Meeetup!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham NC/Triangle Area: Meeetup!", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Meeetup_", "level": 1}, {"title": "Discussion article for the meetup : Durham NC/Triangle Area: Meeetup!", "anchor": "Discussion_article_for_the_meetup___Durham_NC_Triangle_Area__Meeetup_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["24CfLhbByQqv6nSws", "68dHanLWsS6SEyZp9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-09T16:44:07.914Z", "modifiedAt": null, "url": null, "title": "Meetup : Helsinki Meetup", "slug": "meetup-helsinki-meetup-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "morrel", "createdAt": "2013-07-04T17:36:44.083Z", "isAdmin": false, "displayName": "morrel"}, "userId": "Xdtoje5pFmqC2CY6y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sC9MYXSm2JzAsBY3C/meetup-helsinki-meetup-2", "pageUrlRelative": "/posts/sC9MYXSm2JzAsBY3C/meetup-helsinki-meetup-2", "linkUrl": "https://www.lesswrong.com/posts/sC9MYXSm2JzAsBY3C/meetup-helsinki-meetup-2", "postedAtFormatted": "Wednesday, October 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Helsinki%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Helsinki%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsC9MYXSm2JzAsBY3C%2Fmeetup-helsinki-meetup-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Helsinki%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsC9MYXSm2JzAsBY3C%2Fmeetup-helsinki-meetup-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsC9MYXSm2JzAsBY3C%2Fmeetup-helsinki-meetup-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/s3'>Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 October 2013 03:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">J\u00e4mer\u00e4ntaival 3, 02150 Espoo, Finland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to find us:</strong> We\u2019re meeting in <a href=\"http://ayy.fi/jasenille/tilojen-vuokraaminen/sitsi-peli-ja-kokoustilat/takkakabinetti/\" rel=\"nofollow\">Takkakabinetti</a> in Otaniemi. You can find the room by following the signs inside the building.</p>\n\n<p><strong>The theme</strong> of this meetup will be habits. Kaj Sotala will give us an introduction to the subject, after which we\u2019ll discuss habit formation in smaller groups.</p>\n\n<p>Related reading:</p>\n\n<p><a href=\"http://lesswrong.com/lw/gxr/rationality_habits_i_learned_at_the_cfar_workshop/\">Rationality habits</a></p>\n\n<p><a href=\"http://lesswrong.com/lw/ifl/how_i_am_productive/\">Productivity habits</a></p>\n\n<p><a href=\"http://lesswrong.com/lw/hub/common_failure_modes_in_habit_formation/\">Common failure modes in habit formation</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/s3'>Helsinki Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sC9MYXSm2JzAsBY3C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.3733188448283633e-06, "legacy": true, "legacyId": "24365", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup\">Discussion article for the meetup : <a href=\"/meetups/s3\">Helsinki Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 October 2013 03:00:00PM (+0300)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">J\u00e4mer\u00e4ntaival 3, 02150 Espoo, Finland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>How to find us:</strong> We\u2019re meeting in <a href=\"http://ayy.fi/jasenille/tilojen-vuokraaminen/sitsi-peli-ja-kokoustilat/takkakabinetti/\" rel=\"nofollow\">Takkakabinetti</a> in Otaniemi. You can find the room by following the signs inside the building.</p>\n\n<p><strong>The theme</strong> of this meetup will be habits. Kaj Sotala will give us an introduction to the subject, after which we\u2019ll discuss habit formation in smaller groups.</p>\n\n<p>Related reading:</p>\n\n<p><a href=\"http://lesswrong.com/lw/gxr/rationality_habits_i_learned_at_the_cfar_workshop/\">Rationality habits</a></p>\n\n<p><a href=\"http://lesswrong.com/lw/ifl/how_i_am_productive/\">Productivity habits</a></p>\n\n<p><a href=\"http://lesswrong.com/lw/hub/common_failure_modes_in_habit_formation/\">Common failure modes in habit formation</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Helsinki_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/s3\">Helsinki Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Helsinki Meetup", "anchor": "Discussion_article_for_the_meetup___Helsinki_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ymwyTDc96uaAqZ48e", "JTHe5oGvdj6T73o4o", "PYgGpmmk3wQhSt6Yv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-09T19:19:40.795Z", "modifiedAt": null, "url": null, "title": "Advice for a smart 8-year-old bored with school ", "slug": "advice-for-a-smart-8-year-old-bored-with-school", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:02.841Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "James_Miller", "createdAt": "2009-03-05T17:14:38.674Z", "isAdmin": false, "displayName": "James_Miller"}, "userId": "LzF2X9eB9oS3q4BXG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2avAsf898kdPZW4N4/advice-for-a-smart-8-year-old-bored-with-school", "pageUrlRelative": "/posts/2avAsf898kdPZW4N4/advice-for-a-smart-8-year-old-bored-with-school", "linkUrl": "https://www.lesswrong.com/posts/2avAsf898kdPZW4N4/advice-for-a-smart-8-year-old-bored-with-school", "postedAtFormatted": "Wednesday, October 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Advice%20for%20a%20smart%208-year-old%20bored%20with%20school%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdvice%20for%20a%20smart%208-year-old%20bored%20with%20school%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2avAsf898kdPZW4N4%2Fadvice-for-a-smart-8-year-old-bored-with-school%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Advice%20for%20a%20smart%208-year-old%20bored%20with%20school%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2avAsf898kdPZW4N4%2Fadvice-for-a-smart-8-year-old-bored-with-school", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2avAsf898kdPZW4N4%2Fadvice-for-a-smart-8-year-old-bored-with-school", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<p>Although my 8-year-old son likes his teacher, he is frequently bored at school. &nbsp;He attends a high quality suburban public school in the United States. &nbsp;He has a lot of traits in common with LessWrong readers, and we would like advice for what he can do to counter his boredom. &nbsp;Many of you must have found grade school more or less tedious. &nbsp;What were your coping strategies?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2avAsf898kdPZW4N4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 14, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "24366", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 189, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-09T21:44:19.773Z", "modifiedAt": null, "url": null, "title": "Meetup : Complex problems, limited information, and rationality; How should we make decisions in real life?", "slug": "meetup-complex-problems-limited-information-and-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:57.214Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Davidmanheim", "createdAt": "2011-01-18T15:14:54.315Z", "isAdmin": false, "displayName": "Davidmanheim"}, "userId": "DiHrY9qMta2m6MvxJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RqMJ2jTBbRBCCDw5D/meetup-complex-problems-limited-information-and-rationality", "pageUrlRelative": "/posts/RqMJ2jTBbRBCCDw5D/meetup-complex-problems-limited-information-and-rationality", "linkUrl": "https://www.lesswrong.com/posts/RqMJ2jTBbRBCCDw5D/meetup-complex-problems-limited-information-and-rationality", "postedAtFormatted": "Wednesday, October 9th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Complex%20problems%2C%20limited%20information%2C%20and%20rationality%3B%20How%20should%20we%20make%20decisions%20in%20real%20life%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Complex%20problems%2C%20limited%20information%2C%20and%20rationality%3B%20How%20should%20we%20make%20decisions%20in%20real%20life%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRqMJ2jTBbRBCCDw5D%2Fmeetup-complex-problems-limited-information-and-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Complex%20problems%2C%20limited%20information%2C%20and%20rationality%3B%20How%20should%20we%20make%20decisions%20in%20real%20life%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRqMJ2jTBbRBCCDw5D%2Fmeetup-complex-problems-limited-information-and-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRqMJ2jTBbRBCCDw5D%2Fmeetup-complex-problems-limited-information-and-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 253, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/s4'>Complex problems, limited information, and rationality; How should we make decisions in real life?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 October 2013 07:00:45PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">West Los Angeles (At the Westside Tavern Upstair Wine Bar)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Most decisions we make involve complex, poorly understood systems. We'd like to be rational anyways, but how?</p>\n\n<p>Example time:\nI am going to pre-commit here to biking to the meetup. Why? I believe that more exercise would increase my physical fitness in ways that are beneficial.</p>\n\n<p>But... I haven't done the research into the benefits of physical fitness, and haven't done a tradeoff analysis of time costs versus benefits, I don't know how likely dangerous biking accidents are in LA, I don't know enough about my body to be sure that biking is safe, or a useful way for me to get in shape. Should I spend the week until the meetup researching these factors and building a model, or should I spend time getting work and homework done, playing with my kid, and sleeping? Through a combination of trusting experts, laziness, and other things to do, I'm not going to do the research.</p>\n\n<p>And that's where we are with most decisions. What should we do, if we want to be rational? I have some ideas, some questions, and some willingness to shut up and listen to others, and I might even update my beliefs if others have ideas I like.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/s4'>Complex problems, limited information, and rationality; How should we make decisions in real life?</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RqMJ2jTBbRBCCDw5D", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.3735953106185875e-06, "legacy": true, "legacyId": "24367", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Complex_problems__limited_information__and_rationality__How_should_we_make_decisions_in_real_life_\">Discussion article for the meetup : <a href=\"/meetups/s4\">Complex problems, limited information, and rationality; How should we make decisions in real life?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 October 2013 07:00:45PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">West Los Angeles (At the Westside Tavern Upstair Wine Bar)</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Most decisions we make involve complex, poorly understood systems. We'd like to be rational anyways, but how?</p>\n\n<p>Example time:\nI am going to pre-commit here to biking to the meetup. Why? I believe that more exercise would increase my physical fitness in ways that are beneficial.</p>\n\n<p>But... I haven't done the research into the benefits of physical fitness, and haven't done a tradeoff analysis of time costs versus benefits, I don't know how likely dangerous biking accidents are in LA, I don't know enough about my body to be sure that biking is safe, or a useful way for me to get in shape. Should I spend the week until the meetup researching these factors and building a model, or should I spend time getting work and homework done, playing with my kid, and sleeping? Through a combination of trusting experts, laziness, and other things to do, I'm not going to do the research.</p>\n\n<p>And that's where we are with most decisions. What should we do, if we want to be rational? I have some ideas, some questions, and some willingness to shut up and listen to others, and I might even update my beliefs if others have ideas I like.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Complex_problems__limited_information__and_rationality__How_should_we_make_decisions_in_real_life_1\">Discussion article for the meetup : <a href=\"/meetups/s4\">Complex problems, limited information, and rationality; How should we make decisions in real life?</a></h2>", "sections": [{"title": "Discussion article for the meetup : Complex problems, limited information, and rationality; How should we make decisions in real life?", "anchor": "Discussion_article_for_the_meetup___Complex_problems__limited_information__and_rationality__How_should_we_make_decisions_in_real_life_", "level": 1}, {"title": "Discussion article for the meetup : Complex problems, limited information, and rationality; How should we make decisions in real life?", "anchor": "Discussion_article_for_the_meetup___Complex_problems__limited_information__and_rationality__How_should_we_make_decisions_in_real_life_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-10T02:10:48.519Z", "modifiedAt": null, "url": null, "title": "A Voting Puzzle, Some Political Science, and a Nerd Failure Mode", "slug": "a-voting-puzzle-some-political-science-and-a-nerd-failure", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:29.576Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CcC8MocynqKPmMPwL/a-voting-puzzle-some-political-science-and-a-nerd-failure", "pageUrlRelative": "/posts/CcC8MocynqKPmMPwL/a-voting-puzzle-some-political-science-and-a-nerd-failure", "linkUrl": "https://www.lesswrong.com/posts/CcC8MocynqKPmMPwL/a-voting-puzzle-some-political-science-and-a-nerd-failure", "postedAtFormatted": "Thursday, October 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Voting%20Puzzle%2C%20Some%20Political%20Science%2C%20and%20a%20Nerd%20Failure%20Mode&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Voting%20Puzzle%2C%20Some%20Political%20Science%2C%20and%20a%20Nerd%20Failure%20Mode%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCcC8MocynqKPmMPwL%2Fa-voting-puzzle-some-political-science-and-a-nerd-failure%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Voting%20Puzzle%2C%20Some%20Political%20Science%2C%20and%20a%20Nerd%20Failure%20Mode%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCcC8MocynqKPmMPwL%2Fa-voting-puzzle-some-political-science-and-a-nerd-failure", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCcC8MocynqKPmMPwL%2Fa-voting-puzzle-some-political-science-and-a-nerd-failure", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5144, "htmlBody": "<p>In grade school, I read a series of books titled <em>Sideways Stories from Wayside School </em>by Louis Sachar, who you may know as the author of the novel <em>Holes</em> which was made into a movie in 2003. The series included two books of math problems, <em>Sideways Arithmetic from Wayside School </em>and <em>More Sideways Arithmetic from Wayside School, </em>the latter of which included the following problem (paraphrased):</p>\n<p>The students have Mrs. Jewl's class have been given the privilege of voting on the height of the school's new flagpole. She has each of them write down what they think would be the best hight for the flagpole. The votes are distributed as follows:</p>\n<ul>\n<li>1 student votes for 6 feet.</li>\n<li>1 student votes for 10 feet.</li>\n<li>7 students vote for 25 feet.</li>\n<li>1 student votes for 30 feet.</li>\n<li>2 students vote for 50 feet.</li>\n<li>2 students vote for 60 feet.</li>\n<li>1 student votes for 65 feet.</li>\n<li>3 students vote for 75 feet.</li>\n<li>1 student votes for 80 feet, 6 inches.</li>\n<li>4 students vote for 85 feet.</li>\n<li>1 student votes for 91 feet.</li>\n<li>5 students vote for 100 feet.</li>\n</ul>\n<p>At first, Mrs. Jewls declares 25 feet the winning answer, but one of the students who voted for 100 feet convinces her there should be a runoff between 25 feet and 100 feet. In the runoff, each student votes for the height closest to their original answer. But after that round of voting, one of the students who voted for 85 feet wants <em>their </em>turn, so 85 feet goes up against the winner of the <em>previous </em>round of voting, and the students vote the same way, with each student voting for the height closest to their original answer. Then the same thing happens again with the 50 foot option. And so on, with each number, again and again, \"very much like a game of tether ball.\"</p>\n<p>Question: if this process continues until it settles on an answer that can't be beaten by any other answer, how tall will the new flagpole be?</p>\n<p>Answer (<a href=\"http://rot13.com/index.php\">rot13'd</a>):&nbsp;fvkgl-svir srrg, orpnhfr gung'f gur zrqvna inyhr bs gur bevtvany frg bs ibgrf.&nbsp;Naq abj lbh xabj gur fgbel bs zl svefg rapbhagre jvgu gur zrqvna ibgre gurberz.</p>\n<p>Why am I telling you this? There's a minor reason and a major reason. The minor reason is that this shows it is possible to explain little-known academic concepts, at least certain ones, in a way that grade schoolers will understand. It's a data point that fits nicely with <a href=\"/lw/kh/explainers_shoot_high_aim_low/\">what Eliezer has written about how to explain things</a>.&nbsp;The major reason, though, is that a month ago I finished my systematic read-through of <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">the sequences</a>&nbsp;and while I generally agree that they're awesome (perhaps moreso than most people; I didn't see the problem with the metaethics sequence), I thought the mini-discussion of <a href=\"/lw/mg/the_twoparty_swindle/\">political</a> <a href=\"/lw/mh/the_american_system_and_misleading_labels/\">parties</a> and <a href=\"/lw/mi/stop_voting_for_nincompoops/\">voting</a> was on reflection weak and indicative of a broader nerd failure mode.</p>\n<p>TLDR (courtesy of <a href=\"/lw/isk/a_voting_puzzle_some_political_science_and_a_nerd/9vgk\">lavalamp</a>):</p>\n<ol>\n<li>Politicians probably conform to the median voter's views.</li>\n<li>Most voters are not the median, so most people usually dislike the winning politicians.</li>\n<li>But people dislike the politicians for different reasons.</li>\n<li>Nerds should avoid giving advice that boils down to \"behave optimally\". Instead, analyze the reasons for the current failure to behave optimally and give more targeted advice.</li>\n</ol>\n<p><a id=\"more\"></a></p>\n<p>Advance warning for heavy US slant, at least in terms of examples, though the theory is applicable everywhere.</p>\n<h2>The median voter theorem</h2>\n<p>The median voter theorem was first laid out in a paper by Duncan Black titled <a href=\"http://www.corwin.com/upm-data/24492_Dowding_Chapter_01.pdf\">\"On the Rationale of Group Decision-Making,\"</a>&nbsp;which imagine's a situation very much like Mrs. Jewls' class voting on the flagpole height: a committee passes a motion by majority vote, and then it considers various motions to amend the original motion, each of which itself needs a simple majority to pass. Each member of the committee has preferences over the range of possible motions, and furthermore:</p>\n<blockquote>\n<p>While a member&rsquo;s preference curve may be of any shape whatever, there is reason to expect that, in some important practical problems, the valuations actually carried out will tend to take the form of isolated points on single-peaked curves. This would be particularly likely to happen were the committee considering different possible sizes of a numerical quantity and choosing one size in preference to the others. It might be reaching a decision, say, with regard to the price of a product to be marketed by a firm, or the output for a future period, or the wage rate of labor, or the height of a particular tax, or the legal school-leaving age, and so on.</p>\n</blockquote>\n<p>Or, for that matter, the height of a flagpole. Black shows that on his assumptions, the committee will eventually settle on the version of the motion favored by the median committee member.</p>\n<p>Again, you may be asking, so what? Most people don't care about understanding the behavior of committees, especially not compared to their passion for national presidential elections. And elections for political office don't use a tether ball-like system of having head-to-head matchup after head-to-head matchup until you've finally found the candidate the median voter wants. There's one election with two (or if you're lucky, three) major candidates and that's it.</p>\n<p>The relevance to electoral politics comes in when you allow for the possibility of candidates shaping themselves and their platforms to appeal to the median voter. The candidate who does this should be invincible - at least, until the other candidate does the same thing, at which point the election becomes a closer call. The idea of candidates shaping themselves to voter preferences is key; I started off this post with the flagpole example partly to emphasize that. And there are other assumptions you have to make to get to the conclusion that candidates will <em>actually </em>behave this way.</p>\n<p>But before we get in to that, let's compare the median voter picture to the picture Eliezer put forward in the posts linked above:</p>\n<blockquote>\n<p>Forget that Congresspeople on both sides of the \"divide\" are more likely to be lawyers than truck drivers. &nbsp;Forget that in training and in daily life, they have far more in common with each other than they do with a randomly selected US citizen from their own party. Forget that they are more likely to hang out at each other's expensive hotel rooms than drop by your own house. &nbsp;Is there a political divide - a divide of policies and interests - between Professional Politicians on the one hand, and Voters on the other?</p>\n<p>Well, let me put it this way. &nbsp;Suppose that you happen to be socially liberal, fiscally conservative. &nbsp;Who would you vote for?</p>\n<p>Or simplify it further: &nbsp;Suppose that you're a voter who prefers a smaller, less expensive government - should you vote Republican or Democratic? &nbsp;Or, lest I be accused of color favoritism, suppose that your voter preference is to get US troops out of Iraq. &nbsp;Should you vote Democratic or Republican?</p>\n<p>One needs to be careful, at this point, to keep track of the distinction between marketing materials and historical records. &nbsp;I'm not asking which political party stands for the idea of smaller government - which football team has \"Go go smaller government! &nbsp;Go go go!\" as one of its cheers. &nbsp;(Or \"Troops out of Iraq! &nbsp;Yay!\") &nbsp;Rather, over the last several decades, among Republican politicians and Democratic politicians, which group of Professional Politicians shrunk the government while it was in power?</p>\n<p>And by \"shrunk\" I mean \"shrunk\". &nbsp;If you're suckered into an angry, shouting fight over whether Your Politicians or Their Politicians grew the government slightly less slowly, it means you're not seeing the divide between Politicians and Voters. There isn't a grand conspiracy to expand the government, but there's an incentive for each individual politician to send pork to campaign contributors, or borrow today against tomorrow's income. &nbsp;And that creates a divide between the Politicians and the Voters, as a class, for reasons that have nothing to do with colors and slogans.</p>\n</blockquote>\n<p>Eliezer observes that there doesn't seem to be much difference between the two parties, and concludes that they are colluding (albeit probably not by explicit agreement) to advance their own interests at the expense of the voters'. The politicians don't offer the voters any real choice, but get voters to vote for them anyway though misleading party labels and the argument that, if they don't vote for a major-party candidate, they're \"throwing their vote away.\"</p>\n<p>However, the observation that there doesn't seem to be much difference between the two parties can <em>also </em>be explained by the hypothesis that politicians are shaping themselves to appeal to the median voter. This fact alone doesn't show that the median voter model is <em>right... </em>but it does show that the mere fact of there not being much difference between the two parties doesn't show the \"colluding politicians\" model is right <em>either.</em></p>\n<p>So how well does the median voter theorem capture reality? One problem for the model is that it potentially breaks down if the choices don't fit onto a nice, linear spectrum. Suppose, for the sake of a simplified example, that only three people vote in a particular presidential election. Suppose, furthermore, that the three voters have the following set of preferences:</p>\n<ul>\n<li>Alice prefers Obama to Romney, and Romney to Ron Paul</li>\n<li>Bob prefers Romney to Ron Paul, and Ron Paul to Obama</li>\n<li>Carol prefers Ron Paul to Obama, and Obama to Romney</li>\n</ul>\n<p>Given this set of voters and their preferences, in an Obama vs. Romney contest, Obama will win; in a Romney vs. Ron Paul contest, Romney will win; but in a Ron Paul vs. Obama contest, Ron Paul will win.</p>\n<p>However, the median voter theorem seems to be a pretty good model in practice in spite of such problems. Roger Congleton, in <a href=\"http://rdc1.net/forthcoming/medianvt.pdf\">an article in the </a><em><a href=\"http://rdc1.net/forthcoming/medianvt.pdf\">Encyclopedia of Public Choice</a>,&nbsp;</em>writes:</p>\n<blockquote>\n<p>Although theoretical arguments suggest that the applicability of the median voter model&nbsp;may be very limited, the empirical evidence suggests otherwise. There is a large body of evidence that suggests median voter preferences over policies are (largely) of the sort which can be mapped into a single issue space while retaining \"single peakedness\" Poole and Daniels (1985) find that 80-90% of all the recorded votes in the US Congress can be explained with a one dimensional policy space. Stratmann (1996) finds little evidence of cycling across Congressional votes over district specific grants.</p>\n<p>Moreover, the median voter model has a very good empirical track record in public finance as a model of fiscal policy across states and through time. Recent studies show that the median voter model can explain federal, state, and local spending, as well as international tariff policies. Congleton and Shughart (1990) Congleton and Bennett (1995) suggest that the median voter model provides a better explanation of large scale public programs than comparable interest group models. This is not to suggest that the median voter always exercises the same degree of control over public policy irrespective of political institutions. Holcombe (1980) and Frey (1994) report significant policy difference between representative and direct forms of democracy that would not exist unless significant agency problems exist within representative government. Moreover, statistical tests can never prove that a particular model is correct, only that it is more likely to be correct than false. However, in general, the median voter model appears to be quite robust as a model of public policy formation in areas where the median voter can credibly be thought to understand and care about public policy.</p>\n<p>The empirical evidence suggests that the median voter model can serve as a very useful first approximation of governance within democratic polities. As a consequence, the median voter model continues to function as an analytical point of departure for more elaborate models of policy formation within democracies in much the same way that the competitive model serves the micro economics literature.</p>\n</blockquote>\n<p>In the American political system, the effect of the median voter theorem is blunted somewhat by the primary system. It's a commonplace among American political commentators that politicians must appeal to the \"base\" during the primaries, then swing towards the center for the general election. Of course, politicians can't suddenly become <em>perfectly </em>centrist once they secure their party's nomination; their swing towards the center has to be done in a way that's at least superficially consistent with their previous pandering to their base.&nbsp;These observations suggest that, while reality doesn't perfectly match the idealized model, there's still a lot of truth to it.&nbsp;</p>\n<p>(Note: a site search for previous discussion of the median voter theorem on LessWrong turned up a <a href=\"/lw/2qq/politics_as_charity/2o6a\">comment</a> by Carl Shulman that mentioned \"the need to motivate one's base to vote/volunteer/contribute the ideological lumpiness\" as probably having an effect similar to the effect of primaries. I wouldn't have thought they were as important as primaries but I can believe Carl here.)</p>\n<p>The tendency of politicians to position themselves wherever the center of public opinion is currently at can be striking on specific issues. For example, public support for gay rights has increased greatly in the past two decades. In that time period, positions which once got Bill Clinton demonized by the religious right as an agent of the homosexual agenda (like Don't Ask Don't Tell) became the \"conservative\" position. Progress, but in terms of the public stances of politicians, it's progress that came not in the form of dramatic shifts but cautious adjustments.</p>\n<p>By the time of the 2008 campaign, Republican nominee <a href=\"http://en.wikipedia.org/wiki/Political_positions_of_John_McCain#LGBTQ_rights_and_issues\">John McCain</a> was voicing vague support for \"legal agreements\" between same sex couples, while rejecting same-sex marriage. At the same time, he suggested the issue could be punted to the states. Meanwhile, <a href=\"http://en.wikipedia.org/wiki/Social_policy_of_Barack_Obama#Same-sex_marriage\">Obama's</a> position was only slightly more liberal: clearer support for civil unions (but again not full marriage equality), and similar suggestions that the issue could be left to the states.</p>\n<p>Four years later in 2012, Obama <em>finally </em>mentioned in an interview that he'd changed his mind and now supported same-sex marriage. By that time, figures from Rick Santorum to Rick Warren to Sarah Palin had&nbsp;<a href=\"http://slog.thestranger.com/slog/archives/2012/01/03/when-do-we-get-to-meet-elizabeth-santorums-imaginary-gay-friends\">begun telling the press that they, too, have gay friends</a>.&nbsp;Since that time, the Obama administration has only taken modest concrete steps to support gay marriage: a narrowly-worded brief opposing California's Proposition 8, a decision not to defend the Defense of Marriage Act in court, and that's about it.</p>\n<p>From the point of view of the median voter model, the way to explain both the behavior of liberals like Obama and conservatives like McCain and Santorum is that both groups are trying to avoid straying very far from the position of the median voter, so as to not alienate them and lose their vote. It's significant that in 2008, the <a href=\"http://en.wikipedia.org/wiki/Public_opinion_of_same-sex_marriage_in_the_United_States\">polls</a> showed that public opinion was roughly divided into thirds on gay marriage, with about a third totally opposed, a third supporting civil unions, and a third supporting full marriage equality. Obama's announcement that he supported gay marriage came <em>after </em>numerous polls showed 50-some percent of Americans supporting gay marriage.</p>\n<p>Some readers may be wondering how this analysis fits with <a href=\"http://slatestarcodex.com/2013/09/21/the-thin-blue-line-that-stays-bizarrely-horizontal/\">the current polarization in Congress</a>.&nbsp;The answer is, \"perfectly.\" The median voter theorem leads us to expect that politicians running against each other should adopt similar views, but even in its most idealized form, it says nothing about members of the same legislature should have similar views. In fact, it <em>predicts </em>polarized legislatures in situations where (1) members of the legislature are elected by geographic region and (2) the electorate itself is polarized by geographic region.</p>\n<p>This is what we see in the US, where a big-city congressional district can be <em>much </em>more liberal than a rural one. Many members of the House of Representatives probably have <em>more </em>to worry about from a more-extreme primary challenger within their own party than from a general election challenger from the other party.&nbsp;Caveat: I've tried looking up data on the voting records of various House members, and while there's clearly a correlation between the tendencies of their respective districts, the correlation is not as strong as I expected. I'd be curious to hear if anyone out there knows more about this issue of polarization and geography.&nbsp;</p>\n<h2>Voting systems, voting strategies, and knowing your fellow voters</h2>\n<p>So elections in the US may not offer voters much choice, but that's better explained by the median voter theorem than by politicians colluding against voters. Political science also provides a second objection to Eliezer's analysis of the two-party system in America:&nbsp;<a href=\"http://en.wikipedia.org/wiki/Duverger%27s_law\">Duverger's Law</a>,&nbsp;which says that in a system like ours (where everyone votes for one candidate and whoever gets the most votes wins), the system will tend to converge on having two main political parties, due to standard reasoning about not throwing your vote away. A corollary is that you can get a multiparty system by using <a href=\"http://en.wikipedia.org/wiki/Proportional_representation\">proportional representation</a>, which is used in many countries around the world including Spain, Portugal, Italy, Germany, and Israel.</p>\n<p>There are some apparent exceptions to Duverger's Law, such as Canada, which has long had a multiparty system in spite of using a voting system similar to that of the US. However, a friend of mine who follows Canadian politics tells me that what really happens in Canada isn't <em>that </em>far from what you would expect given Duverger's Law. Currently, the three largest parties are the Conservative Party, the New Democratic Party (NDP), and the Liberal Party. It used to be that the NDP was a relatively small party with positions well to the left of the Liberals, but this is no longer true. Instead of offering Canadian voters two different flavors of liberalism, the current situation is that in any given election for any given seat in parliament, the NDP candidate and the Liberal candidate put a lot of effort into arguing over who has the best chance of beating the Conservatives.<sup>1</sup></p>\n<p>So suppose you're an American or Canadian or British voter, looking at the major-party candidates in the next election, and finding that <em>none </em>of them are a good fit for your political views, what you should conclude? First, given that the median voter theorem is a pretty good model of how elections actually work, you should probably take your as evidence that your views are a good ways away from those of the median voter. And if the views of the voters are sufficiently varied, <em>a majority of voters could find themselves in the same position as you.</em></p>\n<p>In the flagpole problem at the start of this post, the only one student originally wanted the height that ends up winning. Actually, there's a subtle joke I left out of my paraphrase: the student who wanted 65 feet&nbsp;was Kathy, who elsewhere in the series was established as hating everyone and loving to see bad things happen. Or, to use the gay marriage example: in the 2008 election, the ~1/3 of voters who supported gay marriage didn't have a major party candidate who supported their views (and voters totally opposed to gay marriage and civil unions may not have been terribly happy with their choices either).</p>\n<p>To throw off the yoke of the existing major parties, it isn't enough for most voters to reject their platforms. They need to reject their platforms in more or less the same direction. In <a href=\"/lw/mi/stop_voting_for_nincompoops/\">\"Stop Voting for Nincompoops,\"</a> Eliezer mentions having anti-interventionist foreign policy views, and based on that, maybe he would say Obama is a nincompoop for being too interventionist, too willing to kill foreigners in the name of fighting terrorism. If so, I'd be sympathetic. But even if a majority of Americans <em>agreed </em>that Obama is a nincompoop, it wouldn't follow that they <em>agree he is a nincompoop for being too willing to kill foreigners in the name of fighting terrorism. </em>Many of them probably think he's a nincompoop for not doing nearly enough to fight terrorism, and maybe even being secretly on the side of the terrorists.<sup>2</sup></p>\n<p>That's because median voter analysis suggests that if none of the main candidates in an election are a good fit for your views, this is a sign that your views are a good ways from those of the median voter, and as a corollary there must be people out there whose views differ from the median voter's in the <em>opposite </em>direction, and therefore would seem even <em>more </em>repugnant to you. (Never forget that half the population is below average.)</p>\n<p>In \"Stop Voting for Nincompoops,\" Eliezer quotes from Douglas Adams' novel <em>So Long And Thanks For All The Fish:</em></p>\n<blockquote>\n<p>\"The leaders are lizards. &nbsp;The people hate the lizards and the lizards rule the people.\"</p>\n<p>\"Odd,\" said Arthur, \"I thought you said it was a democracy.\"</p>\n<p>\"I did,\" said Ford, \"It is.\"</p>\n<p>\"So,\" said Arthur, hoping he wasn't sounding ridiculously obtuse, \"why don't the people get rid of the lizards?\"</p>\n<p>\"It honestly doesn't occur to them,\" said Ford. \"They've all got the vote, so they all pretty much assume that the government they've voted in more or less approximates to the government they want.\"</p>\n<p>\"You mean they actually vote for the lizards?\"</p>\n<p>\"Oh yes,\" said Ford with a shrug, \"of course.\"</p>\n<p>\"But,\" said Arthur, going for the big one again, \"why?\"</p>\n<p>\"Because if they didn't vote for a lizard,\" said Ford, \"the wrong lizard might get in. Got any gin?\"</p>\n</blockquote>\n<p>In light of all the above, let me suggest a modified allegory: the people hate the lizards, and have thought of getting rid of them, but there's disagreement about what to do <em>after </em>getting rid of the lizards. Many people favor self-rule, but a very nearly equal number of people favor replacing the lizards with the Demon Acolytes of Yog-Sothoth. Since a few people actually like the lizards, and almost everyone agrees lizards are better than what those <em>other </em>people want, lizards are what they get.&nbsp;</p>\n<p>Of course, since <a href=\"/lw/i0/are_your_enemies_innately_evil/\">very few people consider themselves villains</a>,&nbsp;to make the story as realistic as possible, we should imagine that the partisans of Demon Acolytes believe the demons are actually Angels of the Light, and that anyone prideful enough to think autonomy is better than being ruled by angels must be profoundly wicked. Either way, the point is that widespread dislike of the current political situation does not imply widespread support for any particular alternative.</p>\n<p>Moving back to the real world again, here's an explanation for US foreign policy under both Bush II and Obama, which I suspect Eliezer would think too cynical, but which I'll mention anyway: maybe the reason the US government is so quick to kill foreigners in the name of fighting terrorism is because the median voter fears terrorism more than they care about the lives of foreigners. I suppose you could argue it isn't so, and the real reason is the median voter doesn't know what impact US foreign policy has on foreigners, but if they cared to know, couldn't they start paying less attention to CNN and more to Al-Jazeera?</p>\n<p>Given all this, how should you vote? Well, you shouldn't vote for a third party candidate <em>because </em>you think a lot of our problems could be solved if everyone just simultaneously resolved to never vote for (anyone they believed to be) a nincompoop. If somehow you actually manage to persuade people to everyone to adopt that policy, don't be surprised if disagreements about who the nincompoops are result in nothing really changing, or worse result in a bunch bizarre elections decided by small pluralities.</p>\n<p>Beyond that though, I'm not actually sure what the proper strategy is. In spite of everything I've said, maybe the \"vote third party to send a message\" argument is (sometimes) right. Or maybe there's something to be said for the argument that your vote isn't going to make a difference anyway so you may as well do whatever makes you feel good. So far in my relatively short time as a voter, I've adopted a mixed approach, protest-voting in my two presidential elections but voting for major-party candidates otherwise. But I'm honestly not sure what I'll do in the future. Maybe a <a href=\"/lw/2ur/probability_and_politics/\">seemingly-infinitesimal chance</a> of affecting the election outcome is <a href=\"/lw/fao/voting_is_like_donating_thousands_of_dollars_to/\">worth it</a>.</p>\n<p>That is not a very exciting way to end an essay this long. Which is why I'm happy to report that that is not how I'm ending this essay, and in fact have been building up to a different general point.</p>\n<h2>A nerd failure mode regarding human affairs</h2>\n<p>So at last, I'm ready to explain what I think the broader nerd failure mode here is: they have a tendency to notice that people are failing to behave optimally and then propose, as a solution to this problem, that people switch to behaving optimally.</p>\n<p>This is related to, if not quite the same as, the problem Randall Munroe pokes at <a href=\"http://xkcd.com/592/\">here</a>. The problem is that if you don't first make a serious effort to figure out <em>why </em>people are failing to behave optimally, that can get in the way of figuring out what a better course of action would be. And it makes it almost impossible to figure out how to get people to <em>actually follow</em> the better course of action.</p>\n<p>If the reason people elect bad leaders is that half the people have views even crazier than those of the leaders they elect, you will not make much progress changing things if you think the problem is a two-party conspiracy against the voters. Or, if you to get people to stop voting for nincompoops, convincing them they should never vote for nincompoops may give you a very different result than you were expecting if they have different ideas from you about who the nincompoops are and what it is about them that qualifies them as nincompoops.</p>\n<p>Many readers of LessWrong will have heard of Chesterton's fence already, but let me quote Chesterton's original words at somewhat greater length than is usual:</p>\n<blockquote>\n<p>In the matter of reforming things, as distinct from deforming them,&nbsp;there is one plain and simple principle; a principle which will probably&nbsp;be called a paradox. There exists in such a case a certain institution&nbsp;or law; let us say for the sake of simplicity, a fence or gate erected&nbsp;across a road. &nbsp;The more modern type of reformer goes gaily up to it&nbsp;and says, \"I don't see the use of this; let us clear it away.\"&nbsp;To which the more intelligent type of reformer will do well to answer:&nbsp;\"If you don't see the use of it, I certainly won't let you clear it away.&nbsp;Go away and think. &nbsp;Then, when you can come back and tell me that you&nbsp;do see the use of it, I may allow you to destroy it.\"</p>\n<p>This paradox rests on the most elementary common sense.&nbsp;The gate or fence did not grow there. &nbsp;It was not set up by somnambulists&nbsp;who built it in their sleep. &nbsp;It is highly improbable that it was put&nbsp;there by escaped lunatics who were for some reason loose in the street.&nbsp;Some person had some reason for thinking it would be a good&nbsp;thing for somebody. &nbsp;And until we know what the reason was,&nbsp;we really cannot judge whether the reason was reasonable.&nbsp;It is extremely probable that we have overlooked some whole&nbsp;aspect of the question, if something set up by human beings&nbsp;like ourselves seems to be entirely meaningless and mysterious.&nbsp;There are reformers who get over this difficulty by assuming&nbsp;that all their fathers were fools; but if that be so,&nbsp;we can only say that folly appears to be a hereditary disease.&nbsp;But the truth is that nobody has any business to destroy a social&nbsp;institution until he has really seen it as an historical institution.&nbsp;If he knows how it arose, and what purposes it was supposed to serve,&nbsp;he may really be able to say that they were bad purposes, or that&nbsp;they have since become bad purposes, or that they are purposes&nbsp;which are no longer served. &nbsp;But if he simply stares at the thing&nbsp;as a senseless monstrosity that has somehow sprung up in his path,&nbsp;it is he and not the traditionalist who is suffering from an illusion.</p>\n</blockquote>\n<p>In spite of being a conservative Catholic apologist, what Chesterton is saying here isn't crazy. Certainly it helps to know what people's reasons for something were before trying to judge whether they were good ones. I wouldn't go quite as far as Chesterton, since sometimes there's such good evidence something's a bad idea that you can reject it without knowing what people were originally thinking.</p>\n<p>But even on much weaker assumptions than Chesterton's, something in the vicinity turns out to be good advice. Even if the fence <em>was </em>built by lunatics, that's worth knowing. It's especially worth knowing whether they're still out there, and whether they're likely to try to rebuild the fence after it's been taken down. If they are likely to try that, you need to know so they can be recaptured before taking the fence down, so that the lunatics don't just rebuild it, making the taking-down a waste of effort.</p>\n<h2>Notes</h2>\n<ol>\n<li>Someone might read this and conclude that, since the two-party system is so awful, and Duverger's Law implies it's a necessary result of our voting system, shouldn't we switch voting systems to something like proportional representation? I'm willing to believe that other systems might be&nbsp;<em>slightly&nbsp;</em>better than what we have in the US. Countries that use proportional representation tend to have higher voter turnout, though it's unclear whether the one causes the other. But does anyone think that proportional representation and more major parties makes, say, Germany's government&nbsp;<em>that much&nbsp;</em>better than the UK's? For more on voting systems, see Yvain's summary of&nbsp;<a href=\"/lw/dp6/imperfect_voting_systems/\">why no voting system is perfect</a>.</li>\n<li>\n<p>Some people reading this might be skeptical of the idea many people would believe something as crazy-sounding as \"Obama is secretly on the side of the terrorists.\" While I think we should be careful about&nbsp;<a href=\"http://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/\">phantom lizardmen</a>&nbsp;and partisan media <a href=\"http://www.patheos.com/blogs/hallq/2013/02/bs-outrage-and-the-atheist-blogosphere/\">selectively reporting on the other side's crazies to gin up outrage</a>, sadly, from what I can tell there genuinely are a large number of people out there who believe such right-wing conspiracy theories about Obama. I'm not trying to make a partisan point here, and say this with full awareness of things like 9/11 conspiracy theories on the left.&nbsp;</p>\n<p>Remember, first, that hardly any of us come into contact with a random sampling of our fellow voters on a daily basis. Furthermore, I grew up in a smallish (pop. ~60k), conservative-leaning town, and occasionally people I barely interacted with in high school will friend me on Facebook, I'll accept because why not, and then I'll start getting their thoughts on politics in my Facebook feed. That may give me a somewhat clearer perspective on this than the averge resident of a liberal big city. I remember when the NSA scandal broke and one girl posted a status update which, while containing civil-libertarian thoughts that I approved of, also contained references to Obama being an illegal president (because, as far as I could tell, birtherism), as well as a reference to Obama's \"terroristic ways,\" whatever that means.</p>\n</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"mPuSAzJN7CyrMiKrf": 4, "mmfk47obrNKhN6waD": 1, "FkzScn5byCs9PxGsA": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CcC8MocynqKPmMPwL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 105, "baseScore": 140, "extendedScore": null, "score": 0.000354, "legacy": true, "legacyId": "24356", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 140, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>In grade school, I read a series of books titled <em>Sideways Stories from Wayside School </em>by Louis Sachar, who you may know as the author of the novel <em>Holes</em> which was made into a movie in 2003. The series included two books of math problems, <em>Sideways Arithmetic from Wayside School </em>and <em>More Sideways Arithmetic from Wayside School, </em>the latter of which included the following problem (paraphrased):</p>\n<p>The students have Mrs. Jewl's class have been given the privilege of voting on the height of the school's new flagpole. She has each of them write down what they think would be the best hight for the flagpole. The votes are distributed as follows:</p>\n<ul>\n<li>1 student votes for 6 feet.</li>\n<li>1 student votes for 10 feet.</li>\n<li>7 students vote for 25 feet.</li>\n<li>1 student votes for 30 feet.</li>\n<li>2 students vote for 50 feet.</li>\n<li>2 students vote for 60 feet.</li>\n<li>1 student votes for 65 feet.</li>\n<li>3 students vote for 75 feet.</li>\n<li>1 student votes for 80 feet, 6 inches.</li>\n<li>4 students vote for 85 feet.</li>\n<li>1 student votes for 91 feet.</li>\n<li>5 students vote for 100 feet.</li>\n</ul>\n<p>At first, Mrs. Jewls declares 25 feet the winning answer, but one of the students who voted for 100 feet convinces her there should be a runoff between 25 feet and 100 feet. In the runoff, each student votes for the height closest to their original answer. But after that round of voting, one of the students who voted for 85 feet wants <em>their </em>turn, so 85 feet goes up against the winner of the <em>previous </em>round of voting, and the students vote the same way, with each student voting for the height closest to their original answer. Then the same thing happens again with the 50 foot option. And so on, with each number, again and again, \"very much like a game of tether ball.\"</p>\n<p>Question: if this process continues until it settles on an answer that can't be beaten by any other answer, how tall will the new flagpole be?</p>\n<p>Answer (<a href=\"http://rot13.com/index.php\">rot13'd</a>):&nbsp;fvkgl-svir srrg, orpnhfr gung'f gur zrqvna inyhr bs gur bevtvany frg bs ibgrf.&nbsp;Naq abj lbh xabj gur fgbel bs zl svefg rapbhagre jvgu gur zrqvna ibgre gurberz.</p>\n<p>Why am I telling you this? There's a minor reason and a major reason. The minor reason is that this shows it is possible to explain little-known academic concepts, at least certain ones, in a way that grade schoolers will understand. It's a data point that fits nicely with <a href=\"/lw/kh/explainers_shoot_high_aim_low/\">what Eliezer has written about how to explain things</a>.&nbsp;The major reason, though, is that a month ago I finished my systematic read-through of <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">the sequences</a>&nbsp;and while I generally agree that they're awesome (perhaps moreso than most people; I didn't see the problem with the metaethics sequence), I thought the mini-discussion of <a href=\"/lw/mg/the_twoparty_swindle/\">political</a> <a href=\"/lw/mh/the_american_system_and_misleading_labels/\">parties</a> and <a href=\"/lw/mi/stop_voting_for_nincompoops/\">voting</a> was on reflection weak and indicative of a broader nerd failure mode.</p>\n<p>TLDR (courtesy of <a href=\"/lw/isk/a_voting_puzzle_some_political_science_and_a_nerd/9vgk\">lavalamp</a>):</p>\n<ol>\n<li>Politicians probably conform to the median voter's views.</li>\n<li>Most voters are not the median, so most people usually dislike the winning politicians.</li>\n<li>But people dislike the politicians for different reasons.</li>\n<li>Nerds should avoid giving advice that boils down to \"behave optimally\". Instead, analyze the reasons for the current failure to behave optimally and give more targeted advice.</li>\n</ol>\n<p><a id=\"more\"></a></p>\n<p>Advance warning for heavy US slant, at least in terms of examples, though the theory is applicable everywhere.</p>\n<h2 id=\"The_median_voter_theorem\">The median voter theorem</h2>\n<p>The median voter theorem was first laid out in a paper by Duncan Black titled <a href=\"http://www.corwin.com/upm-data/24492_Dowding_Chapter_01.pdf\">\"On the Rationale of Group Decision-Making,\"</a>&nbsp;which imagine's a situation very much like Mrs. Jewls' class voting on the flagpole height: a committee passes a motion by majority vote, and then it considers various motions to amend the original motion, each of which itself needs a simple majority to pass. Each member of the committee has preferences over the range of possible motions, and furthermore:</p>\n<blockquote>\n<p>While a member\u2019s preference curve may be of any shape whatever, there is reason to expect that, in some important practical problems, the valuations actually carried out will tend to take the form of isolated points on single-peaked curves. This would be particularly likely to happen were the committee considering different possible sizes of a numerical quantity and choosing one size in preference to the others. It might be reaching a decision, say, with regard to the price of a product to be marketed by a firm, or the output for a future period, or the wage rate of labor, or the height of a particular tax, or the legal school-leaving age, and so on.</p>\n</blockquote>\n<p>Or, for that matter, the height of a flagpole. Black shows that on his assumptions, the committee will eventually settle on the version of the motion favored by the median committee member.</p>\n<p>Again, you may be asking, so what? Most people don't care about understanding the behavior of committees, especially not compared to their passion for national presidential elections. And elections for political office don't use a tether ball-like system of having head-to-head matchup after head-to-head matchup until you've finally found the candidate the median voter wants. There's one election with two (or if you're lucky, three) major candidates and that's it.</p>\n<p>The relevance to electoral politics comes in when you allow for the possibility of candidates shaping themselves and their platforms to appeal to the median voter. The candidate who does this should be invincible - at least, until the other candidate does the same thing, at which point the election becomes a closer call. The idea of candidates shaping themselves to voter preferences is key; I started off this post with the flagpole example partly to emphasize that. And there are other assumptions you have to make to get to the conclusion that candidates will <em>actually </em>behave this way.</p>\n<p>But before we get in to that, let's compare the median voter picture to the picture Eliezer put forward in the posts linked above:</p>\n<blockquote>\n<p>Forget that Congresspeople on both sides of the \"divide\" are more likely to be lawyers than truck drivers. &nbsp;Forget that in training and in daily life, they have far more in common with each other than they do with a randomly selected US citizen from their own party. Forget that they are more likely to hang out at each other's expensive hotel rooms than drop by your own house. &nbsp;Is there a political divide - a divide of policies and interests - between Professional Politicians on the one hand, and Voters on the other?</p>\n<p>Well, let me put it this way. &nbsp;Suppose that you happen to be socially liberal, fiscally conservative. &nbsp;Who would you vote for?</p>\n<p>Or simplify it further: &nbsp;Suppose that you're a voter who prefers a smaller, less expensive government - should you vote Republican or Democratic? &nbsp;Or, lest I be accused of color favoritism, suppose that your voter preference is to get US troops out of Iraq. &nbsp;Should you vote Democratic or Republican?</p>\n<p>One needs to be careful, at this point, to keep track of the distinction between marketing materials and historical records. &nbsp;I'm not asking which political party stands for the idea of smaller government - which football team has \"Go go smaller government! &nbsp;Go go go!\" as one of its cheers. &nbsp;(Or \"Troops out of Iraq! &nbsp;Yay!\") &nbsp;Rather, over the last several decades, among Republican politicians and Democratic politicians, which group of Professional Politicians shrunk the government while it was in power?</p>\n<p>And by \"shrunk\" I mean \"shrunk\". &nbsp;If you're suckered into an angry, shouting fight over whether Your Politicians or Their Politicians grew the government slightly less slowly, it means you're not seeing the divide between Politicians and Voters. There isn't a grand conspiracy to expand the government, but there's an incentive for each individual politician to send pork to campaign contributors, or borrow today against tomorrow's income. &nbsp;And that creates a divide between the Politicians and the Voters, as a class, for reasons that have nothing to do with colors and slogans.</p>\n</blockquote>\n<p>Eliezer observes that there doesn't seem to be much difference between the two parties, and concludes that they are colluding (albeit probably not by explicit agreement) to advance their own interests at the expense of the voters'. The politicians don't offer the voters any real choice, but get voters to vote for them anyway though misleading party labels and the argument that, if they don't vote for a major-party candidate, they're \"throwing their vote away.\"</p>\n<p>However, the observation that there doesn't seem to be much difference between the two parties can <em>also </em>be explained by the hypothesis that politicians are shaping themselves to appeal to the median voter. This fact alone doesn't show that the median voter model is <em>right... </em>but it does show that the mere fact of there not being much difference between the two parties doesn't show the \"colluding politicians\" model is right <em>either.</em></p>\n<p>So how well does the median voter theorem capture reality? One problem for the model is that it potentially breaks down if the choices don't fit onto a nice, linear spectrum. Suppose, for the sake of a simplified example, that only three people vote in a particular presidential election. Suppose, furthermore, that the three voters have the following set of preferences:</p>\n<ul>\n<li>Alice prefers Obama to Romney, and Romney to Ron Paul</li>\n<li>Bob prefers Romney to Ron Paul, and Ron Paul to Obama</li>\n<li>Carol prefers Ron Paul to Obama, and Obama to Romney</li>\n</ul>\n<p>Given this set of voters and their preferences, in an Obama vs. Romney contest, Obama will win; in a Romney vs. Ron Paul contest, Romney will win; but in a Ron Paul vs. Obama contest, Ron Paul will win.</p>\n<p>However, the median voter theorem seems to be a pretty good model in practice in spite of such problems. Roger Congleton, in <a href=\"http://rdc1.net/forthcoming/medianvt.pdf\">an article in the </a><em><a href=\"http://rdc1.net/forthcoming/medianvt.pdf\">Encyclopedia of Public Choice</a>,&nbsp;</em>writes:</p>\n<blockquote>\n<p>Although theoretical arguments suggest that the applicability of the median voter model&nbsp;may be very limited, the empirical evidence suggests otherwise. There is a large body of evidence that suggests median voter preferences over policies are (largely) of the sort which can be mapped into a single issue space while retaining \"single peakedness\" Poole and Daniels (1985) find that 80-90% of all the recorded votes in the US Congress can be explained with a one dimensional policy space. Stratmann (1996) finds little evidence of cycling across Congressional votes over district specific grants.</p>\n<p>Moreover, the median voter model has a very good empirical track record in public finance as a model of fiscal policy across states and through time. Recent studies show that the median voter model can explain federal, state, and local spending, as well as international tariff policies. Congleton and Shughart (1990) Congleton and Bennett (1995) suggest that the median voter model provides a better explanation of large scale public programs than comparable interest group models. This is not to suggest that the median voter always exercises the same degree of control over public policy irrespective of political institutions. Holcombe (1980) and Frey (1994) report significant policy difference between representative and direct forms of democracy that would not exist unless significant agency problems exist within representative government. Moreover, statistical tests can never prove that a particular model is correct, only that it is more likely to be correct than false. However, in general, the median voter model appears to be quite robust as a model of public policy formation in areas where the median voter can credibly be thought to understand and care about public policy.</p>\n<p>The empirical evidence suggests that the median voter model can serve as a very useful first approximation of governance within democratic polities. As a consequence, the median voter model continues to function as an analytical point of departure for more elaborate models of policy formation within democracies in much the same way that the competitive model serves the micro economics literature.</p>\n</blockquote>\n<p>In the American political system, the effect of the median voter theorem is blunted somewhat by the primary system. It's a commonplace among American political commentators that politicians must appeal to the \"base\" during the primaries, then swing towards the center for the general election. Of course, politicians can't suddenly become <em>perfectly </em>centrist once they secure their party's nomination; their swing towards the center has to be done in a way that's at least superficially consistent with their previous pandering to their base.&nbsp;These observations suggest that, while reality doesn't perfectly match the idealized model, there's still a lot of truth to it.&nbsp;</p>\n<p>(Note: a site search for previous discussion of the median voter theorem on LessWrong turned up a <a href=\"/lw/2qq/politics_as_charity/2o6a\">comment</a> by Carl Shulman that mentioned \"the need to motivate one's base to vote/volunteer/contribute the ideological lumpiness\" as probably having an effect similar to the effect of primaries. I wouldn't have thought they were as important as primaries but I can believe Carl here.)</p>\n<p>The tendency of politicians to position themselves wherever the center of public opinion is currently at can be striking on specific issues. For example, public support for gay rights has increased greatly in the past two decades. In that time period, positions which once got Bill Clinton demonized by the religious right as an agent of the homosexual agenda (like Don't Ask Don't Tell) became the \"conservative\" position. Progress, but in terms of the public stances of politicians, it's progress that came not in the form of dramatic shifts but cautious adjustments.</p>\n<p>By the time of the 2008 campaign, Republican nominee <a href=\"http://en.wikipedia.org/wiki/Political_positions_of_John_McCain#LGBTQ_rights_and_issues\">John McCain</a> was voicing vague support for \"legal agreements\" between same sex couples, while rejecting same-sex marriage. At the same time, he suggested the issue could be punted to the states. Meanwhile, <a href=\"http://en.wikipedia.org/wiki/Social_policy_of_Barack_Obama#Same-sex_marriage\">Obama's</a> position was only slightly more liberal: clearer support for civil unions (but again not full marriage equality), and similar suggestions that the issue could be left to the states.</p>\n<p>Four years later in 2012, Obama <em>finally </em>mentioned in an interview that he'd changed his mind and now supported same-sex marriage. By that time, figures from Rick Santorum to Rick Warren to Sarah Palin had&nbsp;<a href=\"http://slog.thestranger.com/slog/archives/2012/01/03/when-do-we-get-to-meet-elizabeth-santorums-imaginary-gay-friends\">begun telling the press that they, too, have gay friends</a>.&nbsp;Since that time, the Obama administration has only taken modest concrete steps to support gay marriage: a narrowly-worded brief opposing California's Proposition 8, a decision not to defend the Defense of Marriage Act in court, and that's about it.</p>\n<p>From the point of view of the median voter model, the way to explain both the behavior of liberals like Obama and conservatives like McCain and Santorum is that both groups are trying to avoid straying very far from the position of the median voter, so as to not alienate them and lose their vote. It's significant that in 2008, the <a href=\"http://en.wikipedia.org/wiki/Public_opinion_of_same-sex_marriage_in_the_United_States\">polls</a> showed that public opinion was roughly divided into thirds on gay marriage, with about a third totally opposed, a third supporting civil unions, and a third supporting full marriage equality. Obama's announcement that he supported gay marriage came <em>after </em>numerous polls showed 50-some percent of Americans supporting gay marriage.</p>\n<p>Some readers may be wondering how this analysis fits with <a href=\"http://slatestarcodex.com/2013/09/21/the-thin-blue-line-that-stays-bizarrely-horizontal/\">the current polarization in Congress</a>.&nbsp;The answer is, \"perfectly.\" The median voter theorem leads us to expect that politicians running against each other should adopt similar views, but even in its most idealized form, it says nothing about members of the same legislature should have similar views. In fact, it <em>predicts </em>polarized legislatures in situations where (1) members of the legislature are elected by geographic region and (2) the electorate itself is polarized by geographic region.</p>\n<p>This is what we see in the US, where a big-city congressional district can be <em>much </em>more liberal than a rural one. Many members of the House of Representatives probably have <em>more </em>to worry about from a more-extreme primary challenger within their own party than from a general election challenger from the other party.&nbsp;Caveat: I've tried looking up data on the voting records of various House members, and while there's clearly a correlation between the tendencies of their respective districts, the correlation is not as strong as I expected. I'd be curious to hear if anyone out there knows more about this issue of polarization and geography.&nbsp;</p>\n<h2 id=\"Voting_systems__voting_strategies__and_knowing_your_fellow_voters\">Voting systems, voting strategies, and knowing your fellow voters</h2>\n<p>So elections in the US may not offer voters much choice, but that's better explained by the median voter theorem than by politicians colluding against voters. Political science also provides a second objection to Eliezer's analysis of the two-party system in America:&nbsp;<a href=\"http://en.wikipedia.org/wiki/Duverger%27s_law\">Duverger's Law</a>,&nbsp;which says that in a system like ours (where everyone votes for one candidate and whoever gets the most votes wins), the system will tend to converge on having two main political parties, due to standard reasoning about not throwing your vote away. A corollary is that you can get a multiparty system by using <a href=\"http://en.wikipedia.org/wiki/Proportional_representation\">proportional representation</a>, which is used in many countries around the world including Spain, Portugal, Italy, Germany, and Israel.</p>\n<p>There are some apparent exceptions to Duverger's Law, such as Canada, which has long had a multiparty system in spite of using a voting system similar to that of the US. However, a friend of mine who follows Canadian politics tells me that what really happens in Canada isn't <em>that </em>far from what you would expect given Duverger's Law. Currently, the three largest parties are the Conservative Party, the New Democratic Party (NDP), and the Liberal Party. It used to be that the NDP was a relatively small party with positions well to the left of the Liberals, but this is no longer true. Instead of offering Canadian voters two different flavors of liberalism, the current situation is that in any given election for any given seat in parliament, the NDP candidate and the Liberal candidate put a lot of effort into arguing over who has the best chance of beating the Conservatives.<sup>1</sup></p>\n<p>So suppose you're an American or Canadian or British voter, looking at the major-party candidates in the next election, and finding that <em>none </em>of them are a good fit for your political views, what you should conclude? First, given that the median voter theorem is a pretty good model of how elections actually work, you should probably take your as evidence that your views are a good ways away from those of the median voter. And if the views of the voters are sufficiently varied, <em>a majority of voters could find themselves in the same position as you.</em></p>\n<p>In the flagpole problem at the start of this post, the only one student originally wanted the height that ends up winning. Actually, there's a subtle joke I left out of my paraphrase: the student who wanted 65 feet&nbsp;was Kathy, who elsewhere in the series was established as hating everyone and loving to see bad things happen. Or, to use the gay marriage example: in the 2008 election, the ~1/3 of voters who supported gay marriage didn't have a major party candidate who supported their views (and voters totally opposed to gay marriage and civil unions may not have been terribly happy with their choices either).</p>\n<p>To throw off the yoke of the existing major parties, it isn't enough for most voters to reject their platforms. They need to reject their platforms in more or less the same direction. In <a href=\"/lw/mi/stop_voting_for_nincompoops/\">\"Stop Voting for Nincompoops,\"</a> Eliezer mentions having anti-interventionist foreign policy views, and based on that, maybe he would say Obama is a nincompoop for being too interventionist, too willing to kill foreigners in the name of fighting terrorism. If so, I'd be sympathetic. But even if a majority of Americans <em>agreed </em>that Obama is a nincompoop, it wouldn't follow that they <em>agree he is a nincompoop for being too willing to kill foreigners in the name of fighting terrorism. </em>Many of them probably think he's a nincompoop for not doing nearly enough to fight terrorism, and maybe even being secretly on the side of the terrorists.<sup>2</sup></p>\n<p>That's because median voter analysis suggests that if none of the main candidates in an election are a good fit for your views, this is a sign that your views are a good ways from those of the median voter, and as a corollary there must be people out there whose views differ from the median voter's in the <em>opposite </em>direction, and therefore would seem even <em>more </em>repugnant to you. (Never forget that half the population is below average.)</p>\n<p>In \"Stop Voting for Nincompoops,\" Eliezer quotes from Douglas Adams' novel <em>So Long And Thanks For All The Fish:</em></p>\n<blockquote>\n<p>\"The leaders are lizards. &nbsp;The people hate the lizards and the lizards rule the people.\"</p>\n<p>\"Odd,\" said Arthur, \"I thought you said it was a democracy.\"</p>\n<p>\"I did,\" said Ford, \"It is.\"</p>\n<p>\"So,\" said Arthur, hoping he wasn't sounding ridiculously obtuse, \"why don't the people get rid of the lizards?\"</p>\n<p>\"It honestly doesn't occur to them,\" said Ford. \"They've all got the vote, so they all pretty much assume that the government they've voted in more or less approximates to the government they want.\"</p>\n<p>\"You mean they actually vote for the lizards?\"</p>\n<p>\"Oh yes,\" said Ford with a shrug, \"of course.\"</p>\n<p>\"But,\" said Arthur, going for the big one again, \"why?\"</p>\n<p>\"Because if they didn't vote for a lizard,\" said Ford, \"the wrong lizard might get in. Got any gin?\"</p>\n</blockquote>\n<p>In light of all the above, let me suggest a modified allegory: the people hate the lizards, and have thought of getting rid of them, but there's disagreement about what to do <em>after </em>getting rid of the lizards. Many people favor self-rule, but a very nearly equal number of people favor replacing the lizards with the Demon Acolytes of Yog-Sothoth. Since a few people actually like the lizards, and almost everyone agrees lizards are better than what those <em>other </em>people want, lizards are what they get.&nbsp;</p>\n<p>Of course, since <a href=\"/lw/i0/are_your_enemies_innately_evil/\">very few people consider themselves villains</a>,&nbsp;to make the story as realistic as possible, we should imagine that the partisans of Demon Acolytes believe the demons are actually Angels of the Light, and that anyone prideful enough to think autonomy is better than being ruled by angels must be profoundly wicked. Either way, the point is that widespread dislike of the current political situation does not imply widespread support for any particular alternative.</p>\n<p>Moving back to the real world again, here's an explanation for US foreign policy under both Bush II and Obama, which I suspect Eliezer would think too cynical, but which I'll mention anyway: maybe the reason the US government is so quick to kill foreigners in the name of fighting terrorism is because the median voter fears terrorism more than they care about the lives of foreigners. I suppose you could argue it isn't so, and the real reason is the median voter doesn't know what impact US foreign policy has on foreigners, but if they cared to know, couldn't they start paying less attention to CNN and more to Al-Jazeera?</p>\n<p>Given all this, how should you vote? Well, you shouldn't vote for a third party candidate <em>because </em>you think a lot of our problems could be solved if everyone just simultaneously resolved to never vote for (anyone they believed to be) a nincompoop. If somehow you actually manage to persuade people to everyone to adopt that policy, don't be surprised if disagreements about who the nincompoops are result in nothing really changing, or worse result in a bunch bizarre elections decided by small pluralities.</p>\n<p>Beyond that though, I'm not actually sure what the proper strategy is. In spite of everything I've said, maybe the \"vote third party to send a message\" argument is (sometimes) right. Or maybe there's something to be said for the argument that your vote isn't going to make a difference anyway so you may as well do whatever makes you feel good. So far in my relatively short time as a voter, I've adopted a mixed approach, protest-voting in my two presidential elections but voting for major-party candidates otherwise. But I'm honestly not sure what I'll do in the future. Maybe a <a href=\"/lw/2ur/probability_and_politics/\">seemingly-infinitesimal chance</a> of affecting the election outcome is <a href=\"/lw/fao/voting_is_like_donating_thousands_of_dollars_to/\">worth it</a>.</p>\n<p>That is not a very exciting way to end an essay this long. Which is why I'm happy to report that that is not how I'm ending this essay, and in fact have been building up to a different general point.</p>\n<h2 id=\"A_nerd_failure_mode_regarding_human_affairs\">A nerd failure mode regarding human affairs</h2>\n<p>So at last, I'm ready to explain what I think the broader nerd failure mode here is: they have a tendency to notice that people are failing to behave optimally and then propose, as a solution to this problem, that people switch to behaving optimally.</p>\n<p>This is related to, if not quite the same as, the problem Randall Munroe pokes at <a href=\"http://xkcd.com/592/\">here</a>. The problem is that if you don't first make a serious effort to figure out <em>why </em>people are failing to behave optimally, that can get in the way of figuring out what a better course of action would be. And it makes it almost impossible to figure out how to get people to <em>actually follow</em> the better course of action.</p>\n<p>If the reason people elect bad leaders is that half the people have views even crazier than those of the leaders they elect, you will not make much progress changing things if you think the problem is a two-party conspiracy against the voters. Or, if you to get people to stop voting for nincompoops, convincing them they should never vote for nincompoops may give you a very different result than you were expecting if they have different ideas from you about who the nincompoops are and what it is about them that qualifies them as nincompoops.</p>\n<p>Many readers of LessWrong will have heard of Chesterton's fence already, but let me quote Chesterton's original words at somewhat greater length than is usual:</p>\n<blockquote>\n<p>In the matter of reforming things, as distinct from deforming them,&nbsp;there is one plain and simple principle; a principle which will probably&nbsp;be called a paradox. There exists in such a case a certain institution&nbsp;or law; let us say for the sake of simplicity, a fence or gate erected&nbsp;across a road. &nbsp;The more modern type of reformer goes gaily up to it&nbsp;and says, \"I don't see the use of this; let us clear it away.\"&nbsp;To which the more intelligent type of reformer will do well to answer:&nbsp;\"If you don't see the use of it, I certainly won't let you clear it away.&nbsp;Go away and think. &nbsp;Then, when you can come back and tell me that you&nbsp;do see the use of it, I may allow you to destroy it.\"</p>\n<p>This paradox rests on the most elementary common sense.&nbsp;The gate or fence did not grow there. &nbsp;It was not set up by somnambulists&nbsp;who built it in their sleep. &nbsp;It is highly improbable that it was put&nbsp;there by escaped lunatics who were for some reason loose in the street.&nbsp;Some person had some reason for thinking it would be a good&nbsp;thing for somebody. &nbsp;And until we know what the reason was,&nbsp;we really cannot judge whether the reason was reasonable.&nbsp;It is extremely probable that we have overlooked some whole&nbsp;aspect of the question, if something set up by human beings&nbsp;like ourselves seems to be entirely meaningless and mysterious.&nbsp;There are reformers who get over this difficulty by assuming&nbsp;that all their fathers were fools; but if that be so,&nbsp;we can only say that folly appears to be a hereditary disease.&nbsp;But the truth is that nobody has any business to destroy a social&nbsp;institution until he has really seen it as an historical institution.&nbsp;If he knows how it arose, and what purposes it was supposed to serve,&nbsp;he may really be able to say that they were bad purposes, or that&nbsp;they have since become bad purposes, or that they are purposes&nbsp;which are no longer served. &nbsp;But if he simply stares at the thing&nbsp;as a senseless monstrosity that has somehow sprung up in his path,&nbsp;it is he and not the traditionalist who is suffering from an illusion.</p>\n</blockquote>\n<p>In spite of being a conservative Catholic apologist, what Chesterton is saying here isn't crazy. Certainly it helps to know what people's reasons for something were before trying to judge whether they were good ones. I wouldn't go quite as far as Chesterton, since sometimes there's such good evidence something's a bad idea that you can reject it without knowing what people were originally thinking.</p>\n<p>But even on much weaker assumptions than Chesterton's, something in the vicinity turns out to be good advice. Even if the fence <em>was </em>built by lunatics, that's worth knowing. It's especially worth knowing whether they're still out there, and whether they're likely to try to rebuild the fence after it's been taken down. If they are likely to try that, you need to know so they can be recaptured before taking the fence down, so that the lunatics don't just rebuild it, making the taking-down a waste of effort.</p>\n<h2 id=\"Notes\">Notes</h2>\n<ol>\n<li>Someone might read this and conclude that, since the two-party system is so awful, and Duverger's Law implies it's a necessary result of our voting system, shouldn't we switch voting systems to something like proportional representation? I'm willing to believe that other systems might be&nbsp;<em>slightly&nbsp;</em>better than what we have in the US. Countries that use proportional representation tend to have higher voter turnout, though it's unclear whether the one causes the other. But does anyone think that proportional representation and more major parties makes, say, Germany's government&nbsp;<em>that much&nbsp;</em>better than the UK's? For more on voting systems, see Yvain's summary of&nbsp;<a href=\"/lw/dp6/imperfect_voting_systems/\">why no voting system is perfect</a>.</li>\n<li>\n<p>Some people reading this might be skeptical of the idea many people would believe something as crazy-sounding as \"Obama is secretly on the side of the terrorists.\" While I think we should be careful about&nbsp;<a href=\"http://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/\">phantom lizardmen</a>&nbsp;and partisan media <a href=\"http://www.patheos.com/blogs/hallq/2013/02/bs-outrage-and-the-atheist-blogosphere/\">selectively reporting on the other side's crazies to gin up outrage</a>, sadly, from what I can tell there genuinely are a large number of people out there who believe such right-wing conspiracy theories about Obama. I'm not trying to make a partisan point here, and say this with full awareness of things like 9/11 conspiracy theories on the left.&nbsp;</p>\n<p>Remember, first, that hardly any of us come into contact with a random sampling of our fellow voters on a daily basis. Furthermore, I grew up in a smallish (pop. ~60k), conservative-leaning town, and occasionally people I barely interacted with in high school will friend me on Facebook, I'll accept because why not, and then I'll start getting their thoughts on politics in my Facebook feed. That may give me a somewhat clearer perspective on this than the averge resident of a liberal big city. I remember when the NSA scandal broke and one girl posted a status update which, while containing civil-libertarian thoughts that I approved of, also contained references to Obama being an illegal president (because, as far as I could tell, birtherism), as well as a reference to Obama's \"terroristic ways,\" whatever that means.</p>\n</li>\n</ol>", "sections": [{"title": "The median voter theorem", "anchor": "The_median_voter_theorem", "level": 1}, {"title": "Voting systems, voting strategies, and knowing your fellow voters", "anchor": "Voting_systems__voting_strategies__and_knowing_your_fellow_voters", "level": 1}, {"title": "A nerd failure mode regarding human affairs", "anchor": "A_nerd_failure_mode_regarding_human_affairs", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "181 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 181, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2TPph4EGZ6trEbtku", "qAJgWCWJJkke4mE8x", "ZXuqNhMDcs6mYtb6i", "k5qPoHFgjyxtvYsm7", "28bAMAxhoX3bwbAKC", "YafmHeLuxfRNRkgN2", "3kLjmvG4BaM6QrDnT", "xNBRkPNHAGQ6EQaLS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-10T22:28:48.005Z", "modifiedAt": null, "url": null, "title": "The selfish reason to write something for Ada Lovelace Day", "slug": "the-selfish-reason-to-write-something-for-ada-lovelace-day", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:59.032Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "sixes_and_sevens", "createdAt": "2009-11-11T14:42:23.502Z", "isAdmin": false, "displayName": "sixes_and_sevens"}, "userId": "n83meJ5yG2WQzygvw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sCNS6yatbpvtQRro3/the-selfish-reason-to-write-something-for-ada-lovelace-day", "pageUrlRelative": "/posts/sCNS6yatbpvtQRro3/the-selfish-reason-to-write-something-for-ada-lovelace-day", "linkUrl": "https://www.lesswrong.com/posts/sCNS6yatbpvtQRro3/the-selfish-reason-to-write-something-for-ada-lovelace-day", "postedAtFormatted": "Thursday, October 10th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20selfish%20reason%20to%20write%20something%20for%20Ada%20Lovelace%20Day&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20selfish%20reason%20to%20write%20something%20for%20Ada%20Lovelace%20Day%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsCNS6yatbpvtQRro3%2Fthe-selfish-reason-to-write-something-for-ada-lovelace-day%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20selfish%20reason%20to%20write%20something%20for%20Ada%20Lovelace%20Day%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsCNS6yatbpvtQRro3%2Fthe-selfish-reason-to-write-something-for-ada-lovelace-day", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsCNS6yatbpvtQRro3%2Fthe-selfish-reason-to-write-something-for-ada-lovelace-day", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 356, "htmlBody": "<p>Last October there was a <a href=\"/lw/ezb/happy_ada_lovelace_day/\">discussion post</a>&nbsp;on Ada Lovelace Day, and it met with something of a lukewarm reception. Fair enough. There are legitimate criticisms of this particular blogosphere event, and people are welcome to subscribe to those criticisms, or not, as they see fit. Personally, I'm quite fond of Ada Lovelace Day, in no small part because I get a chance to write about one of my nerdy interests in a public place with a reasonable expectation that a lay audience will attempt to engage with it. This year, the occasion falls on October 15th, and as a result I'm currently drafting a short piece on Esther Duflo, a development economist responsible for pioneering randomised controlled trials of policy interventions in developing countries. She's&nbsp;<a href=\"http://scholar.google.co.uk/scholar?q=esther+duflo\">rather prolific</a>, has a shelf full of academic awards, and is a <a href=\"http://marginalrevolution.com/marginalrevolution/2013/09/thomson-reuters-predicts-the-2013-nobel-laureate-in-economics.html\">hot tip</a> for a Nobel Memorial Prize over the next few years or so.</p>\n<p>So I was thinking about this: I get to talk about the importance of randomised controlled trials in policy-making; I get to talk about evidence-based philanthropy; I get to wrap it up with a <a href=\"http://flyingmoose.org/heman/mustard.htm\">don't-put-mustard-on-the-cat</a>&nbsp;closing message of how it's not enough to just <em>care</em> about an issue, you have to be <em>informed</em>&nbsp;on it as well, (and by the way, there's this thing called \"effective altruism\" you might want to look up); and I can expect a reasonable number of readers to actually engage with it, because it's ostensibly written about the work of an interesting woman on Ada Lovelace Day.</p>\n<p>You can probably see where I'm going with this by now.</p>\n<p>Whether or not you think it's valuable to publicise the work of women in STEM, it is an excellent opportunity to sneak assorted pro-rationality memes under the radar to an audience that wouldn't necessarily engage with them otherwise. Less Wrong has a lot of eloquent people with knowledge across a wide assortment of different domains. I'm curious as to what we could come up with if we made a concerted effort to do this.</p>\n<p>For that matter (and Harry Potter fanfic aside), it's an interesting question as to what other popular internet phenomena can be co-opted for this purpose.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sCNS6yatbpvtQRro3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 22, "extendedScore": null, "score": 6.2e-05, "legacy": true, "legacyId": "24370", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZpNvwH4CiGtKuyatG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-11T01:23:42.611Z", "modifiedAt": null, "url": null, "title": "Brain Training to maximize returns", "slug": "brain-training-to-maximize-returns", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:00.528Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Goobahman", "createdAt": "2011-01-13T05:09:28.962Z", "isAdmin": false, "displayName": "Goobahman"}, "userId": "cidN68rGuy4wwnvFp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5rqwttogW9akXcH3q/brain-training-to-maximize-returns", "pageUrlRelative": "/posts/5rqwttogW9akXcH3q/brain-training-to-maximize-returns", "linkUrl": "https://www.lesswrong.com/posts/5rqwttogW9akXcH3q/brain-training-to-maximize-returns", "postedAtFormatted": "Friday, October 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Brain%20Training%20to%20maximize%20returns&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABrain%20Training%20to%20maximize%20returns%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5rqwttogW9akXcH3q%2Fbrain-training-to-maximize-returns%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Brain%20Training%20to%20maximize%20returns%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5rqwttogW9akXcH3q%2Fbrain-training-to-maximize-returns", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5rqwttogW9akXcH3q%2Fbrain-training-to-maximize-returns", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 229, "htmlBody": "<p>Hi Everyone,<br /><br />Australia's ABC has recently broadcast a new series called <span style=\"color: #00802a;\"><span style=\"font-size: 14px; line-height: 16px; white-space: nowrap;\"><a style=\"font-family: arial, sans-serif;\" href=\"http://www.abc.net.au/tv/programs/redesign-my-brain-with-todd-sampson/\" target=\"_blank\">'Redesign my Brain'</a>&nbsp;</span></span>with Todd Sampson.<br /><br />The series seeks to explore how much the brain can be improved in areas like memory and recognition. After just one month of training Todd found himself performing considerably better on tests then he had prior.<br />He also competed in the World Memorization Championships, and watched a bloke in Germany play 12 games of chess simultaneously without seeing any of the boards.<br />So other than being a fun show to watch, it got me thinking about the advantages of brain training.&nbsp;<br />I've had a look at some stuff like dual-n-back, luminosity, and other brain training programs, but I've failed to really explore how much utility such training has.</p>\n<p>One of the memory champions was able to remember the order of 25 decks of cards in one hour. But it didn't seem like his ability didn't do much to improve his life beyond providing a fun and enjoyable hobby.<br /><br />So I'd like to ask:<br /><br />Which areas of cognitive training do you think would have the best returns in terms of life optimization?<br /><br />And what do you think would be the best way to go about that training?<br /><br />Would love to hear some success stories.<br /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5rqwttogW9akXcH3q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 4, "extendedScore": null, "score": 1.3751252885481807e-06, "legacy": true, "legacyId": "24371", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-11T16:08:30.516Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-112", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/G9d3FeijDjgCaRCAA/weekly-lw-meetups-112", "pageUrlRelative": "/posts/G9d3FeijDjgCaRCAA/weekly-lw-meetups-112", "linkUrl": "https://www.lesswrong.com/posts/G9d3FeijDjgCaRCAA/weekly-lw-meetups-112", "postedAtFormatted": "Friday, October 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG9d3FeijDjgCaRCAA%2Fweekly-lw-meetups-112%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG9d3FeijDjgCaRCAA%2Fweekly-lw-meetups-112", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG9d3FeijDjgCaRCAA%2Fweekly-lw-meetups-112", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 550, "htmlBody": "<p><strong>This summary was posted to LW main on October 4th. The following week's summary is <a href=\"/lw/it5/new_lw_meetup_saint_petersburg/\">here</a>.</strong></p>\n<p>Irregularly scheduled Less Wrong meetups are taking place in:</p>\n<ul>\n<li><a href=\"/meetups/rs\">Atlanta Lesswrong: October Meetup (First of Two):&nbsp;<span class=\"date\">06 October 2013 06:00PM</span></a></li>\n<li><a href=\"/meetups/qs\">Berlin: Fermi paradox discussion:&nbsp;<span class=\"date\">18 October 2013 07:00PM</span></a></li>\n<li><a href=\"/meetups/rr\">Chicago Open Discussion:&nbsp;<span class=\"date\">05 October 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/rq\">Frankfurt (including effective altruism presentation):&nbsp;<span class=\"date\">27 October 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/re\">Helsinki Meetup:&nbsp;<span class=\"date\">06 October 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/ri\">Munich Meetup in October:&nbsp;<span class=\"date\">05 October 2013 03:00PM</span></a></li>\n<li><a href=\"/meetups/rp\">Paris Meetup: Sunday, October 6: New people, games...:&nbsp;<span class=\"date\">06 October 2013 02:00PM</span></a></li>\n<li><a href=\"/meetups/rv\">Philadelphia - What Bayesianism taught me:&nbsp;<span class=\"date\">06 October 2013 01:00PM</span></a></li>\n<li><a href=\"/meetups/ru\">Tucson Meetup:&nbsp;<span class=\"date\">12 October 2013 02:00PM</span></a></li>\n</ul>\n<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:<a href=\"/meetups/bx\"></a></p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">05 October 2019 01:30PM</span></a></li>\n<li><a href=\"/meetups/r0\">Columbus, OH MEGA-MEETUP, Oct 11-14:&nbsp;<span class=\"date\">12 October 2013 02:33AM</span></a></li>\n<li><a href=\"/meetups/rt\">Durham/RTLW HPMoR discussion, ch. 87-89:&nbsp;<span class=\"date\">05 October 2013 12:00PM</span></a></li>\n<li><a href=\"/meetups/rl\">Washington DC fun and games meetup:&nbsp;<span class=\"date\">06 October 2013 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Columbus.2C_OH\">Columbus</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Research_Triangle.2C_NC_.28Raleigh.2FDurham.2FChapel_Hill.29\">Research Triangle NC</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a></strong><strong><strong>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington DC</strong></a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cincinnati</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Cleveland</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "G9d3FeijDjgCaRCAA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3759423210747309e-06, "legacy": true, "legacyId": "24337", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YwxWvNWEYjwicNQNP", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-11T21:16:20.596Z", "modifiedAt": null, "url": null, "title": "Does the simulation argument even need simulations?", "slug": "does-the-simulation-argument-even-need-simulations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:02.686Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lmm", "createdAt": "2011-10-12T11:42:35.671Z", "isAdmin": false, "displayName": "lmm"}, "userId": "TaAovu5P2hBFpE2mH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HBHsFCeAMSqMD7Td7/does-the-simulation-argument-even-need-simulations", "pageUrlRelative": "/posts/HBHsFCeAMSqMD7Td7/does-the-simulation-argument-even-need-simulations", "linkUrl": "https://www.lesswrong.com/posts/HBHsFCeAMSqMD7Td7/does-the-simulation-argument-even-need-simulations", "postedAtFormatted": "Friday, October 11th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20the%20simulation%20argument%20even%20need%20simulations%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20the%20simulation%20argument%20even%20need%20simulations%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBHsFCeAMSqMD7Td7%2Fdoes-the-simulation-argument-even-need-simulations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20the%20simulation%20argument%20even%20need%20simulations%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBHsFCeAMSqMD7Td7%2Fdoes-the-simulation-argument-even-need-simulations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHBHsFCeAMSqMD7Td7%2Fdoes-the-simulation-argument-even-need-simulations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1068, "htmlBody": "<p>The <a href=\"http://wiki.lesswrong.com/wiki/Simulation_argument\">simulation argument</a>, as I understand it:</p>\n<ol>\n<li>Subjectively, existing as a human in the real, physical universe is indistinguishable from existing as a simulated human in a simulated universe</li>\n<li>Anthropically, there is no reason to privilege one over the other: if there exist k real humans and l simulated humans undergoing one's subjective experience, one's odds of being a real human are k/(k+l)</li>\n<li>Any civilization capable of simulating a universe is quite likely to simulate an enormous number of them<ol>\n<li>Even if most capable civilizations simulate only a few universes for e.g. ethical reasons, civilizations that have no such concerns could simulate such enormous numbers of universes that the expected number of universes simulated by any simulation-capable civilization is still huge</li>\n</ol> </li>\n<li>Our present civilization is likely to reach the point where it can simulate a universe reasonably soon</li>\n<li>By 3. and 4., there exist (at some point in history) huge numbers of simulated universes, and therefore huge numbers of simulated humans living in simulated universes</li>\n<li>By 2. and 5., our odds of being real humans are tiny (unless we reject 4, by assuming that humanity will never reach the stage of running such simulations)<ol> </ol></li>\n</ol>\n<p>When we talk about a simulation we're usually thinking of a computer; crudely, we'd represent the universe as a giant array of bytes in RAM, and have some enormously complicated program that could compute the next state of the simulated universe from the previous one[1]. Fundamentally, we're just storing one big number, then performing a calculation and store another number, and so on. In fact our program is simply another number (witness the DeCSS \"illegal prime\"). This is effectively the <a href=\"/lw/pa/gazp_vs_glut/\">GLUT</a>&nbsp;concept applied to the whole universe.</p>\n<p>But numbers are just... numbers. If we have a computer calculating the fibonacci sequence, it's hard to see that running the calculating program makes this sequence any more real than if we had just conceptualized the rule[2] - or even, to a mathematical Platonist, if we'd never thought of it at all. And we <em>do</em> know the rule (modulo having a theory of quantum gravity), and the initial state of the universe is (to the best of our knowledge) small and simple enough that we could describe it, or another similar but subtly different universe, in terms small enough to write down. At that point, what we have seems in some sense to be a simulated universe, just as real as if we'd run a computer to calculate it all.</p>\n<p>Possible ways out that I can see:</p>\n<ol>\n<li>Bite the bullet: we are most likely not even a computer simulation, just a mathematical construct[3]</li>\n<li>Accept the other conclusion: either simulations are impractical even for posthuman civilizations, or posthuman civilization is unlikely. But if all that's required for a simulation is a mathematical form for the true laws of physics, and knowledge of some early state of the universe, this means humanity is unlikely to ever learn these two things, which is... disturbing, to say the least. This stance also seems to require rejecting mathematical Platonism and adopting some form of finitist/constructivist position, in which a mathematical notion does not exist until we have constructed it</li>\n<li>Argue that something important to the anthropic argument is lost in the move from a computer calculation to a mathematical expression. This seems to require rejecting the Church-Turing thesis and means most established programming theory would be useless in the programming of a simulation[4]</li>\n<li>Some other counter to the simulation argument. To me the anthropic part (i.e. step 2) seems the least certain; it appears to be false under e.g. <a href=\"/lw/572/the_absolute_selfselection_assumption/\">UDASSA</a>, though I don't know enough about anthropics to say more</li>\n</ol>\n<p>Thoughts?</p>\n<p>&nbsp;</p>\n<p>[1] As I understand it there is no contradiction with relativity; we perform the simulation in some particular frame, but obtain the same events whichever frame we choose</p>\n<p>[2] This equivalence is not just speculative. Going back to thinking about computer programs, Haskell (probably the language most likely to be used for a universe simulation, at least at present technology levels) follows lazy evaluation: a value is not calculated unless it is used. Thus if our simulation contained some regions that had no causal effect on subsequent steps (e.g. some people on a spaceship falling into a black hole), the simulation wouldn't bother to evaluate them[5]</p>\n<p>If we upload people who then make phone calls to their relatives to convince them to upload, clearly those people must have been calculated - or at least, enough of them to talk on the phone. But what about a loner who chooses to talk to no-one? Such a person could be more efficiently stored as their initial state plus a counter of how many times the function needs to be run to evaluate them, if anyone were to talk to them. If no-one has their contact details any more, we wouldn't even need to store that much. What about when all humans have uploaded? Sure, you could calculate the world-state for each step explicitly, but that would be wasteful. Our simulated world would still produce the correct outputs if all it did was increment a tick counter</p>\n<p>Practically every programming runtime performs some (more limited) form of this, using dataflow analysis, instruction reordering and dead code elimination - usually without the programmer having to explicitly request it. Thus if your theory of anthropics says that an \"optimized\" simulation is counted differently from a \"full\" one, then there is little hope of constructing such a thing without developing a significant amount of new tools and programming techniques[4]</p>\n<p>[3] Indeed, with an appropriate anthropic argument this might explain why the rules of physics are mathematically simple. I am planning another post on this line of thought</p>\n<p>[4] This is worrying if one is in favour of uploading, particularly forcibly - it would be extremely problematic&nbsp;morally if uploads were in some sense \"less real\" than biological people</p>\n<p>[5] One possible way out is that the laws of physics appear to be information-preserving; to simulate the state of the universe at time t=100 you can't discard any part of the state of the universe at time t=50, and must in some sense have calculated all the intermediate steps (though not necessarily explicitly - the state at t=20 could be spread out between several calculations, never appearing in memory as a single number). I don't think this affects the wider argument though</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HBHsFCeAMSqMD7Td7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 13, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "24321", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 102, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["k6EPphHiBH4WWYFCj", "QmWNbCRMgRBcMK6RK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-12T06:26:16.687Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC: Jobs/project show and tell", "slug": "meetup-washington-dc-jobs-project-show-and-tell-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mGApw2uPC5Cdbtevh/meetup-washington-dc-jobs-project-show-and-tell-0", "pageUrlRelative": "/posts/mGApw2uPC5Cdbtevh/meetup-washington-dc-jobs-project-show-and-tell-0", "linkUrl": "https://www.lesswrong.com/posts/mGApw2uPC5Cdbtevh/meetup-washington-dc-jobs-project-show-and-tell-0", "postedAtFormatted": "Saturday, October 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%3A%20Jobs%2Fproject%20show%20and%20tell&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%3A%20Jobs%2Fproject%20show%20and%20tell%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmGApw2uPC5Cdbtevh%2Fmeetup-washington-dc-jobs-project-show-and-tell-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%3A%20Jobs%2Fproject%20show%20and%20tell%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmGApw2uPC5Cdbtevh%2Fmeetup-washington-dc-jobs-project-show-and-tell-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmGApw2uPC5Cdbtevh%2Fmeetup-washington-dc-jobs-project-show-and-tell-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 70, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/s5'>Washington DC: Jobs/project show and tell</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 October 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Ballston Common mall</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to tell each other about the things we're working on. (I think this would segue well into doing another quick goals discussion).</p>\n\n<p>Thanks to the government shutdown, we're still meeting in Ballston, in the food court.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/s5'>Washington DC: Jobs/project show and tell</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mGApw2uPC5Cdbtevh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.3767352150346447e-06, "legacy": true, "legacyId": "24380", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Jobs_project_show_and_tell\">Discussion article for the meetup : <a href=\"/meetups/s5\">Washington DC: Jobs/project show and tell</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 October 2013 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Ballston Common mall</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to tell each other about the things we're working on. (I think this would segue well into doing another quick goals discussion).</p>\n\n<p>Thanks to the government shutdown, we're still meeting in Ballston, in the food court.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC__Jobs_project_show_and_tell1\">Discussion article for the meetup : <a href=\"/meetups/s5\">Washington DC: Jobs/project show and tell</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC: Jobs/project show and tell", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Jobs_project_show_and_tell", "level": 1}, {"title": "Discussion article for the meetup : Washington DC: Jobs/project show and tell", "anchor": "Discussion_article_for_the_meetup___Washington_DC__Jobs_project_show_and_tell1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-12T12:17:42.908Z", "modifiedAt": null, "url": null, "title": "How habits work and how you may control them", "slug": "how-habits-work-and-how-you-may-control-them", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.585Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5wMTZLZZmZEbXdoMD/how-habits-work-and-how-you-may-control-them", "pageUrlRelative": "/posts/5wMTZLZZmZEbXdoMD/how-habits-work-and-how-you-may-control-them", "linkUrl": "https://www.lesswrong.com/posts/5wMTZLZZmZEbXdoMD/how-habits-work-and-how-you-may-control-them", "postedAtFormatted": "Saturday, October 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20habits%20work%20and%20how%20you%20may%20control%20them&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20habits%20work%20and%20how%20you%20may%20control%20them%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5wMTZLZZmZEbXdoMD%2Fhow-habits-work-and-how-you-may-control-them%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20habits%20work%20and%20how%20you%20may%20control%20them%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5wMTZLZZmZEbXdoMD%2Fhow-habits-work-and-how-you-may-control-them", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5wMTZLZZmZEbXdoMD%2Fhow-habits-work-and-how-you-may-control-them", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3831, "htmlBody": "<p>Some highlights from <a href=\"http://www.amazon.com/Power-Habit-What-Life-Business/dp/1400069289/\">The Power of Habit: Why We Do What We Do in Life And Business</a> by Charles Duhigg, a book which seems like an invaluable resource for pretty much everyone who wants to improve their lives. The below summarizes the first three chapters of the book, as well as the appendix, for I found those to be the most valuable and generally applicable parts. These chapters discuss individual habits, while the rest of the book discusses the habits of companies and individuals. The later chapters also contain plenty of interesting content (some excerpts: [<a href=\"https://www.facebook.com/Xuenay/posts/10152283983813662\">1</a> <a href=\"https://www.facebook.com/Xuenay/posts/10152284361353662\">2</a> <a href=\"https://www.facebook.com/Xuenay/posts/10152284392528662\">3</a>]), and help explain the nature of e.g. some institutional failures.</p>\n<p>(See also <a href=\"/lw/a6t/link_the_nyt_on_everyday_habits/\">two</a> <a href=\"/lw/a8u/link_how_habits_control_our_behavior_and_how_to/\">previous</a> LW discussions on an online article by the author of the book.)</p>\n<p><strong>Chapter One</strong>: <strong>The Habit Loop - How Habits Work</strong></p>\n<p>When a rat first navigates a foreign environment, such as a maze, its brain is full of activity as it works to process the new environment and to learn all the environmental cues. As the environment becomes more familiar, the rat's brain becomes less and less active, until even brain structures related to memory quiet down a week later. Navigating the maze no longer requires higher processing: it has become an automatic habit.</p>\n<p>The process of converting a complicated sequence of actions into an automatic routine is known as \"chunking\", and human brains carry out a similar process. They vary in complexity, from putting toothpaste on your toothbrush before putting it in your mouth, to getting dressed or preparing breakfast, to very complicated processes such as backing one's car out of the driveway. All of these actions initially required considerable effort to learn, but eventually they became so automatic as to be carried out without conscious attention. As soon as we identify the right cue, such as pulling out the car keys, our brain activates the stored habit and lets our conscious minds focus on something else. In order to conserve effort, the brain will attempt to turn almost any routine into a habit.</p>\n<p>However, it can be dangerous to deactivate our brains at the wrong time, for there may be something unanticipated in the environment that will turn a previously-safe routine into something life-threatening. To help avoid such situations, our brains evaluate prospective habits using a three-stage <em>habit loop</em>:<a id=\"more\"></a></p>\n<blockquote>\n<p>From behind a partition, for instance, it&rsquo;s difficult for a rat to know if it&rsquo;s inside a familiar maze or an unfamiliar cupboard with a cat lurking outside. To deal with this uncertainty, the brain spends a lot of effort at the beginning of a habit looking for something&mdash; a cue&mdash; that offers a hint as to which pattern to use. From behind a partition, if a rat hears a click, it knows to use the maze habit. If it hears a meow, it chooses a different pattern. And at the end of the activity, when the reward appears, the brain shakes itself awake and makes sure everything unfolded as expected. <br /><br />This process within our brains is a three-step loop. First, there is a <em>cue</em>, a trigger that tells your brain to go into automatic mode and which habit to use. Then there is the <em>routine</em>, which can be physical or mental or emotional. Finally, there is a <em>reward</em>, which helps your brain figure out if this particular loop is worth remembering for the future.<br /><br />Over time, this loop&mdash; cue, routine, reward; cue, routine, reward&mdash; becomes more and more automatic. The cue and reward become intertwined until a powerful sense of anticipation and craving emerges. Eventually, whether in a chilly MIT laboratory or your driveway, a habit is born.</p>\n</blockquote>\n<p>Unused habits disappear very slowly, if at all. If a rat is trained to find cheese in a particular section of the maze, and the cheese is then moved to a different location, it will obtain a new habit. But once the cheese is moved back to its original location, the old habit re-emerges, almost as if it had been active for the whole time. This is part of the reason why it is so hard to start exercising regularly, or to change one's diet: the habit of relaxing in front of the TV, or snacking on a meal, will still be activated by the old cues and engage the behavioral pattern. On the other hand, if one does manage to establish a habit of ignoring the snacks or going out for a jog, it will eventually become as automatic as any other habit.</p>\n<p>Habits are crucial for our ability to function. People with damage to the basal ganglia, the parts of the brain responsible for habitual behavior, often become mentally paralyzed. Even basic activities, such as opening a door or choosing what to eat, become difficult to perform, and they may need to pause to wonder whether they should tie their left or right foot first, or whether to brush their teeth before or after taking a shower.</p>\n<blockquote>\n<p>In one set of experiments, for example, researchers affiliated with the National Institute on Alcohol Abuse and Alcoholism trained mice to press levers in response to certain cues until the behavior became a habit. The mice were always rewarded with food. Then, the scientists poisoned the food so that it made the animals violently ill, or electrified the floor, so that when the mice walked toward their reward they received a shock. The mice knew the food and cage were dangerous&mdash; when they were offered the poisoned pellets in a bowl or saw the electrified floor panels, they stayed away. When they saw their old cues, however, they unthinkingly pressed the lever and ate the food, or they walked across the floor, even as they vomited or jumped from the electricity. The habit was so ingrained the mice couldn&rsquo;t stop themselves.<br /><br />It&rsquo;s not hard to find an analog in the human world. Consider fast food, for instance. It makes sense&mdash; when the kids are starving and you&rsquo;re driving home after a long day&mdash; to stop, just this once, at McDonald&rsquo;s or Burger King. The meals are inexpensive. It tastes so good. After all, one dose of processed meat, salty fries, and sugary soda poses a relatively small health risk, right? It&rsquo;s not like you do it all the time. <br /><br />But habits emerge without our permission. Studies indicate that families usually don&rsquo;t intend to eat fast food on a regular basis. What happens is that a once a month pattern slowly becomes once a week, and then twice a week&mdash; as the cues and rewards create a habit&mdash; until the kids are consuming an unhealthy amount of hamburgers and fries. When researchers at the University of North Texas and Yale tried to understand why families gradually increased their fast food consumption, they found a series of cues and rewards that most customers never knew were influencing their behaviors. They discovered the habit loop. <br /><br />Every McDonald&rsquo;s, for instance, looks the same&mdash; the company deliberately tries to standardize stores&rsquo; architecture and what employees say to customers, so everything is a consistent cue to trigger eating routines. The foods at some chains are specifically engineered to deliver immediate rewards&mdash; the fries, for instance, are designed to begin disintegrating the moment they hit your tongue, in order to deliver a hit of salt and grease as fast as possible, causing your pleasure centers to light up and your brain to lock in the pattern. All the better for tightening the habit loop.<br /><br />However, even these habits are delicate. When a fast food restaurant closes down, the families that previously ate there will often start having dinner at home, rather than seek out an alternative location. Even small shifts can end the pattern. But since we often don&rsquo;t recognize these habit loops as they grow, we are blind to our ability to control them. By learning to observe the cues and rewards, though, we can change the routines.</p>\n</blockquote>\n<p><strong>Chapter Two: The Craving Brain - How to Create New Habits</strong></p>\n<p>A basic rule of marketing, based on the habit loop, is to attempt to identify a simple obvious cue, and then offer a clear reward from one's product. An early success was in the marketing of Pepsodent, where the marketer instructed people to run their tongue across their teeth and notice the existence of a \"film\" on the teeth. He then argued that by using his toothpaste, people could get rid of the film and obtain beautiful, clean teeth. (In reality, the \"film\" is a harmless membrane that builds up on teeth regardless of how often one eats or brushes their teeth.)</p>\n<p>However, other toothpaste companies had tried similar marketing tactics before, without much success. Another part of Pepsodent's success was that it happened to contain citric acid, as well as other chemicals that act as mild irritants. Their effect is to create a cool, tingling sensation on the tongue and gums of people. This acted as the real reward for the habit - although the sensation itself only happened to occur by coincidence, people came to associate it with having brushed their teeth, and of having a clean mouth. It was when people began craving this reward that tooth brushing really became a habit. When other toothpaste companies realized what was going on, they all proceeded to add similar irritants to their products.</p>\n<blockquote>\n<p>&ldquo;Consumers need some kind of signal that a product is working,&rdquo; Tracy Sinclair, who was a brand manager for Oral-B and Crest Kids Toothpaste, told me. &ldquo;We can make toothpaste taste like anything&mdash; blueberries, green tea&mdash; and as long as it has a cool tingle, people feel like their mouth is clean. The tingling doesn&rsquo;t make the toothpaste work any better. It just convinces people it&rsquo;s doing the job.&rdquo;</p>\n</blockquote>\n<p>When a habit becomes sufficiently established in the brain, the cue no longer just activates the routine - it also makes us crave the reward that is associated with completing the routine. If the cue is present, but we can't engage in the routine or try to prevent ourselves from doing so, the craving will increase in strength until it becomes almost overpowering. Various cues - the sight of a pack of cigarettes, the smell of food, a computer or smartphone chiming to signify the arrival of a new message - can activate the anticipatory mechanism, and the craving to take a smoke, eat a bite, or check one's messages.</p>\n<blockquote>\n<p>Scientists have studied the brains of alcoholics, smokers, and over-eaters and have measured how their neurology&mdash; the structures of their brains and the flow of neurochemicals inside their skulls&mdash; changes as their cravings became ingrained. Particularly strong habits, wrote two researchers at the University of Michigan, produce addiction-like reactions so that &ldquo;wanting evolves into obsessive craving&rdquo; that can force our brains into autopilot, &ldquo;even in the face of strong disincentives, including loss of reputation, job, home, and family\".</p>\n</blockquote>\n<p>The same mechanisms can also be used to encourage good or healthy habits. One chooses a cue, such as going to the gym as soon as one wakes up, and a reward, such as smoothie after each workout. Then one thinks about the smoothie, or the endorphin rush that follows during the exercise. As one allows oneself to anticipate the reward, a craving will begin to ensue, which will make it easier to get oneself to the gym every day. (See also <a href=\"https://www.youtube.com/watch?v=PppCBDHeytg\">PJ Eby on this</a>.)</p>\n<blockquote>\n<p>Cravings are what drive habits. And figuring out how to spark a craving makes creating a new habit easier. It&rsquo;s as true now as it was almost a century ago. Every night, millions of people scrub their teeth in order to get a tingling feeling; every morning, millions put on their jogging shoes to capture an endorphin rush they&rsquo;ve learned to crave.</p>\n</blockquote>\n<p><strong>Chapter Three: The Golden Rule of Habit Change - Why Transformation Occurs.</strong></p>\n<p>The Golden Rule of Habit Change is that one cannot extinguish a bad habit, only change it. One keeps the old cue and the old reward, but changes the routine. Almost any behavior can be changed if the cue and reward stay the same.</p>\n<p>For example, alcoholics rarely crave the actual physical state of intoxication itself. Rather, people drink in order to obtain escape, relaxation, companionship, blunting of anxieties, or an opportunity for emotional release. Organizations such as Alcoholics Anonymous<sup>1</sup> build a system of \"sponsors\" and group meetings, allowing a person in need of relief to talk with their sponsor or attend a group meeting. The cue, a need for relief, stays the same, as does the reward: getting relief. What changes is the behavior: instead of drinking, one obtains their relief by talking to others.</p>\n<p><em>Habit reversal therapy</em> is the formal version of this technique. In one example, Mandy, a 24-year-old graduate student had a compulsive need to bite her nails. The therapist asked Mandy to describe what she felt right before bringing her hand up to her mouth to bite her nails: Mandy described experiencing a feeling of tension. This was the cue for the habit. After some discussion, they established that Mandy bit her fingers when she was bored, and after she had worked through all of her nails, she felt a brief sense of completion. The physical stimulation acted as the reward.</p>\n<blockquote>\n<p>At the end of their first session, the therapist sent Mandy home with an assignment: Carry around an index card, and each time you feel the cue&mdash; a tension in your fingertips&mdash; make a check mark on the card. She came back a week later with twenty-eight checks. She was, by that point, acutely aware of the sensations that preceded her habit. She knew how many times it occurred during class or while watching television. <br /><br />Then the therapist taught Mandy what is known as a &ldquo;competing response.&rdquo; Whenever she felt that tension in her fingertips, he told her, she should immediately put her hands in her pockets or under her legs, or grip a pencil or something else that made it impossible to put her fingers in her mouth. Then Mandy was to search for something that would provide a quick physical stimulation&mdash; such as rubbing her arm or rapping her knuckles on a desk&mdash; anything that would produce a physical response. <br /><br />The cues and rewards stayed the same. Only the routine changed.<br /><br />They practiced in the therapist&rsquo;s office for about thirty minutes and Mandy was sent home with a new assignment: Continue with the index card, but make a check when you feel the tension in your fingertips and a hash mark when you successfully override the habit. <br /><br />A week later, Mandy had bitten her nails only three times and had used the competing response seven times. She rewarded herself with a manicure, but kept using the note cards. After a month, the nail-biting habit was gone. The competing routines had become automatic. One habit had replaced another.</p>\n</blockquote>\n<p>[...]</p>\n<blockquote>\n<p>Say you want to stop snacking at work. Is the reward you&rsquo;re seeking to satisfy your hunger? Or is it to interrupt boredom? If you snack for a brief release, you can easily find another routine&mdash; such as taking a quick walk, or giving yourself three minutes on the Internet&mdash; that provides the same interruption without adding to your waistline.</p>\n<p>If you want to stop smoking, ask yourself, do you do it because you love nicotine, or because it provides a burst of stimulation, a structure to your day, a way to socialize? If you smoke because you need stimulation, studies indicate that some caffeine in the afternoon can increase the odds you&rsquo;ll quit. More than three dozen studies of former smokers have found that identifying the cues and rewards they associate with cigarettes, and then choosing new routines that provide similar payoffs&mdash; a piece of Nicorette, a quick series of push-ups, or simply taking a few minutes to stretch and relax&mdash; makes it more likely they will quit.</p>\n</blockquote>\n<p>For some habits, though, this is not enough. The alcoholics who replace their old behaviors with new ones may manage to stop drinking for a long while, until they run into some particularly stressful event in their lives. At this point, the stress becomes too much for many, who start drinking again. Not everyone does, however, and the difference seems to be in whether people are capable of genuinely believing that things will become better.</p>\n<blockquote>\n<p>However, those alcoholics who believed, like John in Brooklyn, that some higher power had entered their lives were more likely to make it through the stressful periods with their sobriety intact. <br /><br />It wasn&rsquo;t God that mattered, the researchers figured out. It was belief itself that made a difference. Once people learned how to believe in something, that skill started spilling over to other parts of their lives, until they started believing they could change. Belief was the ingredient that made a reworked habit loop into a permanent behavior. <br /><br />&ldquo;I wouldn&rsquo;t have said this a year ago&mdash; that&rsquo;s how fast our understanding is changing,&rdquo; said Tonigan, the University of New Mexico researcher, &ldquo;but belief seems critical. You don&rsquo;t have to believe in God, but you do need the capacity to believe that things will get better. <br /><br />&ldquo;Even if you give people better habits, it doesn&rsquo;t repair why they started drinking in the first place. Eventually they&rsquo;ll have a bad day, and no new routine is going to make everything seem okay. What can make a difference is <em>believing</em> that they can cope with that stress without alcohol.&rdquo;</p>\n</blockquote>\n<p><strong>Appendix: A Reader's Guide to Using These Ideas</strong></p>\n<p>There isn't a single formula for changing habits, but rather thousands. Different people are driven by different cravings, and different habits require different approaches: stopping overeating is different from giving up cigarettes, which is different from how one communicates with their spouse. That said, the author attempts to provide a general framework for changing habits. It consists of four steps: Identify the routine, experiment with rewards, isolate the cue, have a plan.</p>\n<p>The routine involved in the habit is usually the most obvious aspect. For example, maybe somebody always gets up from their desk at afternoon, walks to a cafeteria, buys a cookie, and eats it while chatting with friends. What exactly is the reward here? It could be the cookie itself, the change of scenery, the temporary distraction, the opportunity to socialize with colleagues, or the burst of energy that comes from the blast of sugar.</p>\n<p>To identify the answer, one needs to experiment with rewards. On one day, instead of going out to a cafeteria, they might instead take a walk around the block. Another day, they might go to the cafeteria and buy an apple or chocolate bar and return to their desk without talking to anyone. On yet another day, they might walk to someone's desk to gossip for a few minutes and then return to work. When they do return to their desk, they should take a moment to quickly write down their thoughts or feelings - even just in the form of three random words in their head, like \"relaxed\", \"saw flowers\", \"not hungry\" - and then set a fifteen-minute alarm. If, after fifteen minutes, they still feel the craving, they know that whatever it was that they just did, it didn't give the desired reward. On the other hand, if they replaced the cafeteria visit by going to chat with a friend and the cafeteria craving vanished, then they've identified the reward as being a desire for temporary distraction and socialization.</p>\n<p>Then there is the task of identifying the cue. Experiments have shown that almost all habitual cues fall into one of five categories:</p>\n<ol>\n<li>Location</li>\n<li>Time</li>\n<li>Emotional state</li>\n<li>Other people</li>\n<li>Immediately preceding action</li>\n</ol>\n<p>So when one notices themselves engaging in a habit, they can write down the state of each of these variables. For example, here's one of the notes that the author made while trying to diagnose his own snacking habit:</p>\n<blockquote>\n<p>Where are you? (sitting at my desk)</p>\n<p>What time is it? (3:36 P.M.)</p>\n<p>What's your emotional state? (bored)</p>\n<p>Who else is around? (no one)</p>\n<p>What action preceded the urge? (answered an e-mail)</p>\n</blockquote>\n<p>After making such notes for three days, the pattern became clear: he got an urge to snack sometime between 3:00 and 4:00. The reward was temporary distraction, the kind that comes from gossiping with a friend.</p>\n<p>Now he needed to have a plan for overriding the old habit with a new one, while maintaining the old cue and reward. So he wrote down the following:</p>\n<blockquote>\n<p style=\"padding-left: 30px;\">At 3:30, every day, I will walk to a friend's desk and talk for 10 minutes.</p>\n<p>To make sure I remembered to do this, I set the alarm on my watch for 3: 30.<br /><br />It didn&rsquo;t work immediately. There were some days I was too busy and ignored the alarm, and then fell off the wagon. Other times it seemed like too much work to find a friend willing to chat&mdash; it was easier to get a cookie, and so I gave in to the urge. But on those days that I abided by my plan&mdash; when my alarm went off, I forced myself to walk to a friend&rsquo;s desk and chat for ten minutes&mdash; I found that I ended the workday feeling better. I hadn&rsquo;t gone to the cafeteria, I hadn&rsquo;t eat a cookie, and I felt fine. Eventually, it got be automatic: when the alarm rang, I found a friend and ended the day feeling a small, but real, sense of accomplishment. After a few weeks, I hardly thought about the routine anymore. And when I couldn&rsquo;t find anyone to chat with, I went to the cafeteria and bought tea and drank it with friends.<br /><br />That all happened about six months ago. I don&rsquo;t have my watch anymore&mdash; I lost it at some point. But at about 3:30 every day, I absentmindedly stand up, look around the newsroom for someone to talk to, spend ten minutes gossiping about the news, and then go back to my desk. It occurs almost without me thinking about it. It has become a habit.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p><strong>Footnotes</strong></p>\n<p><sup>1</sup>: How effective is the AA? The book admits that the effectiveness is hard to evaluate, but notes that <em>An estimated 2.1 million people seek help from AA each year, and as many as 10 million alcoholics may have achieved sobriety through the group. AA doesn&rsquo;t work for everyone&mdash; success rates are difficult to measure, because of participants&rsquo; anonymity&mdash; but millions credit the program with saving their lives.</em> It also comments that although scientists have been critical of the AA's unscientific methodology in the past, increasing numbers of researchers have recently become interested in the organization as its methodology fits other findings about habit change.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 2, "5Whwix4cZ3p5otshm": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5wMTZLZZmZEbXdoMD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 73, "baseScore": 96, "extendedScore": null, "score": 0.000236, "legacy": true, "legacyId": "24382", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 96, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Some highlights from <a href=\"http://www.amazon.com/Power-Habit-What-Life-Business/dp/1400069289/\">The Power of Habit: Why We Do What We Do in Life And Business</a> by Charles Duhigg, a book which seems like an invaluable resource for pretty much everyone who wants to improve their lives. The below summarizes the first three chapters of the book, as well as the appendix, for I found those to be the most valuable and generally applicable parts. These chapters discuss individual habits, while the rest of the book discusses the habits of companies and individuals. The later chapters also contain plenty of interesting content (some excerpts: [<a href=\"https://www.facebook.com/Xuenay/posts/10152283983813662\">1</a> <a href=\"https://www.facebook.com/Xuenay/posts/10152284361353662\">2</a> <a href=\"https://www.facebook.com/Xuenay/posts/10152284392528662\">3</a>]), and help explain the nature of e.g. some institutional failures.</p>\n<p>(See also <a href=\"/lw/a6t/link_the_nyt_on_everyday_habits/\">two</a> <a href=\"/lw/a8u/link_how_habits_control_our_behavior_and_how_to/\">previous</a> LW discussions on an online article by the author of the book.)</p>\n<p><strong>Chapter One</strong>: <strong>The Habit Loop - How Habits Work</strong></p>\n<p>When a rat first navigates a foreign environment, such as a maze, its brain is full of activity as it works to process the new environment and to learn all the environmental cues. As the environment becomes more familiar, the rat's brain becomes less and less active, until even brain structures related to memory quiet down a week later. Navigating the maze no longer requires higher processing: it has become an automatic habit.</p>\n<p>The process of converting a complicated sequence of actions into an automatic routine is known as \"chunking\", and human brains carry out a similar process. They vary in complexity, from putting toothpaste on your toothbrush before putting it in your mouth, to getting dressed or preparing breakfast, to very complicated processes such as backing one's car out of the driveway. All of these actions initially required considerable effort to learn, but eventually they became so automatic as to be carried out without conscious attention. As soon as we identify the right cue, such as pulling out the car keys, our brain activates the stored habit and lets our conscious minds focus on something else. In order to conserve effort, the brain will attempt to turn almost any routine into a habit.</p>\n<p>However, it can be dangerous to deactivate our brains at the wrong time, for there may be something unanticipated in the environment that will turn a previously-safe routine into something life-threatening. To help avoid such situations, our brains evaluate prospective habits using a three-stage <em>habit loop</em>:<a id=\"more\"></a></p>\n<blockquote>\n<p>From behind a partition, for instance, it\u2019s difficult for a rat to know if it\u2019s inside a familiar maze or an unfamiliar cupboard with a cat lurking outside. To deal with this uncertainty, the brain spends a lot of effort at the beginning of a habit looking for something\u2014 a cue\u2014 that offers a hint as to which pattern to use. From behind a partition, if a rat hears a click, it knows to use the maze habit. If it hears a meow, it chooses a different pattern. And at the end of the activity, when the reward appears, the brain shakes itself awake and makes sure everything unfolded as expected. <br><br>This process within our brains is a three-step loop. First, there is a <em>cue</em>, a trigger that tells your brain to go into automatic mode and which habit to use. Then there is the <em>routine</em>, which can be physical or mental or emotional. Finally, there is a <em>reward</em>, which helps your brain figure out if this particular loop is worth remembering for the future.<br><br>Over time, this loop\u2014 cue, routine, reward; cue, routine, reward\u2014 becomes more and more automatic. The cue and reward become intertwined until a powerful sense of anticipation and craving emerges. Eventually, whether in a chilly MIT laboratory or your driveway, a habit is born.</p>\n</blockquote>\n<p>Unused habits disappear very slowly, if at all. If a rat is trained to find cheese in a particular section of the maze, and the cheese is then moved to a different location, it will obtain a new habit. But once the cheese is moved back to its original location, the old habit re-emerges, almost as if it had been active for the whole time. This is part of the reason why it is so hard to start exercising regularly, or to change one's diet: the habit of relaxing in front of the TV, or snacking on a meal, will still be activated by the old cues and engage the behavioral pattern. On the other hand, if one does manage to establish a habit of ignoring the snacks or going out for a jog, it will eventually become as automatic as any other habit.</p>\n<p>Habits are crucial for our ability to function. People with damage to the basal ganglia, the parts of the brain responsible for habitual behavior, often become mentally paralyzed. Even basic activities, such as opening a door or choosing what to eat, become difficult to perform, and they may need to pause to wonder whether they should tie their left or right foot first, or whether to brush their teeth before or after taking a shower.</p>\n<blockquote>\n<p>In one set of experiments, for example, researchers affiliated with the National Institute on Alcohol Abuse and Alcoholism trained mice to press levers in response to certain cues until the behavior became a habit. The mice were always rewarded with food. Then, the scientists poisoned the food so that it made the animals violently ill, or electrified the floor, so that when the mice walked toward their reward they received a shock. The mice knew the food and cage were dangerous\u2014 when they were offered the poisoned pellets in a bowl or saw the electrified floor panels, they stayed away. When they saw their old cues, however, they unthinkingly pressed the lever and ate the food, or they walked across the floor, even as they vomited or jumped from the electricity. The habit was so ingrained the mice couldn\u2019t stop themselves.<br><br>It\u2019s not hard to find an analog in the human world. Consider fast food, for instance. It makes sense\u2014 when the kids are starving and you\u2019re driving home after a long day\u2014 to stop, just this once, at McDonald\u2019s or Burger King. The meals are inexpensive. It tastes so good. After all, one dose of processed meat, salty fries, and sugary soda poses a relatively small health risk, right? It\u2019s not like you do it all the time. <br><br>But habits emerge without our permission. Studies indicate that families usually don\u2019t intend to eat fast food on a regular basis. What happens is that a once a month pattern slowly becomes once a week, and then twice a week\u2014 as the cues and rewards create a habit\u2014 until the kids are consuming an unhealthy amount of hamburgers and fries. When researchers at the University of North Texas and Yale tried to understand why families gradually increased their fast food consumption, they found a series of cues and rewards that most customers never knew were influencing their behaviors. They discovered the habit loop. <br><br>Every McDonald\u2019s, for instance, looks the same\u2014 the company deliberately tries to standardize stores\u2019 architecture and what employees say to customers, so everything is a consistent cue to trigger eating routines. The foods at some chains are specifically engineered to deliver immediate rewards\u2014 the fries, for instance, are designed to begin disintegrating the moment they hit your tongue, in order to deliver a hit of salt and grease as fast as possible, causing your pleasure centers to light up and your brain to lock in the pattern. All the better for tightening the habit loop.<br><br>However, even these habits are delicate. When a fast food restaurant closes down, the families that previously ate there will often start having dinner at home, rather than seek out an alternative location. Even small shifts can end the pattern. But since we often don\u2019t recognize these habit loops as they grow, we are blind to our ability to control them. By learning to observe the cues and rewards, though, we can change the routines.</p>\n</blockquote>\n<p><strong id=\"Chapter_Two__The_Craving_Brain___How_to_Create_New_Habits\">Chapter Two: The Craving Brain - How to Create New Habits</strong></p>\n<p>A basic rule of marketing, based on the habit loop, is to attempt to identify a simple obvious cue, and then offer a clear reward from one's product. An early success was in the marketing of Pepsodent, where the marketer instructed people to run their tongue across their teeth and notice the existence of a \"film\" on the teeth. He then argued that by using his toothpaste, people could get rid of the film and obtain beautiful, clean teeth. (In reality, the \"film\" is a harmless membrane that builds up on teeth regardless of how often one eats or brushes their teeth.)</p>\n<p>However, other toothpaste companies had tried similar marketing tactics before, without much success. Another part of Pepsodent's success was that it happened to contain citric acid, as well as other chemicals that act as mild irritants. Their effect is to create a cool, tingling sensation on the tongue and gums of people. This acted as the real reward for the habit - although the sensation itself only happened to occur by coincidence, people came to associate it with having brushed their teeth, and of having a clean mouth. It was when people began craving this reward that tooth brushing really became a habit. When other toothpaste companies realized what was going on, they all proceeded to add similar irritants to their products.</p>\n<blockquote>\n<p>\u201cConsumers need some kind of signal that a product is working,\u201d Tracy Sinclair, who was a brand manager for Oral-B and Crest Kids Toothpaste, told me. \u201cWe can make toothpaste taste like anything\u2014 blueberries, green tea\u2014 and as long as it has a cool tingle, people feel like their mouth is clean. The tingling doesn\u2019t make the toothpaste work any better. It just convinces people it\u2019s doing the job.\u201d</p>\n</blockquote>\n<p>When a habit becomes sufficiently established in the brain, the cue no longer just activates the routine - it also makes us crave the reward that is associated with completing the routine. If the cue is present, but we can't engage in the routine or try to prevent ourselves from doing so, the craving will increase in strength until it becomes almost overpowering. Various cues - the sight of a pack of cigarettes, the smell of food, a computer or smartphone chiming to signify the arrival of a new message - can activate the anticipatory mechanism, and the craving to take a smoke, eat a bite, or check one's messages.</p>\n<blockquote>\n<p>Scientists have studied the brains of alcoholics, smokers, and over-eaters and have measured how their neurology\u2014 the structures of their brains and the flow of neurochemicals inside their skulls\u2014 changes as their cravings became ingrained. Particularly strong habits, wrote two researchers at the University of Michigan, produce addiction-like reactions so that \u201cwanting evolves into obsessive craving\u201d that can force our brains into autopilot, \u201ceven in the face of strong disincentives, including loss of reputation, job, home, and family\".</p>\n</blockquote>\n<p>The same mechanisms can also be used to encourage good or healthy habits. One chooses a cue, such as going to the gym as soon as one wakes up, and a reward, such as smoothie after each workout. Then one thinks about the smoothie, or the endorphin rush that follows during the exercise. As one allows oneself to anticipate the reward, a craving will begin to ensue, which will make it easier to get oneself to the gym every day. (See also <a href=\"https://www.youtube.com/watch?v=PppCBDHeytg\">PJ Eby on this</a>.)</p>\n<blockquote>\n<p>Cravings are what drive habits. And figuring out how to spark a craving makes creating a new habit easier. It\u2019s as true now as it was almost a century ago. Every night, millions of people scrub their teeth in order to get a tingling feeling; every morning, millions put on their jogging shoes to capture an endorphin rush they\u2019ve learned to crave.</p>\n</blockquote>\n<p><strong id=\"Chapter_Three__The_Golden_Rule_of_Habit_Change___Why_Transformation_Occurs_\">Chapter Three: The Golden Rule of Habit Change - Why Transformation Occurs.</strong></p>\n<p>The Golden Rule of Habit Change is that one cannot extinguish a bad habit, only change it. One keeps the old cue and the old reward, but changes the routine. Almost any behavior can be changed if the cue and reward stay the same.</p>\n<p>For example, alcoholics rarely crave the actual physical state of intoxication itself. Rather, people drink in order to obtain escape, relaxation, companionship, blunting of anxieties, or an opportunity for emotional release. Organizations such as Alcoholics Anonymous<sup>1</sup> build a system of \"sponsors\" and group meetings, allowing a person in need of relief to talk with their sponsor or attend a group meeting. The cue, a need for relief, stays the same, as does the reward: getting relief. What changes is the behavior: instead of drinking, one obtains their relief by talking to others.</p>\n<p><em>Habit reversal therapy</em> is the formal version of this technique. In one example, Mandy, a 24-year-old graduate student had a compulsive need to bite her nails. The therapist asked Mandy to describe what she felt right before bringing her hand up to her mouth to bite her nails: Mandy described experiencing a feeling of tension. This was the cue for the habit. After some discussion, they established that Mandy bit her fingers when she was bored, and after she had worked through all of her nails, she felt a brief sense of completion. The physical stimulation acted as the reward.</p>\n<blockquote>\n<p>At the end of their first session, the therapist sent Mandy home with an assignment: Carry around an index card, and each time you feel the cue\u2014 a tension in your fingertips\u2014 make a check mark on the card. She came back a week later with twenty-eight checks. She was, by that point, acutely aware of the sensations that preceded her habit. She knew how many times it occurred during class or while watching television. <br><br>Then the therapist taught Mandy what is known as a \u201ccompeting response.\u201d Whenever she felt that tension in her fingertips, he told her, she should immediately put her hands in her pockets or under her legs, or grip a pencil or something else that made it impossible to put her fingers in her mouth. Then Mandy was to search for something that would provide a quick physical stimulation\u2014 such as rubbing her arm or rapping her knuckles on a desk\u2014 anything that would produce a physical response. <br><br>The cues and rewards stayed the same. Only the routine changed.<br><br>They practiced in the therapist\u2019s office for about thirty minutes and Mandy was sent home with a new assignment: Continue with the index card, but make a check when you feel the tension in your fingertips and a hash mark when you successfully override the habit. <br><br>A week later, Mandy had bitten her nails only three times and had used the competing response seven times. She rewarded herself with a manicure, but kept using the note cards. After a month, the nail-biting habit was gone. The competing routines had become automatic. One habit had replaced another.</p>\n</blockquote>\n<p>[...]</p>\n<blockquote>\n<p>Say you want to stop snacking at work. Is the reward you\u2019re seeking to satisfy your hunger? Or is it to interrupt boredom? If you snack for a brief release, you can easily find another routine\u2014 such as taking a quick walk, or giving yourself three minutes on the Internet\u2014 that provides the same interruption without adding to your waistline.</p>\n<p>If you want to stop smoking, ask yourself, do you do it because you love nicotine, or because it provides a burst of stimulation, a structure to your day, a way to socialize? If you smoke because you need stimulation, studies indicate that some caffeine in the afternoon can increase the odds you\u2019ll quit. More than three dozen studies of former smokers have found that identifying the cues and rewards they associate with cigarettes, and then choosing new routines that provide similar payoffs\u2014 a piece of Nicorette, a quick series of push-ups, or simply taking a few minutes to stretch and relax\u2014 makes it more likely they will quit.</p>\n</blockquote>\n<p>For some habits, though, this is not enough. The alcoholics who replace their old behaviors with new ones may manage to stop drinking for a long while, until they run into some particularly stressful event in their lives. At this point, the stress becomes too much for many, who start drinking again. Not everyone does, however, and the difference seems to be in whether people are capable of genuinely believing that things will become better.</p>\n<blockquote>\n<p>However, those alcoholics who believed, like John in Brooklyn, that some higher power had entered their lives were more likely to make it through the stressful periods with their sobriety intact. <br><br>It wasn\u2019t God that mattered, the researchers figured out. It was belief itself that made a difference. Once people learned how to believe in something, that skill started spilling over to other parts of their lives, until they started believing they could change. Belief was the ingredient that made a reworked habit loop into a permanent behavior. <br><br>\u201cI wouldn\u2019t have said this a year ago\u2014 that\u2019s how fast our understanding is changing,\u201d said Tonigan, the University of New Mexico researcher, \u201cbut belief seems critical. You don\u2019t have to believe in God, but you do need the capacity to believe that things will get better. <br><br>\u201cEven if you give people better habits, it doesn\u2019t repair why they started drinking in the first place. Eventually they\u2019ll have a bad day, and no new routine is going to make everything seem okay. What can make a difference is <em>believing</em> that they can cope with that stress without alcohol.\u201d</p>\n</blockquote>\n<p><strong id=\"Appendix__A_Reader_s_Guide_to_Using_These_Ideas\">Appendix: A Reader's Guide to Using These Ideas</strong></p>\n<p>There isn't a single formula for changing habits, but rather thousands. Different people are driven by different cravings, and different habits require different approaches: stopping overeating is different from giving up cigarettes, which is different from how one communicates with their spouse. That said, the author attempts to provide a general framework for changing habits. It consists of four steps: Identify the routine, experiment with rewards, isolate the cue, have a plan.</p>\n<p>The routine involved in the habit is usually the most obvious aspect. For example, maybe somebody always gets up from their desk at afternoon, walks to a cafeteria, buys a cookie, and eats it while chatting with friends. What exactly is the reward here? It could be the cookie itself, the change of scenery, the temporary distraction, the opportunity to socialize with colleagues, or the burst of energy that comes from the blast of sugar.</p>\n<p>To identify the answer, one needs to experiment with rewards. On one day, instead of going out to a cafeteria, they might instead take a walk around the block. Another day, they might go to the cafeteria and buy an apple or chocolate bar and return to their desk without talking to anyone. On yet another day, they might walk to someone's desk to gossip for a few minutes and then return to work. When they do return to their desk, they should take a moment to quickly write down their thoughts or feelings - even just in the form of three random words in their head, like \"relaxed\", \"saw flowers\", \"not hungry\" - and then set a fifteen-minute alarm. If, after fifteen minutes, they still feel the craving, they know that whatever it was that they just did, it didn't give the desired reward. On the other hand, if they replaced the cafeteria visit by going to chat with a friend and the cafeteria craving vanished, then they've identified the reward as being a desire for temporary distraction and socialization.</p>\n<p>Then there is the task of identifying the cue. Experiments have shown that almost all habitual cues fall into one of five categories:</p>\n<ol>\n<li>Location</li>\n<li>Time</li>\n<li>Emotional state</li>\n<li>Other people</li>\n<li>Immediately preceding action</li>\n</ol>\n<p>So when one notices themselves engaging in a habit, they can write down the state of each of these variables. For example, here's one of the notes that the author made while trying to diagnose his own snacking habit:</p>\n<blockquote>\n<p>Where are you? (sitting at my desk)</p>\n<p>What time is it? (3:36 P.M.)</p>\n<p>What's your emotional state? (bored)</p>\n<p>Who else is around? (no one)</p>\n<p>What action preceded the urge? (answered an e-mail)</p>\n</blockquote>\n<p>After making such notes for three days, the pattern became clear: he got an urge to snack sometime between 3:00 and 4:00. The reward was temporary distraction, the kind that comes from gossiping with a friend.</p>\n<p>Now he needed to have a plan for overriding the old habit with a new one, while maintaining the old cue and reward. So he wrote down the following:</p>\n<blockquote>\n<p style=\"padding-left: 30px;\">At 3:30, every day, I will walk to a friend's desk and talk for 10 minutes.</p>\n<p>To make sure I remembered to do this, I set the alarm on my watch for 3: 30.<br><br>It didn\u2019t work immediately. There were some days I was too busy and ignored the alarm, and then fell off the wagon. Other times it seemed like too much work to find a friend willing to chat\u2014 it was easier to get a cookie, and so I gave in to the urge. But on those days that I abided by my plan\u2014 when my alarm went off, I forced myself to walk to a friend\u2019s desk and chat for ten minutes\u2014 I found that I ended the workday feeling better. I hadn\u2019t gone to the cafeteria, I hadn\u2019t eat a cookie, and I felt fine. Eventually, it got be automatic: when the alarm rang, I found a friend and ended the day feeling a small, but real, sense of accomplishment. After a few weeks, I hardly thought about the routine anymore. And when I couldn\u2019t find anyone to chat with, I went to the cafeteria and bought tea and drank it with friends.<br><br>That all happened about six months ago. I don\u2019t have my watch anymore\u2014 I lost it at some point. But at about 3:30 every day, I absentmindedly stand up, look around the newsroom for someone to talk to, spend ten minutes gossiping about the news, and then go back to my desk. It occurs almost without me thinking about it. It has become a habit.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p><strong id=\"Footnotes\">Footnotes</strong></p>\n<p><sup>1</sup>: How effective is the AA? The book admits that the effectiveness is hard to evaluate, but notes that <em>An estimated 2.1 million people seek help from AA each year, and as many as 10 million alcoholics may have achieved sobriety through the group. AA doesn\u2019t work for everyone\u2014 success rates are difficult to measure, because of participants\u2019 anonymity\u2014 but millions credit the program with saving their lives.</em> It also comments that although scientists have been critical of the AA's unscientific methodology in the past, increasing numbers of researchers have recently become interested in the organization as its methodology fits other findings about habit change.</p>", "sections": [{"title": "Chapter Two: The Craving Brain - How to Create New Habits", "anchor": "Chapter_Two__The_Craving_Brain___How_to_Create_New_Habits", "level": 1}, {"title": "Chapter Three: The Golden Rule of Habit Change - Why Transformation Occurs.", "anchor": "Chapter_Three__The_Golden_Rule_of_Habit_Change___Why_Transformation_Occurs_", "level": 1}, {"title": "Appendix: A Reader's Guide to Using These Ideas", "anchor": "Appendix__A_Reader_s_Guide_to_Using_These_Ideas", "level": 1}, {"title": "Footnotes", "anchor": "Footnotes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "76 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 76, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AgzEatphhmcQLxuq4", "jYA6PqD3QevKtp3Hn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-12T16:43:27.012Z", "modifiedAt": null, "url": null, "title": "Interesting critique of British education by outgoing advisor (warning: some politics)", "slug": "interesting-critique-of-british-education-by-outgoing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:35.804Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bramflakes", "createdAt": "2011-11-01T22:15:00.964Z", "isAdmin": false, "displayName": "bramflakes"}, "userId": "pJEYMdQjRLEJSg8bX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BkDGfZey5p8YdSJvH/interesting-critique-of-british-education-by-outgoing", "pageUrlRelative": "/posts/BkDGfZey5p8YdSJvH/interesting-critique-of-british-education-by-outgoing", "linkUrl": "https://www.lesswrong.com/posts/BkDGfZey5p8YdSJvH/interesting-critique-of-british-education-by-outgoing", "postedAtFormatted": "Saturday, October 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Interesting%20critique%20of%20British%20education%20by%20outgoing%20advisor%20(warning%3A%20some%20politics)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInteresting%20critique%20of%20British%20education%20by%20outgoing%20advisor%20(warning%3A%20some%20politics)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBkDGfZey5p8YdSJvH%2Finteresting-critique-of-british-education-by-outgoing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Interesting%20critique%20of%20British%20education%20by%20outgoing%20advisor%20(warning%3A%20some%20politics)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBkDGfZey5p8YdSJvH%2Finteresting-critique-of-british-education-by-outgoing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBkDGfZey5p8YdSJvH%2Finteresting-critique-of-british-education-by-outgoing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3905, "htmlBody": "<p>The soon-to-be-resigning Dominic Cummings, advisor to the Education Secretary of the Coalition government, has released a <a href=\"http://s3.documentcloud.org/documents/804396/some-thoughts-on-education-and-political.pdf\">250-page manifesto</a> describing the problems of the British educational establishment (\"the blob\" in Whitehall parlance) and offering solutions. I post this here because both his analysis and recommendations are likely to be interesting to LW, in particular an increased emphasis on STEM, broader knowledge of the limits of human reasoning and how they relate to managing complex systems, an appreciation for \"agenty\"-ness in organizational leadership, whole-brain emulation, intelligence enhancement, recursive self-improving AGI, analysis of human interactions on a firm evolutionary-psychological basis, and a rejection of fashionable pseudoscientific theories of psychology and society. Relevant extracts:</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>This essay is aimed mainly at ~15-25 year-olds and those interested in more ambitious education and training for them. Not only are most of them forced into mediocre education but they are also then forced into dysfunctional institutions where many face awful choices: either conform to the patterns set by middle-aged mediocrities (don&rsquo;t pursue excellence, don&rsquo;t challenge bosses&rsquo; errors, and so on) or soon be despised and unemployed. Some of the ideas sketched here may help some to shape their own education and create their own institutions. As people such as Linus Torvald (Linux) or Mark Zuckerberg (Facebook) have shown, the young are capable of much more than the powerful and middle-aged, who control so many institutions, like to admit.2 A signi\ufb01cant change in education and training could also help us partly ameliorate the grim cycle of predictable political dysfunction.&nbsp;</p>\n<p>Although it is generally psychologically unpleasant to focus on our problems and admit the great weaknesses of our institutions, including what we in politics and our leaders do not understand, it is only through honest analysis that progress is possible. As Maxwell said, &lsquo;Thoroughly conscious ignorance is a prelude to every real advance in knowledge.&rsquo; Reliable knowledge about what works (how do people learn, what teaching methods work, how to use technology) should be built cumulatively on the foundation of empirical tests as suggested by physicist Carl Wieman and others (cf. Section 6 and Endnote). New systems and curricula would work in different ways for a school, a University, or an equivalent to a boxing gym for politicians if such a thing existed.</p>\n<p>In particular, it is suggested that we need an &lsquo;Odyssean&rsquo; education so that a substantial fraction of teenagers, students and adults might understand something of our biggest intellectual and practical problems, and be trained to take effective action.</p>\n<p>The Nobel-winning physicist, Murray Gell Mann, one of the architects of the Standard Model of particle physics and namer of the &lsquo;quark&rsquo;,3 has described a scienti\ufb01c and political need for an &lsquo;Odyssean&rsquo; philosophy that can synthesise a) maths and the natural sciences, b) the social sciences, and c) the humanities and arts, into necessarily crude, trans-disciplinary, integrative thinking about complex systems.</p>\n</blockquote>\n<blockquote>\n<p style=\"padding-left: 30px;\">&lsquo;Today the network of relationships linking the human race to itself and to the rest of the biosphere is so complex that all aspects affect all others to an extraordinary degree. Someone should be studying the whole system, however crudely that has to be done, because no gluing together of partial studies of a complex nonlinear system can give a good idea of the behavior of the whole...</p>\n<p style=\"padding-left: 30px;\">&lsquo;Those who study complex adaptive systems are beginning to \ufb01nd some general principles that underlie all such systems, and seeking out those principles requires intensive discussions and collaborations among specialists in a great many \ufb01elds. Of course the careful and inspired study of each specialty remains as vital as ever. But integration of those specialities is urgently needed as well. Important contributions are made by the handful of scholars and scientists who are transforming themselves from specialists into students of simplicity and complexity or of complex adaptive systems in general...&nbsp;</p>\n<p style=\"padding-left: 30px;\">&lsquo;[There is] the distinction (made famous by Nietzsche) between &ldquo;Apollonians&rdquo;, who favor logic, the analytical approach, and a dispassionate weighing of the evidence, and &ldquo;Dionysians&rdquo;&rsquo;, who lean more toward intuition, synthesis, and passion&hellip;[4] But some of us seem to belong to another category: the &ldquo;Odysseans&rdquo;, who combine the two predilections in their quest for connections among ideas&hellip; We need to celebrate the contribution of those who dare take what I call &ldquo;a crude look at the whole&rdquo;&hellip;&nbsp;</p>\n<p style=\"padding-left: 30px;\">&lsquo;&hellip; broadly integrative thinking is relegated to cocktail parties. In academic life, in bureaucracies, and elsewhere, the task of integration is insuf\ufb01ciently respected. Yet anyone at the top of an organization &hellip; has to make decisions as if all aspects of a situation, along with the interaction among those aspects, were being taken into account. Is it reasonable for the leader, reaching down into the organization for help, to encounter specialists and for integrative thinking to take place only when he or she makes the \ufb01nal intuitive judgements?</p>\n<p style=\"padding-left: 30px;\">&lsquo;[A] multiplicity of crude but integrative policy studies, involving not just linear projection but evolution and highly nonlinear simulation and gaming, may provide some modest help in generating a collective foresight function for the human race&hellip;&nbsp;</p>\n<p style=\"padding-left: 30px;\">&lsquo;Given the immense complexity of the numerous interlocking issues facing humanity, foresight demands the ability to identify and gather great quantities of relevant information; the ability to catch glimpses, using that information, of the choices offered by the branching alternative histories of the future, and the wisdom to select simpli\ufb01cations and approximations that do not sacri\ufb01ce the representation of critical qualitative issues, especially issues of values&hellip;</p>\n<p style=\"padding-left: 30px;\">&lsquo;Computers ... can serve us both by learning or adapting themselves and by modelling or simulating systems in the real world that learn or adapt or evolve. .. Powerful computers are essential for assistance in looking into the future, but we must not allow their use to bias the formulation of problems toward the quanti\ufb01able and analyzable at the expense of the important.&rsquo;5</p>\n<p>One of the things that &lsquo;synthesizers&rsquo; need to learn is the way that many themes cut across subjects and generate new subjects. Trans-disciplinary studies of complex systems have been profoundly affected by connected intellectual revolutions in physics, maths, logic, computation, and biology, though these connections have so far barely been integrated in school or university curricula. Ideas about &lsquo;information&rsquo; cut across physics (thermodynamics and entropy),7 computation (bits, qubits, and &lsquo;arti\ufb01cial agents&rsquo;), economics (&lsquo;economic agents&rsquo;, Hayek&rsquo;s description of prices as an &lsquo;information discovery process&rsquo;), evolution (genetic networks),8 the brain (neuronal networks), &lsquo;intelligence failures&rsquo;9 and other subjects. Physics is inseparable from old philosophical questions (it is &lsquo;experimental metaphysics&rsquo;) and it is merging with computer science to produce quantum computation and &lsquo;quantum information theory&rsquo;.10 Evolutionary ideas took hold in economics (Hume, Smith) and biology (Darwin), and they have now been incorporated into computer science (e.g. &lsquo;genetic algorithms&rsquo;) in order to &lsquo;evolve&rsquo; solutions to problems in large search spaces, and they have suggested ideas for engineering solutions. (For example, it is suggested that dangers such as bioterrorism or pandemics should be defended by developing &lsquo;arti\ufb01cial immune systems&rsquo; in which defences operate according to the evolutionary principles of i) generating lots of solutions with random variation, and ii) differential replication of the most effective agents, instead of reliance on traditional centralised institutions that make similar mistakes repeatedly.) Machine intelligence (or &lsquo;automated reasoning&rsquo;) has been shaped by, and in turn is reshaping, ideas about the mind and is now used to design computers, including those that are used to investigate the mind. Behavioural genetics, evolutionary psychology, cognitive science and neuroscience are reshaping not only economics (e.g. fMRI scans of people playing \ufb01nancial games) and history (e.g. the in\ufb02uence of evolved antipathy for out-groups) but also how we design institutions (e.g. how we evolved to succeed in the sexual politics of small, primitive collectivist tribes hence many of the standard features of internal politics). As will be discussed, there are various developments in education that seek to re\ufb02ect these trans-disciplinary themes: e.g. the Nobel-winning neuroscientist, Eric Kandel, plans a new PhD programme combining neuroscience, psychology and art history as part of the &lsquo;Mind, Brain, Behaviour Project&rsquo; at Columbia.11</p>\n</blockquote>\n<p>&nbsp;</p>\n<blockquote>\n<p>Neuroscience, cognitive science, behavioural genetics, and evolutionary biology have developed our understanding of the mind. They and other disciplines have combined with Moore&rsquo;s Law and scanning technology to provide increasingly accurate maps and quantitative models of the brain107 (which Obama promised federal support for in 2013) and rapidly improving brain-computer interfaces.108 We can look inside our minds with tools that our minds have given us and watch the brain watching itself. These developments have undermined the basis for Descartes&rsquo; Ghost in the Machine, Locke&rsquo;s Blank Slate, and Rousseau&rsquo;s Noble Savage (pace the current, sometimes misguided, backlash).109</p>\n<p>The brain consists of ~80 billion (8x1010) neurons and ~100 trillion (1014) synapses. Operating on&nbsp;~20 watts it performs ~1017 \ufb02oating point computations per second. As well as the brain-computer interfaces already underway, various projects plan to map the brain completely (e.g the Connectome Project), simulate the brain (e.g Markram's project), and build new computer architectures modeled on the brain and performing similarly to the brain.</p>\n<p>For example, the most successful government technology developer, DARPA, has made robotics and machine intelligence a priority with projects such as the SyNAPSE Project (led by IBM&rsquo;s Modha) to create &lsquo;a brain inspired electronic &ldquo;chip&rdquo; that mimics that function, size, and power consumption of a biological cortex.&rsquo;110 They have recently announced progress in building this new architecture, fundamentally different to the standard &lsquo;von Neumann architecture&rsquo; of all normal computers.111</p>\n<p>The Human Brain Project is aiming to model a human brain and in 2012 was awarded &euro;1 billion by the EU. In 2005, a single neuron was simulated; in 2008, a cortical column (104 neurons); in 2011, 100 columns (106 neurons). Markram plans for a full rodent brain simulation in 2014 and a human brain simulation 2020-25.</p>\n<p>However, so far SyNAPSE and The Human Brain Project have not demonstrated how simulations connect to observable behaviours. In November 2012, Eliasmith et al (Science, 30/12/2012) published details of a large-scale computational model of the brain (Semantic Pointer Architecture Uni\ufb01ed Network, or &lsquo;Spaun&rsquo;) intended to bridge &lsquo;the brain-behaviour gap&rsquo; by simulating complex behaviour. Spaun is a &lsquo;spiking neuron model&rsquo; of 2.5m simulated neurons organised into subsystems resembling different brain areas. All inputs are images of characters; all outputs are movements of a physically modeled arm. Incoming visual images are compressed by successive layers of the network that extract increasingly complex information, and simple internally generated commands (e.g. &lsquo;draw a number&rsquo;) are &lsquo;expanded&rsquo; into complex mechanical movements. Spaun simulates the working memory and an &lsquo;action selection system&rsquo;. It performs eight tasks such as image recognition, copy drawing, reinforcement learning, and a \ufb02uid reasoning task &lsquo;isomorphic to the induction problems from the Raven&rsquo;s Progressive Matrices (RPM) test for \ufb02uid intelligence&rsquo;. Spaun managed to pass some basic aspects of an IQ test. Spaun is not task-speci\ufb01c so the model could be extended to other tasks and scaled-up in other ways.</p>\n<p>In 2013, Alex Wissner-Gross, a Harvard computer scientist, published a paper in Physical Review in &lsquo;an attempt to describe intelligence as a fundamentally thermodynamic process&rsquo;, proposing that intelligence can spontaneously emerge from the attempt to maximise freedom of action in the future. He built a software programme, &lsquo;ENTROPICA&rsquo;, designed to maximise the production of long-term entropy of any system it \ufb01nds itself in. ENTROPICA then solved various problems including intelligence tests, playing games, social cooperation, trading \ufb01nancial instruments, and &lsquo;balancing&rsquo; a physical system and so on.</p>\n<p style=\"padding-left: 30px;\">&lsquo;We were actually able to successfully reproduce standard intelligence tests and other cognitive behaviors, all without assigning any explicit goals&hellip; &lsquo;</p>\n<p style=\"padding-left: 30px;\">Think of games like chess or Go in which good players try to preserve as much freedom of action as possible. When the best computer programs play Go, they rely on a principle in which the best move is the one which preserves the greatest fraction of possible wins. When computers are equipped with this simple strategy - along with some pruning for ef\ufb01ciency - they begin to approach the level of Go grandmasters&hellip;</p>\n<p style=\"padding-left: 30px;\">&lsquo;Our causal entropy maximization theory predicts that AIs may be fundamentally antithetical to being boxed. If intelligence is a phenomenon that spontaneously emerges through causal entropy maximization, then it might mean that you could effectively reframe the entire de\ufb01nition of Arti\ufb01cial General Intelligence to be a physical effect resulting from a process that tries to avoid being boxed...</p>\n<p style=\"padding-left: 30px;\">&lsquo;The conventional storyline has been that we would \ufb01rst build a really intelligent machine, and then it would spontaneously decide to take over the world&hellip;We may have gotten the order of dependence all wrong. Intelligence and superintelligence may actually emerge from the effort of trying to take control of the world - and speci\ufb01cally, all possible futures - rather than taking control of the world being a behavior that spontaneously emerges from having superhuman machine intelligence&hellip;&nbsp;</p>\n<p style=\"padding-left: 30px;\">&lsquo;The recursive self-improving of an AI can be seen as implicitly inducing a \ufb02ow over the entire space of possible AI programs. In that context, if you look at that \ufb02ow over AI program space, it is conceivable that causal entropy maximization might represent a \ufb01xed point and that a recursively self-improving AI will tend to self-modify so as to do a better and better job of maximizing its future possibilities.</p>\n<p style=\"padding-left: 30px;\">&lsquo;In the problem solving example, I show that cooperation can emerge as a means for the systems to maximize their causal entropy, so it doesn't always have to be competition. If more future possibilities are gained through cooperation rather than competition, then cooperation by itself should spontaneously emerge, speaking to the potential for friendliness.&rsquo; (Interview.)</p>\n</blockquote>\n<blockquote>\n<p>The education of the majority even in rich countries is between awful and mediocre. A tiny number, less than 1 percent, are educated in the basics of how the &lsquo;unreasonable effectiveness of mathematics&rsquo; provides the &lsquo;language of nature&rsquo; and a foundation for our scienti\ufb01c civilisation116 and only a small subset of that &lt;1% then study trans-disciplinary issues concerning the understanding, prediction and control of complex nonlinear systems. Unavoidably, the level of one&rsquo;s mathematical understanding imposes limits on the depth to which one can explore many subjects. For example, it is impossible to follow academic debates about IQ unless one knows roughly what &lsquo;normal distribution&rsquo; and &lsquo;standard deviation&rsquo; mean, and many political decisions, concerning issues such as risk, cannot be wisely taken without at least knowing of the existence of mathematical tools such as conditional probability. Only a few aspects of this problem will be mentioned.</p>\n<p>There is widespread dishonesty about standards in English schools,117 low aspiration even for the brightest children,118 and a common view that only a small fraction of the population, a subset of the most able, should be given a reasonably advanced mathematical and scienti\ufb01c education, while many other able pupils leave school with little more than basic numeracy and some scattered, soon-forgotten facts. A reasonable overall conclusion from international comparisons, many studies, and how universities have behaved, is that overall standards have roughly stagnated over the past thirty years (at best), there are fewer awful schools, the sharp rises in GCSE results re\ufb02ect easier exams rather than real educational improvements, and the skills expected of the top 20 percent of the ability range studying core A Level subjects signi\ufb01cantly declined (while private schools continued to teach beyond A Levels), hence private schools have continued to dominate Oxbridge entry while even the best universities have had to change degree courses substantially</p>\n<p>There is hostility to treating education as a \ufb01eld for objective scienti\ufb01c research to identify what different methods and resources might achieve for different sorts of pupils. The quality of much education research is poor. Randomised control trials (RCTs) are rarely used to evaluate programmes costing huge amounts of money. They were resisted by the medical community for decades (&lsquo;don&rsquo;t challenge my expertise with data&rsquo;)119 and this attitude still pervades education. There are many &lsquo;studies&rsquo; that one cannot rely on and which have not been replicated. Methods are often based on technological constraints of centuries ago, such as lectures. Square wheels are repeatedly reinvented despite the availability of exceptional materials and subject experts are routinely ignored by professional &lsquo;educationalists&rsquo;.120 There is approximately zero connection between a) debates in Westminster and the media about education and b) relevant science, and little desire to make such connections or build the systems necessary; almost everybody prefers the current approach despite occasional talk of &lsquo;evidence-based policy&rsquo; (this problem is one of the reasons we asked Ben Goldacre to review the DfE&rsquo;s analysis division). The political implications of discussing the effects of evolutionary in\ufb02uences on the variance of various characteristics (such as intelligence (&lsquo;g&rsquo;) and conscientiousness) and the gaps between work done by natural scientists and much &lsquo;social science&rsquo; commentary have also prevented rational public discussion (cf. Endnote on IQ).121</p>\n<p>Westminster and Whitehall have distorted incentives to learn and improve,122 have simultaneously taken control of curricula and exams and undermined the credibility of both, and have then blamed universities for the failures of state schools123 and put enormous pressure on Universities and academics not to speak publicly about problems with exams, which has made rational discussion of exams impossible. Most people with power in the education system are more worried about being accused of &lsquo;elitism&rsquo; (and &lsquo;dividing children into sheep and goats&rsquo;) than they are about problems caused by poor teaching and exams and they would rather live with those problems than deal with those accusations.124</p>\n<p>[124 E.g. Almost everybody the DfE consulted 2011-13 about curriculum and exam reform was much more concerned about accusations of elitism than about the lack of ambition for the top 20%. Although they would not put it like this, most prominent people in the education world tacitly accept that failing to develop the talents of the most able is a price worth paying to be able to pose as defenders of &lsquo;equality&rsquo;. The insistence that ~95% of pupils be able to take the same exam at 16 means (if one assumes symmetrical exclusions) that the exam must embrace plus and minus two standard deviations on the cognitive ability range: i.e. they exclude only the bottom 2.5% (i.e. an IQ of &lt;~70) and top 2.5% (i.e an IQ of &gt;~130, which is the average Physics PhD).]</p>\n<p>There is huge variation in school performance (on exams that are sub-optimal) among schools with the poorest children. About a quarter of primaries have over a quarter of their pupils leave each year who are not properly prepared for basic secondary studies and few such pupils enjoy a turnaround at secondary;125 other primaries (including in the poorest areas) have &lt;5% in such a desperate situation. Consider a basic benchmark: getting four-\ufb01fths of pupils to at least a &lsquo;C&rsquo; in existing English and Maths GCSE. A small minority of state schools achieve this, while others with similar funding and similarly impoverished pupils struggle to get two-\ufb01fths to this level. This wide variety in performance combined with severe limits on direct parent choice means the system is partly rationed by house price.126</p>\n<p>This wide variety in performance also strongly suggests that the block to achieving this basic benchmark is the management and quality of teaching in the school; the block is not poverty,127 IQ, money,128 lack of innovation, or a lack of understanding about how to teach basics. Making a transition to a school system in which ~4/5 meet this basic level is therefore an issue of <em>doing things we already know how to do</em>; the obstacles are political and bureaucratic (such as replacing management and bad teachers despite political resistance and legal complexity), although this must not blind us to the fact that most variation in performance is due to within school factors (including genetics) rather than between school factors (see below).</p>\n<p><em>There are various problems with maths and science education&hellip;</em></p>\n<p>The Royal Society estimates that ~300,000 per year need some sort of post-GCSE Maths course but only ~100,000 do one now. About 6/10 now get at least a C in English and Maths GCSE; most never do any more maths after GCSE.129 There is no widely respected &lsquo;maths for non-maths specialists&rsquo; 16-18 course (see below).130 About 70-80,000 (~1/10 of the cohort) do Maths A Level each year (of these ~\u2153 come from private schools and grammars)131 and ~1-2% also do Further Maths. In the last year for which we have data, ~0.5% (3,580 pupils) went on to get A or A* in each of A Level Maths, Further Maths, and Physics.132 Further, many universities only demand GCSE Maths as a condition of entry even for scienti\ufb01c degrees, so ~20% of HE Engineering entrants, ~40% of Chemistry and Economics entrants, and ~60-70% of Biology and Computer Science entrants do not have A Level Maths. Less than10% of undergraduate bioscience degree courses demand A Level Maths.&nbsp;</p>\n<p>Because of how courses have been devised, ~4/5 pupils leave England&rsquo;s schools without basic knowledge of subjects like logarithms and exponential functions which are fundamental to many theoretical and practical problems (such as compound interest and interpreting a simple chart on a log scale), and unaware of the maths and physics of Newton (basic calculus and mechanics). Less than one in ten has a grasp of the maths of probability developed in the 19th Century such as &lsquo;normal distributions&rsquo; and the Central Limit Theorem (&lsquo;bell curves&rsquo;) and conditional probability.133 Only the 1-2% doing Further Maths study complex numbers, matrices and basic linear algebra. Basic logic and set theory (developed c. 1850-1940) do not feature in Maths or Further Maths A levels, so almost nobody leaves school with even a vague idea of the modern axiomatic approach to maths unless they go to a very unusual school or teach themselves.</p>\n<p>133 Gigerenzer&rsquo;s &lsquo;Reckoning With Risk&rsquo; has terrifying stats on the inability of trained medical professionals making life and death decisions to understand the basics of conditional probability, which is not covered in the pre-16 curriculum (cf. Endnote). Current A Level modules have conditional probability and normal distributions in S1 and S2 (not compulsory), so one could have an A* in A Level Maths and Further Maths without knowing what these are. Data on who does which modules is not published by exam boards.</p>\n<p>(...)</p>\n<p>The education world generally resists \ufb01ercely the idea that a large fraction of children can or should be introduced to advanced ideas but we could substantially raise expectations without embracing &lsquo;Ender&rsquo;s Game&rsquo;. It is almost never asked: how could we explore rigorously how ambitious it is realistic to be? <strong>If you ask, including in the Royal Society, &lsquo;what proportion of kids with an IQ of X could master integration given a great teacher?&rsquo;, you will get only blank looks and &lsquo;I don&rsquo;t think anyone has researched that&rsquo;.</strong> Given the lack of empirical research into what pupils with different levels of cognitive ability are capable of with good teachers, <em>research that obviously should be undertaken</em>, and given excellent schools (private or state) show high performance is possible, it is important to err on the side of over-ambition rather than continue the current low expectations. Programmes in America have shown that &lsquo;adolescents scoring 500 or higher on SAT-M or SAT-V by age 13 (top 1 in 200), can assimilate a full high school course (e.g., chemistry, English, and mathematics) in three weeks at summer residential programs for intellectually precocious youth; yet, those scoring 700 or more (top 1 in 10,000), can assimilate at least twice this amount&hellip;&rsquo; (Lubinski, 2010). (See Endnote.)</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BkDGfZey5p8YdSJvH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 39, "extendedScore": null, "score": 9.3e-05, "legacy": true, "legacyId": "24383", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-12T21:18:54.754Z", "modifiedAt": null, "url": null, "title": "On the importance of taking limits: Infinite Spheres of Utility", "slug": "on-the-importance-of-taking-limits-infinite-spheres-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:02.245Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "aspera", "createdAt": "2012-03-20T21:28:32.004Z", "isAdmin": false, "displayName": "aspera"}, "userId": "W58nKSarCzqXj8TPS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jRK5syD9MXGc4jYgv/on-the-importance-of-taking-limits-infinite-spheres-of", "pageUrlRelative": "/posts/jRK5syD9MXGc4jYgv/on-the-importance-of-taking-limits-infinite-spheres-of", "linkUrl": "https://www.lesswrong.com/posts/jRK5syD9MXGc4jYgv/on-the-importance-of-taking-limits-infinite-spheres-of", "postedAtFormatted": "Saturday, October 12th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20the%20importance%20of%20taking%20limits%3A%20Infinite%20Spheres%20of%20Utility&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20the%20importance%20of%20taking%20limits%3A%20Infinite%20Spheres%20of%20Utility%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjRK5syD9MXGc4jYgv%2Fon-the-importance-of-taking-limits-infinite-spheres-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20the%20importance%20of%20taking%20limits%3A%20Infinite%20Spheres%20of%20Utility%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjRK5syD9MXGc4jYgv%2Fon-the-importance-of-taking-limits-infinite-spheres-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjRK5syD9MXGc4jYgv%2Fon-the-importance-of-taking-limits-infinite-spheres-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2411, "htmlBody": "<p>I had a discussion recently with some Less Wrongers about a decision problem involving infinities, which appears to have a paradoxical solution. We have been warned by Jaynes and others to be careful about taking the proper limits when infinities are involved in a problem, and I thought this would be a good example to show that we can get answers that make sense out of problems that seem not to.</p>\n<p><a id=\"more\"></a></p>\n<p>The problem is the \"Infinite Spheres of Utility.\" To quote a description from <a href=\"http://www.philosophyetc.net/2006/03/infinite-spheres-of-utility.html\">Philosophy et cetera</a>,</p>\n<blockquote>\n<p>Imagine a universe containing infinitely many immortal people, partitioned into two \"spheres\". In one sphere [sphere A], all the inhabitants live a blissful existence, whereas the members of the other sphere [sphere B] suffer unbearable agony. Now compare the following two variations:</p>\n<ol>\n<li>Everyone starts off in the blissful sphere. But each day, one more person gets permanently transferred across to the agony sphere, where they reside for the rest of eternity.</li>\n<li>Everyone starts off in the agony sphere. But each day, one more person gets permanently transferred across to the blissful sphere, where they reside for the rest of eternity.</li>\n</ol>\n<p>&nbsp;</p>\n</blockquote>\n<p>At first consideration, the problem appears to cause a paradox:</p>\n<blockquote>\n<p>Which scenario is better? The answer, paradoxically, appears to be \"both\".</p>\n<ul>\n<li>At any moment in time, there will be infinitely many people in the original sphere, and only a finite number who have been transferred across. So option 1 is better.</li>\n</ul>\n<ul>\n<li>However, each particular person will spend only a finite amount of time in the first sphere, whereas they will spend an eternity in their post-transfer home. So option 2 is better.</li>\n</ul>\n<p>&nbsp;</p>\n</blockquote>\n<p class=\"MsoNormal\">Given these reasonable but hard-to-reconcile viewpoints, how do we make a decision?</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<h2>Deciding how to decide</h2>\n<p class=\"MsoNormal\">We first need to decide which kind of decision analysis we want to use to choose a starting sphere. For example, we might simply have an arbitrary preference for putting everyone in the bad sphere. Paradox over. Even if we want to use a form of utilitarianism, we have more than one type to choose from. One of the simplest and most intuitive is <em>additive utilitarianism</em>, in which we define a utility for each person, add them all together, and make the choice with the larger total utility. We can think of the paradox above as resulting from our insistence that the solution conform to this kind of decision analysis: it appears that there should be more than one conflicting solution. In fact, we will see that there are three mutually exclusive solutions conforming to additive utilitarianism, and we can get to any one of them by solving the finite problem and choosing how to take the infinite limit.</p>\n<p>&nbsp;</p>\n<h2>The Finite Problem</h2>\n<p>We will begin by creating an analogous finite problem using additive utilitarianism.</p>\n<p>Define <img title=\"u_A\" src=\"http://www.codecogs.com/png.latex?u_A\" alt=\"\" align=\"bottom\" /> and <img title=\"u_B\" src=\"http://www.codecogs.com/png.latex?u_B\" alt=\"\" align=\"bottom\" /> as the utilities for one person living for one day in spheres <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\" /> or <img title=\"B\" src=\"http://www.codecogs.com/png.latex?B\" alt=\"\" align=\"bottom\" />, respectively, which carry units of [utility/(day*person)]. We might, for example, choose <img title=\"u_A=1\" src=\"http://www.codecogs.com/png.latex?u_A=1\" alt=\"\" align=\"bottom\" /> and <img title=\"u_B=-1\" src=\"http://www.codecogs.com/png.latex?u_B=-1\" alt=\"\" align=\"bottom\" />, but we will certainly choose <img title=\"u_A &gt; u_B\" src=\"http://www.codecogs.com/png.latex?u_A &gt; u_B\" alt=\"\" align=\"bottom\" />. If we make \"choice 1,\" we begin with all people in sphere A. If we make \"choice 2,\" we begin with all people in sphere <img title=\"B\" src=\"http://www.codecogs.com/png.latex?B\" alt=\"\" align=\"bottom\" />. The total utility is the sum of <img title=\"u_A\" src=\"http://www.codecogs.com/png.latex?u_A\" alt=\"\" align=\"bottom\" /> and <img title=\"u_B\" src=\"http://www.codecogs.com/png.latex?u_B\" alt=\"\" align=\"bottom\" /> over all people and all days, or</p>\n<p><img title=\"U =\\sum_{t= 0}^{s}u_A n_{A,t} + u_B n_{B,t},\" src=\"http://www.codecogs.com/png.latex?U =\\sum_{t= 0}^{s}u_A n_{A,t} + u_B n_{B,t}\" alt=\"\" align=\"bottom\" /></p>\n<p>where <img title=\"t\" src=\"http://www.codecogs.com/png.latex?t\" alt=\"\" align=\"bottom\" /> is the day, <img title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\" /> is the number of days each person lives, <img title=\"n_{A,t}\" src=\"http://www.codecogs.com/png.latex?n_{A,t}\" alt=\"\" align=\"bottom\" /> and <img title=\"n_{B,t}\" src=\"http://www.codecogs.com/png.latex?n_{B,t}\" alt=\"\" align=\"bottom\" /> are the number of people in spheres <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\" /> and <img title=\"B\" src=\"http://www.codecogs.com/png.latex?B\" alt=\"\" align=\"bottom\" /> on day <img title=\"t\" src=\"http://www.codecogs.com/png.latex?t\" alt=\"\" align=\"bottom\" />, respectively, and I've implicitly summed over the number of people for brevity. We have assumed a linear utility function that values every person&rsquo;s utility equally on every day, but we could easily generalize to other functional forms. Define <img title=\"r\" src=\"http://www.codecogs.com/png.latex?r\" alt=\"\" align=\"bottom\" /> as the people-transfer rate, which the problem dictates to be</p>\n<p><img title=\"r=1[person/day]\" src=\"http://www.codecogs.com/png.latex?r=1[person/day]\" alt=\"\" align=\"bottom\" />.</p>\n<p>For choice 1, we begin with <img title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\" /> people in sphere <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\" /> and lose <img title=\"r\" src=\"http://www.codecogs.com/png.latex?r\" alt=\"\" align=\"bottom\" /> of them per day so</p>\n<p><img title=\"n_{A,t} = n - t r\" src=\"http://www.codecogs.com/png.latex?n_{A,t} = n - t r\" alt=\"\" align=\"bottom\" /></p>\n<p>and</p>\n<p><img title=\"n_{B,t} = t r\" src=\"http://www.codecogs.com/png.latex?n_{B,t} = t r\" alt=\"\" align=\"bottom\" />,</p>\n<p>with these variables only defined for <img title=\"t &lt; s\" src=\"http://www.codecogs.com/png.latex?t &lt; s\" alt=\"\" align=\"bottom\" />, beyond which everyone would be dead. Note that even though <img title=\"r\" src=\"http://www.codecogs.com/png.latex?r\" alt=\"\" align=\"bottom\" /> is 1, we'll end up with a unit error if we don't carry it around. If we go with choice 2 instead, these variables are switched, so</p>\n<p><img title=\"n_{A,t} = t r\" src=\"http://www.codecogs.com/png.latex?n_{A,t} = t r\" alt=\"\" align=\"bottom\" /></p>\n<p>and</p>\n<p><img title=\"n_{B,t} = n - t r\" src=\"http://www.codecogs.com/png.latex?n_{B,t} = n - t r\" alt=\"\" align=\"bottom\" />.</p>\n<p>According to the problem statement, the transfer rate is constant, but we could again easily generalize to any transfer function if we wanted to (even non-monotonic ones, or ones that depend on the number of people in either sphere). Call the total utilities for each scheme <img title=\"U_1\" src=\"http://www.codecogs.com/png.latex?U_1\" alt=\"\" align=\"bottom\" /> and <img title=\"U_2\" src=\"http://www.codecogs.com/png.latex?U_2\" alt=\"\" align=\"bottom\" />. From here on out, I will replace sums with integrals, because the graphs will look better and the math takes up less space. We take on a small error in the process, but it won&rsquo;t affect the conclusion.</p>\n<p>Then the total utilities are</p>\n<p><img title=\"U_1 = \\int_{0}^{s}dt\\:\\:u_A (n - t r) + u_B t r\" src=\"http://www.codecogs.com/png.latex?U_1 = \\int_{0}^{s}dt\\:\\:u_A (n - t r) + u_B t r\" alt=\"\" align=\"bottom\" /></p>\n<p>&nbsp;for choice 1, and</p>\n<p><img title=\"U_2 = \\int_{0}^{s}dt\\:\\:u_A t r + u_B (n - t r),\" src=\"http://www.codecogs.com/png.latex?U_2 = \\int_{0}^{s}dt\\:\\:u_A t r + u_B (n - t r),\" alt=\"\" align=\"bottom\" /></p>\n<p>for choice 2, so that the differential utility is</p>\n<p><img title=\"\\Delta U = U_1-U_2 = \\int_0^s dt\\:\\:u_A(n - 2t r) + u_B (2t r - n).\" src=\"http://www.codecogs.com/png.latex?\\Delta U = U_1-U_2 = \\int_0^s dt\\:\\:u_A(n - 2t r) + u_B (2t r - n).\" alt=\"\" align=\"bottom\" /></p>\n<p>Integrating over <img title=\"dt\" src=\"http://www.codecogs.com/png.latex?dt\" alt=\"\" align=\"bottom\" />, we have</p>\n<p><img title=\"\\Delta U = (ns - s^2 r)(u_A-u_B).\" src=\"http://www.codecogs.com/png.latex?\\Delta U = (ns - s^2 r)(u_A-u_B).\" alt=\"\" align=\"bottom\" /></p>\n<p>When this function is positive, choice 1 is better. When it&rsquo;s negative, choice 2 is better. Keep in mind that the second factor is always positive, since we set <img title=\"u_A&gt;u_B\" src=\"http://www.codecogs.com/png.latex?u_A&gt;u_B\" alt=\"\" align=\"bottom\" />.</p>\n<p>First, let's confirm that this result makes sense for finite <img title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\" /> and <img title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\" />. Figure 1 shows what happens as we increase the number of people in the problem, but keep their life spans fixed. The differential utility forms a line, with the first part of the line below the horizontal axis. For a small number of people, it&rsquo;s better to start everyone in sphere <img title=\"B\" src=\"http://www.codecogs.com/png.latex?B\" alt=\"\" align=\"bottom\" />, since everyone will be quickly transferred to <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\" />. But for a large number of people, it&rsquo;s better to let them live out most of their lives on average in sphere <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\" /> before getting transferred to sphere <img title=\"B\" src=\"http://www.codecogs.com/png.latex?B\" alt=\"\" align=\"bottom\" />. As <img title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\" /> approaches infinity, choice 1 is clearly better, since we have positive <img title=\"\\Delta U\" src=\"http://www.codecogs.com/png.latex?\\Delta U\" alt=\"\" align=\"bottom\" />.</p>\n<p align=\"center\"><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_isp_2.png\" alt=\"\" width=\"417\" height=\"258\" /></p>\n<p align=\"center\"><strong>Figure 1: Fixed lifespan.</strong> As the population increases, choice 1 becomes monotonically better.</p>\n<p>&nbsp;</p>\n<p>In contrast, Figure 2 shows what happens as a fixed number of people become long-lived. This time, we get a parabola. If everyone dies quickly, it&rsquo;s better to start in sphere <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\" />, since many of them will die before they get transferred. If they live a long time, it&rsquo;s better to start in sphere <img title=\"B\" src=\"http://www.codecogs.com/png.latex?B\" alt=\"\" align=\"bottom\" />, and then live out most of their lives after being transferred to sphere <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\" />. As <img title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\" /> approaches infinity, choice 2 is clearly better, since we want people living out their immortality in a good universe, which corresponds to <img title=\"\\Delta U\" src=\"http://www.codecogs.com/png.latex?\\Delta U\" alt=\"\" align=\"bottom\" /> being negative.</p>\n<p align=\"center\"><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_isp_1.png\" alt=\"\" width=\"417\" height=\"276\" /></p>\n<p align=\"center\"><strong>Figure 2: Fixed populaiton. </strong>As life span increases, choice 2 becomes better.</p>\n<p>&nbsp;</p>\n<h2>The Infinite Limit</h2>\n<p><span style=\"font-size: small;\">What happens if both </span><img style=\"font-size: small;\" title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> and </span><img style=\"font-size: small;\" title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> go to infinity, as stated in the problem? If we just plug </span><img style=\"font-size: small;\" title=\"\\{s,n\\}=\\{\\infty,\\infty\\}\" src=\"http://www.codecogs.com/png.latex?\\{s,n\\}=\\{\\infty,\\infty\\}\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> into our </span><img style=\"font-size: small;\" title=\"\\Delta U\" src=\"http://www.codecogs.com/png.latex?\\Delta U\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> equation, the answer is undefined, and that is the substance of the paradox. But of course, mathematics lets us carefully take limits of functions of multiple variables. Figure 3 is a density plot of </span><img style=\"font-size: small;\" title=\"\\Delta U\" src=\"http://www.codecogs.com/png.latex?\\Delta U\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> as a function of both </span><img style=\"font-size: small;\" title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> and </span><img style=\"font-size: small;\" title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\">. The value of </span><img style=\"font-size: small;\" title=\"\\Delta U\" src=\"http://www.codecogs.com/png.latex?\\Delta U\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> is mapped on to color. When we refer to a system with &ldquo;an infinite number of immortal people,&rdquo; we&rsquo;re talking about a point far away from the origin, where both </span><img style=\"font-size: small;\" title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> and </span><img style=\"font-size: small;\" title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> are positive and infinite. But from the density plot, it&rsquo;s clear that we could be talking about many different points with different values, and we have to specify which direction we go to get there. For example, if we follow the red dotted line out from the origin, we will find that choice 1 gets better and better the more we increase </span><img style=\"font-size: small;\" title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> and </span><img style=\"font-size: small;\" title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\">. If we follow the blue dotted line, choice 2 is better for large </span><img style=\"font-size: small;\" title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> and </span><img style=\"font-size: small;\" title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\">. Finally, if we follow the green dotted line out to infinity, we find that </span><img style=\"font-size: small;\" title=\"\\Delta U = 0\" src=\"http://www.codecogs.com/png.latex?\\Delta U = 0\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> for all </span><img style=\"font-size: small;\" title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> and </span><img style=\"font-size: small;\" title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\">, meaning that our choices are equally good (or bad). </span></p>\n<p align=\"center\"><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_isp_0.png\" alt=\"Density plot of differential utility\" width=\"511\" height=\"390\" /></p>\n<p align=\"center\"><strong>Figure 3: Utility vs. population and life span.</strong>&nbsp;The best choice depends on what path we follow to take the limit.</p>\n<p><span style=\"font-size: small;\">Practically, these lines represent situations in which we choose a finite number of people and a finite life span, and then monitor what happens to </span><img style=\"font-size: small;\" title=\"\\Delta U\" src=\"http://www.codecogs.com/png.latex?\\Delta U\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> as we increase them both at a constant (but not necessarily equal) rate. We find that the answer depends on the difference in rates. To formalize this result, we can set </span><img style=\"font-size: small;\" title=\"n = \\alpha*s\" src=\"http://www.codecogs.com/png.latex?n = \\alpha*s\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> in the </span><img style=\"font-size: small;\" title=\"\\Delta U\" src=\"http://www.codecogs.com/png.latex?\\Delta U\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> equation, where </span><img style=\"font-size: small;\" title=\"\\alpha\" src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> has units of [people/day]</span><span style=\"font-size: small;\">, and is allowed to be fractional. That way, every time we double </span><img style=\"font-size: small;\" title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\">, we multiply </span><img style=\"font-size: small;\" title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> by </span><img style=\"font-size: small;\" title=\"2\\alpha\" src=\"http://www.codecogs.com/png.latex?2\\alpha\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\">, and we move towards infinity that way. Put another way, we <em>constrain</em> ourselves to a linear relationship between </span><img style=\"font-size: small;\" title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> and </span><img style=\"font-size: small;\" title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\" />&nbsp;(we could take a non-linear path out to infinity if we cared to)<span style=\"font-size: small;\">. We then have </span></p>\n<p><img title=\"\\Delta U_{linear} = s^2 (\\alpha - r)(u_A - u_B).\" src=\"http://www.codecogs.com/png.latex?\\Delta U_{linear} = s^2 (\\alpha - r)(u_A - u_B).\" alt=\"\" align=\"bottom\" /></p>\n<p><span style=\"font-size: small;\">The limit of this function as </span><img style=\"font-size: small;\" title=\"s \\rightarrow \\infty\" src=\"http://www.codecogs.com/png.latex?s \\rightarrow \\infty\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> is </span><img style=\"font-size: small;\" title=\"+\\infty\" src=\"http://www.codecogs.com/png.latex?+\\infty\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> if </span><img style=\"font-size: small;\" title=\"\\alpha &lt;r\" src=\"http://www.codecogs.com/png.latex?\\alpha &lt;r\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\">, </span><img style=\"font-size: small;\" title=\"-\\infty\" src=\"http://www.codecogs.com/png.latex?-\\infty\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> if </span><img style=\"font-size: small;\" title=\"\\alpha&gt;r\" src=\"http://www.codecogs.com/png.latex?\\alpha&gt;r\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\">, and </span><img style=\"font-size: small;\" title=\"0\" src=\"http://www.codecogs.com/png.latex?0\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> if </span><img style=\"font-size: small;\" title=\"\\alpha = r\" src=\"http://www.codecogs.com/png.latex?\\alpha = r\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\">, which covers all of the possible linear paths to infinite population and infinite lifespan.</span></p>\n<p>&nbsp;</p>\n<h2>Expected Personal Utility</h2>\n<p><span style=\"font-size: small;\"> Another interesting question to ask is \"If I were one of the people in the problem, what would I expect my differential utility to be?\" We can answer this question using the same method as above. I define my utility per day in each sphere as </span><img style=\"font-size: small;\" title=\"u_{A}\" src=\"http://www.codecogs.com/png.latex?u_{A}\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> and </span><img style=\"font-size: small;\" title=\"u_{B}\" src=\"http://www.codecogs.com/png.latex?u_{B}\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\"> in units of [utility/day].</span><span style=\"font-size: small;\">&nbsp;The expected number of days that I will be in the starting sphere for a finite population is </span><img style=\"font-size: small;\" title=\"n/(2r)\" src=\"http://www.codecogs.com/png.latex?n/(2r)\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\">, and the number of days in the second sphere for a finite population and life span is </span><img style=\"font-size: small;\" title=\"s - n/2r\" src=\"http://www.codecogs.com/png.latex?s - n/2r\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\">. My expected utility is therefore given by </span></p>\n<p><img title=\"U_1 = u_A\\left(\\frac{n}{2r}\\right) + u_B\\left(s-\\frac{n}{2r}\\right)\" src=\"http://www.codecogs.com/png.latex?U_1 = u_A\\left(\\frac{n}{2r}\\right) + u_B\\left(s-\\frac{n}{2r}\\right)\" alt=\"\" align=\"bottom\" /></p>\n<p><span style=\"font-size: small;\"> for choice 1, and</span></p>\n<p><img title=\"U_2 = u_A\\left(s-\\frac{n}{2r}\\right) + u_B\\left(\\frac{n}{2r}\\right)\" src=\"http://www.codecogs.com/png.latex?U_2 = u_A\\left(s-\\frac{n}{2r}\\right) + u_B\\left(\\frac{n}{2r}\\right)\" alt=\"\" align=\"bottom\" /></p>\n<p><span style=\"font-size: small;\">for choice 2. The differential utility is the difference between these, or </span></p>\n<p><img title=\"\\Delta U = \\left(u_A - u_B\\right)\\left(\\frac{n}{r} - s\\right).\" src=\"http://www.codecogs.com/png.latex?\\Delta U = \\left(u_A - u_B\\right)\\left(\\frac{n}{r} - s\\right).\" alt=\"\" align=\"bottom\" /></p>\n<p><span style=\"font-size: small;\">If we just plug in infinities, the answer is undefined. But if we use the same method as above, and define </span><img style=\"font-size: small;\" title=\"n = \\alpha*s\" src=\"http://www.codecogs.com/png.latex?n = \\alpha*s\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\">, then we can write</span></p>\n<p><img title=\"\\Delta U = s\\left(u_A - u_B\\right)\\left(\\frac{\\alpha}{r} - 1\\right),\" src=\"http://www.codecogs.com/png.latex?\\Delta U = s\\left(u_A - u_B\\right)\\left(\\frac{\\alpha}{r} - 1\\right),\" alt=\"\" align=\"bottom\" /></p>\n<p><span style=\"font-size: small;\"> and we have well defined limits for all slopes </span><img style=\"font-size: small;\" title=\"\\alpha\\in\\{0,\\infty\\}\" src=\"http://www.codecogs.com/png.latex?\\alpha\\in\\{0,\\infty\\}\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\">. If I choose </span><img style=\"font-size: small;\" title=\"\\alpha&lt;r\" src=\"http://www.codecogs.com/png.latex?\\alpha&lt;r\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\">, I find that my lifespan grows faster than the population grows, and I'm better off starting in the bad sphere, expecting to be transfered before half my life is over. If I choose </span><img style=\"font-size: small;\" title=\"\\alpha&gt;r\" src=\"http://www.codecogs.com/png.latex?\\alpha&gt;r\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\">, the population grows too quickly, and I would prefer to start in the good sphere, since on average I'll die before I live more than half my life in the bad sphere. And as before, if </span><img style=\"font-size: small;\" title=\"\\alpha=r\" src=\"http://www.codecogs.com/png.latex?\\alpha=r\" alt=\"\" align=\"bottom\" /><span style=\"font-size: small;\">, I'm indifferent to the two plans, since on average I will live half my life in each sphere either way.</span></p>\n<p><span style=\"font-size: small;\"><br /></span></p>\n<h2>The Answer</h2>\n<p class=\"MsoNormal\">If we want to make a decision based on additive utility, the infinite problem is<em> ill posed</em>; it has no unique solution unless we take on additional assumptions. In particular, we introduced an additional parameter&nbsp;<img style=\"font-size: small;\" title=\"\\alpha\" src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\" align=\"bottom\" />,&nbsp;making clear three well defined solutions that span the space of possible solutions. In doing so, we solved a&nbsp;<em>similar&nbsp;</em>well posed problem, but not the original one.</p>\n<p class=\"MsoNormal\">So why bother, if we didn't solve the problem? It's worth working through the math because tt gave us an intuition for how the system works, including a quantitative understanding of crossing points, intercepts, and curvatures of the utility curves. While the problem statement corresponds to an impossible situation, the finite problem is quite possible, and taking linear limits could very well correspond to a real physical process. If we notice a paradox and then stop thinking, we lose an opportunity to gain a better understanding of the decision process.</p>\n<p class=\"MsoNormal\">We could also consider using a different paradigm of decision theory that might deal with infinite quantities better. With that said, I think that additive utilitarianism represents well the intuitive paradox that presents each choice as better than the other. Our graphs with one variable fixed showed that these are both intuitively valid viewpoints when taken alone, and their reconciliation is the challenging part.</p>\n<p><span style=\"font-size: small;\">If you're interested in similar problems, the <a href=\"http://en.wikipedia.org/wiki/St._Petersburg_paradox\">St. Petersburg Paradox</a>&nbsp;also involves diverging utillities, and it&nbsp;has has been \"dispelled\" <a href=\"http://www.jstor.org/stable/2525406\">here</a>.</span></p>\n<p>&nbsp;</p>\n<p><em>Author's notes:&nbsp;</em><em>[last edit Oct.15, 2013]</em>&nbsp;<em>The subject of unbounded utilities is not new here at Less Wrong.&nbsp;<a href=\"/user/Stuart_Armstrong/overview/\">Stuart_Armstrong</a>&nbsp;has a well written analysis of the&nbsp;<a href=\"/lw/giu/naturalism_versus_unbounded_or_unmaximisable/\">Heaven and Hell problem</a>, among others.&nbsp;<a href=\"/user/PhilGoetz/overview/\">PhilGoetz</a>&nbsp;has a useful&nbsp;<a href=\"/lw/15a/a_note_on_hypotheticals/\">note on infinities</a>. And on a related topic,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Infinite_set_atheism\">infinite set atheism</a>&nbsp;abounds. I have been unable to find on LW examples of careful mathematical treatments of apparent paradoxes that result from misuse of infinites in utilitarian calculations, and I hope this article serves as a concrete example of how easy it is to defeat (some) such problems. If there are other examples at LW, please post them in the comments: I&rsquo;d love to read them. It is interesting to note that&nbsp;<a href=\"http://en.wikipedia.org/wiki/Pascal's_Wager\">Pascal's Wager</a>, a famous related problem, is rather more difficult to solve. It involves not only infinities, but also a hypothesis space whose cardinality and particular members are not obvious.</em></p>\n<p><em>Thanks to <a href=\"/user/Mestroyer/overview/\">Mestroyer</a> for bringing this problem to my attention,&nbsp;<a href=\"/user/Manfred/overview/\">Manfred</a> for helping me work through it,&nbsp;</em><em><span style=\"font-size: 11.0pt; line-height: 115%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font: minor-latin; mso-fareast-font-family: Calibri; mso-fareast-theme-font: minor-latin; mso-hansi-theme-font: minor-latin; mso-bidi-font-family: &quot;Times New Roman&quot;; mso-bidi-theme-font: minor-bidi; mso-ansi-language: EN-US; mso-fareast-language: EN-US; mso-bidi-language: AR-SA;\">and <a href=\"/user/VincentYu/overview/\">VincentYu</a> for pointing out the issues surrounding additive utilitarianism as a decision paradigm.</span></em></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2aa": 1, "L3NcKBNTvQaFXwv9u": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jRK5syD9MXGc4jYgv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 35, "extendedScore": null, "score": 1.3775611958384617e-06, "legacy": true, "legacyId": "24361", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I had a discussion recently with some Less Wrongers about a decision problem involving infinities, which appears to have a paradoxical solution. We have been warned by Jaynes and others to be careful about taking the proper limits when infinities are involved in a problem, and I thought this would be a good example to show that we can get answers that make sense out of problems that seem not to.</p>\n<p><a id=\"more\"></a></p>\n<p>The problem is the \"Infinite Spheres of Utility.\" To quote a description from <a href=\"http://www.philosophyetc.net/2006/03/infinite-spheres-of-utility.html\">Philosophy et cetera</a>,</p>\n<blockquote>\n<p>Imagine a universe containing infinitely many immortal people, partitioned into two \"spheres\". In one sphere [sphere A], all the inhabitants live a blissful existence, whereas the members of the other sphere [sphere B] suffer unbearable agony. Now compare the following two variations:</p>\n<ol>\n<li>Everyone starts off in the blissful sphere. But each day, one more person gets permanently transferred across to the agony sphere, where they reside for the rest of eternity.</li>\n<li>Everyone starts off in the agony sphere. But each day, one more person gets permanently transferred across to the blissful sphere, where they reside for the rest of eternity.</li>\n</ol>\n<p>&nbsp;</p>\n</blockquote>\n<p>At first consideration, the problem appears to cause a paradox:</p>\n<blockquote>\n<p>Which scenario is better? The answer, paradoxically, appears to be \"both\".</p>\n<ul>\n<li>At any moment in time, there will be infinitely many people in the original sphere, and only a finite number who have been transferred across. So option 1 is better.</li>\n</ul>\n<ul>\n<li>However, each particular person will spend only a finite amount of time in the first sphere, whereas they will spend an eternity in their post-transfer home. So option 2 is better.</li>\n</ul>\n<p>&nbsp;</p>\n</blockquote>\n<p class=\"MsoNormal\">Given these reasonable but hard-to-reconcile viewpoints, how do we make a decision?</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<h2 id=\"Deciding_how_to_decide\">Deciding how to decide</h2>\n<p class=\"MsoNormal\">We first need to decide which kind of decision analysis we want to use to choose a starting sphere. For example, we might simply have an arbitrary preference for putting everyone in the bad sphere. Paradox over. Even if we want to use a form of utilitarianism, we have more than one type to choose from. One of the simplest and most intuitive is <em>additive utilitarianism</em>, in which we define a utility for each person, add them all together, and make the choice with the larger total utility. We can think of the paradox above as resulting from our insistence that the solution conform to this kind of decision analysis: it appears that there should be more than one conflicting solution. In fact, we will see that there are three mutually exclusive solutions conforming to additive utilitarianism, and we can get to any one of them by solving the finite problem and choosing how to take the infinite limit.</p>\n<p>&nbsp;</p>\n<h2 id=\"The_Finite_Problem\">The Finite Problem</h2>\n<p>We will begin by creating an analogous finite problem using additive utilitarianism.</p>\n<p>Define <img title=\"u_A\" src=\"http://www.codecogs.com/png.latex?u_A\" alt=\"\" align=\"bottom\"> and <img title=\"u_B\" src=\"http://www.codecogs.com/png.latex?u_B\" alt=\"\" align=\"bottom\"> as the utilities for one person living for one day in spheres <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\"> or <img title=\"B\" src=\"http://www.codecogs.com/png.latex?B\" alt=\"\" align=\"bottom\">, respectively, which carry units of [utility/(day*person)]. We might, for example, choose <img title=\"u_A=1\" src=\"http://www.codecogs.com/png.latex?u_A=1\" alt=\"\" align=\"bottom\"> and <img title=\"u_B=-1\" src=\"http://www.codecogs.com/png.latex?u_B=-1\" alt=\"\" align=\"bottom\">, but we will certainly choose <img title=\"u_A > u_B\" src=\"http://www.codecogs.com/png.latex?u_A > u_B\" alt=\"\" align=\"bottom\">. If we make \"choice 1,\" we begin with all people in sphere A. If we make \"choice 2,\" we begin with all people in sphere <img title=\"B\" src=\"http://www.codecogs.com/png.latex?B\" alt=\"\" align=\"bottom\">. The total utility is the sum of <img title=\"u_A\" src=\"http://www.codecogs.com/png.latex?u_A\" alt=\"\" align=\"bottom\"> and <img title=\"u_B\" src=\"http://www.codecogs.com/png.latex?u_B\" alt=\"\" align=\"bottom\"> over all people and all days, or</p>\n<p><img title=\"U =\\sum_{t= 0}^{s}u_A n_{A,t} + u_B n_{B,t},\" src=\"http://www.codecogs.com/png.latex?U =\\sum_{t= 0}^{s}u_A n_{A,t} + u_B n_{B,t}\" alt=\"\" align=\"bottom\"></p>\n<p>where <img title=\"t\" src=\"http://www.codecogs.com/png.latex?t\" alt=\"\" align=\"bottom\"> is the day, <img title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\"> is the number of days each person lives, <img title=\"n_{A,t}\" src=\"http://www.codecogs.com/png.latex?n_{A,t}\" alt=\"\" align=\"bottom\"> and <img title=\"n_{B,t}\" src=\"http://www.codecogs.com/png.latex?n_{B,t}\" alt=\"\" align=\"bottom\"> are the number of people in spheres <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\"> and <img title=\"B\" src=\"http://www.codecogs.com/png.latex?B\" alt=\"\" align=\"bottom\"> on day <img title=\"t\" src=\"http://www.codecogs.com/png.latex?t\" alt=\"\" align=\"bottom\">, respectively, and I've implicitly summed over the number of people for brevity. We have assumed a linear utility function that values every person\u2019s utility equally on every day, but we could easily generalize to other functional forms. Define <img title=\"r\" src=\"http://www.codecogs.com/png.latex?r\" alt=\"\" align=\"bottom\"> as the people-transfer rate, which the problem dictates to be</p>\n<p><img title=\"r=1[person/day]\" src=\"http://www.codecogs.com/png.latex?r=1[person/day]\" alt=\"\" align=\"bottom\">.</p>\n<p>For choice 1, we begin with <img title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\"> people in sphere <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\"> and lose <img title=\"r\" src=\"http://www.codecogs.com/png.latex?r\" alt=\"\" align=\"bottom\"> of them per day so</p>\n<p><img title=\"n_{A,t} = n - t r\" src=\"http://www.codecogs.com/png.latex?n_{A,t} = n - t r\" alt=\"\" align=\"bottom\"></p>\n<p>and</p>\n<p><img title=\"n_{B,t} = t r\" src=\"http://www.codecogs.com/png.latex?n_{B,t} = t r\" alt=\"\" align=\"bottom\">,</p>\n<p>with these variables only defined for <img title=\"t < s\" src=\"http://www.codecogs.com/png.latex?t < s\" alt=\"\" align=\"bottom\">, beyond which everyone would be dead. Note that even though <img title=\"r\" src=\"http://www.codecogs.com/png.latex?r\" alt=\"\" align=\"bottom\"> is 1, we'll end up with a unit error if we don't carry it around. If we go with choice 2 instead, these variables are switched, so</p>\n<p><img title=\"n_{A,t} = t r\" src=\"http://www.codecogs.com/png.latex?n_{A,t} = t r\" alt=\"\" align=\"bottom\"></p>\n<p>and</p>\n<p><img title=\"n_{B,t} = n - t r\" src=\"http://www.codecogs.com/png.latex?n_{B,t} = n - t r\" alt=\"\" align=\"bottom\">.</p>\n<p>According to the problem statement, the transfer rate is constant, but we could again easily generalize to any transfer function if we wanted to (even non-monotonic ones, or ones that depend on the number of people in either sphere). Call the total utilities for each scheme <img title=\"U_1\" src=\"http://www.codecogs.com/png.latex?U_1\" alt=\"\" align=\"bottom\"> and <img title=\"U_2\" src=\"http://www.codecogs.com/png.latex?U_2\" alt=\"\" align=\"bottom\">. From here on out, I will replace sums with integrals, because the graphs will look better and the math takes up less space. We take on a small error in the process, but it won\u2019t affect the conclusion.</p>\n<p>Then the total utilities are</p>\n<p><img title=\"U_1 = \\int_{0}^{s}dt\\:\\:u_A (n - t r) + u_B t r\" src=\"http://www.codecogs.com/png.latex?U_1 = \\int_{0}^{s}dt\\:\\:u_A (n - t r) + u_B t r\" alt=\"\" align=\"bottom\"></p>\n<p>&nbsp;for choice 1, and</p>\n<p><img title=\"U_2 = \\int_{0}^{s}dt\\:\\:u_A t r + u_B (n - t r),\" src=\"http://www.codecogs.com/png.latex?U_2 = \\int_{0}^{s}dt\\:\\:u_A t r + u_B (n - t r),\" alt=\"\" align=\"bottom\"></p>\n<p>for choice 2, so that the differential utility is</p>\n<p><img title=\"\\Delta U = U_1-U_2 = \\int_0^s dt\\:\\:u_A(n - 2t r) + u_B (2t r - n).\" src=\"http://www.codecogs.com/png.latex?\\Delta U = U_1-U_2 = \\int_0^s dt\\:\\:u_A(n - 2t r) + u_B (2t r - n).\" alt=\"\" align=\"bottom\"></p>\n<p>Integrating over <img title=\"dt\" src=\"http://www.codecogs.com/png.latex?dt\" alt=\"\" align=\"bottom\">, we have</p>\n<p><img title=\"\\Delta U = (ns - s^2 r)(u_A-u_B).\" src=\"http://www.codecogs.com/png.latex?\\Delta U = (ns - s^2 r)(u_A-u_B).\" alt=\"\" align=\"bottom\"></p>\n<p>When this function is positive, choice 1 is better. When it\u2019s negative, choice 2 is better. Keep in mind that the second factor is always positive, since we set <img title=\"u_A>u_B\" src=\"http://www.codecogs.com/png.latex?u_A>u_B\" alt=\"\" align=\"bottom\">.</p>\n<p>First, let's confirm that this result makes sense for finite <img title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\"> and <img title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\">. Figure 1 shows what happens as we increase the number of people in the problem, but keep their life spans fixed. The differential utility forms a line, with the first part of the line below the horizontal axis. For a small number of people, it\u2019s better to start everyone in sphere <img title=\"B\" src=\"http://www.codecogs.com/png.latex?B\" alt=\"\" align=\"bottom\">, since everyone will be quickly transferred to <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\">. But for a large number of people, it\u2019s better to let them live out most of their lives on average in sphere <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\"> before getting transferred to sphere <img title=\"B\" src=\"http://www.codecogs.com/png.latex?B\" alt=\"\" align=\"bottom\">. As <img title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\"> approaches infinity, choice 1 is clearly better, since we have positive <img title=\"\\Delta U\" src=\"http://www.codecogs.com/png.latex?\\Delta U\" alt=\"\" align=\"bottom\">.</p>\n<p align=\"center\"><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_isp_2.png\" alt=\"\" width=\"417\" height=\"258\"></p>\n<p align=\"center\"><strong>Figure 1: Fixed lifespan.</strong> As the population increases, choice 1 becomes monotonically better.</p>\n<p>&nbsp;</p>\n<p>In contrast, Figure 2 shows what happens as a fixed number of people become long-lived. This time, we get a parabola. If everyone dies quickly, it\u2019s better to start in sphere <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\">, since many of them will die before they get transferred. If they live a long time, it\u2019s better to start in sphere <img title=\"B\" src=\"http://www.codecogs.com/png.latex?B\" alt=\"\" align=\"bottom\">, and then live out most of their lives after being transferred to sphere <img title=\"A\" src=\"http://www.codecogs.com/png.latex?A\" alt=\"\" align=\"bottom\">. As <img title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\"> approaches infinity, choice 2 is clearly better, since we want people living out their immortality in a good universe, which corresponds to <img title=\"\\Delta U\" src=\"http://www.codecogs.com/png.latex?\\Delta U\" alt=\"\" align=\"bottom\"> being negative.</p>\n<p align=\"center\"><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_isp_1.png\" alt=\"\" width=\"417\" height=\"276\"></p>\n<p align=\"center\"><strong>Figure 2: Fixed populaiton. </strong>As life span increases, choice 2 becomes better.</p>\n<p>&nbsp;</p>\n<h2 id=\"The_Infinite_Limit\">The Infinite Limit</h2>\n<p><span style=\"font-size: small;\">What happens if both </span><img style=\"font-size: small;\" title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> and </span><img style=\"font-size: small;\" title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> go to infinity, as stated in the problem? If we just plug </span><img style=\"font-size: small;\" title=\"\\{s,n\\}=\\{\\infty,\\infty\\}\" src=\"http://www.codecogs.com/png.latex?\\{s,n\\}=\\{\\infty,\\infty\\}\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> into our </span><img style=\"font-size: small;\" title=\"\\Delta U\" src=\"http://www.codecogs.com/png.latex?\\Delta U\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> equation, the answer is undefined, and that is the substance of the paradox. But of course, mathematics lets us carefully take limits of functions of multiple variables. Figure 3 is a density plot of </span><img style=\"font-size: small;\" title=\"\\Delta U\" src=\"http://www.codecogs.com/png.latex?\\Delta U\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> as a function of both </span><img style=\"font-size: small;\" title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> and </span><img style=\"font-size: small;\" title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\">. The value of </span><img style=\"font-size: small;\" title=\"\\Delta U\" src=\"http://www.codecogs.com/png.latex?\\Delta U\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> is mapped on to color. When we refer to a system with \u201can infinite number of immortal people,\u201d we\u2019re talking about a point far away from the origin, where both </span><img style=\"font-size: small;\" title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> and </span><img style=\"font-size: small;\" title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> are positive and infinite. But from the density plot, it\u2019s clear that we could be talking about many different points with different values, and we have to specify which direction we go to get there. For example, if we follow the red dotted line out from the origin, we will find that choice 1 gets better and better the more we increase </span><img style=\"font-size: small;\" title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> and </span><img style=\"font-size: small;\" title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\">. If we follow the blue dotted line, choice 2 is better for large </span><img style=\"font-size: small;\" title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> and </span><img style=\"font-size: small;\" title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\">. Finally, if we follow the green dotted line out to infinity, we find that </span><img style=\"font-size: small;\" title=\"\\Delta U = 0\" src=\"http://www.codecogs.com/png.latex?\\Delta U = 0\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> for all </span><img style=\"font-size: small;\" title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> and </span><img style=\"font-size: small;\" title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\">, meaning that our choices are equally good (or bad). </span></p>\n<p align=\"center\"><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_isp_0.png\" alt=\"Density plot of differential utility\" width=\"511\" height=\"390\"></p>\n<p align=\"center\"><strong>Figure 3: Utility vs. population and life span.</strong>&nbsp;The best choice depends on what path we follow to take the limit.</p>\n<p><span style=\"font-size: small;\">Practically, these lines represent situations in which we choose a finite number of people and a finite life span, and then monitor what happens to </span><img style=\"font-size: small;\" title=\"\\Delta U\" src=\"http://www.codecogs.com/png.latex?\\Delta U\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> as we increase them both at a constant (but not necessarily equal) rate. We find that the answer depends on the difference in rates. To formalize this result, we can set </span><img style=\"font-size: small;\" title=\"n = \\alpha*s\" src=\"http://www.codecogs.com/png.latex?n = \\alpha*s\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> in the </span><img style=\"font-size: small;\" title=\"\\Delta U\" src=\"http://www.codecogs.com/png.latex?\\Delta U\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> equation, where </span><img style=\"font-size: small;\" title=\"\\alpha\" src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> has units of [people/day]</span><span style=\"font-size: small;\">, and is allowed to be fractional. That way, every time we double </span><img style=\"font-size: small;\" title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\">, we multiply </span><img style=\"font-size: small;\" title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> by </span><img style=\"font-size: small;\" title=\"2\\alpha\" src=\"http://www.codecogs.com/png.latex?2\\alpha\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\">, and we move towards infinity that way. Put another way, we <em>constrain</em> ourselves to a linear relationship between </span><img style=\"font-size: small;\" title=\"s\" src=\"http://www.codecogs.com/png.latex?s\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> and </span><img style=\"font-size: small;\" title=\"n\" src=\"http://www.codecogs.com/png.latex?n\" alt=\"\" align=\"bottom\">&nbsp;(we could take a non-linear path out to infinity if we cared to)<span style=\"font-size: small;\">. We then have </span></p>\n<p><img title=\"\\Delta U_{linear} = s^2 (\\alpha - r)(u_A - u_B).\" src=\"http://www.codecogs.com/png.latex?\\Delta U_{linear} = s^2 (\\alpha - r)(u_A - u_B).\" alt=\"\" align=\"bottom\"></p>\n<p><span style=\"font-size: small;\">The limit of this function as </span><img style=\"font-size: small;\" title=\"s \\rightarrow \\infty\" src=\"http://www.codecogs.com/png.latex?s \\rightarrow \\infty\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> is </span><img style=\"font-size: small;\" title=\"+\\infty\" src=\"http://www.codecogs.com/png.latex?+\\infty\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> if </span><img style=\"font-size: small;\" title=\"\\alpha <r\" src=\"http://www.codecogs.com/png.latex?\\alpha <r\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\">, </span><img style=\"font-size: small;\" title=\"-\\infty\" src=\"http://www.codecogs.com/png.latex?-\\infty\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> if </span><img style=\"font-size: small;\" title=\"\\alpha>r\" src=\"http://www.codecogs.com/png.latex?\\alpha>r\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\">, and </span><img style=\"font-size: small;\" title=\"0\" src=\"http://www.codecogs.com/png.latex?0\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> if </span><img style=\"font-size: small;\" title=\"\\alpha = r\" src=\"http://www.codecogs.com/png.latex?\\alpha = r\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\">, which covers all of the possible linear paths to infinite population and infinite lifespan.</span></p>\n<p>&nbsp;</p>\n<h2 id=\"Expected_Personal_Utility\">Expected Personal Utility</h2>\n<p><span style=\"font-size: small;\"> Another interesting question to ask is \"If I were one of the people in the problem, what would I expect my differential utility to be?\" We can answer this question using the same method as above. I define my utility per day in each sphere as </span><img style=\"font-size: small;\" title=\"u_{A}\" src=\"http://www.codecogs.com/png.latex?u_{A}\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> and </span><img style=\"font-size: small;\" title=\"u_{B}\" src=\"http://www.codecogs.com/png.latex?u_{B}\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\"> in units of [utility/day].</span><span style=\"font-size: small;\">&nbsp;The expected number of days that I will be in the starting sphere for a finite population is </span><img style=\"font-size: small;\" title=\"n/(2r)\" src=\"http://www.codecogs.com/png.latex?n/(2r)\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\">, and the number of days in the second sphere for a finite population and life span is </span><img style=\"font-size: small;\" title=\"s - n/2r\" src=\"http://www.codecogs.com/png.latex?s - n/2r\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\">. My expected utility is therefore given by </span></p>\n<p><img title=\"U_1 = u_A\\left(\\frac{n}{2r}\\right) + u_B\\left(s-\\frac{n}{2r}\\right)\" src=\"http://www.codecogs.com/png.latex?U_1 = u_A\\left(\\frac{n}{2r}\\right) + u_B\\left(s-\\frac{n}{2r}\\right)\" alt=\"\" align=\"bottom\"></p>\n<p><span style=\"font-size: small;\"> for choice 1, and</span></p>\n<p><img title=\"U_2 = u_A\\left(s-\\frac{n}{2r}\\right) + u_B\\left(\\frac{n}{2r}\\right)\" src=\"http://www.codecogs.com/png.latex?U_2 = u_A\\left(s-\\frac{n}{2r}\\right) + u_B\\left(\\frac{n}{2r}\\right)\" alt=\"\" align=\"bottom\"></p>\n<p><span style=\"font-size: small;\">for choice 2. The differential utility is the difference between these, or </span></p>\n<p><img title=\"\\Delta U = \\left(u_A - u_B\\right)\\left(\\frac{n}{r} - s\\right).\" src=\"http://www.codecogs.com/png.latex?\\Delta U = \\left(u_A - u_B\\right)\\left(\\frac{n}{r} - s\\right).\" alt=\"\" align=\"bottom\"></p>\n<p><span style=\"font-size: small;\">If we just plug in infinities, the answer is undefined. But if we use the same method as above, and define </span><img style=\"font-size: small;\" title=\"n = \\alpha*s\" src=\"http://www.codecogs.com/png.latex?n = \\alpha*s\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\">, then we can write</span></p>\n<p><img title=\"\\Delta U = s\\left(u_A - u_B\\right)\\left(\\frac{\\alpha}{r} - 1\\right),\" src=\"http://www.codecogs.com/png.latex?\\Delta U = s\\left(u_A - u_B\\right)\\left(\\frac{\\alpha}{r} - 1\\right),\" alt=\"\" align=\"bottom\"></p>\n<p><span style=\"font-size: small;\"> and we have well defined limits for all slopes </span><img style=\"font-size: small;\" title=\"\\alpha\\in\\{0,\\infty\\}\" src=\"http://www.codecogs.com/png.latex?\\alpha\\in\\{0,\\infty\\}\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\">. If I choose </span><img style=\"font-size: small;\" title=\"\\alpha<r\" src=\"http://www.codecogs.com/png.latex?\\alpha<r\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\">, I find that my lifespan grows faster than the population grows, and I'm better off starting in the bad sphere, expecting to be transfered before half my life is over. If I choose </span><img style=\"font-size: small;\" title=\"\\alpha>r\" src=\"http://www.codecogs.com/png.latex?\\alpha>r\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\">, the population grows too quickly, and I would prefer to start in the good sphere, since on average I'll die before I live more than half my life in the bad sphere. And as before, if </span><img style=\"font-size: small;\" title=\"\\alpha=r\" src=\"http://www.codecogs.com/png.latex?\\alpha=r\" alt=\"\" align=\"bottom\"><span style=\"font-size: small;\">, I'm indifferent to the two plans, since on average I will live half my life in each sphere either way.</span></p>\n<p><span style=\"font-size: small;\"><br></span></p>\n<h2 id=\"The_Answer\">The Answer</h2>\n<p class=\"MsoNormal\">If we want to make a decision based on additive utility, the infinite problem is<em> ill posed</em>; it has no unique solution unless we take on additional assumptions. In particular, we introduced an additional parameter&nbsp;<img style=\"font-size: small;\" title=\"\\alpha\" src=\"http://www.codecogs.com/png.latex?\\alpha\" alt=\"\" align=\"bottom\">,&nbsp;making clear three well defined solutions that span the space of possible solutions. In doing so, we solved a&nbsp;<em>similar&nbsp;</em>well posed problem, but not the original one.</p>\n<p class=\"MsoNormal\">So why bother, if we didn't solve the problem? It's worth working through the math because tt gave us an intuition for how the system works, including a quantitative understanding of crossing points, intercepts, and curvatures of the utility curves. While the problem statement corresponds to an impossible situation, the finite problem is quite possible, and taking linear limits could very well correspond to a real physical process. If we notice a paradox and then stop thinking, we lose an opportunity to gain a better understanding of the decision process.</p>\n<p class=\"MsoNormal\">We could also consider using a different paradigm of decision theory that might deal with infinite quantities better. With that said, I think that additive utilitarianism represents well the intuitive paradox that presents each choice as better than the other. Our graphs with one variable fixed showed that these are both intuitively valid viewpoints when taken alone, and their reconciliation is the challenging part.</p>\n<p><span style=\"font-size: small;\">If you're interested in similar problems, the <a href=\"http://en.wikipedia.org/wiki/St._Petersburg_paradox\">St. Petersburg Paradox</a>&nbsp;also involves diverging utillities, and it&nbsp;has has been \"dispelled\" <a href=\"http://www.jstor.org/stable/2525406\">here</a>.</span></p>\n<p>&nbsp;</p>\n<p><em>Author's notes:&nbsp;</em><em>[last edit Oct.15, 2013]</em>&nbsp;<em>The subject of unbounded utilities is not new here at Less Wrong.&nbsp;<a href=\"/user/Stuart_Armstrong/overview/\">Stuart_Armstrong</a>&nbsp;has a well written analysis of the&nbsp;<a href=\"/lw/giu/naturalism_versus_unbounded_or_unmaximisable/\">Heaven and Hell problem</a>, among others.&nbsp;<a href=\"/user/PhilGoetz/overview/\">PhilGoetz</a>&nbsp;has a useful&nbsp;<a href=\"/lw/15a/a_note_on_hypotheticals/\">note on infinities</a>. And on a related topic,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Infinite_set_atheism\">infinite set atheism</a>&nbsp;abounds. I have been unable to find on LW examples of careful mathematical treatments of apparent paradoxes that result from misuse of infinites in utilitarian calculations, and I hope this article serves as a concrete example of how easy it is to defeat (some) such problems. If there are other examples at LW, please post them in the comments: I\u2019d love to read them. It is interesting to note that&nbsp;<a href=\"http://en.wikipedia.org/wiki/Pascal's_Wager\">Pascal's Wager</a>, a famous related problem, is rather more difficult to solve. It involves not only infinities, but also a hypothesis space whose cardinality and particular members are not obvious.</em></p>\n<p><em>Thanks to <a href=\"/user/Mestroyer/overview/\">Mestroyer</a> for bringing this problem to my attention,&nbsp;<a href=\"/user/Manfred/overview/\">Manfred</a> for helping me work through it,&nbsp;</em><em><span style=\"font-size: 11.0pt; line-height: 115%; font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;; mso-ascii-theme-font: minor-latin; mso-fareast-font-family: Calibri; mso-fareast-theme-font: minor-latin; mso-hansi-theme-font: minor-latin; mso-bidi-font-family: &quot;Times New Roman&quot;; mso-bidi-theme-font: minor-bidi; mso-ansi-language: EN-US; mso-fareast-language: EN-US; mso-bidi-language: AR-SA;\">and <a href=\"/user/VincentYu/overview/\">VincentYu</a> for pointing out the issues surrounding additive utilitarianism as a decision paradigm.</span></em></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "Deciding how to decide", "anchor": "Deciding_how_to_decide", "level": 1}, {"title": "The Finite Problem", "anchor": "The_Finite_Problem", "level": 1}, {"title": "The Infinite Limit", "anchor": "The_Infinite_Limit", "level": 1}, {"title": "Expected Personal Utility", "anchor": "Expected_Personal_Utility", "level": 1}, {"title": "The Answer", "anchor": "The_Answer", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "58 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PpTN7GP2FsPyHfKrs", "LZwFMXvwTGCbWcaiq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2013-10-13T12:26:03.879Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne Social Meetup", "slug": "meetup-melbourne-social-meetup-8", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:59.118Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Maelin", "createdAt": "2009-05-28T03:32:36.549Z", "isAdmin": false, "displayName": "Maelin"}, "userId": "CE5vuYfsSRTeG2KWd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PJwCpaWb9cbTMGWcb/meetup-melbourne-social-meetup-8", "pageUrlRelative": "/posts/PJwCpaWb9cbTMGWcb/meetup-melbourne-social-meetup-8", "linkUrl": "https://www.lesswrong.com/posts/PJwCpaWb9cbTMGWcb/meetup-melbourne-social-meetup-8", "postedAtFormatted": "Sunday, October 13th 2013", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20Social%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20Social%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPJwCpaWb9cbTMGWcb%2Fmeetup-melbourne-social-meetup-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20Social%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPJwCpaWb9cbTMGWcb%2Fmeetup-melbourne-social-meetup-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPJwCpaWb9cbTMGWcb%2Fmeetup-melbourne-social-meetup-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 140, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/s6'>Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 October 2013 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">5 / 52 Leicester St, Carlton</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's regular monthly Social Meetup will be running as normal on the third Friday evening of the month. All welcome from 6:30pm, feel free to arrive later if that is easier for you.</p>\n\n<p>Our social meetups are friendly, informal events where we chat about topics of interest and often play board games. Sometimes we will also play parlour games like Mafia (a.k.a. Werewolf) or Resistance. We usually order some sort of take-away dinner for any that wish to partake.</p>\n\n<p>Just ring number 5 on the buzzer when you arrive in the foyer and we'll buzz you up. If you get lost or have any problems, feel free to call me (Richard) on 0421231789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/s6'>Melbourne Social Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PJwCpaWb9cbTMGWcb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.3784015088464434e-06, "legacy": true, "legacyId": "24387", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup\">Discussion article for the meetup : <a href=\"/meetups/s6\">Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 October 2013 06:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">5 / 52 Leicester St, Carlton</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's regular monthly Social Meetup will be running as normal on the third Friday evening of the month. All welcome from 6:30pm, feel free to arrive later if that is easier for you.</p>\n\n<p>Our social meetups are friendly, informal events where we chat about topics of interest and often play board games. Sometimes we will also play parlour games like Mafia (a.k.a. Werewolf) or Resistance. We usually order some sort of take-away dinner for any that wish to partake.</p>\n\n<p>Just ring number 5 on the buzzer when you arrive in the foyer and we'll buzz you up. If you get lost or have any problems, feel free to call me (Richard) on 0421231789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/s6\">Melbourne Social Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}