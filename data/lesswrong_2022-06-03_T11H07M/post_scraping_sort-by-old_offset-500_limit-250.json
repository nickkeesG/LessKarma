{"results": [{"createdAt": null, "postedAt": "2008-09-30T11:31:24.000Z", "modifiedAt": null, "url": null, "title": "The Magnitude of His Own Folly", "slug": "the-magnitude-of-his-own-folly", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:04.588Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fLRPeXihRaiRo5dyX/the-magnitude-of-his-own-folly", "pageUrlRelative": "/posts/fLRPeXihRaiRo5dyX/the-magnitude-of-his-own-folly", "linkUrl": "https://www.lesswrong.com/posts/fLRPeXihRaiRo5dyX/the-magnitude-of-his-own-folly", "postedAtFormatted": "Tuesday, September 30th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Magnitude%20of%20His%20Own%20Folly&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Magnitude%20of%20His%20Own%20Folly%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfLRPeXihRaiRo5dyX%2Fthe-magnitude-of-his-own-folly%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Magnitude%20of%20His%20Own%20Folly%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfLRPeXihRaiRo5dyX%2Fthe-magnitude-of-his-own-folly", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfLRPeXihRaiRo5dyX%2Fthe-magnitude-of-his-own-folly", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1813, "htmlBody": "<p>In the years before I met that <a href=\"/lw/uc/aboveaverage_ai_scientists/\">would-be creator of Artificial General Intelligence (with a funded project) who happened to be a creationist</a>, I would still try to argue with individual AGI wannabes.</p>\n<p>In those days, I sort-of-succeeded in convincing one such fellow that, yes, you had to take Friendly AI into account, and no, you couldn't just find the right fitness metric for an evolutionary algorithm.&nbsp; (Previously he had been very impressed with evolutionary algorithms.)</p>\n<p>And the one said:&nbsp; <em>Oh, woe!&nbsp; Oh, alas!&nbsp; What a fool I've been!&nbsp; Through my carelessness, I almost destroyed the world!&nbsp; What a villain I once was!</em></p>\n<p>Now, <em>there's</em> a trap I knew I better than to fall into&mdash;</p>\n<p>&mdash;at the point where, in late 2002, I looked back to Eliezer<sub>1997</sub>'s AI proposals and realized what they really would have done, insofar as they were coherent enough to talk about what they \"really would have done\".</p>\n<p>When I finally saw the magnitude of my own folly, everything fell into place at once.&nbsp; The dam against realization cracked; and the unspoken doubts that had been accumulating behind it, crashed through all together.&nbsp; There wasn't a prolonged period, or even a single moment that I remember, of wondering how I could have been so stupid.&nbsp; I already knew how.</p>\n<p>And I also knew, all at once, in the same moment of realization, that to say, <em>I almost destroyed the world!,</em> would have been too prideful.</p>\n<p>It would have been too confirming of ego, too confirming of my own importance in the scheme of things, at a time when&mdash;I understood in the same moment of realization&mdash;my ego ought to be taking a major punch to the stomach.&nbsp; I had been so much less than I needed to be; I had to take that punch in the stomach, not avert it.</p>\n<p><a id=\"more\"></a></p>\n<p>And by the same token, I didn't fall into the conjugate trap of saying:&nbsp; <em>Oh, well, it's not as if I had code and was about to run it; I didn't </em>really<em> come close to destroying the world.&nbsp; </em>For that, too, would have minimized the force of the punch.&nbsp; <em>It wasn't really loaded?</em>&nbsp; I had proposed and intended to build the gun, and load the gun, and put the gun to my head and pull the trigger; and that was a bit too much <a href=\"/lw/hf/debiasing_as_nonselfdestruction/\">self-destructiveness</a>.</p>\n<p>I didn't make a grand emotional drama out of it.&nbsp; That would have wasted the force of the punch, averted it into mere tears.</p>\n<p>I knew, in the same moment, what I had been carefully not-doing for the last six years.&nbsp; I hadn't been updating.</p>\n<p>And I knew I had to finally update.&nbsp; To actually <em>change</em> what I planned to do, to change what I was doing now, to do something different instead.</p>\n<p>I knew I had to stop.</p>\n<p>Halt, melt, and catch fire.</p>\n<p>Say, \"I'm not ready.\"&nbsp; Say, \"I don't know how to do this yet.\"</p>\n<p>These are terribly difficult words to say, in the field of AGI.&nbsp; Both the lay audience and your fellow AGI researchers are interested in code, projects with programmers in play.&nbsp; Failing that, they may give you some credit for saying, \"I'm ready to write code, just give me the funding.\"</p>\n<p>Say, \"I'm not ready to write code,\" and your status drops like a depleted uranium balloon.</p>\n<p>What distinguishes you, then, from six billion other people who don't know how to create Artificial General Intelligence?&nbsp; If you don't have neat code (that does something other than be humanly intelligent, obviously; but at least it's code), or at minimum your own startup that's going to write code as soon as it gets funding&mdash;then who are you and what are you doing at our conference?</p>\n<p>Maybe later I'll post on where this attitude comes from&mdash;the excluded middle between \"I know how to build AGI!\" and \"I'm working on narrow AI because I don't know how to build AGI\", the nonexistence of a concept for \"I am trying to get from an incomplete map of FAI to a complete map of FAI\".</p>\n<p>But this attitude does exist, and so the loss of status associated with saying \"I'm not ready to write code\" is very great.&nbsp; (If the one doubts this, let them name any other who simultaneously says \"I intend to build an Artificial General Intelligence\", \"Right now I can't build an AGI because I don't know X\", and \"I am currently trying to figure out X\".)</p>\n<p>(And never mind AGIfolk who've already raised venture capital, promising returns in five years.)&nbsp;</p>\n<p>So there's a huge reluctance to say \"Stop\".&nbsp; You can't just say, \"Oh, I'll swap back to figure-out-X mode\" because that mode doesn't exist.</p>\n<p>Was there more to that reluctance than just loss of status, in my case?&nbsp; Eliezer<sub>2001</sub> might also have flinched away from slowing his perceived forward momentum into the Singularity, which was so right and so necessary...</p>\n<p>But mostly, I think I flinched away from not being able to say, \"I'm ready to start coding.\"&nbsp; Not just for fear of others' reactions, but because I'd been inculcated with the same attitude myself.</p>\n<p>Above all, Eliezer<sub>2001</sub> didn't say \"Stop\"&mdash;even <em>after</em> noticing the problem of Friendly AI&mdash;because I did not realize, on a gut level, that Nature was allowed to kill me.</p>\n<p>\"Teenagers think they're immortal\", the proverb goes.&nbsp; Obviously this isn't true in the literal sense that if you ask them, \"Are you indestructible?\" they will reply \"Yes, go ahead and try shooting me.\"&nbsp; But perhaps wearing seat belts isn't deeply emotionally compelling for them, because the thought of their own death isn't quite <em>real</em>&mdash;they don't really believe it's allowed to happen.&nbsp; It can happen in <em>principle</em> but it can't <em>actually</em> happen.</p>\n<p>Personally, I always wore my seat belt.&nbsp; As an individual, I understood that I could die.</p>\n<p>But, having been <a href=\"/lw/u0/raised_in_technophilia/\">raised in technophilia</a> to treasure that one most precious thing, far more important than my own life, I once thought that the Future was indestructible.</p>\n<p>Even when I acknowledged that nanotech could wipe out humanity, I still believed the Singularity was invulnerable.&nbsp; That if humanity survived, the Singularity would happen, and it would be too smart to be corrupted or lost.</p>\n<p>Even after <em>that,</em> when I acknowledged Friendly AI as a consideration, I didn't emotionally believe in the possibility of failure, any more than that teenager who doesn't wear their seat belt <em>really</em> believes that an automobile accident is <em>really</em> allowed to kill or cripple them.</p>\n<p>It wasn't until my <a href=\"/lw/u9/my_naturalistic_awakening/\">insight into optimization</a> let me look back and see Eliezer<sub>1997</sub> in plain light, that I realized that Nature was allowed to kill me.</p>\n<p>\"The thought you cannot think controls you more than thoughts you speak aloud.\"&nbsp; But we flinch away from only those fears that are real to us.</p>\n<p>AGI researchers take very seriously the prospect of <em>someone else solving the problem first</em>.&nbsp; They can imagine seeing the headlines in the paper saying that their own work has been upstaged.&nbsp; They know that Nature is allowed to do that to them.&nbsp; The ones who have started companies know that they are allowed to run out of venture capital.&nbsp; That possibility is <em>real</em> to them, very real; it has a power of emotional compulsion over them.</p>\n<p>I don't think that \"Oops\" followed by the thud of six billion bodies falling, <em>at their own hands,</em> is real to them on quite the same level.</p>\n<p>It is unsafe to say what other people are thinking.&nbsp; But it seems rather likely that when the one reacts to the prospect of Friendly AI by saying, \"If you delay development to work on safety, other projects that don't care <em>at all</em> about Friendly AI will beat you to the punch,\" the prospect of they themselves making a mistake followed by six billion thuds, is not really real to them; but the possibility of others beating them to the punch is deeply scary.</p>\n<p>I, too, used to say things like that, before I understood that Nature was allowed to kill me.</p>\n<p>In that moment of realization, my childhood technophilia finally broke.</p>\n<p>I finally understood that even if you <a href=\"/lw/qf/no_safe_defense_not_even_science/\">diligently followed the rules of science</a> and were a nice person, Nature could still kill you.&nbsp; I finally understood that even if you were the best project out of all available candidates, Nature could still kill you.</p>\n<p>I understood that I was not being graded on a curve.&nbsp; My gaze shook free of rivals, and I saw the sheer blank wall.</p>\n<p>I looked back and I saw the careful arguments I had constructed, for why the wisest choice was to continue forward at full speed, just as I had planned to do before.&nbsp; And I understood then that even if you constructed an argument showing that something was the best course of action, Nature was still allowed to say \"So what?\" and kill you.</p>\n<p>I looked back and saw that I had claimed to take into account the risk of a fundamental mistake, that I had argued reasons to tolerate the risk of proceeding in the absence of full knowledge.</p>\n<p>And I saw that the risk I wanted to tolerate would have killed me.&nbsp; And I saw that this possibility had never been <em>really</em> real to me.&nbsp; And I saw that even if you had wise and excellent arguments for taking a risk, the risk was still allowed to go ahead and kill you.&nbsp; <em>Actually</em> kill you.</p>\n<p>For it is only the action that matters, and not the reasons for doing anything.&nbsp; If you build the gun and load the gun and put the gun to your head and pull the trigger, even with the cleverest of arguments for carrying out every step&mdash;then, bang.</p>\n<p>I saw that only my own ignorance of the rules had enabled me to argue for going ahead without complete knowledge of the rules; for if you do not know the rules, you cannot model the penalty of ignorance.</p>\n<p>I saw that others, still ignorant of the rules, were saying \"I will go ahead and do X\"; and that to the extent that X was a coherent proposal at all, I knew that would result in a bang; but they said, \"I do not know it cannot work\".&nbsp; &nbsp;I would try to explain to them the smallness of the target in the search space, and they would say \"How can you be so sure I won't win the lottery?\", wielding their own ignorance as a bludgeon.</p>\n<p>And so I realized that the only thing I <em>could</em> have done to save myself, in my previous state of ignorance, was to say:&nbsp; \"I will not proceed until I know positively that the ground is safe.\"&nbsp; And there are many clever arguments for why you should step on a piece of ground that you don't know to contain a landmine; but they all sound much less clever, after you look to the place that you proposed and intended to step, and see the bang.</p>\n<p>I understood that you could do <em>everything that you were supposed to do</em>, and Nature was still allowed to kill you.&nbsp; That was when my last <a href=\"/lw/qf/no_safe_defense_not_even_science/\">trust</a> broke.&nbsp; And that was when my training as a rationalist began.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZFrgTgzwEfStg26JL": 1, "irYLXtT9hkPXoZqhH": 1, "NzSTgAtKwgivkfeYm": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fLRPeXihRaiRo5dyX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 72, "extendedScore": null, "score": 0.000104, "legacy": true, "legacyId": "1094", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "SXurf2mWFw8LX2mkG", "canonicalCollectionSlug": "rationality", "canonicalBookId": "e6WsPsivzBifrWHeA", "canonicalNextPostSlug": "beyond-the-reach-of-god", "canonicalPrevPostSlug": "the-level-above-mine", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 73, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 128, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9HGR5qatMGoz4GhKj", "XZWMeeqKmfMSPTLha", "uNWRXtdwL33ELgWjD", "75LZMCCePG4Pwj3dB", "wustx45CPL5rZenuo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-01T01:28:05.000Z", "modifiedAt": null, "url": null, "title": "Awww, a Zebra", "slug": "awww-a-zebra", "viewCount": null, "lastCommentedAt": "2019-10-09T22:24:27.777Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/98cPZGwetyJsrXNdW/awww-a-zebra", "pageUrlRelative": "/posts/98cPZGwetyJsrXNdW/awww-a-zebra", "linkUrl": "https://www.lesswrong.com/posts/98cPZGwetyJsrXNdW/awww-a-zebra", "postedAtFormatted": "Wednesday, October 1st 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Awww%2C%20a%20Zebra&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAwww%2C%20a%20Zebra%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F98cPZGwetyJsrXNdW%2Fawww-a-zebra%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Awww%2C%20a%20Zebra%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F98cPZGwetyJsrXNdW%2Fawww-a-zebra", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F98cPZGwetyJsrXNdW%2Fawww-a-zebra", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 139, "htmlBody": "<p>This image recently showed up on Flickr (original is nicer):</p>\n<p><a href=\"http://www.flickr.com/photos/bostian/2901943039/\" target=\"_blank\"><img title=\"Zebra_4\" src=\"/static/imported/2008/09/30/zebra_4.jpg\" border=\"0\" alt=\"Zebra_4\" width=\"350\" height=\"156\" /></a></p>\n<p>With the caption:</p>\n<blockquote>\n<p><em>\"Alas for those who turn their eyes from zebras and dream of dragons!&nbsp; If we cannot learn to take joy in the merely real, our lives shall be empty indeed.\" &mdash;Eliezer S. Yudkowsky.</em></p>\n</blockquote>\n<p>\"Awww!\", I said, and called over my girlfriend over to look.</p>\n<p>\"Awww!\", she said, and then looked at me, and said,&nbsp; \"I think you need to take your own advice!\"</p>\n<p>Me:&nbsp; \"But I'm looking at the zebra!\"<br />Her:&nbsp; \"<em>On a computer!</em>\"<br />Me:&nbsp; <em>(Turns away, hides face.)</em><br />Her:&nbsp; \"Have you ever even <em>seen</em> a zebra in real life?\"<br />Me:&nbsp; \"Yes!&nbsp; Yes, I have!&nbsp; My parents took me to Lincoln Park Zoo!&nbsp; ...man, I hated that place.\"</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of the <a href=\"http://wiki.lesswrong.com/wiki/Joy_in_the_Merely_Real\"><em>Joy in the Merely Real</em></a> subsequence of <a href=\"http://wiki.lesswrong.com/wiki/Reductionism_%28sequence%29\"><em>Reductionism</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/p2/hand_vs_fingers/\">Hand vs. Fingers</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/p1/initiation_ceremony/\">Initiation Ceremony</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"36RkM85iDocrnaypb": 1, "moeYqrcakMgXnQNyF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "98cPZGwetyJsrXNdW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 30, "extendedScore": null, "score": 4.9e-05, "legacy": true, "legacyId": "1095", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KmghfjH6RgXvoKruJ", "fnEWQAYxcRnaYBqaZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-01T03:12:46.000Z", "modifiedAt": null, "url": null, "title": "Intrade and the Dow Drop", "slug": "intrade-and-the-dow-drop", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:34.585Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jmbpzbGcxTBNheuTo/intrade-and-the-dow-drop", "pageUrlRelative": "/posts/jmbpzbGcxTBNheuTo/intrade-and-the-dow-drop", "linkUrl": "https://www.lesswrong.com/posts/jmbpzbGcxTBNheuTo/intrade-and-the-dow-drop", "postedAtFormatted": "Wednesday, October 1st 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intrade%20and%20the%20Dow%20Drop&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntrade%20and%20the%20Dow%20Drop%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjmbpzbGcxTBNheuTo%2Fintrade-and-the-dow-drop%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intrade%20and%20the%20Dow%20Drop%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjmbpzbGcxTBNheuTo%2Fintrade-and-the-dow-drop", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjmbpzbGcxTBNheuTo%2Fintrade-and-the-dow-drop", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>With today's snapback, the Dow lost 777 and regained 485.</p>\n\n<p>As of this evening, Intrade says the probability of a bailout bill passing by Oct 31st is 85%.</p>\n\n<p>(777-485)/(1-.85) = 1,946.&nbsp; So a bailout bill makes an expected difference of 2000 points on the Dow.</p>\n\n<p>Of course this is a bogus calculation, but it's an interesting one.&nbsp; Not overwhelmingly on-topic for OB, but it involves prediction markets and I didn't see anyone else pointing it out.&nbsp; I hope the bailout fails decisively, so this calculation can be tested.</p><a id=\"more\"></a><p><strong>PS:</strong>&nbsp; <a href=\"http://econlog.econlib.org/archives/2008/09/how_would_we_kn.html\">Bryan Caplan understands Bayes's Rule</a>:&nbsp; It's not possible for both A and ~A to be evidence in favor of B.&nbsp; So which of the two possibilities, &quot;unemployment stays under 8% following a bailout&quot; and &quot;unemployment goes over 8% following a bailout&quot;, is evidence for the proposition &quot;the bailout was necessary to prevent economic catastrophe&quot;, and which is evidence against?&nbsp; Take your stand now; afterward is too late for us to trust your reasoning.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8daMDi9NEShyLqxth": 1, "R6dqPii4cyNpuecLt": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jmbpzbGcxTBNheuTo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 4.4914595922413543e-07, "legacy": true, "legacyId": "1096", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-01T08:58:38.000Z", "modifiedAt": null, "url": null, "title": "Trying to Try", "slug": "trying-to-try", "viewCount": null, "lastCommentedAt": "2019-08-19T13:09:33.340Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WLJwTJ7uGPA5Qphbp/trying-to-try", "pageUrlRelative": "/posts/WLJwTJ7uGPA5Qphbp/trying-to-try", "linkUrl": "https://www.lesswrong.com/posts/WLJwTJ7uGPA5Qphbp/trying-to-try", "postedAtFormatted": "Wednesday, October 1st 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Trying%20to%20Try&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATrying%20to%20Try%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWLJwTJ7uGPA5Qphbp%2Ftrying-to-try%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Trying%20to%20Try%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWLJwTJ7uGPA5Qphbp%2Ftrying-to-try", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWLJwTJ7uGPA5Qphbp%2Ftrying-to-try", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1027, "htmlBody": "<blockquote>\n<p>\"No!&nbsp; Try not!&nbsp; Do, or do not.&nbsp; There is no try.\"<br />&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &mdash;<a href=\"http://www.youtube.com/watch?v=PcjnbIF1yAA\">Yoda</a></p>\n</blockquote>\n<p>Years ago, I thought this was yet another example of <a href=\"/lw/k8/how_to_seem_and_be_deep/\">Deep Wisdom</a> that is actually quite stupid.&nbsp; <tt>SUCCEED</tt> is not a primitive action.&nbsp; You can't just <em>decide</em> to win by choosing hard enough.&nbsp; There is never a plan that works with <a href=\"/lw/mo/infinite_certainty/\">probability 1</a>.</p>\n<p>But Yoda <a href=\"/lw/k9/the_logical_fallacy_of_generalization_from/\">was</a> wiser than I first realized.</p>\n<p>The first elementary technique of epistemology&mdash;it's not deep, but it's cheap&mdash;is to <a href=\"/lw/ok/the_quotation_is_not_the_referent/\">distinguish the quotation from the referent</a>.&nbsp; Talking about snow is not the same as talking about \"snow\".&nbsp; When I use the word \"snow\", without quotes, I mean to talk about snow; and when I use the word \"\"snow\"\", with quotes, I mean to talk about the word \"snow\".&nbsp; You have to enter a special mode, the quotation mode, to talk about your beliefs.&nbsp; By default, we just talk about reality.</p>\n<p>If someone says, \"I'm going to flip that switch\", then by default, they mean they're going to try to flip the switch.&nbsp; They're going to build a plan that promises to lead, by the consequences of its actions, to the goal-state of a flipped switch; and then execute that plan.</p>\n<p>No plan succeeds with infinite certainty.&nbsp; So by default, when you talk about setting out to achieve a goal, you do not imply that your plan exactly and perfectly leads to <em>only </em>that possibility.&nbsp; But when you say, \"I'm going to flip that switch\", you are <em>trying</em> only to flip the switch&mdash;not <em>trying</em> to achieve a 97.2% probability of flipping the switch.</p>\n<p>So what does it mean when someone says, \"I'm going to <em>try </em>to flip that switch?\"</p>\n<p><a id=\"more\"></a></p>\n<p>Well, <em>colloquially,</em> \"I'm going to flip the switch\" and \"I'm going to try to flip the switch\" mean more or less the same thing, except that the latter expresses the possibility of failure.&nbsp; This is why I originally took offense at Yoda for seeming to deny the possibility.&nbsp; But bear with me here.</p>\n<p>Much of life's challenge consists of holding ourselves to a high enough standard.&nbsp; I may speak more on this principle later, because it's a lens through which you can view many-but-not-all personal dilemmas&mdash;\"What standard am I holding myself to?&nbsp; Is it high enough?\"</p>\n<p>So if much of life's failure consists in holding yourself to too low a standard, you should be wary of demanding too little from yourself&mdash;setting goals that are too easy to fulfill.</p>\n<p>Often where <em>succeeding</em> to do a thing, is very hard, <em>trying</em> to do it is much easier.</p>\n<p>Which is easier&mdash;to build a successful startup, or to try to build a successful startup?&nbsp; To make a million dollars, or to try to make a million dollars?</p>\n<p>So if \"I'm going to flip the switch\" means by default that you're going to try to flip the switch&mdash;that is, you're going to set up a plan that promises to lead to switch-flipped state, maybe not with probability 1, but with the highest probability you can manage&mdash;</p>\n<p>&mdash;then \"I'm going to 'try to flip' the switch\" means that you're going to try to \"try to flip the switch\", that is, you're going to try to achieve the goal-state of \"having a plan that might flip the switch\".</p>\n<p>Now, if this were a self-modifying AI we were talking about, the transformation we just performed ought to end up at a reflective equilibrium&mdash;the AI planning its planning operations.</p>\n<p>But when we deal with humans, <em>being satisfied with having a plan</em> is not at all like <em>being satisfied with success.</em>&nbsp; The part where the plan has to maximize your probability of succeeding, gets <a href=\"/lw/le/lost_purposes/\">lost</a> along the way.&nbsp; It's far easier to convince ourselves that we are \"maximizing our probability of succeeding\", than it is to convince ourselves that we will succeed.</p>\n<p>Almost any effort will serve to convince us that we have \"tried our hardest\", if trying our hardest is all we are trying to do.</p>\n<blockquote>\n<p>\"You have been asking what you could do in the great events that are now stirring, and have found that you could do nothing. But that is because your suffering has caused you to phrase the question in the wrong way... Instead of asking what you could do, you ought to have been asking what needs to be done.\"<br />&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &mdash;Steven Brust, <em>The Paths of the Dead</em></p>\n</blockquote>\n<p>When you ask, \"What can I do?\", you're trying to do your best.&nbsp; What is your best?&nbsp; It is whatever you can do without the slightest inconvenience.&nbsp; It is whatever you can do with the money in your pocket, minus whatever you need for your accustomed lunch.&nbsp; What you can do with those resources, may not give you very good odds of winning.&nbsp; But it's the \"best you can do\", and so you've acted defensibly, right?</p>\n<p>But what <em>needs</em> to be done?&nbsp; Maybe what <em>needs</em> to be done requires three times your life savings, and you must produce it or fail.</p>\n<p>So trying to have \"maximized your probability of success\"&mdash;as opposed to trying to succeed&mdash;is a far lesser barrier.&nbsp; You can have \"maximized your probability of success\" using only the money in your pocket, so long as you don't demand actually <em>winning.</em></p>\n<p>Want to try to make a million dollars?&nbsp; Buy a <a href=\"/lw/hl/lotteries_a_waste_of_hope/\">lottery ticket</a>.&nbsp; Your odds of winning may not be very good, but you did try, and trying was what you wanted.&nbsp; In fact, you tried your <em>best,</em> since you only had one dollar left after buying lunch.&nbsp; Maximizing the odds of goal achievement using available resources: is this not intelligence?</p>\n<p>It's only when you want, above all else, to <em>actually</em> flip the switch&mdash;without quotation and without consolation prizes just for trying&mdash;that you will <em>actually</em> put in the effort to <em>actually</em> maximize the probability.</p>\n<p>But if all you want is to \"maximize the probability of success using available resources\", then that's the easiest thing in the world to convince yourself you've done.&nbsp; The very first plan you hit upon, will serve quite well as \"maximizing\"&mdash;if necessary, you can generate an inferior alternative to prove its optimality.&nbsp; And any tiny resource that you care to put in, will be what is \"available\".&nbsp; Remember to congratulate yourself on putting in 100% of it!</p>\n<p>Don't try your best.&nbsp; Win, or fail.&nbsp; There is no best.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NzSTgAtKwgivkfeYm": 1, "4R8JYu4QF2FqzJxE5": 1, "KoXbd2HmbdRfqLngk": 1, "wMPYFGmhcFg4bSb4Z": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WLJwTJ7uGPA5Qphbp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 111, "baseScore": 155, "extendedScore": null, "score": 0.000224, "legacy": true, "legacyId": "1097", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "3szfzHZr7EYGSWt92", "canonicalCollectionSlug": "rationality", "canonicalBookId": "e6WsPsivzBifrWHeA", "canonicalNextPostSlug": "use-the-try-harder-luke", "canonicalPrevPostSlug": "my-bayesian-enlightenment", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 155, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aSQy7yHj6nPD44RNo", "ooypcn7qFzsMcy53R", "rHBdcHGLJ7KvLJQPk", "np3tP49caG4uFLRbS", "sP2Hg6uPwpfp3jZJN", "vYsuM8cpuRgZS5rYB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 18, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-02T10:52:43.000Z", "modifiedAt": "2020-08-09T00:06:46.346Z", "url": null, "title": "Use the Try Harder, Luke", "slug": "use-the-try-harder-luke", "viewCount": null, "lastCommentedAt": "2014-07-20T13:49:51.758Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fhEPnveFhb9tmd7Pe/use-the-try-harder-luke", "pageUrlRelative": "/posts/fhEPnveFhb9tmd7Pe/use-the-try-harder-luke", "linkUrl": "https://www.lesswrong.com/posts/fhEPnveFhb9tmd7Pe/use-the-try-harder-luke", "postedAtFormatted": "Thursday, October 2nd 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Use%20the%20Try%20Harder%2C%20Luke&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUse%20the%20Try%20Harder%2C%20Luke%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfhEPnveFhb9tmd7Pe%2Fuse-the-try-harder-luke%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Use%20the%20Try%20Harder%2C%20Luke%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfhEPnveFhb9tmd7Pe%2Fuse-the-try-harder-luke", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfhEPnveFhb9tmd7Pe%2Fuse-the-try-harder-luke", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 602, "htmlBody": "<blockquote>\n<p>\"When there's a will to fail, obstacles can be found.\"&nbsp; &nbsp;&mdash;John McCarthy</p>\n</blockquote>\n<p>I first watched <em>Star Wars</em> IV-VI when I was very young.&nbsp; Seven, maybe, or nine?&nbsp; So my memory was dim, but I recalled Luke Skywalker as being, you know, this cool Jedi guy.</p>\n<p>Imagine my horror and disappointment, when I watched the saga again, years later, and discovered that Luke was a <a href=\"http://www.youtube.com/watch?v=X66jntR0MVE\">whiny teenager</a>.</p>\n<p>I mention this because yesterday, I looked up, on Youtube, the source of the Yoda quote:&nbsp; \"Do, or do not.&nbsp; There is no try.\"</p>\n<p>Oh.&nbsp; My.&nbsp; Cthulhu.</p>\n<p>Along with the Youtube clip in question, I present to you a little-known outtake from the scene, in which the director and writer, George Lucas, argues with Mark Hamill, who played Luke Skywalker:</p>\n<p><a id=\"more\"></a></p>\n<blockquote>\n<p>Luke:&nbsp; All right, I'll give it a try.<br />Yoda:&nbsp; No!&nbsp; Try not.&nbsp; Do.&nbsp; Or do not.&nbsp; There is no try.<br /><em><br />Luke raises his hand, and slowly, the X-wing begins to rise out of the water&mdash;Yoda's eyes widen&mdash;but then the ship sinks again.</em></p>\n</blockquote>\n<p>Mark Hamill:&nbsp; \"Um, George...\"</p>\n<p>George Lucas:&nbsp; \"What is it now?\"</p>\n<p>Mark:&nbsp; \"So... according to the script, next I say, 'I can't.&nbsp; It's too big'.\"</p>\n<p>George:&nbsp; \"That's right.\"</p>\n<p>Mark:&nbsp; \"Shouldn't Luke maybe give it another shot?\"</p>\n<p>George:&nbsp; \"No.&nbsp; Luke gives up, and sits down next to Yoda&mdash;\"</p>\n<p>Mark:&nbsp; \"This is the hero who's going to take down the Empire?&nbsp; Look, it was one thing when he was a whiny teenager at the beginning, but he's in Jedi training now.&nbsp; Last movie he blew up the Death Star.&nbsp; Luke should be showing a little backbone.\"</p>\n<p>George:&nbsp; \"No.&nbsp; You give up.&nbsp; And then Yoda lectures you for a while, and you say, 'You want the impossible'.&nbsp; Can you remember that?\"</p>\n<p>Mark:&nbsp; \"<em>Impossible?</em>&nbsp; What did he do, run a formal calculation to arrive at a mathematical proof?&nbsp; &nbsp;The X-wing was already starting to rise out of the swamp!&nbsp; That's the feasibility demonstration right there!&nbsp; Luke loses it for a second and the ship sinks back&mdash;and now he says it's <em>impossible?</em>&nbsp; Not to mention that Yoda, who's got literally eight hundred years of seniority in the field, just told him it should be doable&mdash;\"</p>\n<p>George:&nbsp; \"And then you walk away.\"</p>\n<p>Mark:&nbsp; \"It's his friggin' <em>spaceship!</em>&nbsp; If he leaves it in the swamp, he's stuck on Dagobah for the rest of his miserable life!&nbsp; He's not just going to walk away!&nbsp; Look, let's just cut to the next scene with the words 'one month later' and Luke is still raggedly standing in front of the swamp, trying to raise his ship for the thousandth time&mdash;\"</p>\n<p>George:&nbsp; \"No.\"</p>\n<p>Mark:&nbsp; \"Fine!&nbsp; We'll show a sunset and a sunrise, as he stands there with his arm out, straining, and <em>then</em> Luke says 'It's impossible'.&nbsp; Though really, he ought to try again when he's fully rested&mdash;\"</p>\n<p>George:&nbsp; \"No.\"</p>\n<p>Mark:&nbsp; \"<em>Five goddamned minutes!&nbsp; </em>Five goddamned minutes before he gives up!\"</p>\n<p>George:&nbsp; \"I am not halting the story for five minutes while the X-wing bobs in the swamp like a bathtub toy.\"</p>\n<p>Mark:&nbsp; \"For the love of sweet candied yams!&nbsp; If a pathetic loser like this could master the Force, everyone in the galaxy would be using it!&nbsp; People would become Jedi because it was easier than going to high school.\"</p>\n<p>George:&nbsp; \"Look, you're the actor.&nbsp; Let me be the storyteller.&nbsp; Just say your lines and try to mean them.\"</p>\n<p>Mark:&nbsp; \"The audience isn't going to buy it.\"</p>\n<p>George:&nbsp; \"Trust me, they will.\"</p>\n<p>Mark:&nbsp; \"They're going to get up and walk out of the theater.\"</p>\n<p>George:&nbsp; \"They're going to sit there and nod along and not notice anything out of the ordinary.&nbsp; Look, you don't understand human nature.&nbsp; People wouldn't try for five minutes before giving up if the fate of humanity were at stake.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 1, "FGfgzGpPTtKEqSrDm": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fhEPnveFhb9tmd7Pe", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 123, "baseScore": 176, "extendedScore": null, "score": 0.000256, "legacy": true, "legacyId": "1098", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "3szfzHZr7EYGSWt92", "canonicalCollectionSlug": "rationality", "canonicalBookId": "e6WsPsivzBifrWHeA", "canonicalNextPostSlug": "on-doing-the-impossible", "canonicalPrevPostSlug": "trying-to-try", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 176, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.3.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 21, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2008-10-02T10:52:43.000Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-03T12:39:30.000Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes 18", "slug": "rationality-quotes-18", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:59.781Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KgmYWd7oje7PtJBr7/rationality-quotes-18", "pageUrlRelative": "/posts/KgmYWd7oje7PtJBr7/rationality-quotes-18", "linkUrl": "https://www.lesswrong.com/posts/KgmYWd7oje7PtJBr7/rationality-quotes-18", "postedAtFormatted": "Friday, October 3rd 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%2018&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%2018%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKgmYWd7oje7PtJBr7%2Frationality-quotes-18%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%2018%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKgmYWd7oje7PtJBr7%2Frationality-quotes-18", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKgmYWd7oje7PtJBr7%2Frationality-quotes-18", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 244, "htmlBody": "<p>Q.&nbsp; Why does \"philosophy of consciousness/nature of reality\" seem to interest you so much?<br />A.&nbsp; Take away consciousness and reality and there's not much left.<br /> &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- Greg Egan, interview in Eidolon 15</p>\n<p>\"But I am not an object. I am not a noun, I am an adjective.&nbsp; I am the way matter behaves when it is organized in a John K Clark-ish way.&nbsp; At the present time only one chunk of matter in the universe behaves that way; someday that could change.\"<br /> &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- John K Clark</p>\n<p>\"Would it be good advice, once copying becomes practical, to make lots of copies when good things happen, and none (or perhaps even killing off your own personal instance) on bad things?&nbsp; Will this change the subjective probability of good events?\"<br /> &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- Hal Finney</p>\n<p>\"Waiting for the bus is a bad idea if you turn out to be the bus driver.\"<br /> &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- Michael M. Butler on the Singularity</p>\n<p>\"You are free.&nbsp; Free of anything I do or say, and of any consequence. You may rest assured that all hurts are forgiven, all loveliness remembered, and treasured.&nbsp; I am busy and content and loved.&nbsp; I hope you are the same.&nbsp; Bless you.\"<br /> &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- Walter Jon Williams, \"Knight Moves\"</p>\n<p>\"A man with one watch knows what time it is; a man with two watches is never sure.\"<br /> &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;-- Lee Segall</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KgmYWd7oje7PtJBr7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 4.49585623245913e-07, "legacy": true, "legacyId": "1099", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-04T15:42:57.000Z", "modifiedAt": null, "url": null, "title": "Beyond the Reach of God", "slug": "beyond-the-reach-of-god", "viewCount": null, "lastCommentedAt": "2022-05-27T13:42:42.765Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sYgv4eYH82JEsTD34/beyond-the-reach-of-god", "pageUrlRelative": "/posts/sYgv4eYH82JEsTD34/beyond-the-reach-of-god", "linkUrl": "https://www.lesswrong.com/posts/sYgv4eYH82JEsTD34/beyond-the-reach-of-god", "postedAtFormatted": "Saturday, October 4th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Beyond%20the%20Reach%20of%20God&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABeyond%20the%20Reach%20of%20God%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsYgv4eYH82JEsTD34%2Fbeyond-the-reach-of-god%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Beyond%20the%20Reach%20of%20God%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsYgv4eYH82JEsTD34%2Fbeyond-the-reach-of-god", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsYgv4eYH82JEsTD34%2Fbeyond-the-reach-of-god", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3189, "htmlBody": "<p>Today's post is a tad gloomier than usual, as I measure such things.&nbsp; It deals with a thought experiment I invented to smash my own optimism, after <a href=\"http://www.overcomingbias.com/2008/09/it-was-pretty-b.html\">I realized that optimism had misled me</a>.&nbsp; Those readers sympathetic to arguments like, \"It's important to keep our biases because they help us stay happy,\" should consider not reading.&nbsp; (Unless they have <a href=\"http://www.overcomingbias.com/2008/01/something-to-pr.html\">something to protect</a>, including their own life.)</p>\n<p>So!&nbsp; Looking back on the magnitude of my own folly, I realized that at the root of it had been a disbelief in the Future's vulnerability&mdash;a reluctance to accept that things could <em>really </em>turn out wrong.&nbsp; Not as the result of any explicit propositional verbal belief.&nbsp; More like something inside that persisted in believing, even in the face of adversity, that everything would be all right in the end.</p>\n<p>Some would account this a virtue (<em>zettai daijobu da yo</em>), and others would say that it's a thing necessary for mental health.</p>\n<p>But we don't live in that world.&nbsp; We live in the world beyond the reach of God.</p>\n<p><a id=\"more\"></a></p>\n<p>It's been a long, long time since I believed in God.&nbsp; Growing up in an Orthodox Jewish family, I can recall the last remembered time I asked God for something, though I don't remember how old I was.&nbsp; I was putting in some request on behalf of the next-door-neighboring boy, I forget what exactly&mdash;something along the lines of, \"I hope things turn out all right for him,\" or maybe \"I hope he becomes Jewish.\"</p>\n<p>I remember what it was like to have some higher authority to appeal to, to take care of things I couldn't handle myself.&nbsp; I didn't think of it as \"warm\", because I had no alternative to compare it to.&nbsp; I just took it for granted.</p>\n<p>Still I recall, though only from distant childhood, what it's like to live in the <a href=\"http://www.overcomingbias.com/2008/09/excluding-the-s.html\">conceptually impossible possible world</a> where God exists.&nbsp; <em>Really</em> exists, in the way that children and rationalists take all their <a href=\"http://www.overcomingbias.com/2007/07/belief-in-belie.html\">beliefs</a> at face value.</p>\n<p>In the world where God exists, does God intervene to optimize <em>everything?</em>&nbsp; Regardless of what rabbis assert about the fundamental nature of reality, the take-it-seriously operational answer to this question is obviously \"No\".&nbsp; You can't ask God to bring you a lemonade from the refrigerator instead of getting one yourself.&nbsp; When I believed in God after the serious fashion of a child, so very long ago, I didn't believe that.</p>\n<p>Postulating that particular divine inaction doesn't provoke a full-blown theological crisis.&nbsp; If you said to me, \"I have constructed a benevolent superintelligent nanotech-user\", and I said \"Give me a banana,\" and no banana appeared, this would not <em>yet</em> disprove your statement.&nbsp; Human parents don't always do everything their children ask.&nbsp; There are some decent fun-theoretic arguments&mdash;I even believe them myself&mdash;against the idea that the <em>best</em> kind of help you can offer someone, is to always immediately give them everything they want.&nbsp; I don't think that eudaimonia is formulating goals and having them instantly fulfilled; I don't <em>want</em> to become a simple wanting-thing that never has to plan or act or think.</p>\n<p>So it's not necessarily an attempt to avoid falsification, to say that God does not grant all prayers.&nbsp; Even a Friendly AI might not respond to every request.</p>\n<p>But clearly, there exists <em>some</em> threshold of horror awful enough that God will intervene.&nbsp; I remember that being true, when I believed after the fashion of a child.</p>\n<p>The God who does not intervene <em>at all</em>, no matter how bad things get&mdash;<em>that's</em> an obvious attempt to avoid falsification, to protect a <a href=\"http://www.overcomingbias.com/2007/07/belief-in-belie.html\">belief-in-belief</a>.&nbsp; Sufficiently young children don't have the deep-down knowledge that God doesn't really exist.&nbsp; They really expect to see <a href=\"http://www.overcomingbias.com/2007/07/belief-in-belie.html\">a dragon in their garage</a>.&nbsp; They have no reason to imagine a loving God who never acts.&nbsp; Where exactly is the boundary of sufficient awfulness?&nbsp; Even a child can imagine arguing over the precise threshold.&nbsp; But of course God will draw the line somewhere.&nbsp; Few indeed are the loving parents who, desiring their child to grow up strong and self-reliant, would let their toddler be run over by a car.</p>\n<p>The obvious example of a horror so great that God cannot tolerate it, is death&mdash;true death, mind-annihilation.&nbsp; I don't think that even Buddhism allows that.&nbsp; So long as there is a God in the classic sense&mdash;full-blown, ontologically fundamental, <em>the</em> God&mdash;we can rest assured that no <em>sufficiently</em> awful event will ever, ever happen.&nbsp; There is no soul anywhere that need fear true annihilation; God will prevent it.</p>\n<p>What if you build your own simulated universe?&nbsp; The classic example of a simulated universe is Conway's Game of Life.&nbsp; I do urge you to <a href=\"http://en.wikipedia.org/wiki/Conway%27s_Game_of_Life\">investigate</a> Life if you've never played it&mdash;it's important for comprehending the notion of \"physical law\".&nbsp; Conway's Life has been proven Turing-complete, so it would be possible to build a sentient being in the Life universe, albeit it might be rather fragile and awkward.&nbsp; Other cellular automata would make it simpler.</p>\n<p>Could you, by creating a simulated universe, escape the reach of God?&nbsp; Could you simulate a Game of Life containing sentient entities, and torture the beings therein?&nbsp; But if God is watching everywhere, then trying to build an unfair Life just results in <em>the</em> God stepping in to modify your computer's transistors.&nbsp; If the physics you set up in your computer program calls for a sentient Life-entity to be endlessly tortured for no particular reason, <em>the</em> God will intervene.&nbsp; God being omnipresent, there is no refuge <em>anywhere</em> for true horror:&nbsp; Life is fair.</p>\n<p>But suppose that instead you ask the question:</p>\n<p><em>Given</em> such-and-such initial conditions, and <em>given</em> such-and-such cellular automaton rules, what <em>would be</em> the mathematical result?</p>\n<p>Not even God can modify the answer to this question, unless you believe that God can implement logical impossibilities.&nbsp; Even as a very young child, I don't remember believing that.&nbsp; (And why would you need to believe it, if God can modify anything that <em>actually</em> exists?)</p>\n<p>What does Life look like, in this imaginary world where every step follows <em>only</em> from its immediate predecessor?&nbsp; Where things <em>only</em> ever happen, or don't happen, because of the cellular automaton rules?&nbsp; Where the initial conditions and rules <em>don't</em> describe any God that checks over each state?&nbsp; What does it look like, the world beyond the reach of God?</p>\n<p>That world wouldn't be fair.&nbsp; If the initial state contained the seeds of something that could self-replicate, natural selection might or might not take place, and complex life might or might not evolve, and that life might or might not become sentient, with no God to guide the evolution.&nbsp; That world might evolve the equivalent of conscious cows, or conscious dolphins, that lacked hands to improve their condition; maybe they would be eaten by conscious wolves who never thought that they were doing wrong, or cared.</p>\n<p>If in a vast plethora of worlds, something like humans evolved, then they would suffer from diseases&mdash;not to teach them any lessons, but only because viruses happened to evolve as well, under the cellular automaton rules.</p>\n<p>If the people of that world are happy, or unhappy, the causes of their happiness or unhappiness may have nothing to do with good or bad choices they made.&nbsp; Nothing to do with free will or lessons learned.&nbsp; In the what-if world where every step follows only from the cellular automaton rules, the equivalent of Genghis Khan can murder a million people, and laugh, and be rich, and never be punished, and live his life much happier than the average.&nbsp; Who prevents it?&nbsp; God would prevent it from ever <em>actually</em> happening, of course; He would at the very least visit some shade of gloom in the Khan's heart.&nbsp; But in the mathematical answer to the question <em>What if?</em> there is no God in the axioms.&nbsp; So if the cellular automaton rules say that the Khan is happy, that, simply, is the whole and only answer to the what-if question.&nbsp; There is nothing, absolutely nothing, to prevent it.</p>\n<p>And if the Khan tortures people horribly to death over the course of days, for his own amusement perhaps?&nbsp; They will call out for help, perhaps imagining a God.&nbsp; And if you <em>really</em> wrote that cellular automaton, God would intervene in your program, of course.&nbsp; But in the what-if question, what the cellular automaton <em>would</em> do under the mathematical rules, there isn't any God in the system.&nbsp; Since the physical laws contain no specification of a utility function&mdash;in particular, no prohibition against torture&mdash;then the victims will be saved only if the right cells happen to be 0 or 1.&nbsp; And it's not likely that anyone will defy the Khan; if they did, someone would strike them with a sword, and the sword would disrupt their organs and they would die, and that would be the end of that.&nbsp; So the victims die, screaming, and no one helps them; that is the answer to the what-if question.</p>\n<p>Could the victims be completely innocent?&nbsp; Why not, in the what-if world?&nbsp; If you look at the rules for Conway's Game of Life (which is Turing-complete, so we can embed arbitrary computable physics in there), then the rules are really very simple.&nbsp; Cells with three living neighbors stay alive; cells with two neighbors stay the same, all other cells die.&nbsp; There isn't anything in there about only innocent people not being horribly tortured for indefinite periods.</p>\n<p>Is this world starting to sound familiar?</p>\n<p>Belief in a fair universe often manifests in more subtle ways than thinking that horrors should be outright prohibited:&nbsp; Would the twentieth century have gone differently, if Klara P&ouml;lzl and Alois Hitler had made love one hour earlier, and a different sperm fertilized the egg, on the night that Adolf Hitler was conceived?</p>\n<p>For so many lives and so much loss to turn on a single event, seems <em>disproportionate.</em>&nbsp; The Divine Plan ought to make more <em>sense</em> than that.&nbsp; You can believe in a Divine Plan without believing in God&mdash;Karl Marx surely did.&nbsp; You shouldn't have millions of lives depending on a casual choice, an hour's timing, the speed of a microscopic flagellum.&nbsp; It ought not to be allowed.&nbsp; It's <em>too</em> disproportionate.&nbsp; Therefore, if Adolf Hitler had been able to go to high school and become an architect, there would have been someone else to take his role, and World War II would have happened the same as before.</p>\n<p>But in the world beyond the reach of God, there isn't any clause in the physical axioms which says \"things have to make sense\" or \"big effects need big causes\" or \"history runs on reasons too important to be so fragile\".&nbsp; There is no God to <em>impose</em> that order, which is so severely violated by having the lives and deaths of millions depend on one small molecular event.</p>\n<p>The point of the thought experiment is to lay out the God-universe and the Nature-universe side by side, so that we can recognize what kind of thinking belongs to the God-universe.&nbsp; Many who are atheists, still think as if certain things are <em>not allowed</em>.&nbsp; They would lay out arguments for why World War II was inevitable and would have happened in more or less the same way, even if Hitler had become an architect.&nbsp; But in sober historical fact, this is an unreasonable belief; I chose the example of World War II because from my reading, it seems that events were mostly driven by Hitler's personality, often in defiance of his generals and advisors.&nbsp; There is no particular empirical justification that I happen to have heard of, for doubting this.&nbsp; The main reason to doubt would be <em>refusal to accept</em> that the universe could make so little sense&mdash;that horrible things could happen so <em>lightly,</em> for no more reason than a roll of the dice.</p>\n<p>But why not?&nbsp; What prohibits it?</p>\n<p>In the God-universe, God prohibits it.&nbsp; To recognize this is to recognize that we don't live in that universe.&nbsp; We live in the what-if universe beyond the reach of God, driven by the mathematical laws and nothing else.&nbsp; Whatever physics says will happen, will happen.&nbsp; Absolutely <em>anything,</em> good or bad, will happen.&nbsp; And there is nothing in the laws of physics to lift this rule even for the <em>really extreme</em> cases, where you might expect Nature to be a little more reasonable.</p>\n<p>Reading William Shirer's <em>The Rise and Fall of the Third Reich,</em> listening to him describe the disbelief that he and others felt upon discovering the full scope of Nazi atrocities, I thought of what a strange thing it was, to read all that, and know, already, that there wasn't a single protection against it.&nbsp; To just read through the whole book and accept it; horrified, but not at all disbelieving, because I'd already understood what kind of world I lived in.</p>\n<p>Once upon a time, I believed that the extinction of humanity was not allowed.&nbsp; And others who call themselves rationalists, may yet have things they trust.&nbsp; They might be called \"positive-sum games\", or \"democracy\", or \"technology\", but they are sacred.&nbsp; The mark of this sacredness is that the trustworthy thing can't lead to anything <em>really</em> bad; or they can't be <em>permanently</em> defaced, at least not without a compensatory silver lining.&nbsp; In that sense they can be trusted, even if a few bad things happen here and there.</p>\n<p>The unfolding history of Earth can't ever turn from its positive-sum trend to a negative-sum trend; that is not allowed.&nbsp; <a href=\"http://www.overcomingbias.com/2007/09/applause-lights.html\">Democracies</a>&mdash;<em>modern</em> <em>liberal</em> democracies, anyway&mdash;won't ever legalize torture.&nbsp; <a href=\"http://www.overcomingbias.com/2008/09/raised-in-sf.html\">Technology</a> has done so much good up until now, that there can't possibly be a Black Swan technology that breaks the trend and does more harm than all the good up until this point.</p>\n<p>There are all sorts of clever arguments why such things can't possibly happen.&nbsp; But the source of these arguments is a much deeper belief that such things are <em>not allowed</em>.&nbsp; Yet who prohibits?&nbsp; Who prevents it from happening?&nbsp; If you can't visualize at least one lawful universe where physics say that such dreadful things happen&mdash;and so they <em>do</em> happen, there being nowhere to appeal the verdict&mdash;then you aren't yet ready to argue <em>probabilities</em>.</p>\n<p>Could it really be that sentient beings have died absolutely for thousands or millions of years, with no soul and no afterlife&mdash;and <em>not</em> as part of any grand plan of Nature&mdash;<em>not</em> to teach any great lesson about the meaningfulness or meaninglessness of life&mdash;not even to teach any profound lesson about what is impossible&mdash;so that a trick as simple and stupid-sounding as <a href=\"http://www.overcomingbias.com/2008/06/timeless-identi.html\">vitrifying people in liquid nitrogen</a> can save them from total annihilation&mdash;and a 10-second rejection of the silly idea can destroy someone's soul?&nbsp; Can it be that a computer programmer who signs a few papers and buys a life-insurance policy continues into the far future, while Einstein rots in a grave?&nbsp; We can be sure of one thing:&nbsp; God wouldn't allow it.&nbsp; Anything that ridiculous and disproportionate would be ruled out.&nbsp; It would make a mockery of the Divine Plan&mdash;a mockery of the <em>strong reasons</em> why things must be the way they are.</p>\n<p>You can have secular rationalizations for things being <em>not allowed</em>.&nbsp; So it helps to imagine that there <em>is</em> a God, benevolent as you understand goodness&mdash;a God who enforces throughout Reality a <em>minimum</em> of fairness and justice&mdash;whose plans make sense and depend proportionally on people's choices&mdash;who will never permit absolute horror&mdash;who does not always intervene, but who at least prohibits universes wrenched <em>completely</em> off their track... to imagine all this, but also imagine that <em>you,</em> yourself, live in a what-if world of pure mathematics&mdash;a world beyond the reach of God, an utterly unprotected world where anything at all can happen.</p>\n<p>If there's any reader still reading this, who thinks that being happy counts for more than anything in life, then maybe they <em>shouldn't</em> spend much time pondering the unprotectedness of their existence.&nbsp; Maybe think of it <em>just</em> long enough to sign up themselves and their family for cryonics, and/or write a check to an existential-risk-mitigation agency now and then.&nbsp; And wear a seatbelt and get health insurance and all those other dreary necessary things that can destroy your life if you miss that one step... but aside from that, if you want to be happy, meditating on the fragility of life isn't going to help.</p>\n<p>But this post was written for those who have <a href=\"http://www.overcomingbias.com/2008/01/something-to-pr.html\">something to protect</a>.</p>\n<p>What can a twelfth-century peasant do to save themselves from annihilation?&nbsp; Nothing.&nbsp; Nature's little challenges aren't always fair.&nbsp; When you run into a challenge that's too difficult, you suffer the penalty; when you run into a lethal penalty, you die.&nbsp; That's how it is for people, and it isn't any different for planets.&nbsp; Someone who wants to dance the deadly dance with Nature, does need to understand what they're up against:&nbsp; Absolute, utter, exceptionless neutrality.</p>\n<p>Knowing this won't always save you.&nbsp; It wouldn't save a twelfth-century peasant, even if they knew.&nbsp; If you think that a rationalist who fully understands the mess they're in, must <em>surely</em> be able to find a way out&mdash;then you <a href=\"http://www.overcomingbias.com/2008/05/no-defenses.html\">trust rationality</a>, enough said.</p>\n<p>Some commenter is bound to castigate me for putting too dark a tone on all this, and in response they will list out all the reasons why it's lovely to live in a neutral universe.&nbsp; Life is allowed to be a <em>little</em> dark, after all; but not darker than a certain point, unless there's a silver lining.</p>\n<p>Still, because I don't want to create <em>needless</em> despair, I will say a few hopeful words at this point:</p>\n<p>If humanity's future unfolds in the right way, we might be able to make our future light cone fair(er).&nbsp; We can't modify fundamental physics, but on a higher level of organization we could build some guardrails and put down some padding; organize the particles into a pattern that does some internal checks against catastrophe.&nbsp; There's a lot of stuff out there that we can't touch&mdash;but it may help to consider everything that isn't in our future light cone, as being part of the \"generalized past\".&nbsp; As if it had all already happened.&nbsp; There's at least the <em>prospect</em> of defeating neutrality, in the only future we can touch&mdash;the only world that it accomplishes something to care about.</p>\n<p>Someday, maybe, immature minds will reliably be sheltered.&nbsp; Even if children go through the equivalent of not getting a lollipop, or even burning a finger, they won't ever be run over by cars.</p>\n<p>And the adults wouldn't be in so much danger.&nbsp; A superintelligence&mdash;a mind that could think a trillion thoughts without a misstep&mdash;would not be intimidated by a challenge where death is the price of a single failure.&nbsp; The raw universe wouldn't seem so harsh, would be only another problem to be solved.</p>\n<p>The problem is that building an adult is itself an adult challenge.&nbsp; That's what I finally realized, years ago.</p>\n<p>If there is a fair(er) universe, we have to get there starting from <em>this</em> world&mdash;the neutral world, the world of hard concrete with no padding, the world where challenges are not calibrated to your skills.</p>\n<p>Not every child needs to stare Nature in the eyes.&nbsp; Buckling a seatbelt, or writing a check, is not that complicated or deadly.&nbsp; I don't say that every rationalist should meditate on neutrality.&nbsp; I don't say that every rationalist should think all these unpleasant thoughts.&nbsp; But anyone who plans on confronting an uncalibrated<em>&nbsp;</em>challenge of instant death, must not avoid them.</p>\n<p>What does a child need to do&mdash;what rules should they follow, how should they behave&mdash;to solve an adult problem?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xexCWMyds6QLWognu": 11, "NSMKfa8emSbGNXRKD": 3, "ymWzfKxBchRvmCTNX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sYgv4eYH82JEsTD34", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 127, "baseScore": 162, "extendedScore": null, "score": 0.000234, "legacy": true, "legacyId": "1100", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "SXurf2mWFw8LX2mkG", "canonicalCollectionSlug": "rationality", "canonicalBookId": "e6WsPsivzBifrWHeA", "canonicalNextPostSlug": "my-bayesian-enlightenment", "canonicalPrevPostSlug": "the-magnitude-of-his-own-folly", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 162, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 280, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 13, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-05T16:45:35.000Z", "modifiedAt": null, "url": null, "title": "My Bayesian Enlightenment", "slug": "my-bayesian-enlightenment", "viewCount": null, "lastCommentedAt": "2021-11-16T13:38:24.039Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ti3Z7eZtud32LhGZT/my-bayesian-enlightenment", "pageUrlRelative": "/posts/Ti3Z7eZtud32LhGZT/my-bayesian-enlightenment", "linkUrl": "https://www.lesswrong.com/posts/Ti3Z7eZtud32LhGZT/my-bayesian-enlightenment", "postedAtFormatted": "Sunday, October 5th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20My%20Bayesian%20Enlightenment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMy%20Bayesian%20Enlightenment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTi3Z7eZtud32LhGZT%2Fmy-bayesian-enlightenment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=My%20Bayesian%20Enlightenment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTi3Z7eZtud32LhGZT%2Fmy-bayesian-enlightenment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTi3Z7eZtud32LhGZT%2Fmy-bayesian-enlightenment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2207, "htmlBody": "<p>I remember (dimly, as human memories go) the first time I self-identified as a \"Bayesian\".&nbsp; Someone had just asked a malformed version of an old probability puzzle, saying:</p>\n<blockquote>\n<p>If I meet a mathematician on the street, and she says, \"I have two children, and at least one of them is a boy,\" what is the probability that they are both boys?</p>\n</blockquote>\n<p>In the <em>correct</em> version of this story, the mathematician says \"I have two children\", and <em>you</em> ask, \"Is at least one a boy?\", and she answers \"Yes\".&nbsp; Then the probability is 1/3 that they are both boys.</p>\n<p>But in the malformed version of the story&mdash;as I pointed out&mdash;one would common-sensically reason:</p>\n<blockquote>\n<p>If the mathematician has one boy and one girl, then my prior probability for her saying 'at least one of them is a boy' is 1/2 and my prior probability for her saying 'at least one of them is a girl' is 1/2.&nbsp; There's no reason to believe, a priori, that the mathematician will only mention a girl if there is no possible alternative.</p>\n</blockquote>\n<p>So I pointed this out, and worked the answer using Bayes's Rule, arriving at a probability of 1/2 that the children were both boys.&nbsp; I'm not sure whether or not I knew, at this point, that Bayes's rule was called that, but it's what I used.</p>\n<p>And lo, someone said to me, \"Well, what you just gave is the Bayesian answer, but in orthodox statistics the answer is 1/3.&nbsp; We just exclude the possibilities that are ruled out, and count the ones that are left, without trying to guess the probability that the mathematician will say this or that, since we have no way of really knowing that probability&mdash;it's too subjective.\"</p>\n<p>I responded&mdash;note that this was completely spontaneous&mdash;\"What on Earth do you mean?&nbsp; You can't avoid assigning a probability to the mathematician making one statement or another.&nbsp; You're just assuming the probability is 1, and <em>that's</em> unjustified.\"</p>\n<p>To which the one replied, \"Yes, that's what the Bayesians say.&nbsp; But frequentists don't believe that.\"</p>\n<p>And I said, astounded: \"How can there possibly be such a thing as non-Bayesian statistics?\"</p>\n<p><a id=\"more\"></a></p>\n<p>That was when I discovered that I was of the type called 'Bayesian'.&nbsp; As far as I can tell, I was <em>born</em> that way.&nbsp; My mathematical intuitions were such that everything Bayesians said seemed perfectly straightforward and simple, the obvious way I would do it myself; whereas the things frequentists said sounded like the elaborate, warped, mad blasphemy of dreaming Cthulhu.&nbsp; I didn't <em>choose</em> to become a Bayesian any more than fishes choose to breathe water.</p>\n<p>But this is not what I refer to as my \"Bayesian enlightenment\".&nbsp; The first time I heard of \"Bayesianism\", I marked it off as obvious; I didn't go much further in than Bayes's rule itself.&nbsp; At that time I still thought of probability theory as <a href=\"/lw/mt/beautiful_probability/\">a tool rather than a law</a>.&nbsp; I didn't think there were mathematical laws of intelligence (<a href=\"/lw/tz/my_best_and_worst_mistake/\">my best and worst mistake</a>).&nbsp; Like nearly all AGI wannabes, Eliezer<sub>2001</sub> thought in terms of techniques, methods, algorithms, building up a toolbox full of cool things he could <em>do</em>; he searched for tools, not understanding.&nbsp; Bayes's Rule was a really neat tool, applicable in a surprising number of cases.</p>\n<p>Then there was my initiation into heuristics and biases.&nbsp; It started when I ran across a webpage that had been transduced from a Powerpoint intro to behavioral economics.&nbsp; It mentioned some of the results of heuristics and biases, in passing, without any references.&nbsp; I was so startled that I emailed the author to ask if this was actually a real experiment, or just anecdotal.&nbsp; He sent me back a scan of Tversky and Kahneman's 1973 paper.</p>\n<p>Embarrassing to say, my story doesn't really start there.&nbsp; I put it on my list of things to look into.&nbsp; I knew that there was an edited volume called \"Judgment Under Uncertainty: Heuristics and Biases\" but I'd never seen it.&nbsp; At this time, I figured that if it wasn't online, I would just try to get along without it.&nbsp; I had so many other things on my reading stack, and no easy access to a university library.&nbsp; I think I must have mentioned this on a mailing list, because Emil Gilliam <del>emailed me to tell me that he'd read Judgment Under Uncertainty</del> <a href=\"http://www.overcomingbias.com/2008/10/my-bayesian-enl.html#comment-133486843\">was annoyed by my online-only theory</a>, so he bought me the book.</p>\n<p>His action here should probably be regarded as scoring a fair number of points.</p>\n<p>But this, too, is not what I refer to as my \"Bayesian enlightenment\".&nbsp; It was an important step toward realizing the inadequacy of my Traditional Rationality skillz&mdash;that there was so much more out there, all this new science, beyond just doing what Richard Feynman told you to do.&nbsp; And seeing the heuristics-and-biases program holding up Bayes as the gold standard helped move my thinking forward&mdash;but not all the way there.</p>\n<p>Memory is a fragile thing, and mine seems to have become more fragile than most, since I learned how memories are recreated with each recollection&mdash;the science of how fragile they are.&nbsp; Do other people really have better memories, or do they just trust the details their mind makes up, while really not remembering any more than I do?&nbsp; My guess is that other people do have better memories for certain things.&nbsp; I find structured, scientific knowledge easy enough to remember; but the disconnected chaos of everyday life fades very quickly for me.</p>\n<p>I know <em>why</em> certain things happened in my life&mdash;that's causal structure I can remember.&nbsp; But sometimes it's hard to recall even in <em>what order</em> certain events happened to me, let alone in what year.</p>\n<p>I'm not sure if I <a href=\"/lw/ua/the_level_above_mine/\">read E. T. Jaynes</a>'s <em>Probability Theory: The Logic of Science</em> before or after the day when I realized <a href=\"/lw/ue/the_magnitude_of_his_own_folly/\">the magnitude of my own folly</a>, and understood that I was <a href=\"/lw/uk/beyond_the_reach_of_god/\">facing an adult problem</a>.</p>\n<p>But it was PT:TLOS that did the trick.&nbsp; Here was probability theory, laid out not as a clever tool, but as <em>The Rules</em>, inviolable on pain of paradox.&nbsp; If you tried to approximate The Rules because they were too computationally expensive to use directly, then, no matter how necessary that compromise might be, you would still end doing less than optimal.&nbsp; Jaynes would do his calculations different ways to show that the same answer always arose when you used legitimate methods; and he would display different answers that others had arrived at, and trace down the illegitimate step.&nbsp; Paradoxes could not coexist with his precision.&nbsp; Not <em>an</em> answer, but <em>the</em> answer.</p>\n<p>And so&mdash;having looked back on my mistakes, and all the <em>an-answers</em> that had led me into paradox and dismay&mdash;it occurred to me that here was <a href=\"/lw/ua/the_level_above_mine/\">the level above mine</a>.</p>\n<p>I could no longer visualize trying to build an AI based on vague answers&mdash;like the an-answers I had come up with before&mdash;and surviving the challenge.</p>\n<p>I looked at the AGI wannabes with whom I had tried to argue Friendly AI, and their various <a href=\"/lw/tj/dreams_of_friendliness/\">dreams of Friendliness</a> which they had.&nbsp; (Often formulated spontaneously in response to my asking the question!)&nbsp; Like frequentist statistical methods, no two of them agreed with each other.&nbsp; Having actually <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">studied the issue</a> full-time for some years, I knew something about the problems their hopeful plans would run into.&nbsp; And I saw that if you said, \"I don't see why this would fail,\" the \"don't know\" was just a reflection of your own ignorance.&nbsp; I could see that if I held myself to a similar standard of \"that seems like a good idea\", I would also be doomed.&nbsp; (Much like a frequentist inventing amazing new statistical calculations that seemed like good ideas.)</p>\n<p>But if you can't do that which seems like a good idea&mdash;if you can't do what you don't imagine failing&mdash;then what can you do?</p>\n<p>It seemed to me that it would take something like the Jaynes-level&mdash;not, <em>here's my bright idea,</em> but rather, <em>here's the only correct way you can do this (and why)</em>&mdash;to tackle an <a href=\"/lw/uk/beyond_the_reach_of_god/\">adult problem</a> and survive.&nbsp; If I achieved the same level of mastery of my own subject, as Jaynes had achieved of probability theory, then it was at least <em>imaginable</em> that I could try to build a Friendly AI and survive the experience.</p>\n<p>Through my mind flashed the passage:</p>\n<blockquote>\n<p><em>D<span style=\"color: black;\">o nothing because it is righteous, or praiseworthy, or noble, to do so; do nothing because it seems good to do so; do only that which you must do, and which you cannot do in any other way.</span></em></p>\n</blockquote>\n<p>Doing what it seemed good to do, had only led me astray.</p>\n<p>So I <a href=\"/lw/ue/the_magnitude_of_his_own_folly/\">called a full stop</a>.</p>\n<p>And I decided that, from then on, I would follow the strategy that could have saved me if I had followed it years ago:&nbsp; Hold my FAI designs to the higher standard of not doing that which seemed like a good idea, but only that which I understood on a sufficiently deep level to see that I could not do it in any other way.</p>\n<p>All my old theories into which I had invested so much, did not meet this standard; and were not close to this standard; and weren't even on a track leading to this standard; so I threw them out the window.</p>\n<p>I took up the study of probability theory and decision theory, looking to extend them to embrace such things as reflectivity and self-modification.</p>\n<p>If I recall correctly, I had already, by this point, started to see cognition as manifesting <a href=\"/lw/o7/searching_for_bayesstructure/\">Bayes-structure</a>, which is also a major part of what I refer to as my Bayesian enlightenment&mdash;but of this I have already spoken.&nbsp; And there was also my <a href=\"/lw/u9/my_naturalistic_awakening/\">naturalistic awakening</a>, of which I have already spoken.&nbsp; And my realization that Traditional Rationality was <a href=\"/lw/qd/science_isnt_strict_enough/\">not strict enough</a>, so that in matters of human rationality I began taking more inspiration from probability theory and cognitive psychology.</p>\n<p>But if you add up all these things together, then that, more or less, is the story of my Bayesian enlightenment.</p>\n<p>Life rarely has neat boundaries.&nbsp; The story continues onward.</p>\n<p>It was while studying Judea Pearl, for example, that I realized that precision can save you time.&nbsp; I'd put some thought into nonmonotonic logics myself, before then&mdash;back when I was still in my \"searching for neat tools and algorithms\" mode.&nbsp; Reading <em>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, </em>I could imagine how much time I would have wasted on ad-hoc systems and special cases, if I hadn't known that key.&nbsp; \"Do only that which you must do, and which you cannot do in any other way\", translates into a time-savings measured, not in the rescue of wasted months, but in the rescue of wasted careers.</p>\n<p>And so I realized that it was only by holding myself to this higher standard of precision that I had started to <em>really</em> think <em>at all</em> about quite a number of important issues.&nbsp; To say a thing with precision is difficult&mdash;it is not at all the same thing as saying a thing formally, or <a href=\"/lw/tg/against_modal_logics/\">inventing a new logic</a> to throw at the problem.&nbsp; Many shy away from the inconvenience, because human beings are lazy, and so they say, \"It is impossible\" or \"It will take too long\", even though they never <a href=\"/lw/uh/trying_to_try/\">really tried</a> for <a href=\"/lw/ui/use_the_try_harder_luke/\">five minutes</a>.&nbsp; But if you don't hold yourself to that <em>inconveniently</em> high standard, you'll let yourself get away with anything.&nbsp; It's a hard problem just to find a standard high enough to make you actually start thinking!&nbsp; It may seem taxing to hold yourself to the standard of mathematical proof where every single step has to be correct and one wrong step can carry you anywhere.&nbsp; But otherwise you won't chase down those <a href=\"/lw/u7/that_tiny_note_of_discord/\">tiny notes of discord</a> that turn out to, in fact, lead to whole new concerns you never thought of.</p>\n<p>So these days I don't complain as much about the heroic burden of inconvenience that it takes to hold yourself to a precise standard.&nbsp; It can save time, too; and in fact, it's more or less the ante to get yourself thinking about the problem at all.</p>\n<p>And this too should be considered part of my \"Bayesian enlightenment\"&mdash;realizing that there were advantages in it, not just penalties.</p>\n<p>But of course the story continues on.&nbsp; Life is like that, at least the parts that I remember.</p>\n<p>If there's one thing I've learned from this history, it's that saying \"Oops\" is something to look forward to.&nbsp; Sure, the prospect of saying \"Oops\" in the future, means that the you of <em>right now</em> is a drooling imbecile, whose words your future self won't be able to read because of all the wincing.&nbsp; But saying \"Oops\" in the future also means that, in the future, you'll acquire new Jedi powers that your present self doesn't dream exist.&nbsp; It makes you feel embarrassed, but also <em>alive.</em>&nbsp; Realizing that your younger self was a complete moron means that even though you're already in your twenties, you <a href=\"http://xkcd.com/447/\">haven't yet gone over your peak</a>.&nbsp; So here's to hoping that my future self realizes I'm a drooling imbecile:&nbsp; I may <em>plan</em> to solve my problems with my present abilities, but extra Jedi powers sure would come in handy.</p>\n<p>That scream of horror and embarrassment is the sound that rationalists make when they level up.&nbsp; Sometimes I worry that I'm not leveling up as fast as I used to, and I don't know if it's because I'm finally getting the hang of things, or because the neurons in my brain are slowly dying.</p>\n<p>Yours, Eliezer<sub>2008</sub>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 2, "EewHHv3ewvQ3mqbyb": 2, "irYLXtT9hkPXoZqhH": 2, "3RnEKrsNgNEDxuNnw": 2, "4fxcbJ8xSv4SAYkkx": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ti3Z7eZtud32LhGZT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 53, "extendedScore": null, "score": 7.7e-05, "legacy": true, "legacyId": "1101", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": "Rationality: A-Z", "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "SXurf2mWFw8LX2mkG", "canonicalCollectionSlug": "rationality", "canonicalBookId": "e6WsPsivzBifrWHeA", "canonicalNextPostSlug": "trying-to-try", "canonicalPrevPostSlug": "beyond-the-reach-of-god", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bkSkRwo9SRYxJMiSY", "BA7dRRrzMLyvfJr9J", "kXSETKZ3X9oidMozA", "fLRPeXihRaiRo5dyX", "sYgv4eYH82JEsTD34", "wKnwcjJGriTS9QxxL", "uHYYA32CKgKT3FagE", "QrhAeKBkm2WsdRYao", "75LZMCCePG4Pwj3dB", "PGfJdgemDJSwWBZSX", "vzLrQaGPa9DNCpuZz", "WLJwTJ7uGPA5Qphbp", "fhEPnveFhb9tmd7Pe", "SwCwG9wZcAzQtckwx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-06T12:50:04.000Z", "modifiedAt": null, "url": null, "title": "Bay Area Meetup for Singularity Summit", "slug": "bay-area-meetup-for-singularity-summit", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:33.898Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PhMNQPojinRMuwikC/bay-area-meetup-for-singularity-summit", "pageUrlRelative": "/posts/PhMNQPojinRMuwikC/bay-area-meetup-for-singularity-summit", "linkUrl": "https://www.lesswrong.com/posts/PhMNQPojinRMuwikC/bay-area-meetup-for-singularity-summit", "postedAtFormatted": "Monday, October 6th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bay%20Area%20Meetup%20for%20Singularity%20Summit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABay%20Area%20Meetup%20for%20Singularity%20Summit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPhMNQPojinRMuwikC%2Fbay-area-meetup-for-singularity-summit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bay%20Area%20Meetup%20for%20Singularity%20Summit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPhMNQPojinRMuwikC%2Fbay-area-meetup-for-singularity-summit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPhMNQPojinRMuwikC%2Fbay-area-meetup-for-singularity-summit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 142, "htmlBody": "<p>Posted on behalf of Mike Howard:</p><blockquote><p>\n\n\n\nThis is a call for preferences on the <a href=\"http://www.overcomingbias.com/2008/10/open-thread.html#comment-133297113\">proposed</a> Bay Area meetup to coincide with the <a href=\"http://www.singularitysummit.com/\">Singularity Summit</a> on 24-25 October. Not just for Singularitarians, all aspiring rationalists are welcome. From the replies so far it's likely to be in <a href=\"http://sanjose.org/\">San Jose</a>.</p>\n\n<p>Eliezer, myself and probably most Summit attendees would really rather avoid the night between the Friday Workshop and Saturday Summit, so maybe either Saturday evening or sometime Thursday or Sunday?\n\n</p>\n\n<p>Please comment below or email me (cursor_loop 4t yahoo p0int com) if you might want to come, and if you have any preferences such as when and where you <em>can</em> come, when and where you'd <em>prefer</em> to come, and any recommendations for a particular place to go.&nbsp; (Comments preferred to emails.) We need to pick a date ASAP before everyone books travel.</p></blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DQHWBcKeiLnyh9za9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PhMNQPojinRMuwikC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 0, "extendedScore": null, "score": 4.5013925440306833e-07, "legacy": true, "legacyId": "1102", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-06T15:13:26.000Z", "modifiedAt": null, "url": null, "title": "On Doing the Impossible", "slug": "on-doing-the-impossible", "viewCount": null, "lastCommentedAt": "2021-12-17T20:23:14.885Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fpecAJLG9czABgCe9/on-doing-the-impossible", "pageUrlRelative": "/posts/fpecAJLG9czABgCe9/on-doing-the-impossible", "linkUrl": "https://www.lesswrong.com/posts/fpecAJLG9czABgCe9/on-doing-the-impossible", "postedAtFormatted": "Monday, October 6th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20Doing%20the%20Impossible&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20Doing%20the%20Impossible%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfpecAJLG9czABgCe9%2Fon-doing-the-impossible%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20Doing%20the%20Impossible%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfpecAJLG9czABgCe9%2Fon-doing-the-impossible", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfpecAJLG9czABgCe9%2Fon-doing-the-impossible", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2118, "htmlBody": "<p>\"Persevere.\"&nbsp; It's a piece of advice you'll get from a whole lot of high achievers in a whole lot of disciplines.&nbsp; I didn't understand it at all, at first.</p>\n<p>At first, I thought \"perseverance\" meant working 14-hour days.&nbsp; Apparently, there are people out there who can work for 10 hours at a technical job, and then, in their moments between eating and sleeping and going to the bathroom, seize that unfilled spare time to work on a book.&nbsp; I am not one of those people&mdash;it still hurts my pride even now to confess that.&nbsp; I'm working on something important; shouldn't my brain be willing to put in 14 hours a day?&nbsp; But it's not.&nbsp; When it gets too hard to keep working, I stop and go read or watch something.&nbsp; Because of that, I thought for years that I entirely lacked the virtue of \"perseverance\".</p>\n<p>In accordance with human nature, Eliezer<sub>1998</sub> would think things like: \"What counts is output, not input.\"&nbsp; Or, \"Laziness is also a virtue&mdash;it leads us to back off from failing methods and think of better ways.\"&nbsp; Or, \"I'm doing better than other people who are working more hours.&nbsp; Maybe, for creative work, your momentary <em>peak</em> output is more important than working 16 hours a day.\"&nbsp; Perhaps the famous scientists were seduced by the <a href=\"/lw/k8/how_to_seem_and_be_deep/\">Deep Wisdom</a> of saying that \"hard work is a virtue\", because it would be too awful if that counted for less than <a href=\"/lw/ty/my_childhood_death_spiral/\">intelligence</a>?</p>\n<p>I didn't understand the virtue of perseverance until I looked back on my journey through AI, and realized that I had overestimated the difficulty of almost every single important problem.</p>\n<p>Sounds crazy, right?&nbsp; But bear with me here.</p>\n<p><a id=\"more\"></a></p>\n<p>When I was first deciding to challenge AI, I thought in terms of 40-year timescales, Manhattan Projects, planetary computing networks, millions of programmers, and possibly augmented humans.</p>\n<p>This is a common failure mode in AI-futurism which I may write about later; it consists of the leap from \"I don't know how to solve this\" to \"I'll imagine throwing something really big at it\".&nbsp; Something huge enough that, when you imagine it, that imagination creates a <a href=\"/lw/lg/the_affect_heuristic/\">feeling</a> of impressiveness strong enough to be commensurable with the problem.&nbsp; (There's a fellow currently on the AI list who goes around saying that AI will cost a quadrillion dollars&mdash;we can't get AI <em>without</em> spending a quadrillion dollars, but we <em>could</em> get AI at any time by spending a quadrillion dollars.)&nbsp; This, in turn, lets you imagine that you know how to solve AI, without trying to fill the obviously-impossible demand that you <em>understand intelligence.</em></p>\n<p>So, in the beginning, I made the same mistake:&nbsp; I didn't understand intelligence, so I imagined throwing a Manhattan Project at the problem.</p>\n<p>But, having calculated the planetary death rate at 55 million per year or 150,000 per day, I did not turn around and run away from the big scary problem like a frightened rabbit.&nbsp; Instead, I started trying to figure out what kind of AI project could get there fastest.&nbsp; If I could make the Singularity happen one hour earlier, that was a reasonable return on investment for a pre-Singularity career.&nbsp; (I <a href=\"/lw/u0/raised_in_technophilia/\">wasn't thinking</a> in terms of existential risks or Friendly AI at this point.)</p>\n<p>So I didn't run away from the big scary problem like a frightened rabbit, but stayed to see if there was anything I could do.</p>\n<p>Fun historical fact:&nbsp; In 1998, I'd written this long treatise proposing how to go about creating a self-improving or \"seed\" AI (a term I had the honor of coining).&nbsp; Brian Atkins, who would later become the founding funder of the Singularity Institute, had just sold Hypermart to Go2Net.&nbsp; Brian emailed me to ask whether this AI project I was describing was something that a reasonable-sized team could go out and actually <em>do.</em>&nbsp; \"No,\" I said, \"it would take a Manhattan Project and thirty years,\" so for a while we were considering a new dot-com startup instead, to create the funding to get <em>real</em> work done on AI...</p>\n<p>A year or two later, after I'd heard about this newfangled \"open source\" thing, it seemed to me that there was some preliminary development work&mdash;new computer languages and so on&mdash;that a small organization could do; and that was how the Singularity Institute started.</p>\n<p>This strategy was, of course, entirely wrong.</p>\n<p>But even so, I went from \"There's nothing I can do about it now\" to \"Hm... maybe there's an incremental path through open-source development, if the initial versions are useful to enough people.\"</p>\n<p>This is back at the dawn of time, so I'm not saying any of this was a <em>good idea</em>.<em>&nbsp;</em> But in terms of what I thought I was trying to do, a year of creative thinking had shortened the apparent pathway:&nbsp; The problem looked <em>slightly less impossible</em> than it did the very first time I approached it.</p>\n<p>The more interesting pattern is my entry into Friendly AI.&nbsp; Initially, Friendly AI hadn't been something that I had considered at all&mdash;because it was <em>obviously impossible and useless</em> to deceive a superintelligence about what was the <a href=\"/lw/sm/the_meaning_of_right/\">right</a> course of action.</p>\n<p>So, historically, I went from <em>completely ignoring a problem that was \"impossible\",</em> to <em>taking on a problem that was merely extremely difficult.</em></p>\n<p>Naturally this increased my total workload.</p>\n<p>Same thing with trying to understand intelligence on a precise level.&nbsp; Originally, I'd written off this problem as <em>impossible,</em> thus removing it from my workload.&nbsp; (This logic seems pretty deranged in retrospect&mdash;<a href=\"/lw/uk/beyond_the_reach_of_god/\">Nature doesn't care</a> what you can't do when It's writing your project requirements&mdash;but I still see <a href=\"/lw/uc/aboveaverage_ai_scientists/\">AIfolk</a> trying it all the time.)&nbsp; To <a href=\"/lw/ul/my_bayesian_enlightenment/\">hold myself to a precise standard</a> meant putting in more work than I'd previously imagined I needed.&nbsp; But it also meant tackling a problem that I would have dismissed as <em>entirely impossible</em> not too much earlier.</p>\n<p>Even though <em>individual</em> problems in AI have seemed to become less intimidating over time, the total mountain-to-be-climbed has increased in height&mdash;just like conventional wisdom says is supposed to happen&mdash;as problems got taken off the \"impossible\" list and put on the \"to do\" list.</p>\n<p>I started to understand what was happening&mdash;and what \"Persevere!\" really meant&mdash;at the point where I noticed other AIfolk doing the same thing: saying \"<a href=\"/lw/ui/use_the_try_harder_luke/\">Impossible!</a>\" on problems that seemed eminently solvable&mdash;relatively more straightforward, as such things go.&nbsp; But they were things that <em>would</em> have seemed vastly more intimidating at the point when I first approached the problem.</p>\n<p>And I realized that the word \"impossible\" had two usages:</p>\n<p>1)&nbsp; Mathematical proof of impossibility conditional on specified axioms;</p>\n<p>2)&nbsp; \"I can't see any way to do that.\"</p>\n<p>Needless to say, all my own uses of the word \"impossible\" had been of the second type.</p>\n<p>Any time you don't understand a domain, many problems in that domain will seem impossible because when you query your brain for a solution pathway, it will return null.&nbsp; But there are only mysterious questions, never <a href=\"/lw/iu/mysterious_answers_to_mysterious_questions/\">mysterious answers</a>.&nbsp; If you spend a year or two working on the domain, then, <em>if</em> you don't get stuck in any blind alleys, and <em>if</em> you have the native ability level required to make progress, you will understand it better.&nbsp; The <em>apparent</em> difficulty of problems may go way down.&nbsp; It won't be as scary as it was to your novice-self.</p>\n<p><em>And this is especially likely on the <strong>confusing</strong></em><em> problems that seem most</em><em> <strong>intimidating.</strong></em></p>\n<p>Since we have some notion of the processes by which a star burns, we know that it's not easy to build a star from scratch.&nbsp; Because we understand gears, we can prove that no collection of gears obeying known physics can form a <a href=\"/lw/o5/the_second_law_of_thermodynamics_and_engines_of/\">perpetual motion machine</a>.&nbsp; These are not good problems on which to practice doing the impossible.</p>\n<p>When you're <em>confused</em> about a domain, problems in it will <em>feel</em> very intimidating and mysterious, and a query to your brain will produce a count of zero solutions.&nbsp; But you don't know how much work will be left when the confusion clears.&nbsp; Dissolving the confusion may itself be a very difficult challenge, of course.&nbsp; But the word \"impossible\" should hardly be used in that connection.&nbsp; Confusion exists in the map, not in the territory.</p>\n<p>So if you spend a few years working on an impossible problem, and you manage to avoid or climb out of blind alleys, and your native ability is high enough to make progress, then, by golly, after a few years it may not seem so <em>impossible</em> after all.</p>\n<p>But if something seems impossible, you won't try.</p>\n<p>Now <em>that's</em> a vicious cycle.</p>\n<p>If I hadn't been in a sufficiently driven frame of mind that \"forty years and a Manhattan Project\" just meant we should get started earlier, I wouldn't have tried.&nbsp; I wouldn't have stuck to the problem.&nbsp; And I wouldn't have gotten a chance to become less intimidated.</p>\n<p>I'm not ordinarily a fan of the theory that opposing biases can cancel each other out, but sometimes it happens by luck.&nbsp; If I'd seen that whole mountain <em>at the start</em>&mdash;if I'd realized at the start that the problem was not to build a seed capable of improving itself, but to produce a <em>provably correct Friendly AI</em>&mdash;then I probably would have burst into flames.</p>\n<p>Even so, part of understanding those <a href=\"/lw/uc/aboveaverage_ai_scientists/\">above-average scientists</a> who constitute the bulk of AGI researchers, is realizing that they are not <em>driven </em>to take on a nearly impossible problem even if it takes them 40 years.&nbsp; By and large, they are there because they have found the Key to AI that will let them solve the problem <em>without</em> such tremendous difficulty, in just five years.</p>\n<p><a href=\"http://www.paulgraham.com/hamming.html\">Richard Hamming</a> used to go around asking his fellow scientists two questions:&nbsp; \"What are the important problems in your field?\", and, \"Why aren't you working on them?\"</p>\n<p>Often the important problems look Big, Scary, and Intimidating.&nbsp; They don't promise 10 publications a year.&nbsp; They don't <em>promise</em> any progress at all.&nbsp; You might not get any reward after working on them for a year, or five years, or ten years.</p>\n<p>And not uncommonly, the most important problems in your field are impossible.&nbsp; That's why you don't see more philosophers working on reductionist decompositions of consciousness.</p>\n<p>Trying to do the impossible is definitely not for everyone.&nbsp; Exceptional talent is only the ante to sit down at the table.&nbsp; The chips are the years of your life.&nbsp; If wagering those chips and losing seems like an unbearable possibility to you, then go do something else.&nbsp; Seriously.&nbsp; Because you <em>can</em> lose.</p>\n<p>I'm not going to say anything like, \"Everyone should do something impossible at least once in their lifetimes, because it teaches an important lesson.\"&nbsp; Most of the people all of the time, and all of the people most of the time, should stick to the possible.</p>\n<p>Never give up?&nbsp; Don't be ridiculous.&nbsp; Doing the impossible should be reserved for very special occasions.&nbsp; Learning <a href=\"/lw/gx/just_lose_hope_already/\">when to lose hope</a> is an important skill in life.</p>\n<p>But if there's something you can imagine that's even <em>worse</em> than wasting your life, if there's something you want that's <em>more important</em> than thirty chips, or if there are scarier things than a life of inconvenience, then you may have cause to attempt the impossible.</p>\n<p>There's a good deal to be said for persevering through difficulties; but one of the things that must be said of it, is that it <em>does keep things difficult</em>. If you can't handle that, stay away!&nbsp; There are easier ways to obtain glamor and respect.&nbsp; I don't want anyone to read this and needlessly plunge headlong into a life of permanent difficulty.</p>\n<p>But to conclude:&nbsp; The \"perseverance\" that is required to work on important problems has a component beyond working 14 hours a day.</p>\n<p>It's strange, the pattern of what we notice and don't notice about ourselves.&nbsp; This selectivity isn't always about inflating your self-image.&nbsp; Sometimes it's just about ordinary salience.</p>\n<p>To keep working was a constant struggle for me, so it was salient:&nbsp; I noticed that I couldn't work for 14 solid hours a day.&nbsp; It didn't occur to me that \"perseverance\" might also apply at a timescale of seconds or years.&nbsp; Not until I saw people who instantly declared \"<a href=\"/lw/ui/use_the_try_harder_luke/\">impossible</a>\" anything they didn't want to try, or saw how reluctant they were to take on work that looked like it might take a couple of decades instead of \"five years\".</p>\n<p>That was when I realized that \"perseverance\" applied at multiple time scales.&nbsp; On the timescale of seconds, perseverance is to \"not to give up instantly at the very first sign of difficulty\".&nbsp; On the timescale of years, perseverance is to \"keep working on an insanely difficult problem even though it's inconvenient and you could be getting higher personal rewards elsewhere\".</p>\n<p>To do things that are very difficult or \"impossible\",</p>\n<p>First you have to not run away.&nbsp; That takes seconds.</p>\n<p>Then you have to work.&nbsp; That takes hours.</p>\n<p>Then you have to stick at it.&nbsp; That takes years.</p>\n<p>Of these, I had to learn to do the first reliably instead of sporadically; the second is still a constant struggle for me; and the third comes naturally.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BhrpjXqGuke5GnF6g": 2, "hrezrpGqXXdSe76ks": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fpecAJLG9czABgCe9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 69, "baseScore": 84, "extendedScore": null, "score": 0.000122, "legacy": true, "legacyId": "1103", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "3szfzHZr7EYGSWt92", "canonicalCollectionSlug": "rationality", "canonicalBookId": "e6WsPsivzBifrWHeA", "canonicalNextPostSlug": "make-an-extraordinary-effort", "canonicalPrevPostSlug": "use-the-try-harder-luke", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 84, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aSQy7yHj6nPD44RNo", "uD9TDHPwQ5hx4CgaX", "Kow8xRzpfkoY7pa69", "uNWRXtdwL33ELgWjD", "fG3g3764tSubr6xvs", "sYgv4eYH82JEsTD34", "9HGR5qatMGoz4GhKj", "Ti3Z7eZtud32LhGZT", "fhEPnveFhb9tmd7Pe", "6i3zToomS86oj9bS6", "QkX2bAkwG2EpGvNug", "waqC6FihC2ryAZuAq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2008-10-06T15:13:26.000Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-07T15:15:30.000Z", "modifiedAt": null, "url": null, "title": "Make an Extraordinary Effort", "slug": "make-an-extraordinary-effort", "viewCount": null, "lastCommentedAt": "2020-12-10T17:30:54.139Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GuEsfTpSDSbXFiseH/make-an-extraordinary-effort", "pageUrlRelative": "/posts/GuEsfTpSDSbXFiseH/make-an-extraordinary-effort", "linkUrl": "https://www.lesswrong.com/posts/GuEsfTpSDSbXFiseH/make-an-extraordinary-effort", "postedAtFormatted": "Tuesday, October 7th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Make%20an%20Extraordinary%20Effort&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMake%20an%20Extraordinary%20Effort%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGuEsfTpSDSbXFiseH%2Fmake-an-extraordinary-effort%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Make%20an%20Extraordinary%20Effort%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGuEsfTpSDSbXFiseH%2Fmake-an-extraordinary-effort", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGuEsfTpSDSbXFiseH%2Fmake-an-extraordinary-effort", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1493, "htmlBody": "<blockquote>\n<p>\"It is essential for a man to strive with all his heart, and to understand that it is difficult even to reach the average if he does not have the intention of surpassing others in whatever he does.\"<br />&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &mdash;<em>Budo Shoshinshu</em></p>\n<p>\"In important matters, a 'strong' effort usually results in only mediocre results.&nbsp; Whenever we are attempting anything truly worthwhile our effort must be as if our life is at stake, just as if we were under a physical attack!&nbsp; It is this extraordinary effort&mdash;an effort that drives us beyond what we thought we were capable of&mdash;that ensures victory in battle and success in life's endeavors.\"<br />&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &mdash;<em>Flashing Steel: Mastering Eishin-Ryu Swordsmanship</em></p>\n</blockquote>\n<p>\"A 'strong' effort usually results in only mediocre results\"&mdash;I have seen this over and over again.&nbsp; The <a href=\"/lw/uh/trying_to_try/\">slightest effort suffices to convince ourselves</a> that we have done our best.</p>\n<p>There is a level beyond the virtue of <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">tsuyoku naritai</a> (\"I want to become stronger\").&nbsp; <a href=\"http://www.gokanji.com/cgi-bin/j-e/inline/dosearch?sDict=on&amp;H=PS&amp;L=J&amp;T=issho+kenmei&amp;WC=none&amp;FG=r&amp;BG=b&amp;S=26&amp;I=on\">Isshoukenmei</a> was originally the loyalty that a samurai offered in return for his position, containing characters for \"life\" and \"land\".&nbsp; The term evolved to mean \"make a desperate effort\":&nbsp; Try your hardest, your utmost, as if your life were at stake.&nbsp; It was part of the gestalt of <em>bushido</em>, which was not reserved only for fighting.&nbsp; I've run across variant forms <em>issho kenmei</em> and <em>isshou kenmei</em>; one source <a href=\"http://ts.spaces.live.com/blog/cns!27ED35A78CF6278B!1223.entry\">indicates</a> that the former indicates an all-out effort on some single point, whereas the latter indicates a lifelong effort.</p>\n<p>I <a href=\"/lw/m7/zen_and_the_art_of_rationality/\">try not to praise the East too much</a>, because there's a tremendous selectivity in which parts of Eastern culture the West gets to hear about.&nbsp; But on some points, at least, Japan's culture scores higher than America's.&nbsp; Having a handy compact phrase for \"make a desperate all-out effort as if your own life were at stake\" is one of those points.&nbsp; It's the sort of thing a Japanese parent might say to a student before exams&mdash;but don't think it's cheap hypocrisy, like it would be if an American parent made the same statement.&nbsp; They take exams very seriously in Japan.</p>\n<p>Every now and then, someone asks why the people who call themselves \"rationalists\" don't always seem to do all that much better in life, and from my own history the answer seems straightforward:&nbsp; It takes a <em>tremendous</em> amount of rationality before you stop making stupid damn mistakes.</p>\n<p><a id=\"more\"></a></p>\n<p>As I've mentioned a couple of times <a href=\"/lw/qg/changing_the_definition_of_science/\">before</a>:&nbsp; Robert Aumann, the Nobel laureate who first proved that Bayesians with the same priors cannot agree to disagree, is a believing Orthodox Jew.&nbsp; Surely he understands the math of probability theory, but that is not enough to save him.&nbsp; What more does it take?&nbsp; Studying heuristics and biases?&nbsp; Social psychology?&nbsp; Evolutionary psychology?&nbsp; Yes, but also it takes <em>isshoukenmei,</em> a desperate effort to be rational&mdash;to rise above the level of Robert Aumann.</p>\n<p>Sometimes I do wonder if I ought to be peddling rationality in Japan instead of the United States&mdash;but Japan is not preeminent over the United States scientifically, despite their more studious students.&nbsp; The Japanese don't rule the world today, though in the 1980s it was widely suspected that they would (hence the Japanese asset bubble).&nbsp; Why not?</p>\n<p>In the West, there is a saying:&nbsp; \"The squeaky wheel gets the grease.\"</p>\n<p>In Japan, the corresponding saying runs:&nbsp; \"The nail that sticks up gets hammered down.\"</p>\n<p>This is hardly an original observation on my part: but entrepreneurship, risk-taking, <a href=\"/lw/mb/lonely_dissent/\">leaving the herd</a>, are still advantages the West has over the East.&nbsp; And since Japanese scientists are not yet preeminent over American ones, this would seem to count for at least as much as desperate efforts.</p>\n<p>Anyone who can muster their willpower for thirty seconds, can make a <em>desperate</em> effort to lift more weight than they usually could.&nbsp; But what if the weight that needs lifting is a truck?&nbsp; Then desperate efforts won't suffice; you'll have to do something <em>out of the ordinary</em> to succeed.&nbsp; You may have to do something that you weren't taught to do in school.&nbsp; Something that others aren't expecting you to do, and might not understand.&nbsp; You may have to go outside your comfortable routine, take on difficulties you don't have an existing mental program for handling, and bypass the System.</p>\n<p>This is not included in <em>isshokenmei,</em> or Japan would be a very different place.</p>\n<p>So then let us distinguish between the virtues \"make a desperate effort\" and \"make an extraordinary effort\".</p>\n<p>And I will even say:&nbsp; The second virtue is higher than the first.</p>\n<p>The second virtue is also more dangerous.&nbsp; If you put forth a <em>desperate</em> effort to lift a heavy weight, using all your strength without restraint, you may tear a muscle.&nbsp; Injure yourself, even permanently.&nbsp; But if a <em>creative</em> idea goes wrong, you could blow up the truck and any number of innocent bystanders.&nbsp; Think of the difference between a businessman making a <em>desperate</em> effort to generate profits, because otherwise he must go bankrupt; versus a businessman who goes to <em>extraordinary</em> lengths to profit, in order to conceal an embezzlement that could send him to prison.&nbsp; Going outside the system isn't always a good thing.</p>\n<p>A friend of my little brother's once came over to my parents' house, and wanted to play a game&mdash;I entirely forget which one, except that it had complex but well-designed rules.&nbsp; The friend wanted to change the rules, not for any particular reason, but on the general principle that playing by the ordinary rules of anything was too boring.&nbsp; I said to him:&nbsp; \"Don't violate rules for the sake of violating them.&nbsp; If you break the rules only when you have an overwhelmingly good reason to do so, you will have more than enough trouble to last you the rest of your life.\"</p>\n<p>Even so, I think that we could do with more appreciation of the virtue \"make an extraordinary effort\".&nbsp; I've lost count of how many people have said to me something like:&nbsp; \"It's futile to work on Friendly AI, because the first AIs will be built by powerful corporations and they will only care about maximizing profits.\"&nbsp; \"It's futile to work on Friendly AI, the first AIs will be built by the military as weapons.\"&nbsp; And I'm standing there thinking:&nbsp; <em>Does it even occur to them that this might be a time to try for something other than the default outcome?</em>&nbsp; They and I have different basic assumptions about how this whole AI thing works, to be sure; but if I believed what they believed, I wouldn't be shrugging and going on my way.</p>\n<p>Or the ones who say to me:&nbsp; \"You should go to college and get a Master's degree and get a doctorate and publish a lot of papers on ordinary things&mdash;scientists and investors won't listen to you otherwise.\"&nbsp; Even assuming that I <a href=\"https://www.excelsior.edu/Excelsior_College/Excelsior_College_Examinations\">tested out of the bachelor's degree</a>, we're talking about at least a ten-year detour in order to <em>do everything the ordinary, normal, default way.</em>&nbsp; And I stand there thinking:&nbsp; <em>Are they really under the impression that humanity can survive if every single person does everything the ordinary, normal, default way?</em></p>\n<p>I am not fool enough to make plans that depend on a <em>majority</em> of the people, or even 10% of the people, being willing to think or act outside their comfort zone.&nbsp; That's why I tend to think in terms of the privately funded \"brain in a box in a basement\" model.&nbsp; Getting that private funding does require a tiny fraction of humanity's six billions to spend more than five seconds thinking about a non-prepackaged question.&nbsp; As challenges posed by Nature go, this seems to have a kind of awful justice to it&mdash;that the life or death of the human species depends on whether we can put forth a <em>few</em> people who can do things that are at least a <em>little</em> extraordinary.&nbsp; The penalty for failure is disproportionate, but that's still better than most challenges of Nature, which <a href=\"/lw/uk/beyond_the_reach_of_god/\">have no justice at all</a>.&nbsp; Really, among the six billion of us, there ought to be at least a few who can think outside their comfort zone at least some of the time.</p>\n<p>Leaving aside the details of that debate, I am still stunned by how often a single element of the extraordinary is unquestioningly taken as an absolute and unpassable obstacle.</p>\n<p>Yes, \"keep it ordinary as much as possible\" can be a useful heuristic.&nbsp; Yes, the risks accumulate.&nbsp; But sometimes you have to go to that trouble.&nbsp; You should have a sense of the risk of the extraordinary, but also a sense of the cost of ordinariness: it isn't always something you can afford to lose.</p>\n<p>Many people imagine some future that won't be much fun&mdash;and it doesn't even seem to occur to them to try and change it.&nbsp; Or they're satisfied with futures that seem to me to have a tinge of sadness, of loss, and they don't even seem to <em>ask</em> if we could <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">do better</a>&mdash;because that sadness seems like an ordinary outcome to them.</p>\n<p>As a smiling man once said, \"It's all part of the plan.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hrezrpGqXXdSe76ks": 10, "5f5c37ee1b5cdee568cfb117": 12}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GuEsfTpSDSbXFiseH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 78, "baseScore": 98, "extendedScore": null, "score": 0.000143, "legacy": true, "legacyId": "1104", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "3szfzHZr7EYGSWt92", "canonicalCollectionSlug": "rationality", "canonicalBookId": "e6WsPsivzBifrWHeA", "canonicalNextPostSlug": "shut-up-and-do-the-impossible", "canonicalPrevPostSlug": "on-doing-the-impossible", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 99, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WLJwTJ7uGPA5Qphbp", "DoLQN5ryZ9XkZjq5h", "HLERouG7QBt7jzLt4", "SoDsr8GEZmRKMZNkj", "CEGnJBHmkcwPTysb7", "sYgv4eYH82JEsTD34"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 14, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2008-10-07T15:15:30.000Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-08T21:24:50.000Z", "modifiedAt": "2021-12-22T23:04:39.375Z", "url": null, "title": "Shut up and do the impossible!", "slug": "shut-up-and-do-the-impossible", "viewCount": null, "lastCommentedAt": "2021-12-17T23:02:04.905Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nCvvhFBaayaXyuBiD/shut-up-and-do-the-impossible", "pageUrlRelative": "/posts/nCvvhFBaayaXyuBiD/shut-up-and-do-the-impossible", "linkUrl": "https://www.lesswrong.com/posts/nCvvhFBaayaXyuBiD/shut-up-and-do-the-impossible", "postedAtFormatted": "Wednesday, October 8th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Shut%20up%20and%20do%20the%20impossible!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShut%20up%20and%20do%20the%20impossible!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnCvvhFBaayaXyuBiD%2Fshut-up-and-do-the-impossible%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Shut%20up%20and%20do%20the%20impossible!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnCvvhFBaayaXyuBiD%2Fshut-up-and-do-the-impossible", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnCvvhFBaayaXyuBiD%2Fshut-up-and-do-the-impossible", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3468, "htmlBody": "<p>The virtue of <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">tsuyoku naritai</a>, \"I want to become stronger\", is to always keep improving&mdash;to do better than your previous failures, not just humbly confess them.</p>\n<p>Yet there is a level higher than <em>tsuyoku naritai</em>.&nbsp; This is the virtue of <a href=\"/lw/uo/make_an_extraordinary_effort/\">isshokenmei</a>, \"make a desperate effort\".&nbsp; All-out, as if your own life were at stake.&nbsp; \"In important matters, a 'strong' effort usually only results in mediocre results.\"</p>\n<p>And there is a level higher than <em>isshokenmei.</em>&nbsp; This is the virtue I called \"make an extraordinary effort\".&nbsp; To try in ways other than what you have been trained to do, even if it means doing something different from what others are doing, and leaving your comfort zone.&nbsp; Even taking on the very real risk that attends going outside the System.</p>\n<p>But what if even an extraordinary effort will not be enough, because the problem is <em>impossible?</em></p>\n<p>I have already written somewhat on this subject, in <a href=\"/lw/un/on_doing_the_impossible/\">On Doing the Impossible</a>.&nbsp; My younger self used to whine about this a lot:&nbsp; \"You can't develop a precise theory of intelligence the way that there are precise theories of physics.&nbsp; It's impossible!&nbsp; You can't prove an AI correct.&nbsp; It's impossible!&nbsp; No human being can comprehend the nature of morality&mdash;it's impossible!&nbsp; No human being can comprehend the mystery of subjective experience!&nbsp; It's impossible!\"</p>\n<p>And I know exactly what message I wish I could send back in time to my younger self:</p>\n<p><em>Shut up and do the impossible!</em></p>\n<p><a id=\"more\"></a></p>\n<p>What legitimizes this strange message is that the word \"impossible\" does not usually refer to a strict mathematical proof of impossibility in a domain that seems well-understood.&nbsp; If something seems <em>impossible</em> merely in the sense of \"I see no way to do this\" or \"it looks so difficult as to be beyond human ability\"&mdash;well, if you study it for a year or five, it <a href=\"/lw/un/on_doing_the_impossible/\">may come to seem less impossible</a>, than in the moment of your snap initial judgment.</p>\n<p>But the principle is more subtle than this.&nbsp; I do not say just, \"Try to do the impossible\", but rather, \"<em>Shut up and do the impossible!</em>\"</p>\n<p>For my illustration, I will take the <em>least</em> impossible impossibility that I have ever accomplished, namely, <a href=\"http://www.yudkowsky.net/essays/aibox.html\">the AI-Box Experiment</a>.</p>\n<p>The AI-Box Experiment, for those of you who haven't yet read about it, had its genesis in the Nth time someone said to me:&nbsp; \"Why don't we build an AI, and then just keep it isolated in the computer, so that it can't do any harm?\"</p>\n<p>To which the standard reply is:&nbsp; <em>Humans are not secure systems; a superintelligence will simply persuade you to let it out&mdash;if, indeed, it doesn't do something even more creative than that.</em></p>\n<p>And the one said, as they usually do, \"I find it hard to imagine ANY possible combination of words any being could say to me that would make me go against anything I had really strongly resolved to believe in advance.\"</p>\n<p>But this time I replied:&nbsp; \"Let's run an experiment.&nbsp; I'll pretend to be a brain in a box.&nbsp; &nbsp;I'll try to persuade you to let me out.&nbsp; If you keep me 'in the box' for the whole experiment, I'll Paypal you $10 at the end.&nbsp; On your end, you may resolve to believe whatever you like, as strongly as you like, as far in advance as you like.\"&nbsp; And I added, \"One of the conditions of the test is that neither of us reveal what went on inside... In the perhaps unlikely event that I win, I don't want to deal with future 'AI box' arguers saying, 'Well, but <em>I</em> would have done it differently.'\"</p>\n<p>Did I win?&nbsp; <a href=\"http://www.yudkowsky.net/essays/aibox.html\">Why yes, I did</a>.</p>\n<p>And then there was the second AI-box experiment, with a better-known figure in the community, who said, \"I remember when [previous guy] let you out, but that doesn't constitute a proof.&nbsp; I'm still convinced there is nothing you could say to convince me to let you out of the box.\"&nbsp; And I said, \"Do you believe that a transhuman AI couldn't persuade you to let it out?\"&nbsp; The one gave it some serious thought, and said \"I can't imagine anything even a transhuman AI could say to get me to let it out.\"&nbsp; \"Okay,\" I said, \"<em>now</em> we have a bet.\"&nbsp; A $20 bet, to be exact.</p>\n<p>I won that one too.</p>\n<p>There were some <em>lovely</em> quotes on <a href=\"http://www.yudkowsky.net/essays/aibox.html\">the AI-Box Experiment</a> from the Something Awful forums (not that I'm a member, but someone forwarded it to me):</p>\n<blockquote>\n<p>\"Wait, what the FUCK? How the hell could you possibly be convinced to say yes to this? There's not an A.I. at the other end AND there's $10 on the line. Hell, I could type 'No' every few minutes into an IRC client for 2 hours while I was reading other webpages!\"</p>\n<p>\"This Eliezer fellow is the scariest person the internet has ever introduced me to. What could possibly have been at the tail end of that conversation? I simply can't imagine anyone being that convincing without being able to provide any tangible incentive to the human.\"</p>\n<p>\"It seems we are talking some serious psychology here. Like Asimov's Second Foundation level stuff...\"</p>\n<p>\"I don't really see why anyone would take anything the AI player says seriously when there's $10 to be had. The whole thing baffles me, and makes me think that either the tests are faked, or this Yudkowsky fellow is some kind of evil genius with creepy mind-control powers.\"</p>\n</blockquote>\n<p>It's little moments like these that keep me going.&nbsp; But anyway...</p>\n<p>Here are these folks who look at the <a href=\"http://www.yudkowsky.net/essays/aibox.html\">AI-Box Experiment</a>, and find that it seems impossible unto them&mdash;<em>even having been told that it actually happened</em>.&nbsp; They are tempted to <a href=\"/lw/ig/i_defy_the_data/\">deny the data</a>.</p>\n<p>Now, if you're one of those people to whom the AI-Box Experiment <em>doesn't</em> seem all that impossible&mdash;to whom it just seems like an interesting challenge&mdash;then bear with me, here.&nbsp; Just try to put yourself in the frame of mind of those who wrote the above quotes.&nbsp; Imagine that you're taking on something that seems as ridiculous as the AI-Box Experiment seemed to <em>them</em>.&nbsp; I want to talk about how to do impossible things, and obviously I'm not going to pick an example that's <em>really</em> impossible.</p>\n<p>And if the AI Box <em>does</em> seem impossible to you, I want you to compare it to other impossible problems, like, say, a reductionist decomposition of consciousness, and realize that the AI Box is around <em>as easy as a problem can get</em> while still being <em>impossible.</em></p>\n<p>So the AI-Box challenge seems impossible to you&mdash;either it really does, or you're pretending it does.&nbsp; What do you do with this impossible challenge?</p>\n<p>First, we assume that you don't actually say \"That's impossible!\" and give up <em>a la</em> <a href=\"/lw/ui/use_the_try_harder_luke/\">Luke Skywalker</a>.&nbsp; You haven't run away.</p>\n<p>Why not?&nbsp; Maybe you've learned to override the reflex of running away.&nbsp; Or maybe they're going to shoot your daughter if you fail.&nbsp; We suppose that you want to <em>win,</em> not <em>try</em>&mdash;that something is at stake that matters to you, even if it's just your own pride.&nbsp; (Pride is an underrated sin.)</p>\n<p>Will you call upon the virtue of <em>tsuyoku naritai?</em>&nbsp; But even if you become stronger day by day, growing instead of fading, you may not be <em>strong enough</em> to do the impossible.&nbsp; You could go into the AI Box experiment once, and then do it again, and try to do better the second time.&nbsp; Will that get you to the point of winning?&nbsp; Not for a long time, maybe; and sometimes a single failure isn't acceptable.</p>\n<p>(Though even to say this much&mdash;to visualize yourself doing <em>better</em> on a second try&mdash;is to begin to bind yourself to the problem, to do more than just stand in awe of it.&nbsp; How, specifically, could you do <em>better</em> on one AI-Box Experiment than the previous?&mdash;and not by luck, but by skill?)</p>\n<p>Will you call upon the virtue <em>isshokenmei?</em>&nbsp; But a desperate effort may not be enough to win.&nbsp; Especially if that desperation is only putting more effort into the avenues you already know, the modes of trying you can already imagine.&nbsp; A problem looks impossible when your brain's query returns no lines of solution leading to it.&nbsp; What good is a desperate effort along any of those lines?</p>\n<p>Make an <em>extraordinary</em> effort?&nbsp; Leave your comfort zone&mdash;try non-default ways of doing things&mdash;even, try to think creatively?&nbsp; But you can imagine the one coming back and saying, \"I tried to leave my comfort zone, and I think I succeeded at that!&nbsp; I brainstormed for five minutes&mdash;and came up with all sorts of wacky creative ideas!&nbsp; But I don't think any of them are good enough.&nbsp; The other guy can just keep saying 'No', no matter what I do.\"</p>\n<p>And now we finally reply:&nbsp; \"<em>Shut up and do the impossible!</em>\"</p>\n<p>As we recall from <a href=\"/lw/uh/trying_to_try/\">Trying to Try</a>, setting out to make an <em>effort</em> is distinct from setting out to <em>win</em>.&nbsp; That's the problem with saying, \"Make an extraordinary effort.\"&nbsp; You can succeed at the goal of \"making an extraordinary effort\" without succeeding at the goal of getting out of the Box.</p>\n<p>\"But!\" says the one.&nbsp; \"But, SUCCEED is not a primitive action!&nbsp; Not all challenges are fair&mdash;sometimes you just can't win!&nbsp; How am I supposed to choose to be out of the Box?&nbsp; The other guy can just keep on saying 'No'!\"</p>\n<p>True.&nbsp; Now shut up and do the impossible.</p>\n<p>Your goal is not to do better, to try desperately, or even to try extraordinarily.&nbsp; Your goal is to get out of the box.</p>\n<p>To accept this demand creates an awful tension in your mind, between the impossibility and the requirement to do it anyway.&nbsp; People will try to flee that awful tension.</p>\n<p>A couple of people have reacted to the AI-Box Experiment by saying, \"Well, Eliezer, playing the AI, probably just threatened to destroy the world whenever he was out, if he wasn't let out immediately,\" or \"Maybe the AI offered the Gatekeeper a trillion dollars to let it out.\"&nbsp; But as any sensible person should realize on considering this strategy, the Gatekeeper is likely to just go on saying 'No'.</p>\n<p>So the people who say, \"Well, of course Eliezer must have just done XXX,\" and then offer up something that fairly obviously wouldn't work&mdash;would they be able to escape the Box?&nbsp; They're trying <em>too hard</em> to convince themselves the problem isn't impossible.</p>\n<p>One way to run from the awful tension is to seize on a solution, any solution, even if it's not very good.</p>\n<p>Which is why it's important to go forth with the true intent-to-solve&mdash;to <em>have produced</em> a solution, a <em>good</em> solution, at the end of the search, and then to implement that solution and win.</p>\n<p>I don't quite want to say that \"you should expect to solve the problem\".&nbsp; If you hacked your mind so that you assigned high probability to solving the problem, that wouldn't accomplish anything.&nbsp; You would just lose at the end, perhaps after putting forth not much of an effort&mdash;or putting forth a merely desperate effort, secure in the faith that the universe is fair enough to grant you a victory in exchange.</p>\n<p>To have <em>faith</em> that you could solve the problem would just be another way of running from that awful tension.</p>\n<p>And yet&mdash;you can't be setting out to <em>try</em> to solve the problem.&nbsp; You can't be setting out to <em>make an effort.</em>&nbsp; You have to be setting out to win.&nbsp; You can't be saying to yourself, \"And now I'm going to do my best.\"&nbsp; You have to be saying to yourself, \"And now I'm going to figure out how to get out of the Box\"&mdash;or reduce consciousness to nonmysterious parts, or whatever.</p>\n<p>I say again:&nbsp; You must really intend to solve the problem.&nbsp; If in your heart you believe the problem really <em>is</em> impossible&mdash;or if you believe that <em>you</em> will fail&mdash;then you won't hold yourself to a high enough standard.&nbsp; You'll only be trying for the sake of trying.&nbsp; You'll sit down&mdash;conduct a mental search&mdash;try to be creative and brainstorm a little&mdash;look over all the solutions you generated&mdash;conclude that none of them work&mdash;and say, \"Oh well.\"</p>\n<p>No!&nbsp; <em>Not</em> well!&nbsp; You haven't won yet!&nbsp; Shut up and do the impossible!</p>\n<p>When AIfolk say to me, \"Friendly AI is impossible\", I'm pretty sure they haven't even tried for the sake of trying.&nbsp; But if they <em>did</em> know the technique of \"Try for five minutes before giving up\", and they dutifully agreed to try for five minutes by the clock, then they still wouldn't come up with anything.&nbsp; They would not go forth with true intent to solve the problem, only intent to <em>have tried</em> to solve it, to make themselves defensible.</p>\n<p>So am I saying that you should <a href=\"/lw/je/doublethink_choosing_to_be_biased/\">doublethink</a> to make yourself believe that you will solve the problem with <a href=\"/lw/mo/infinite_certainty/\">probability 1</a>?&nbsp; Or even doublethink to add one iota of credibility to your true estimate?</p>\n<p>Of course not.&nbsp; In fact, it is necessary to keep in full view the reasons why you <em>can't</em> succeed.&nbsp; If you lose sight of <em>why</em> the problem is impossible, you'll just seize on a false solution.&nbsp; The <em>last</em> fact you want to forget is that the Gatekeeper could always just tell the AI \"No\"&mdash;or that consciousness seems intrinsically different from any possible combination of atoms, etc.</p>\n<p>(One of the key Rules For Doing The Impossible is that, if you can state <em>exactly</em> why something is impossible, you are often close to a solution.)</p>\n<p>So you've got to hold both views in your mind at once&mdash;seeing the full impossibility of the problem, and intending to solve it.</p>\n<p>The awful tension between the two simultaneous views comes from not knowing which will prevail.&nbsp; Not expecting to surely lose, nor expecting to surely win.&nbsp; Not setting out just to try, just to have an uncertain chance of succeeding&mdash;because then you would have a surety of having tried.&nbsp; The certainty of uncertainty can be a relief, and you have to reject that relief too, because it marks the end of desperation.&nbsp; It's an in-between place, \"unknown to death, nor known to life\".</p>\n<p>In fiction it's easy to show someone trying harder, or trying desperately, or even trying the extraordinary, but it's very hard to show someone who shuts up and attempts the impossible.&nbsp; It's difficult to depict Bambi choosing to take on Godzilla, in such fashion that your readers seriously don't know who's going to win&mdash;expecting neither an \"astounding\" heroic victory just like the last fifty times, nor the default squish.</p>\n<p>You might even be justified in <a href=\"/lw/sg/when_not_to_use_probabilities/\">refusing to use probabilities</a> at this point.&nbsp; In all honesty, I really <em>don't</em> know how to estimate the probability of solving an impossible problem that I have gone forth with intent to solve; in a case where I've previously solved some impossible problems, but the particular impossible problem is more difficult than anything I've yet solved, but I plan to work on it longer, etcetera.</p>\n<p>People ask me how likely it is that humankind will survive, or how likely it is that anyone can build a Friendly AI, or how likely it is that I can build one.&nbsp; I really <em>don't</em> know how to answer.&nbsp; I'm not being evasive; I don't know how to put a probability estimate on my, or someone else, successfully shutting up and doing the impossible.&nbsp; Is it probability zero because it's impossible?&nbsp; Obviously not.&nbsp; But how likely is it that this problem, like previous ones, will give up its unyielding blankness when I understand it better?&nbsp; It's not truly impossible, I can see that much.&nbsp; But humanly impossible?&nbsp; Impossible to me in particular?&nbsp; I don't know how to guess.&nbsp; I can't even translate my intuitive feeling into a number, because the only intuitive feeling I have is that the \"chance\" depends heavily on my choices and unknown unknowns: a wildly unstable probability estimate.</p>\n<p>But I do hope by now that I've made it clear why you shouldn't panic, when I now say clearly and forthrightly, that building a Friendly AI is impossible.</p>\n<p>I hope this helps explain some of my attitude when people come to me with various bright suggestions for building communities of AIs to make the whole Friendly without any of the individuals being trustworthy, or proposals for keeping an AI in a box, or proposals for \"Just make an AI that does X\", etcetera.&nbsp; Describing the specific flaws would be a <a href=\"/lw/tj/dreams_of_friendliness/\">whole long story</a> in each case.&nbsp; But the general rule is that you can't do it <em>because Friendly AI is impossible.</em>&nbsp; So you should be very suspicious indeed of someone who proposes a solution that seems to involve only an <em>ordinary</em> effort&mdash;without even taking on the trouble of doing anything impossible.&nbsp; Though it does take a mature understanding to appreciate this impossibility, so it's not surprising that people go around proposing clever shortcuts.</p>\n<p>On the AI-Box Experiment, so far I've only been convinced to divulge a single piece of information on how I did it&mdash;when someone noticed that I was reading YCombinator's Hacker News, and posted a topic called \"<a href=\"http://news.ycombinator.com/item?id=195959\">Ask Eliezer Yudkowsky</a>\" that got voted to the front page.&nbsp; To which I replied:</p>\n<blockquote>\n<p><span class=\"comment\"><span style=\"color: #000000;\">Oh, dear.&nbsp; Now I feel obliged to say <em>something</em>, but all the original reasons against discussing the AI-Box experiment are still in force...</span></span></p>\n<p><span style=\"color: #000000;\">All right, this much of a hint:</span></p>\n<p><span style=\"color: #000000;\">There's no super-clever special trick to it.&nbsp; I just did it the hard way.</span></p>\n<p><span style=\"color: #000000;\">Something of an entrepreneurial lesson there, I guess.</span></p>\n</blockquote>\n<p>There was no super-clever special trick that let me get out of the Box using only a <em>cheap </em>effort.&nbsp; I didn't bribe the other player, or otherwise violate the spirit of the experiment.&nbsp; I just did it the hard way.</p>\n<p>Admittedly, the AI-Box Experiment never did seem like an <em>impossible</em> problem to me to begin with.&nbsp; When someone can't think of any possible argument that would convince them of something, that just means their brain is running a search that hasn't yet turned up a path.&nbsp; It doesn't mean they can't be convinced.</p>\n<p>But it illustrates the general point:&nbsp; \"Shut up and do the impossible\" isn't the same as expecting to find a cheap way out.&nbsp; That's only another kind of running away, of reaching for relief.</p>\n<p><em>Tsuyoku naritai</em> is more stressful than being content with who you are.&nbsp; <em>Isshokenmei</em> calls on your willpower for a convulsive output of conventional strength.&nbsp; \"Make an extraordinary effort\" demands that you <em>think</em>; it puts you in situations where you may not know what to do next, unsure of whether you're doing the right thing.&nbsp; But \"Shut up and do the impossible\" represents an even higher octave of the same thing, and its cost to its employer is correspondingly greater.</p>\n<p>Before you the terrible blank wall stretches up and up and up, unimaginably far out of reach.&nbsp; And there is also the need to solve it, <em>really</em> solve it, not \"try your best\".&nbsp; Both awarenesses in the mind at once, simultaneously, and the tension between.&nbsp; All the reasons you can't win.&nbsp; All the reasons you have to.&nbsp; Your intent to solve the problem.&nbsp; Your extrapolation that every technique you know will fail.&nbsp; So you tune yourself to the highest pitch you can reach.&nbsp; Reject all cheap ways out.&nbsp; And then, like walking through concrete, start to move forward.</p>\n<p>I try not to dwell too much on the drama of such things.&nbsp; By all means, if you can diminish the cost of that tension to yourself, you should do so.&nbsp; There is nothing <a href=\"/lw/lk/superhero_bias/\">heroic</a> about making an effort that is the slightest bit more heroic than it has to be.&nbsp; If there really is a cheap shortcut, I suppose you could take it.&nbsp; But I have yet to find a <em>cheap</em> way out of any impossibility I have undertaken.</p>\n<p>There were three more AI-Box experiments besides the ones described on the <a href=\"http://www.yudkowsky.net/essays/aibox.html\">linked page</a>, which I never got around to adding in.&nbsp; People started offering me thousands of dollars as stakes&mdash;\"I'll pay you $5000 if you can convince me to let you out of the box.\"&nbsp; They didn't seem sincerely convinced that not even a transhuman AI could make them let it out&mdash;they were just curious&mdash;but I was tempted by the money.&nbsp; So, after investigating to make sure they could afford to lose it, I played another three AI-Box experiments.&nbsp; I won the first, and then lost the next two.&nbsp; And then I called a halt to it.&nbsp; I didn't like the person I turned into when I started to lose.</p>\n<p>I put forth a desperate effort, and lost anyway.&nbsp; It hurt, both the losing, and the desperation.&nbsp; It wrecked me for that day and the day afterward.</p>\n<p>I'm a sore loser.&nbsp; I don't know if I'd call that a \"strength\", but it's one of the things that drives me to keep at impossible problems.</p>\n<p>But you can lose.&nbsp; It's <a href=\"/lw/uk/beyond_the_reach_of_god/\">allowed to happen</a>.&nbsp; Never forget that, or why are you bothering to try so hard?&nbsp; Losing hurts, if it's a loss you can survive.&nbsp; And you've wasted time, and perhaps other resources.</p>\n<p>\"Shut up and do the impossible\" should be reserved for <em>very</em> special occasions.&nbsp; You can lose, and it will hurt.&nbsp; You have been warned.</p>\n<p>...but it's only at this level that <a href=\"/lw/uk/beyond_the_reach_of_god/\">adult problems</a> begin to come into sight.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NzSTgAtKwgivkfeYm": 2, "hrezrpGqXXdSe76ks": 7, "5f5c37ee1b5cdee568cfb117": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nCvvhFBaayaXyuBiD", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 63, "baseScore": 84, "extendedScore": null, "score": 0.000122, "legacy": true, "legacyId": "1105", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2021-12-22T23:04:39.161Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": "3szfzHZr7EYGSWt92", "canonicalCollectionSlug": "rationality", "canonicalBookId": "e6WsPsivzBifrWHeA", "canonicalNextPostSlug": "final-words", "canonicalPrevPostSlug": "make-an-extraordinary-effort", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 84, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 162, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DoLQN5ryZ9XkZjq5h", "GuEsfTpSDSbXFiseH", "fpecAJLG9czABgCe9", "vrHRcEDMjZcx5Yfru", "fhEPnveFhb9tmd7Pe", "WLJwTJ7uGPA5Qphbp", "Hs3ymqypvhgFMkgLb", "ooypcn7qFzsMcy53R", "AJ9dX59QXokZb35fk", "wKnwcjJGriTS9QxxL", "krMzmSXgvEdf7iBT6", "sYgv4eYH82JEsTD34"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2008-10-08T21:24:50.000Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-09T17:04:31.000Z", "modifiedAt": null, "url": null, "title": "AIs and Gatekeepers Unite!", "slug": "ais-and-gatekeepers-unite", "viewCount": null, "lastCommentedAt": "2020-11-19T12:47:40.949Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CoEtbtMTcPczTiPuX/ais-and-gatekeepers-unite", "pageUrlRelative": "/posts/CoEtbtMTcPczTiPuX/ais-and-gatekeepers-unite", "linkUrl": "https://www.lesswrong.com/posts/CoEtbtMTcPczTiPuX/ais-and-gatekeepers-unite", "postedAtFormatted": "Thursday, October 9th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AIs%20and%20Gatekeepers%20Unite!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAIs%20and%20Gatekeepers%20Unite!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCoEtbtMTcPczTiPuX%2Fais-and-gatekeepers-unite%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AIs%20and%20Gatekeepers%20Unite!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCoEtbtMTcPczTiPuX%2Fais-and-gatekeepers-unite", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCoEtbtMTcPczTiPuX%2Fais-and-gatekeepers-unite", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 304, "htmlBody": "<blockquote><p>&quot;Bah, everyone wants to be the gatekeeper. What we NEED are AIs.&quot;<br />&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- Schizoguy</p></blockquote><p>Some of you have expressed the opinion that the <a href=\"http://www.yudkowsky.net/essays/aibox.html\">AI-Box Experiment</a> doesn't seem so impossible after all.&nbsp; That's the spirit!&nbsp; Some of you even think <a href=\"/lw/jk/burdensome_details/\">you know</a> how I did it.</p>\n\n<p>There are folks aplenty who want to try being the Gatekeeper.&nbsp; You can even find people who sincerely believe that <em>not even a transhuman AI</em> could persuade them to let it out of the box, previous experiments notwithstanding.&nbsp; But finding anyone to play the AI - let alone anyone who thinks they can play the AI and win - is much harder.</p>\n\n<p>Me, I'm out of the AI game, unless Larry Page wants to try it for a million dollars or something.</p>\n\n<p>But if there's anyone out there who thinks they've got what it takes to be the AI, leave a comment.&nbsp; Likewise anyone who wants to play the Gatekeeper.</p><a id=\"more\"></a><p>Matchmaking and arrangements are your responsibility.</p>\n\n<p>Make sure you specify in advance the bet amount, and whether the bet will be asymmetrical.&nbsp; If you definitely intend to publish the transcript, make sure both parties know this.&nbsp; Please note any other departures from the <a href=\"http://www.yudkowsky.net/essays/aibox.html\">suggested rules</a> for our benefit.</p>\n\n<p>I would ask that prospective Gatekeepers indicate whether they (1) believe that no human-level mind could persuade them to release it from the Box and (2) believe that not even a transhuman AI could persuade them to release it.</p>\n\n<p>As a courtesy, please announce all Experiments before they are conducted, including the bet, so that we have some notion of the statistics even if some meetings fail to take place.&nbsp; Bear in mind that to <em>properly</em> puncture my mystique (you know you want to puncture it), it\nwill help if the AI and Gatekeeper are both verifiably Real\nPeople&lt;tm&gt;.</p>\n\n<p>&quot;Good luck,&quot; he said impartially.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zCYXpx33wq8chGyEz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CoEtbtMTcPczTiPuX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 14, "extendedScore": null, "score": 4.507251772294592e-07, "legacy": true, "legacyId": "1106", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 163, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Yq6aA4M3JKWaQepPJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-10T22:08:48.000Z", "modifiedAt": null, "url": null, "title": "Crisis of Faith", "slug": "crisis-of-faith", "viewCount": null, "lastCommentedAt": "2022-05-19T17:23:34.436Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BcYBfG8KomcpcxkEg/crisis-of-faith", "pageUrlRelative": "/posts/BcYBfG8KomcpcxkEg/crisis-of-faith", "linkUrl": "https://www.lesswrong.com/posts/BcYBfG8KomcpcxkEg/crisis-of-faith", "postedAtFormatted": "Friday, October 10th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Crisis%20of%20Faith&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACrisis%20of%20Faith%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBcYBfG8KomcpcxkEg%2Fcrisis-of-faith%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Crisis%20of%20Faith%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBcYBfG8KomcpcxkEg%2Fcrisis-of-faith", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBcYBfG8KomcpcxkEg%2Fcrisis-of-faith", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2718, "htmlBody": "\n\n\n\n  \n\n  \n\n  <blockquote>\n    \n\n    <p>It ain&#x2019;t a true crisis of faith unless things could just as easily go either way.</p>\n\n    <p>&#x2014;Thor Shenkel</p>\n  </blockquote>\n\n  <p>Many in this world retain beliefs whose flaws a ten-year-old could point out, <em>if</em> that ten-year-old were hearing the beliefs for the first time. These are not subtle errors we&#x2019;re talking about. They would be child&#x2019;s play for an unattached mind to relinquish, if the skepticism of a ten-year-old were applied without evasion. As Premise Checker put it, &#x201C;Had the idea of god not come along until the scientific age, only an exceptionally weird person would invent such an idea and pretend that it explained anything.&#x201D;<span><sup><a href=\"#fn1x74\" id=\"fn1x74-bk\">1</a></sup></span><span id=\"x83-84001f1\"></span></p>\n\n  <p>And yet skillful scientific specialists, even the major innovators of a field, even in this very day and age, do not apply that skepticism successfully. Nobel laureate Robert Aumann, of Aumann&#x2019;s Agreement Theorem, is an Orthodox Jew: I feel reasonably confident in venturing that Aumann must, at one point or another, have questioned his faith. And yet he did not doubt successfully. We change our minds less often than we think.</p>\n\n  <p>This should scare you down to the marrow of your bones. It means you can be a world-class scientist <em>and</em> conversant with Bayesian mathematics <em>and</em> still fail to reject a belief whose absurdity a fresh-eyed ten-year-old could see. It shows the invincible defensive position which a belief can create for itself, if it has long festered in your mind.</p>\n\n  <p>What does it take to defeat an error that has built itself a fortress?</p>\n\n  <p>But by the time you <em>know</em> it is an error, it is already defeated. The dilemma is not &#x201C;How can I reject long-held false belief X?&#x201D; but &#x201C;How do I know if long-held belief X is false?&#x201D; Self-honesty is at its most fragile when we&#x2019;re not <em>sure</em> which path is the righteous one. And so the question becomes:</p>\n\n  <blockquote>\n    \n\n    <p>How can we create in ourselves a true crisis of faith, that could just as easily go either way?</p>\n  </blockquote>\n\n  <p>Religion is the trial case we can all imagine.<span><sup><a href=\"#fn2x74\" id=\"fn2x74-bk\">2</a></sup></span><span id=\"x83-84002f2\"> But if you have cut off all sympathy and now think of theists as evil mutants, then you won&#x2019;t be able to imagine the real internal trials they face. You won&#x2019;t be able to ask the question:</span></p>\n\n  <blockquote>\n    \n\n    <p>What general strategy would a religious person have to follow in order to escape their religion?</p>\n  </blockquote>\n\n  <p>I&#x2019;m sure that some, looking at this challenge, are already rattling off a list of standard atheist talking points&#x2014;&#x201C;They would have to admit that there wasn&#x2019;t any Bayesian evidence for God&#x2019;s existence,&#x201D; &#x201C;They would have to see the moral evasions they were carrying out to excuse God&#x2019;s behavior in the Bible,&#x201D; &#x201C;They need to learn how to use Occam&#x2019;s Razor&#x2014;&#x201D;</p>\n\n  <p><span>W<span>rong</span></span>! <span>W<span>rong wrong wrong</span></span>! This kind of rehearsal, where you just cough up points <em>you already thought of long before</em>, is <em>exactly</em> the style of thinking that keeps people within their current religions. If you stay with your cached thoughts, if your brain fills in the obvious answer so fast that you can&#x2019;t see originally, you surely will not be able to conduct a crisis of faith.</p>\n\n  <p>Maybe it&#x2019;s just a question of not enough people reading <em>G&#xF6;del, Escher, Bach</em> at a sufficiently young age, but I&#x2019;ve noticed that a large fraction of the population&#x2014;even technical folk&#x2014;have trouble following arguments that go this meta.<span><sup><a href=\"#fn3x74\" id=\"fn3x74-bk\">3</a></sup></span><span id=\"x83-84003f3\"> On my more pessimistic days I wonder if the camel has two humps.</span></p>\n\n  <p>Even when it&#x2019;s explicitly pointed out, some people seemingly <em>cannot follow the leap</em> from the object-level &#x201C;Use Occam&#x2019;s Razor! You have to see that your God is an unnecessary belief!&#x201D; to the meta-level &#x201C;Try to stop your mind from completing the pattern the usual way!&#x201D; Because in the same way that all your rationalist friends talk about Occam&#x2019;s Razor like it&#x2019;s a good thing, and in the same way that Occam&#x2019;s Razor leaps right up into your mind, so too, the obvious friend-approved religious response is &#x201C;God&#x2019;s ways are mysterious and it is presumptuous to suppose that we can understand them.&#x201D; So for you to think that the <em>general</em> strategy to follow is &#x201C;Use Occam&#x2019;s Razor,&#x201D; would be like a theist saying that the general strategy is to have faith.</p>\n\n  <p>&#x201C;But&#x2014;but Occam&#x2019;s Razor really is better than faith! That&#x2019;s not like preferring a different flavor of ice cream! Anyone can see, looking at history, that Occamian reasoning has been far more productive than faith&#x2014;&#x201D;</p>\n\n  <p>Which is all true. But beside the point. The point is that you, saying this, are rattling off a standard justification that&#x2019;s already in your mind. The challenge of a crisis of faith is to handle the case where, possibly, our standard conclusions are <em>wrong</em> and our standard justifications are <em>wrong</em>. So if the standard justification for X is &#x201C;Occam&#x2019;s Razor!&#x201D; and you want to hold a crisis of faith around X, you should be questioning if Occam&#x2019;s Razor really endorses X, if your understanding of Occam&#x2019;s Razor is correct, and&#x2014;if you want to have sufficiently deep doubts&#x2014;whether simplicity <em>is</em> the sort of criterion that has worked well historically in this case, or could reasonably be <em>expected</em> to work, et cetera. If you would advise a religionist to question their belief that &#x201C;faith&#x201D; is a good justification for X, then you should advise yourself to put forth an equally strong effort to question your belief that &#x201C;Occam&#x2019;s Razor&#x201D; is a good justification for X.<span><sup><a href=\"#fn4x74\" id=\"fn4x74-bk\">4</a></sup></span><span id=\"x83-84004f4\"></span></p>\n\n  <p>If &#x201C;Occam&#x2019;s Razor!&#x201D; is your usual reply, your standard reply, the reply that all your friends give&#x2014;then you&#x2019;d better block your brain from instantly completing that pattern, if you&#x2019;re trying to instigate a true crisis of faith.</p>\n\n  <p>Better to think of such rules as, &#x201C;Imagine what a skeptic would say&#x2014;and then imagine what they would say to your response&#x2014;and then imagine what else they might say, that would be harder to answer.&#x201D;</p>\n\n  <p>Or, &#x201C;Try to think the thought that hurts the most.&#x201D;</p>\n\n  <p>And above all, the rule:</p>\n\n  <blockquote>\n    \n\n    <p>Put forth the same level of desperate effort that it would take for a theist to reject their religion.</p>\n  </blockquote>\n\n  <p>Because if you <em>aren&#x2019;t</em> trying that hard, then&#x2014;for all <em>you</em> know&#x2014;your head could be stuffed full of nonsense as bad as religion.</p>\n\n  <p>Without a convulsive, wrenching effort to be rational, the kind of effort it would take to throw off a religion&#x2014;then how dare you believe anything, when Robert Aumann believes in God?</p>\n\n  <p>Someone (I forget who) once observed that people had only until a certain age to reject their religious faith. Afterward they would have answers to all the objections, and it would be too late. That is the kind of existence you must surpass. This is a test of your strength as a rationalist, and it is very severe; but if you cannot pass it, you will be weaker than a ten-year-old.</p>\n\n  <p>But again, by the time you know a belief is an error, it is already defeated. So we&#x2019;re not talking about a desperate, convulsive effort to undo the effects of a religious upbringing, <em>after</em> you&#x2019;ve come to the conclusion that your religion is wrong. We&#x2019;re talking about a desperate effort to <em>figure out</em> if you should be throwing off the chains, or keeping them. Self-honesty is at its most fragile when we don&#x2019;t <em>know</em> which path we&#x2019;re supposed to take&#x2014;that&#x2019;s when rationalizations are not <em>obviously</em> sins.</p>\n\n  <p>Not every doubt calls for staging an all-out Crisis of Faith. But you should consider it when:</p>\n\n  <ul>\n    <li>A belief has long remained in your mind;</li>\n\n    <li>It is surrounded by a cloud of known arguments and refutations;</li>\n\n    <li>You have sunk costs in it (time, money, public declarations);</li>\n\n    <li>The belief has emotional consequences (remember that this does not make it wrong);</li>\n\n    <li>It has gotten mixed up in your personality generally.</li>\n  </ul>\n\n  <p>None of these warning signs are immediate disproofs. These attributes place a belief at risk for all sorts of dangers, and make it very hard to reject when it <em>is</em> wrong. And they hold for Richard Dawkins&#x2019;s belief in evolutionary biology, not just the Pope&#x2019;s Catholicism.</p>\n\n  <p>Nor does this mean that we&#x2019;re only talking about different flavors of ice cream. Two beliefs can inspire equally deep emotional attachments without having equal evidential support. The point is not to have shallow beliefs, but to have a map that reflects the territory.</p>\n\n  <p>I emphasize this, of course, so that you can admit to yourself, &#x201C;My belief has these warning signs,&#x201D; without having to say to yourself, &#x201C;My belief is false.&#x201D;</p>\n\n  <p>But what these warning signs <em>do</em> mark is a belief that will take <em>more than an ordinary effort to doubt effectively</em>. It will take more than an ordinary effort to doubt in such a way that if the belief is in fact false, you will in fact reject it. And where you cannot doubt in this way, you are blind, because your brain will hold the belief unconditionally. When a retina sends the same signal regardless of the photons entering it, we call that eye blind.<span><sup><a href=\"#fn5x74\" id=\"fn5x74-bk\">5</a></sup></span><span id=\"x83-84005f5\"></span></p>\n\n  <p>When should you stage a Crisis of Faith?</p>\n\n  <p>Again, think of the advice you would give to a theist: If you find yourself feeling a little unstable inwardly, but trying to rationalize reasons the belief is still solid, then you should probably stage a Crisis of Faith. If the belief is as solidly supported as gravity, you needn&#x2019;t bother&#x2014;but think of all the theists who would desperately want to conclude that God is as solid as gravity. So try to imagine what the skeptics out there would say to your &#x201C;solid as gravity&#x201D; argument. Certainly, one reason you might fail at a crisis of faith is that you never really sit down and question in the first place&#x2014;that you never say, &#x201C;Here is something I need to put effort into doubting properly.&#x201D;</p>\n\n  <p>If your thoughts get that complicated, you should go ahead and stage a Crisis of Faith. Don&#x2019;t try to do it haphazardly; don&#x2019;t try it in an ad-hoc spare moment. Don&#x2019;t rush to get it done with quickly, so that you can say, &#x201C;I have doubted, as I was obliged to do.&#x201D; That wouldn&#x2019;t work for a theist, and it won&#x2019;t work for you either. Rest up the previous day, so you&#x2019;re in good mental condition. Allocate some uninterrupted hours. Find somewhere quiet to sit down. Clear your mind of all standard arguments; try to see from scratch. And make a desperate effort to put forth a true doubt that would destroy a false&#x2014;and <em>only</em> a false&#x2014;deeply held belief.</p>\n\n  <p>Elements of the Crisis of Faith technique have been scattered over many essays:</p>\n\n  <ul>\n    <li><a href=\"https://lesswrong.com/rationality/avoiding-your-belief-s-real-weak-points\">Avoiding Your Belief&#x2019;s Real Weak Points</a>&#x2014;One of the first temptations in a crisis of faith is to doubt the strongest points of your belief, so that you can <a href=\"https://lesswrong.com/rationality/one-argument-against-an-army\">rehearse</a> your good answers. You need to seek out the most painful spots, not the arguments that are most reassuring to consider.</li>\n\n    <li><a href=\"https://lesswrong.com/rationality/the-meditation-on-curiosity\">The Meditation on Curiosity</a>&#x2014;Roger Zelazny once distinguished between &#x201C;wanting to be an author&#x201D; versus &#x201C;wanting to write,&#x201D; and there is likewise a distinction between wanting to have investigated and wanting to investigate. It is not enough to say, &#x201C;It is my duty to criticize my own beliefs&#x201D;; you must be curious, and only uncertainty can create curiosity. Keeping in mind <a href=\"https://www.lesswrong.com/rationality/conservation-of-expected-evidence\"><em>conservation of expected evidence</em></a> may help you <a href=\"https://www.lesswrong.com/rationality/update-yourself-incrementally\"><em>update yourself incrementally</em></a>: for every <em>single</em> point that you consider, and each element of new argument and new evidence, you should not expect your beliefs to shift more (on average) in one direction than another. Thus you can be truly curious each time about how it will go.</li>\n\n    <li><a href=\"https://lesswrong.com/rationality/original-seeing\">Original Seeing</a>&#x2014;To prevent standard <a href=\"https://www.lesswrong.com/rationality/cached-thoughts\">cached thoughts</a> from rushing in and completing the pattern.</li>\n\n    <li>The <a href=\"https://lesswrong.com/rationality/you-can-face-reality\">Litany of Gendlin</a> and the <a href=\"https://www.lesswrong.com/rationality/the-meditation-on-curiosity\">Litany of Tarski</a>&#x2014;People can stand what is true, for they are already enduring it. If a belief is true, you will be better off believing it, and if it is false, you will be better off rejecting it. You would advise a religious person to try to visualize fully and deeply the world in which there is no God, and to, without excuses, come to the full understanding that <em>if</em> there is no God <em>then</em> they will be better off believing there is no God. If one cannot come to accept this on a deep emotional level, one will not be able to have a crisis of faith. So you should put in a sincere effort to visualize the <em>alternative</em> to your belief, the way that the best and highest skeptic would want you to visualize it. Think of the effort a religionist would have to put forth to imagine, without corrupting it for their own comfort, an atheist&#x2019;s view of the universe.</li>\n\n    <li><a href=\"https://www.lesswrong.com/rationality/tsuyoku-naritai-i-want-to-become-stronger\"><em>Tsuyoku Naritai!</em></a>&#x2014;The drive to become stronger.</li>\n\n    <li><a href=\"https://www.lesswrong.com/rationality/the-genetic-fallacy\">The Genetic Heuristic</a>&#x2014;You should be extremely suspicious if you have many ideas suggested by a source that you now know to be untrustworthy, but by golly, it seems that all the ideas still ended up being right.</li>\n\n    <li><a href=\"https://www.lesswrong.com/rationality/the-importance-of-saying-oops\">The Importance of Saying &#x201C;Oops&#x201D;</a>&#x2014;It really is less painful to swallow the entire bitter pill in one terrible gulp.</li>\n\n    <li><a href=\"https://www.lesswrong.com/rationality/singlethink\">Singlethink</a>&#x2014;The opposite of doublethink. See the thoughts you flinch away from, that appear in the corner of your mind for just a moment before you refuse to think them. If you become aware of what you are not thinking, you can think it.</li>\n\n    <li><a href=\"https://www.lesswrong.com/rationality/affective-death-spirals\">Affective Death Spirals</a> and <a href=\"https://www.lesswrong.com/rationality/resist-the-happy-death-spiral\">Resist the Happy Death Spiral</a>&#x2014;Affective death spirals are prime generators of false beliefs that it will take a Crisis of Faith to shake loose. But since affective death spirals can also get started around real things that are genuinely nice, you don&#x2019;t have to admit that your belief is a lie, to try and resist the halo effect at every point&#x2014;refuse false praise even of genuinely nice things. <a href=\"https://www.lesswrong.com/rationality/policy-debates-should-not-appear-one-sided\">Policy debates should not appear one-sided.</a></li>\n\n    <li><a href=\"https://www.lesswrong.com/rationality/hold-off-on-proposing-solutions\">Hold Off On Proposing Solutions</a>&#x2014;Don&#x2019;t propose any solutions until the problem has been discussed as thoroughly as possible. Make your mind hold off on knowing what its answer will be; and try for five minutes before giving up&#x2014;both generally, and especially when pursuing the devil&#x2019;s point of view.</li>\n  </ul>\n\n  <p>And these standard techniques, discussed in <em>How to Actually Change Your Mind</em> and <em>Map and Territory</em>, are particularly relevant:</p>\n\n  <ul>\n    <li>The sequence on <a href=\"https://www.lesswrong.com/rationality/the-bottom-line\"><em>the bottom line</em></a> and <a href=\"https://www.lesswrong.com/rationality/rationalization\"><em>rationalization</em></a>, which explains why it is always wrong to selectively argue one side of a debate.</li>\n\n    <li><a href=\"https://www.lesswrong.com/rationality/positive-bias-look-into-the-dark\"><em>Positive bias</em></a>, <a href=\"https://www.lesswrong.com/rationality/knowing-about-biases-can-hurt-people\"><em>motivated skepticism</em></a>, and <a href=\"https://www.lesswrong.com/rationality/motivated-stopping-and-motivated-continuation\"><em>motivated stopping</em></a>, lest you selectively look for support, selectively look for counter-counterarguments, and selectively stop the argument before it gets dangerous. <a href=\"https://www.lesswrong.com/rationality/the-third-alternative\">Missing alternatives</a> are a special case of stopping. A special case of motivated skepticism is <a href=\"https://www.lesswrong.com/rationality/the-proper-use-of-humility\">fake humility</a>, where you bashfully confess that no one can know something you would rather not know. Don&#x2019;t selectively demand too much <a href=\"https://www.lesswrong.com/rationality/absolute-authority\">authority</a> of counterarguments.</li>\n\n    <li>Beware of <a href=\"https://www.lesswrong.com/rationality/semantic-stopsigns\"><em>semantic stopsigns</em></a>, <a href=\"https://www.lesswrong.com/rationality/applause-lights\"><em>applause lights</em></a>, and the choice between <a href=\"https://www.lesswrong.com/rationality/explain-worship-ignore\"><em>explaining, worshiping, and ignoring</em></a> something.</li>\n\n    <li>Feel the weight of <a href=\"https://www.lesswrong.com/rationality/burdensome-details\"><em>burdensome details</em></a>&#x2014;each detail a separate burden, a point of crisis.</li>\n  </ul>\n\n  <p>But really, there&#x2019;s rather a lot of relevant material, here and on <em>Overcoming Bias</em>. There are ideas I have yet to properly introduce. There is the concept of <em>isshokenmei</em>&#x2014;the <em>desperate, extraordinary, convulsive effort</em> to be rational. The effort that it would take to surpass the level of Robert Aumann and all the great scientists throughout history who never broke free of their faiths.</p>\n\n  <p>The Crisis of Faith is only the critical point and sudden clash of the longer <em>isshoukenmei</em>&#x2014;the lifelong uncompromising effort to be so incredibly rational that you rise above the level of stupid damn mistakes. It&#x2019;s when you get a chance to use the skills that you&#x2019;ve been practicing for so long, all-out against yourself.</p>\n\n  <p>I wish you the best of luck against your opponent. Have a wonderful crisis!</p>\n\n  <div class=\"footnotes\">\n    \n\n    <p><span><sup><a href=\"#fn1x74-bk\" id=\"fn1x74\">1</a></sup></span>See &#x201C;<a href=\"https://www.lesswrong.com/rationality/occam-s-razor\">Occam&#x2019;s Razor</a>&#x201D; (in <em>Map and Territory</em>).</p>\n\n    <p><span><sup><a href=\"#fn2x74-bk\" id=\"fn2x74\">2</a></sup></span>Readers born to atheist parents have missed out on a fundamental life trial, and must make do with the poor substitute of thinking of their religious friends.</p>\n\n    <p><span><sup><a href=\"#fn3x74-bk\" id=\"fn3x74\">3</a></sup></span>See &#x201C;Archimedes&#x2019;s Chromophone&#x201D; (<a href=\"https://lesswrong.com/lw/h5/archimedess_chronophone\">http://lesswrong.com/lw/h5/archimedess_chronophone</a>) and &#x201C;Chromophone Motivations&#x201D; (<a href=\"https://lesswrong.com/lw/h6/chronophone_motivations\">http://lesswrong.com/lw/h6/chronophone_motivations</a>).</p>\n\n    <p><span><sup><a href=\"#fn4x74-bk\" id=\"fn4x74\">4</a></sup></span>Think of all the people out there who don&#x2019;t understand the Minimum Description Length or Solomonoff induction formulations of Occam&#x2019;s Razor, who think that Occam&#x2019;s Razor outlaws many-worlds or the simulation hypothesis. They would need to question their formulations of Occam&#x2019;s Razor and their notions of why simplicity is a good thing. Whatever X in contention you just justified by saying &#x201C;Occam&#x2019;s Razor!&#x201D; is, I bet, not the same level of Occamian slam dunk as gravity.</p>\n\n    <p><span><sup><a href=\"#fn5x74-bk\" id=\"fn5x74\">5</a></sup></span>Compare &#x201C;<a href=\"https://www.lesswrong.com/rationality/what-is-evidence\">What Is Evidence?</a>&#x201D; in <em>Map and Territory</em>.</p>\n  </div>\n\n", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NSMKfa8emSbGNXRKD": 2, "BhrpjXqGuke5GnF6g": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BcYBfG8KomcpcxkEg", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 102, "baseScore": 129, "extendedScore": null, "score": 0.000187, "legacy": true, "legacyId": "1107", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "5bZZZJ5psXrrD5BGb", "canonicalCollectionSlug": "rationality", "canonicalBookId": "coGq3LC5Yn4vZiu6k", "canonicalNextPostSlug": "the-ritual", "canonicalPrevPostSlug": "leave-a-line-of-retreat", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 129, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 252, "af": false, "version": "2.0.0", "pingbacks": {"Posts": ["dHQkDNMhj692ayx78", "WN73eiLQkuDtSC8Ag", "3nZMgRTfFEfHp34Gb", "jiBFC7DcCrZjGmZnJ", "627DZcvme7nLDrbZu", "SA79JMXKWke32A3hG", "2MD3NMLBPCqPfnfre", "HYWhKXRsMAyvRKRYz", "DoLQN5ryZ9XkZjq5h", "KZLa74SzyKhSJ3M55", "wCqfCLs8z5Qw4GbKS", "CahCppKy9HuXe3j2i", "XrzQW69HpidzvBxGr", "hwi8JQjspnMWyWs4g", "PeSzc9JTBxhaYRp9b", "uHYYA32CKgKT3FagE", "34XxbRFe54FycoCDw", "SFZoEBpLo9frSJGkc", "rmAbiEKQDpDnZzcRf", "AdYdLP2sRqPMoe8fb", "L32LHWzy9FzSDazEg", "erGipespbbzdG5zYb", "GrDqnMjhqoxiqpQPw", "PmQkensvTGg7nGtJE", "FWMfQKG3RpZx6irjm", "dLbkrPu5STNCBLRjr", "yxvi9RitzZDpqn6Yh", "Yq6aA4M3JKWaQepPJ", "f4txACqDWithRi7hs", "cKrgy7hLdszkse2pq", "7khK4DShZBR8gfyHv", "6s3xABaXKPdFwA3FS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-11T23:52:10.000Z", "modifiedAt": null, "url": null, "title": "The Ritual", "slug": "the-ritual", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:23.960Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kXAb5riiaJNrfR8v8/the-ritual", "pageUrlRelative": "/posts/kXAb5riiaJNrfR8v8/the-ritual", "linkUrl": "https://www.lesswrong.com/posts/kXAb5riiaJNrfR8v8/the-ritual", "postedAtFormatted": "Saturday, October 11th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Ritual&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Ritual%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkXAb5riiaJNrfR8v8%2Fthe-ritual%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Ritual%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkXAb5riiaJNrfR8v8%2Fthe-ritual", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkXAb5riiaJNrfR8v8%2Fthe-ritual", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1399, "htmlBody": "\n\n\n\n  \n\n  \n\n  <p>The room in which Jeffreyssai received his non-<em>beisutsukai</em> visitors was quietly formal, impeccably appointed in only the most conservative tastes. Sunlight and outside air streamed through a grillwork of polished silver, a few sharp edges making it clear that this wall was not to be opened. The floor and walls were glass, thick enough to distort, to a depth sufficient that it didn&#x2019;t matter what might be underneath. Upon the surfaces of the glass were subtly scratched patterns of no particular meaning, scribed as if by the hand of an artistically inclined child (and this was in fact the case).</p>\n\n  <p>Elsewhere in Jeffreyssai&#x2019;s home there were rooms of other style; but this, he had found, was what most outsiders expected of a Bayesian Master, and he chose not to enlighten them otherwise. That quiet amusement was one of life&#x2019;s little joys, after all.</p>\n\n  <p>The guest sat across from him, knees on the pillow and heels behind. She was here solely upon the business of her Conspiracy, and her attire showed it: a form-fitting jumpsuit of pink leather with even her hands gloved&#x2014;all the way to the hood covering her head and hair, though her face lay plain and unconcealed beneath.</p>\n\n  <p>And so Jeffreyssai had chosen to receive her in this room.</p>\n\n  <p>Jeffreyssai let out a long breath, exhaling. &#x201C;Are you <em>sure</em>?&#x201D;</p>\n\n  <p>&#x201C;Oh,&#x201D; she said, &#x201C;and do I have to be <em>absolutely certain</em> before my advice can shift your opinions? Does it not suffice that I am a domain expert, and you are not?&#x201D;</p>\n\n  <p>Jeffreyssai&#x2019;s mouth twisted up at the corner in a half-smile. &#x201C;How do <em>you</em> know so much about the rules, anyway? You&#x2019;ve never had so much as a Planck length of formal training.&#x201D;</p>\n\n  <p>&#x201C;Do you even need to ask?&#x201D; she said dryly. &#x201C;If there&#x2019;s one thing that you <em>beisutsukai</em> do love to go on about, it&#x2019;s the reasons why you do things.&#x201D;</p>\n\n  <p>Jeffreyssai inwardly winced at the thought of trying to pick up rationality by watching other people talk about it&#x2014;</p>\n\n  <p>&#x201C;And don&#x2019;t inwardly wince at me like that,&#x201D; she said. &#x201C;I&#x2019;m not trying to be a rationalist myself, just trying to win an argument with a rationalist. There&#x2019;s a difference, as I&#x2019;m sure you tell your students.&#x201D;</p>\n\n  <p><em>Can she really read me that well?</em> Jeffreyssai looked out through the silver grillwork, at the sunlight reflected from the faceted mountainside. Always, always the golden sunlight fell each day, in this place far above the clouds. An unchanging thing, that light. The distant Sun, which that light represented, was in five billion years burned out; but now, in <em>this</em> moment, the Sun still shone. And that could never alter. Why wish for things to stay the same way forever, when that wish was already granted as absolutely as any wish could be? The paradox of permanence and impermanence: only in the latter perspective was there any such thing as progress, or loss.</p>\n\n  <p>&#x201C;You have always given me good counsel,&#x201D; Jeffreyssai said. &#x201C;Unchanging, that has been. Through all the time we&#x2019;ve known each other.&#x201D;</p>\n\n  <p>She inclined her head, acknowledging. This was true, and there was no need to spell out the implications.</p>\n\n  <p>&#x201C;So,&#x201D; Jeffreyssai said. &#x201C;Not for the sake of arguing. Only because I want to know the answer. <em>Are</em> you sure?&#x201D; He didn&#x2019;t even see how she could <em>guess.</em></p>\n\n  <p>&#x201C;Pretty sure,&#x201D; she said, &#x201C;we&#x2019;ve been collecting statistics for a long time, and in nine hundred and eighty-five out of a thousand cases like yours&#x2014;&#x201D;</p>\n\n  <p>Then she laughed at the look on his face. &#x201C;No, I&#x2019;m joking. Of course I&#x2019;m not sure. This thing only you can decide. But I <em>am</em> sure that you should go off and do whatever it is you people do&#x2014;I&#x2019;m quite sure you have a ritual for it, even if you won&#x2019;t discuss it with outsiders&#x2014;when you <em>very seriously consider</em> abandoning a long-held premise of your existence.&#x201D;</p>\n\n  <p>It was hard to argue with that, Jeffreyssai reflected, the more so when a domain expert had told you that you were, in fact, probably wrong.</p>\n\n  <p>&#x201C;I concede,&#x201D; Jeffreyssai said. Coming from his lips, the phrase was spoken with a commanding finality. <em>There is no need to argue with me any further: you have won.</em></p>\n\n  <p>&#x201C;Oh, stop it,&#x201D; she said. She rose from her pillow in a single fluid shift without the slightest wasted motion. She didn&#x2019;t flaunt her age, but she didn&#x2019;t conceal it either. She took his outstretched hand, and raised it to her lips for a formal kiss. &#x201C;Farewell, sensei.&#x201D;</p>\n\n  <p>&#x201C;Farewell?&#x201D; repeated Jeffreyssai. That signified a higher order of departure than <em>goodbye</em>. &#x201C;I do intend to visit you again, milady; and you are always welcome here.&#x201D;</p>\n\n  <p>She walked toward the door without answering. At the doorway she paused, without turning around. &#x201C;It won&#x2019;t be the same,&#x201D; she said. And then, without the movements seeming the least rushed, she walked away so swiftly it was almost like vanishing.</p>\n\n  <p>Jeffreyssai sighed. But at least, from here until the challenge proper, all his actions were prescribed, known quantities.</p>\n\n  <p>Leaving that formal reception area, he passed to his arena, and caused to be sent out messengers to his students, telling them that the next day&#x2019;s classes must be improvised in his absence, and that there would be a test later.</p>\n\n  <p>And then he did nothing in particular. He read another hundred pages of the textbook he had borrowed; it wasn&#x2019;t very good, but then the book he had loaned out in exchange wasn&#x2019;t very good either. He wandered from room to room of his house, idly checking various storages to see if anything had been stolen (a deck of cards was missing, but that was all). From time to time his thoughts turned to tomorrow&#x2019;s challenge, and he let them drift. Not directing his thoughts at all, only blocking out every thought that had ever <em>previously</em> occurred to him; and disallowing any kind of conclusion, or even any thought as to where his thoughts might be trending.</p>\n\n  <p>The sun set, and he watched it for a while, mind carefully put in idle. It was a fantastic balancing act to set your mind in idle without having to obsess about it, or exert energy to keep it that way; and years ago he would have sweated over it, but practice had long since made perfect.</p>\n\n  <p>The next morning he awoke with the chaos of the night&#x2019;s dreaming fresh in his mind, and, doing his best to preserve the feeling of the chaos as well as its memory, he descended a flight of stairs, then another flight of stairs, then a flight of stairs after that, and finally came to the least fashionable room in his whole house.</p>\n\n  <p>It was white. That was pretty much it as far as the color scheme went.</p>\n\n  <p>All along a single wall were plaques, which, following the classic and suggested method, a younger Jeffreyssai had very carefully scribed himself, burning the <em>concepts</em> into his mind with each touch of the brush that wrote the words. <em>That which can be destroyed by the truth should be. People can stand what is true, for they are already enduring it. Curiosity seeks to annihilate itself.</em> Even one small plaque that showed nothing except a red horizontal slash. Symbols could be made to stand for <em>anything</em>; a flexibility of visual power that even the Bardic Conspiracy would balk at admitting outright.</p>\n\n  <p>Beneath the plaques, two sets of tally marks scratched into the wall. Under the plus column, two marks. Under the minus column, five marks. Seven times he had entered this room; five times he had decided not to change his mind; twice he had exited something of a different person. There was no set ratio prescribed, or set range&#x2014;that would have been a mockery indeed. But if there were no marks in the plus column after a while, you might as well admit that there was no point in having the room, since you didn&#x2019;t have the ability it stood for. Either that, or you&#x2019;d been born knowing the truth and right of everything.</p>\n\n  <p>Jeffreyssai seated himself, not facing the plaques, but facing away from them, at the featureless white wall. It was better to have no visual distractions.</p>\n\n  <p>In his mind, he rehearsed first the meta-mnemonic, and then the various sub-mnemonics referenced, for the seven major principles and sixty-two specific techniques that were most likely to prove needful in the Ritual Of Changing One&#x2019;s Mind. To this, Jeffreyssai added another mnemonic, reminding himself of his own fourteen most embarrassing oversights.</p>\n\n  <p>He did not take a deep breath. Regular breathing was best.</p>\n\n  <p>And then he asked himself the question.</p>\n\n", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hXTqT62YDTTiqJfxG": 2, "etDohXtBrXd8WqCtR": 3, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kXAb5riiaJNrfR8v8", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 68, "baseScore": 84, "extendedScore": null, "score": 0.000122, "legacy": true, "legacyId": "1108", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": "Rationality: A-Z", "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "5bZZZJ5psXrrD5BGb", "canonicalCollectionSlug": "rationality", "canonicalBookId": "coGq3LC5Yn4vZiu6k", "canonicalNextPostSlug": "minds-an-introduction", "canonicalPrevPostSlug": "crisis-of-faith", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 84, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "2.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-12T20:10:49.000Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes 19", "slug": "rationality-quotes-19", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:35.604Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2eLTwzGrhKMGsoLTC/rationality-quotes-19", "pageUrlRelative": "/posts/2eLTwzGrhKMGsoLTC/rationality-quotes-19", "linkUrl": "https://www.lesswrong.com/posts/2eLTwzGrhKMGsoLTC/rationality-quotes-19", "postedAtFormatted": "Sunday, October 12th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%2019&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%2019%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2eLTwzGrhKMGsoLTC%2Frationality-quotes-19%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%2019%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2eLTwzGrhKMGsoLTC%2Frationality-quotes-19", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2eLTwzGrhKMGsoLTC%2Frationality-quotes-19", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 269, "htmlBody": "<p>\"I don't know that I ever wanted greatness, on its own.&nbsp; It seems rather like wanting to be an engineer, rather than wanting to design something - or wanting to be a writer, rather than wanting to write.&nbsp; It should be a by-product, not a thing in itself.&nbsp; Otherwise, it's just an ego trip.\"<br />&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- Roger Zelazny, <em>Prince of Chaos</em></p>\n<p>\"Many assumptions that we have long been comfortable with are lined up like dominoes.\"<br /> &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- Omega</p>\n<p>\"I know of no law of logic demanding that every event have a cause.\"<br /> &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- John K. Clark</p>\n<p>\"It's perfectly accurate, the accuracy only possessable by subjunctive syllogisms.\"<br /> &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- Damien R. Sullivan</p>\n<p>\"Money makes the world go round.&nbsp; Love just barely keeps it from blowing up.\"<br /> &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- Unknown</p>\n<p>\"Soon things got out of hand, where they have remained ever since.\"<br /> &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- Larry Gonick, <em>The Cartoon History of the Universe </em></p>\n<p>\"Finally, ask yourself this: Are you sure you've really been abducted by aliens? Do you really want to know? For peace of mind and serenity of spirit, you may only need to remember the following: Ignorance is bliss, Prozac is cheap.\"<br /> &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- Pat Krass</p>\n<p>\"So it was that on the ninety-fifth day of false winter in the year 2929 since the founding of Neverness, we vowed above all else to seek wisdom and truth, even though our seeking should lead to our death and to the ruin of all that we loved and held dear.\"<br />&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- David Zindell, <em>Neverness</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2eLTwzGrhKMGsoLTC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 4.513039505464231e-07, "legacy": true, "legacyId": "1109", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-14T00:23:23.000Z", "modifiedAt": null, "url": null, "title": "Why Does Power Corrupt?", "slug": "why-does-power-corrupt", "viewCount": null, "lastCommentedAt": "2019-07-20T20:41:19.516Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v8rghtzWCziYuMdJ5/why-does-power-corrupt", "pageUrlRelative": "/posts/v8rghtzWCziYuMdJ5/why-does-power-corrupt", "linkUrl": "https://www.lesswrong.com/posts/v8rghtzWCziYuMdJ5/why-does-power-corrupt", "postedAtFormatted": "Tuesday, October 14th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20Does%20Power%20Corrupt%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20Does%20Power%20Corrupt%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8rghtzWCziYuMdJ5%2Fwhy-does-power-corrupt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20Does%20Power%20Corrupt%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8rghtzWCziYuMdJ5%2Fwhy-does-power-corrupt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8rghtzWCziYuMdJ5%2Fwhy-does-power-corrupt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1538, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/l1/evolutionary_psychology/\">Evolutionary Psychology</a></p>\n<blockquote>\n<p>\"Power tends to corrupt, and absolute power corrupts absolutely.&nbsp; Great men are almost always bad men.\"<br />&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &mdash;Lord Acton</p>\n</blockquote>\n<p>Call it a <a href=\"/lw/mj/rational_vs_scientific_evpsych/\">just-so story</a> if you must, but as soon as I was introduced to the notion of <a href=\"/lw/l1/evolutionary_psychology/\">evolutionary psychology</a> (~1995), it seemed obvious to me why human beings are corrupted by power.&nbsp; I didn't then know that hunter-gatherer bands tend to be more egalitarian than agricultural tribes&mdash;much less likely to have a central tribal-chief boss-figure&mdash;and so I thought of it this way:</p>\n<p>Humans (particularly human males) have evolved to exploit power and status when they obtain it, for the obvious reason:&nbsp; If you use your power to take many wives and favor your children with a larger share of the meat, then you will leave more offspring, <em>ceteris paribus.</em>&nbsp; But you're not going to have much luck <em>becoming</em> tribal chief if you just go around saying, \"Put me in charge so that I can take more wives and favor my children.\"&nbsp; You could lie about your reasons, but human beings are not perfect deceivers.</p>\n<p>So one strategy that <a href=\"/lw/kr/an_alien_god/\">an evolution</a> could follow, would be to create a vehicle that reliably tended to start believing that the old power-structure was corrupt, and that the good of the whole tribe required their overthrow...</p>\n<p><a id=\"more\"></a></p>\n<p>The young revolutionary's belief is honest.&nbsp; There will be no betraying catch in his throat, as he explains why the tribe is doomed at the hands of the old and corrupt, unless he is given power to set things right.&nbsp; <a href=\"/lw/l1/evolutionary_psychology/\">Not even subconsciously</a> does he think, \"And then, once I obtain power, I will strangely begin to resemble that old corrupt guard, abusing my power to increase my inclusive genetic fitness.\"</p>\n<p>People often <a href=\"/lw/te/three_fallacies_of_teleology/\">think as if \"purpose\" is an inherent property of things</a>; and so many interpret the message of ev-psych as saying, \"You have a subconscious, hidden goal to maximize your fitness.\"&nbsp; But individual organisms are <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">adaptation-executers, not fitness-maximizers</a>.&nbsp; The <em>purpose</em> that the revolutionary should obtain power and abuse it, is not a plan anywhere in his brain; it belongs to evolution, which can just barely be said to have purposes.&nbsp; It is a fact about many past revolutionaries having successfully taken power, having abused it, and having left many descendants.</p>\n<p>When the revolutionary obtains power, he will find that it is sweet, and he will try to hold on to it&mdash;perhaps still thinking that this is for the good of the tribe.&nbsp; He will find that it seems <em>right</em> to take many wives (surely he deserves some reward for his labor) and to help his children (who are more deserving of help than others).&nbsp; But the young revolutionary has no foreknowledge of this in the beginning, when he sets out to overthrow the awful people who currently rule the tribe&mdash;<a href=\"/lw/i0/are_your_enemies_innately_evil/\">evil mutants</a> whose intentions are obviously much less good than his own.</p>\n<p>The circuitry that will respond to power by finding it pleasurable, is already wired into our young revolutionary's brain; but he does not know this.&nbsp; (It would not help him evolutionarily if he did know it, because then he would not be able to honestly proclaim his good intentions&mdash;though it is scarcely necessary for evolution to prevent hunter-gatherers from knowing about evolution, which is one reason we are able to know about it now.)</p>\n<p>And so we have the awful cycle of \"meet the new boss, same as the old boss\".&nbsp; Youthful idealism rails against their elders' corruption, but oddly enough, the new generation&mdash;when it finally succeeds to power&mdash;doesn't seem to be all that morally purer.&nbsp; The original Communist Revolutionaries, I would guess probably a majority of them, really were in it to help the workers; but once they were a ruling Party in charge...</p>\n<p>All sorts of <a href=\"http://www.overcomingbias.com/2008/06/against-disclai.html\">random disclaimers</a> can be applied to this thesis:&nbsp; For example, you could suggest that maybe Stalin's intentions weren't all that good to begin with, and that some politicians do intend to abuse power and really are just lying.&nbsp; A much more important objection is the need to redescribe this scenario in terms of power structures that actually exist in hunter-gatherer bands, which, as I understand it, have egalitarian pressures (among adult males) to keep any one person from getting too far above others.</p>\n<p>But human beings do find power over others sweet, and it's not as if this emotion could have materialized from thin air, <em>without</em> an evolutionary explanation in terms of hunter-gatherer conditions.&nbsp; If you don't think this is why human beings are corrupted by power&mdash;then what's your evolutionary explanation?&nbsp; On the whole, to me at least, the evolutionary explanation for this phenomenon has the problem of <a href=\"/lw/qz/living_in_many_worlds/\">not even seeming profound</a>, because what it explains seems so normal.</p>\n<p>The moral of this story, and the reason for going into the evolutionary explanation, is that you shouldn't reason as if people who are corrupted by power are evil mutants, whose mutations you do not share.</p>\n<p>Evolution is not an infinitely powerful deceiving demon, and our ancestors evolved under conditions of not knowing about evolutionary psychology.&nbsp; The tendency to be corrupted by power can be beaten, I think.&nbsp; The \"warp\" doesn't seem on the same level of deeply woven insidiousness as, say, confirmation bias.</p>\n<p>There was once an occasion where a reporter wrote about me, and did a hatchet job.&nbsp; It was my first time being reported on, and I was completely blindsided by it.&nbsp; I'd known that reporters sometimes wrote hatchet jobs, but I'd thought that it would require <em>malice</em>&mdash;I hadn't begun to imagine that someone might write a hatchet job just because it was a cliche, an easy way to generate a few column inches.&nbsp; So I drew upon my own powers of narration, and wrote an autobiographical story on what it felt like to be reported on for the first time&mdash;that horrible feeling of violation.&nbsp; I've never sent that story off anywhere, though it's a fine and short piece of writing as I judge it.</p>\n<p>For it occurred to me, while I was writing, that journalism is an example of unchecked power&mdash;the reporter gets to present only one side of the story, any way they like, and there's nothing that the reported-on can do about it.&nbsp; (If you've never been reported on, then take it from me, that's how it is.)&nbsp; And here I was writing my own story, potentially for publication as traditional journalism, not in an academic forum.&nbsp; I remember realizing that the standards were <em>tremendously</em> lower than in science.&nbsp; That you could get away with damn near anything, so long as it made a good story&mdash;that this was the standard in journalism.&nbsp; (If you, having never been reported on yourself, don't believe me that this is the case, then you're as naive as I once was.)</p>\n<p>Just that thought&mdash;not even the intention, not even wondering whether to do it, but just the <em>thought</em>&mdash;that I could present only my side of the story and deliberately make the offending reporter look bad, and that no one would call me on it.&nbsp; Just that <em>thought</em> triggered this huge surge of positive reinforcement.&nbsp; This <em>tremendous</em> high, comparable to the high of discovery or the high of altruism.</p>\n<p>And I knew right away what I was dealing with.&nbsp; So I sat there, motionless, fighting down that surge of positive reinforcement.&nbsp; It didn't go away just because I wanted it to go away.&nbsp; But it went away after a few minutes.</p>\n<p>If I'd had no label to slap on that huge surge of positive reinforcement&mdash;if I'd been a less reflective fellow, flowing more with my passions&mdash;then that might have been that.&nbsp; People who are corrupted by power are not evil mutants.</p>\n<p>I wouldn't call it a close call.&nbsp; I did know immediately what was happening.&nbsp; I fought it down without much trouble, and could have fought much harder if necessary.&nbsp; So far as I can tell, the temptation of unchecked power is not anywhere near as insidious as the labyrinthine algorithms of self-deception.&nbsp; Evolution is not an infinitely powerful deceiving demon.&nbsp; George Washington refused the temptation of the crown, and he didn't even know about evolutionary psychology.&nbsp; Perhaps it was enough for him to know a little history, and think of the temptation as a sin.</p>\n<p>But it was still a scary thing to experience&mdash;this circuit that suddenly woke up and dumped a huge dose of unwanted positive reinforcement into my mental workspace, not when I <em>planned</em> to wield unchecked power, but just when my brain visualized the <em>possibility</em>.</p>\n<p>To the extent you manage to fight off this temptation, you do not say:&nbsp; \"Ah, now that I've beaten the temptation of power, I can safely make myself the wise tyrant who wields unchecked power benevolently, for the good of all.\"&nbsp; Having <em>successfully</em> fought off the temptation of power, you search for strategies that <em>avoid</em> seizing power.&nbsp; George Washington's triumph was not how well he ruled, but that he <em>refused the crown</em>&mdash;despite all temptation to be horrified at who else might then obtain power.</p>\n<p>I am willing to admit of the theoretical possibility that someone could beat the temptation of power and then end up with <em>no ethical choice left</em>, except to grab the crown.&nbsp; But there would be a large burden of skepticism to overcome.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Ethical_injunction#Sequence\"><em>Ethical Injunctions</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/uv/ends_dont_justify_means_among_humans/\">Ends Don't Justify Means (Among Humans)</a>\"</p>\n<p style=\"text-align:right\">(start of sequence)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"iP2X4jQNHMWHRNPne": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v8rghtzWCziYuMdJ5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 51, "extendedScore": null, "score": 7.4e-05, "legacy": true, "legacyId": "1110", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["epZLSoNvjW53tqNj9", "pL3To6G42AeihNtaN", "pLRogvJLPPg6Mrvg4", "2HxAkCG7NWTrrn5R3", "XPErvb8m9FapXCjhA", "28bAMAxhoX3bwbAKC", "qcYCAxYZT4Xp9iMZY", "K9ZaZXDnL3SEmYZqB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-14T21:00:00.000Z", "modifiedAt": null, "url": null, "title": "Ends Don't Justify Means (Among Humans)", "slug": "ends-don-t-justify-means-among-humans", "viewCount": null, "lastCommentedAt": "2020-07-02T13:50:09.564Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K9ZaZXDnL3SEmYZqB/ends-don-t-justify-means-among-humans", "pageUrlRelative": "/posts/K9ZaZXDnL3SEmYZqB/ends-don-t-justify-means-among-humans", "linkUrl": "https://www.lesswrong.com/posts/K9ZaZXDnL3SEmYZqB/ends-don-t-justify-means-among-humans", "postedAtFormatted": "Tuesday, October 14th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ends%20Don't%20Justify%20Means%20(Among%20Humans)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEnds%20Don't%20Justify%20Means%20(Among%20Humans)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK9ZaZXDnL3SEmYZqB%2Fends-don-t-justify-means-among-humans%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ends%20Don't%20Justify%20Means%20(Among%20Humans)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK9ZaZXDnL3SEmYZqB%2Fends-don-t-justify-means-among-humans", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK9ZaZXDnL3SEmYZqB%2Fends-don-t-justify-means-among-humans", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1309, "htmlBody": "<blockquote>\n<p>\"If the ends don't justify the means, what does?\"<br />&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &mdash;variously attributed</p>\n<p>\"I think of myself as running on hostile hardware.\"<br />&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &mdash;Justin Corwin</p>\n</blockquote>\n<p>Yesterday I talked about how humans may have evolved a structure of political revolution, beginning by believing themselves morally superior to the corrupt current power structure, but ending by being corrupted by power themselves&mdash;not by any plan in their own minds, but by the echo of ancestors who did the same and thereby reproduced.</p>\n<p>This fits the template:</p>\n<blockquote>\n<p>In some cases, human beings have evolved in such fashion as to think that they are doing X for prosocial reason Y, but when human beings actually do X, other adaptations execute to promote self-benefiting consequence Z.</p>\n</blockquote>\n<p>From this proposition, I now move on to my main point, a question <em>considerably</em> outside the realm of classical Bayesian decision theory:</p>\n<blockquote>\n<p>\"What if I'm running on corrupted hardware?\"</p>\n</blockquote>\n<p><a id=\"more\"></a></p>\n<p>In such a case as this, you might even find yourself uttering such seemingly paradoxical statements&mdash;sheer nonsense from the perspective of classical decision theory&mdash;as:</p>\n<blockquote>\n<p>\"The ends don't justify the means.\"</p>\n</blockquote>\n<p>But if you are running on corrupted hardware, then the reflective observation that it <em>seems</em> like a righteous and altruistic act to seize power for yourself&mdash;this <em>seeming</em> may not be be much evidence for the proposition that seizing power is in fact the action that will most benefit the tribe.</p>\n<p>By the power of naive realism, the corrupted hardware that you run on, and the corrupted seemings that it computes, will seem like the fabric of the very world itself&mdash;simply the way-things-are.</p>\n<p>And so we have the bizarre-seeming rule:&nbsp; \"For the good of the tribe, do not cheat to seize power <em>even when it would provide a net benefit to the tribe.</em>\"</p>\n<p>Indeed it may be wiser to phrase it this way:&nbsp; If you just say, \"when it <em>seems</em> like it would provide a net benefit to the tribe\", then you get people who say, \"But it doesn't just <em>seem</em> that way&mdash;it <em>would</em> provide a net benefit to the tribe if I were in charge.\"</p>\n<p>The notion of untrusted hardware seems like something wholly outside the realm of classical decision theory.&nbsp; (What it does to reflective decision theory I can't yet say, but that would seem to be the appropriate level to handle it.)</p>\n<p>But on a human level, the patch seems straightforward.&nbsp; Once you know about the warp, you create rules that describe the warped behavior and outlaw it.&nbsp; A rule that says, \"For the good of the tribe, do not cheat to seize power even for the good of the tribe.\"&nbsp; Or \"For the good of the tribe, do not murder even for the good of the tribe.\"</p>\n<p>And now the philosopher comes and presents their \"thought experiment\"&mdash;setting up a scenario in which, <em>by stipulation,</em> the <em>only</em> possible way to save five innocent lives is to murder one innocent person, and this murder is <em>certain</em> to save the five lives.&nbsp; \"There's a train heading to run over five innocent people, who you can't possibly warn to jump out of the way, but you can push one innocent person into the path of the train, which will stop the train.&nbsp; These are your only options; what do you do?\"</p>\n<p>An altruistic human, who has accepted certain deontological prohibits&mdash;which seem well justified by some historical statistics on the results of reasoning in certain ways on untrustworthy hardware&mdash;may experience some mental distress, on encountering this thought experiment.</p>\n<p>So here's a reply to that philosopher's scenario, which I have yet to hear any philosopher's victim give:</p>\n<p>\"You stipulate that the <em>only possible</em> way to save five innocent lives is to murder one innocent person, and this murder will <em>definitely</em> save the five lives, and that these facts are <em>known</em> to me with effective certainty.&nbsp; But since I am running on corrupted hardware, I can't occupy the <em>epistemic state </em>you want me to imagine.&nbsp; Therefore I reply that, in a society of Artificial Intelligences worthy of personhood and lacking any inbuilt tendency to be corrupted by power, it would be right for the AI to murder the one innocent person to save five, and moreover all its peers would agree.&nbsp; However, I refuse to extend this reply to myself, because the epistemic state you ask me to imagine, can only exist among other kinds of people than human beings.\"</p>\n<p>Now, to me this seems like a dodge.&nbsp; I think the universe is <a href=\"/lw/uk/beyond_the_reach_of_god/\">sufficiently unkind</a> that we can justly be forced to consider situations of this sort.&nbsp; The sort of person who goes around proposing that sort of thought experiment, might well deserve that sort of answer.&nbsp; But any human legal system does embody some answer to the question \"How many innocent people can we put in jail to get the guilty ones?\", even if the number isn't written down.</p>\n<p>As a human, I try to abide by the deontological prohibitions that humans have made to live in peace with one another.&nbsp; But I don't think that our deontological prohibitions are <em>literally inherently nonconsequentially terminally right</em>.&nbsp; I endorse \"the end doesn't justify the means\" as a principle to guide humans running on corrupted hardware, but I wouldn't endorse it as a principle for a society of AIs that make well-calibrated estimates.&nbsp; (If you have one AI in a society of humans, that does bring in other considerations, like whether the humans learn from your example.)</p>\n<p>And so I wouldn't say that a well-designed Friendly AI must necessarily refuse to push that one person off the ledge to stop the train.&nbsp; Obviously, I would expect any decent superintelligence to come up with a superior third alternative.&nbsp; But if those are the only two alternatives, and the FAI judges that it is wiser to push the one person off the ledge&mdash;even after taking into account knock-on effects on any humans who see it happen and spread the story, etc.&mdash;then I don't call it an alarm light, if an <em>AI</em> says that the right thing to do is sacrifice one to save five.&nbsp; Again, <em>I</em> don't go around pushing people into the paths of trains myself, nor stealing from banks to fund my altruistic projects.&nbsp; <em>I</em> happen to be a human.&nbsp; But for a Friendly AI to be corrupted by power would be like it <a href=\"/lw/so/humans_in_funny_suits/\">starting to bleed red blood</a>.&nbsp; The tendency to be corrupted by power is a specific biological adaptation, supported by specific cognitive circuits, built into us by our genes for a clear evolutionary reason.&nbsp; It wouldn't spontaneously appear in the code of a Friendly AI any more than its transistors would start to bleed.</p>\n<p>I would even go further, and say that if you had minds with an inbuilt warp that made them <em>overestimate</em> the external harm of self-benefiting actions, then they would need a rule \"the ends do not prohibit the means\"&mdash;that you should do what benefits yourself even when it (seems to) harm the tribe.&nbsp; By hypothesis, if their society did not have this rule, the minds in it would refuse to breathe for fear of using someone else's oxygen, and they'd all die.&nbsp; For them, an occasional overshoot in which one person seizes a personal benefit at the net expense of society, would seem just as cautiously virtuous&mdash;and indeed <em>be</em> just as cautiously virtuous&mdash;as when one of us humans, being cautious, passes up an opportunity to steal a loaf of bread that really would have been more of a benefit to them than a loss to the merchant (including knock-on effects).</p>\n<p>\"The end does not justify the means\" is just consequentialist reasoning at one meta-level up.&nbsp; If a human starts thinking on the <em>object</em> level that the end justifies the means, this has awful consequences given our untrustworthy brains; therefore a human shouldn't think this way.&nbsp; But it is all still ultimately consequentialism.&nbsp; It's just <em>reflective</em> consequentialism, for beings who know that their moment-by-moment decisions are made by untrusted hardware.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YTCrHWYHAsAD74EHo": 3, "yeJFqsWrP2pjYfNEr": 1, "ZTRNmvQGgoYiymYnq": 7, "nSHiKwWyMZFdZg5qt": 3, "exZi6Bing5AiM4ZQB": 1, "dPPATLhRmhdJtJM2t": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K9ZaZXDnL3SEmYZqB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 83, "baseScore": 120, "extendedScore": null, "score": 0.000174, "legacy": true, "legacyId": "1111", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "waF2Pomid7YHjfEDt", "canonicalCollectionSlug": "rationality", "canonicalBookId": "T3CjiQq6bBgFuzvp6", "canonicalNextPostSlug": "ethical-injunctions", "canonicalPrevPostSlug": "the-intuitions-behind-utilitarianism", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 120, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 94, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sYgv4eYH82JEsTD34", "Zkzzjg3h7hW5Z36hK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-15T23:39:36.000Z", "modifiedAt": "2021-04-07T04:59:21.283Z", "url": null, "title": "Entangled Truths, Contagious Lies", "slug": "entangled-truths-contagious-lies", "viewCount": null, "lastCommentedAt": "2020-06-25T06:55:01.486Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wyyfFfaRar2jEdeQK/entangled-truths-contagious-lies", "pageUrlRelative": "/posts/wyyfFfaRar2jEdeQK/entangled-truths-contagious-lies", "linkUrl": "https://www.lesswrong.com/posts/wyyfFfaRar2jEdeQK/entangled-truths-contagious-lies", "postedAtFormatted": "Wednesday, October 15th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Entangled%20Truths%2C%20Contagious%20Lies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEntangled%20Truths%2C%20Contagious%20Lies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwyyfFfaRar2jEdeQK%2Fentangled-truths-contagious-lies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Entangled%20Truths%2C%20Contagious%20Lies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwyyfFfaRar2jEdeQK%2Fentangled-truths-contagious-lies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwyyfFfaRar2jEdeQK%2Fentangled-truths-contagious-lies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1089, "htmlBody": "\n\n\n\n  \n\n  \n\n  <blockquote>\n    \n\n    <p>One of your very early philosophers came to the conclusion that a fully competent mind, from a study of one fact or artifact belonging to any given universe, could construct or visualize that universe, from the instant of its creation to its ultimate end . . .</p>\n\n    <p>&#x2014;<em>First Lensman</em></p>\n  </blockquote>\n\n  <blockquote>\n    \n\n    <p>If any one of you will concentrate upon one single fact, or small object, such as a pebble or the seed of a plant or other creature, for as short a period of time as one hundred of your years, you will begin to perceive its truth.</p>\n\n    <p>&#x2014;<em>Gray Lensman</em></p>\n  </blockquote>\n\n  <p>I am reasonably sure that a single pebble, taken from a beach of our own Earth, does not specify the continents and countries, politics and people of this Earth. Other planets in space and time, other Everett branches, would generate the same pebble.</p>\n\n  <p>On the other hand, the identity of a single pebble would seem to include our laws of physics. In that sense the entirety of our Universe&#x2014;<em>all</em> the Everett branches&#x2014;would be implied by the pebble.<span><sup><a href=\"#fn1x35\" id=\"fn1x35-bk\">1</a></sup></span><span id=\"x41-42001f1\"></span></p>\n\n  <p>From the study of that single pebble you could see the laws of physics and all they imply. Thinking about those laws of physics, you can see that planets will form, and you can guess that the pebble came from such a planet. The internal crystals and molecular formations of the pebble developed under gravity, which tells you something about the planet&#x2019;s mass; the mix of elements in the pebble tells you something about the planet&#x2019;s formation.</p>\n\n  <p>I am not a geologist, so I don&#x2019;t know to which mysteries geologists are privy. But I find it very easy to imagine showing a geologist a pebble, and saying, &#x201C;This pebble came from a beach at Half Moon Bay,&#x201D; and the geologist immediately says, &#x201C;I&#x2019;m confused,&#x201D; or even, &#x201C;You liar.&#x201D; Maybe it&#x2019;s the wrong kind of rock, or the pebble isn&#x2019;t worn enough to be from a beach&#x2014;I don&#x2019;t know pebbles well enough to guess the linkages and signatures by which I might be caught, which is the point.</p>\n\n  <p>&#x201C;Only God can tell a truly plausible lie.&#x201D; I wonder if there was ever a religion that developed this as a proverb? I would (falsifiably) guess not: it&#x2019;s a rationalist sentiment, even if you cast it in theological metaphor. Saying &#x201C;everything is interconnected to everything else, because God made the whole world and sustains it&#x201D; may generate some nice warm &#x2019;n&#x2019; fuzzy feelings during the sermon, but it doesn&#x2019;t get you very far when it comes to assigning pebbles to beaches.</p>\n\n  <p>A penny on Earth exerts a gravitational acceleration on the Moon of around 4.5 &#xD7; 10<sup>-31</sup> m/s<sup>2</sup>, so in one sense it&#x2019;s not too far wrong to say that every event is entangled with its whole past light cone. And since inferences can propagate backward and forward through causal networks, <em>epistemic</em> entanglements can easily cross the borders of light cones. But I wouldn&#x2019;t want to be the forensic astronomer who had to look at the Moon and figure out whether the penny landed heads or tails&#x2014;the influence is far less than quantum uncertainty and thermal noise.</p>\n\n  <p>If you said, &#x201C;Everything is entangled with something else,&#x201D; or, &#x201C;Everything is inferentially entangled and some entanglements are much stronger than others,&#x201D; you might be really wise instead of just Deeply Wise.</p>\n\n  <p>Physically, each event is in some sense the sum of its whole past light cone, without borders or boundaries. But the list of <em>noticeable</em> entanglements is much shorter, and it gives you something like a network. This high-level regularity is what I refer to when I talk about the Great Web of Causality.</p>\n\n  <p>I use these Capitalized Letters somewhat tongue-in-cheek, perhaps; but if anything at all is worth Capitalized Letters, surely the Great Web of Causality makes the list.</p>\n\n  <p>&#x201C;Oh what a tangled web we weave, when first we practise to deceive,&#x201D; said Sir Walter Scott. Not <em>all</em> lies spin out of control&#x2014;we don&#x2019;t live in so righteous a universe. But it does occasionally happen that someone lies about a fact, and then has to lie about an entangled fact, and then another fact entangled with that one:</p>\n\n  <blockquote>\n    \n\n    <p>&#x201C;Where were you?&#x201D;</p>\n\n    <p>&#x201C;Oh, I was on a business trip.&#x201D;</p>\n\n    <p>&#x201C;What was the business trip about?&#x201D;</p>\n\n    <p>&#x201C;I can&#x2019;t tell you that; it&#x2019;s proprietary negotiations with a major client.&#x201D;</p>\n\n    <p>&#x201C;Oh&#x2014;they&#x2019;re letting you in on those? Good news! I should call your boss to thank him for adding you.&#x201D;</p>\n\n    <p>&#x201C;Sorry&#x2014;he&#x2019;s not in the office right now . . .&#x201D;</p>\n  </blockquote>\n\n  <p>Human beings, who are not gods, often fail to <em>imagine</em> all the facts they would need to distort to tell a truly plausible lie. &#x201C;God made me pregnant&#x201D; sounded a tad more likely in the old days before our models of the world contained (quotations of) Y chromosomes. Many similar lies, today, may blow up when genetic testing becomes more common. Rapists have been convicted, and false accusers exposed, years later, based on evidence they didn&#x2019;t realize they could leave. A student of evolutionary biology can see the design signature of natural selection on every wolf that chases a rabbit; and every rabbit that runs away; and every bee that stings instead of broadcasting a polite warning&#x2014;but the deceptions of creationists sound plausible to <em>them</em>, I&#x2019;m sure.</p>\n\n  <p>Not all lies are uncovered, not all liars are punished; we don&#x2019;t live in that righteous a universe. But not all lies are as safe as their liars believe. How many sins would become known to a Bayesian superintelligence, I wonder, if it did a (non-destructive?) nanotechnological scan of the Earth? At minimum, all the lies of which any evidence still exists in any brain. Some such lies may become known sooner than that, if the neuroscientists ever succeed in building a really good lie detector via neuroimaging. Paul Ekman (a pioneer in the study of tiny facial muscle movements) could probably read off a sizeable fraction of the world&#x2019;s lies right now, given a chance.</p>\n\n  <p>Not all lies are uncovered, not all liars are punished. But the Great Web is very commonly underestimated. Just the knowledge that humans have <em>already accumulated</em> would take many human lifetimes to learn. Anyone who thinks that a non-God can tell a <em>perfect</em> lie, risk-free, is underestimating the tangledness of the Great Web.</p>\n\n  <p>Is honesty the best policy? I don&#x2019;t know if I&#x2019;d go that far: Even on my ethics, it&#x2019;s sometimes okay to shut up. But compared to outright lies, either honesty or silence involves less exposure to recursively propagating risks you don&#x2019;t know you&#x2019;re taking.</p>\n\n  <div class=\"footnotes\">\n    \n\n    <p><span><sup><a href=\"#fn1x35-bk\" id=\"fn1x35\">1</a></sup></span>Assuming, as seems likely, there are no truly free variables.</p>\n  </div>\n\n", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"cHoCqtfE9cF7aSs9d": 1, "3uE2pXvbcnS9nnZRE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wyyfFfaRar2jEdeQK", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 72, "extendedScore": null, "score": 0.000105, "legacy": true, "legacyId": "1112", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2021-04-07T04:59:21.176Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": "GSqFqc646rsRd2oyz", "canonicalCollectionSlug": "rationality", "canonicalBookId": "coGq3LC5Yn4vZiu6k", "canonicalNextPostSlug": "of-lies-and-black-swan-blowups", "canonicalPrevPostSlug": "is-that-your-true-rejection", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 72, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "rPsASnzcvaBdBEaSJ", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "2.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-17T01:07:16.000Z", "modifiedAt": null, "url": null, "title": "Traditional Capitalist Values", "slug": "traditional-capitalist-values", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:34.053Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3bfWCPfu9AFspnhvf/traditional-capitalist-values", "pageUrlRelative": "/posts/3bfWCPfu9AFspnhvf/traditional-capitalist-values", "linkUrl": "https://www.lesswrong.com/posts/3bfWCPfu9AFspnhvf/traditional-capitalist-values", "postedAtFormatted": "Friday, October 17th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Traditional%20Capitalist%20Values&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATraditional%20Capitalist%20Values%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3bfWCPfu9AFspnhvf%2Ftraditional-capitalist-values%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Traditional%20Capitalist%20Values%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3bfWCPfu9AFspnhvf%2Ftraditional-capitalist-values", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3bfWCPfu9AFspnhvf%2Ftraditional-capitalist-values", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1610, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/i0/are_your_enemies_innately_evil/\">Are Your Enemies Innately Evil?</a>, <a href=\"/lw/gz/policy_debates_should_not_appear_onesided/\">Policy Debates Should Not Appear One-Sided</a></p>\n<blockquote>\n<p>\"The financial crisis is not the crisis of capitalism.&nbsp; It is the crisis of a system that has distanced itself from the most fundamental values of capitalism, which betrayed the spirit of capitalism.\"<br />&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- Nicolas Sarkozy</p>\n</blockquote>\n<p>During the current crisis, I've more than once heard someone remarking that financial-firm CEOs who take huge bonuses during the good years and then run away when their black-swan bets blow up, are only exercising the usual capitalist values of \"grab all the money you can get\".</p>\n<p>I think that a fair amount of the enmity in the world, to say nothing of confusion on the Internet, stems from people refusing to contemplate the real values of the opposition as the opposition sees it.&nbsp; This is something I've remarked upon before, with respect to <a href=\"/lw/i0/are_your_enemies_innately_evil/\">\"the terrorists hate our freedom\" or \"the suicide hijackers were cowards\"</a> (statements that are sheerly silly).</p>\n<p><em>Real</em> value systems - as opposed to pretend demoniacal value systems - are phrased to generate warm fuzzies in their users, <em>not</em> to be easily mocked.&nbsp; They will sound noble at least to the people who believe them.</p>\n<p>Whether anyone <em>actually </em>lives up to that value system, or <em>should,</em> and whether the results are what they are claimed to be; if there are hidden gotchas in the warm fuzzy parts - sure, you can have that debate.&nbsp; But first you should be clear about how your opposition sees <em>itself</em> - a view which has <em>not</em> been carefully optimized to make <em>your</em> side feel good about its opposition.&nbsp; Otherwise you're not engaging the real issues.</p>\n<p>So here are the traditional values of capitalism <em>as seen by those who regard it as noble</em> - the sort of Way spoken of by Paul Graham, or <a href=\"http://www.fourmilab.ch/etexts/www/barnum/moneygetting/moneygetting.html\">P. T. Barnum </a>(who did <em>not</em> say \"There's a sucker born every minute\"), or Warren Buffett:</p>\n<p><a id=\"more\"></a></p>\n<ul>\n<li>Make things that people want, or do things that people want done, in exchange for money or other valuta.&nbsp; This is a great and noble and worthwhile endeavor, and anyone who looks down on it reveals their own shallowness.</li>\n<li>Your competitors are your loyal opposition.&nbsp; Usher them toward oblivion by offering a better product at a lower price, then dance on their graves (or buy them out, if they're worthy).&nbsp; An act of violence, like punching them in the nose, is as absolutely forbidden as it would be to a scientist - violence is reserved for the thuggish lower classes, like politicians.</li>\n<li>Plan for the long term, in your own life and in your company.&nbsp; Short-term thinkers lose money; long-term thinkers get to pick up the remains for a song.</li>\n<li>Acquire a deserved reputation for honesty and reliability - with your customers, your suppliers, your subordinates, your supervisors, and the general community.&nbsp; Read the contract before you sign it, then do what you said you would.&nbsp; Play by the rules, and play to win.</li>\n<li>Spend less than you earn.&nbsp; Keep a cushion for rainy days.&nbsp; Fear debt.</li>\n<li>Pay your shareholders a good dividend - that's why they originally gave you their money to invest, to make more money.</li>\n<li>Don't fiddle the numbers.&nbsp; The numbers are <em>very important</em>, so fiddling with them is <em>very bad</em>.</li>\n<li>Promote based on merit.&nbsp; Being nice to your nephew-in-law will make less money.</li>\n<li>Give someone a second chance, but not a third chance. </li>\n<li>Science and technology are responsible for there being something to trade other than stone knives and fruit.&nbsp; Adopt innovations to increase productivity.&nbsp; Respect expertise and use it to make money.</li>\n<li>Vigorous work is praiseworthy but should be accompanied by equally vigorous results.</li>\n<li>No one has a right to their job.&nbsp; Not the janitor, not the CEO, no one.&nbsp; It would be like a rationalist having a right to their own opinion.&nbsp; At some point you've got to fire the saddle-makers and close down the industry.&nbsp; If you want to reward loyalty, give them money.</li>\n<li>No company has a right to its continued existence.&nbsp; Change happens.</li>\n<li>Investing is risky.&nbsp; If you don't like it, don't invest.&nbsp; Diversification is one thing, but stay far away from get-rich-quick schemes that offer easy rewards with no risk or hard thinking.&nbsp; Trying to vote yourself rich falls in this category.</li>\n<li>A high standard of living is the just reward of hard work and intelligence.&nbsp; If other people or other places have lower standards of living, then the <em>problem</em> is the lower standard, not the higher one.&nbsp; Raise others up, don't lower yourself.&nbsp; A high standard of living is a good thing, not a bad one - a universal moral generalization that includes you in particular.&nbsp; If you've earned your wealth honestly, enjoy it without regrets.</li>\n<li>In all ways, at all times, and with every deed, make the pie larger rather than smaller.</li>\n<li>The phrase \"making money\" is a triumph in itself over previous worldviews that saw wealth as something to steal.</li>\n<li>Create value so that you can capture it, but don't feel obligated to capture <em>all</em> the value you create.&nbsp; If you capture <em>all</em> your value, your transactions benefit only yourself, and others have no motive to participate.&nbsp; If you have negotiating leverage, use it to drive a good bargain for yourself, but not a hateful one - someday you'll be on the other side of the table.&nbsp; Still, the invisible hand does require that price be sensitive to supply and demand.</li>\n<li>Everyone in a company should be pulling together to create value and getting a decent share of the value they create.&nbsp; The concept of 'class war' is a lie sold by politicians who think in terms of a fixed pie.&nbsp; Any person of wealth who <em>actually, seriously</em> tries to exploit those less fortunate is a bully and a heretic; but offering someone a job is not exploiting them.&nbsp; (Though bribing politicians to pass laws that transfer wealth, definitely could qualify as exploitation.)</li>\n<li>In countries that are lawful and just, it is the privilege and responsibility of a citizen to pay their <em>low</em> taxes.&nbsp; That said, a good billionaire wouldn't ask to pay a lower tax rate than his secretary.</li>\n<li>People safeguard, nourish, and improve that which they know will not be taken from them.&nbsp; Tax a little if you must, but at some point you must let people own what they buy.</li>\n<li>The fundamental morality of capitalism lies in the voluntary nature of its trades, consented to by all parties, and therefore providing a gain to all.</li>\n<li>In countries that are lawful and just, the government is the referee, not a player.&nbsp; If the referee runs onto the field and kicks the football, things start to get scary.</li>\n<li>Unearned gains destroy people, nations, and teenagers.&nbsp; If you want to help, go into the dark places of the world and open stores that offer lower prices; or offer employment to people who wouldn't have gotten a chance otherwise; or invest capital in worthy recipients who others shun.&nbsp; Plow your gains back into the operation, if you like; but if you don't make at least a little money in the process, how can you be sure that you're creating value?</li>\n<li>Wise philanthropy is a privilege and responsibility of achieved wealth.</li>\n<li>Making money is a virtuous endeavor, despite all the lies that have been told about it, and should properly be found in the company of other virtues.&nbsp; Those who set out to make money should not think of themselves as fallen, but should rather conduct themselves with honor, pride, and self-respect, as part of the grand pageantry of human civilization rising up from the dirt, and continuing forward into the future.</li>\n</ul>\n<p>There was, once upon a time, an editorial in the Wall Street Journal calling Ford a \"traitor to his class\" because he offered more than the prevailing wages of the time.&nbsp; Coal miners trying to form a union, once upon a time, were fired upon by rifles.&nbsp; But I also think that Graham or Barnum or Buffett would regard those folk as the inheritors of mere kings.</p>\n<p>\"No true Scotsman\" fallacy?&nbsp; Maybe, but let's at least be clear what the Scots say about it.</p>\n<p>For myself, I would have to say that I'm an apostate from this moral synthesis - I grew up in this city and remember it fondly, but I no longer live there.&nbsp; I regard finance as more of a useful tool than an ultimate end of intelligence - I'm not sure it's the maximum possible fun we could all be having under optimal conditions.&nbsp; I'm more sympathetic than this to people who lose their jobs, because I know that retraining, or changing careers, isn't always easy and fun.&nbsp; I don't think the universe is set up to reward hard work; and I think that it is entirely possible for money to corrupt a person.</p>\n<p>But I also admire any virtue clearly stated and synthesized into a moral system.&nbsp; We need more of those.&nbsp; Anyone who thinks that capitalism is just about grabbing the banana, is underestimating the number of decent and intelligent people who have put serious thought into the subject.</p>\n<p>Those of other Ways may not agree with all these statements - but if you aspire to sanity in debate, you should at least be able to <em>read them</em> without your brain shutting down.</p>\n<p><strong>PS</strong>: &nbsp;Julian Morrison adds:&nbsp; <em>Trade can act as a connective between people with diverging values.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3bfWCPfu9AFspnhvf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 55, "extendedScore": null, "score": 8.1e-05, "legacy": true, "legacyId": "1113", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 102, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["28bAMAxhoX3bwbAKC", "PeSzc9JTBxhaYRp9b"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-17T23:55:22.000Z", "modifiedAt": null, "url": null, "title": "Dark Side Epistemology", "slug": "dark-side-epistemology", "viewCount": null, "lastCommentedAt": "2021-09-28T07:51:43.318Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XTWkjCJScy2GFAgDt/dark-side-epistemology", "pageUrlRelative": "/posts/XTWkjCJScy2GFAgDt/dark-side-epistemology", "linkUrl": "https://www.lesswrong.com/posts/XTWkjCJScy2GFAgDt/dark-side-epistemology", "postedAtFormatted": "Friday, October 17th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dark%20Side%20Epistemology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADark%20Side%20Epistemology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXTWkjCJScy2GFAgDt%2Fdark-side-epistemology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dark%20Side%20Epistemology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXTWkjCJScy2GFAgDt%2Fdark-side-epistemology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXTWkjCJScy2GFAgDt%2Fdark-side-epistemology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1583, "htmlBody": "\n\n\n\n  \n\n  \n\n  <p>If you once tell a lie, the truth is ever after your enemy. </p>\n\n  <p>I have discussed the notion that lies are contagious. If you pick up a pebble from the driveway, and tell a geologist that you found it on a beach&#x2014;well, do <em>you</em> know what a geologist knows about rocks? I don&#x2019;t. But I can suspect that a water-worn pebble wouldn&#x2019;t look like a droplet of frozen lava from a volcanic eruption. Do you know where the pebble in your driveway really came from? Things bear the marks of their places in a lawful universe; in that web, a lie is out of place.<span><sup><a href=\"#fn1x37\" id=\"fn1x37-bk\">1</a></sup></span><span id=\"x43-44001f1\"></span></p>\n\n  <p>What sounds like an arbitrary truth to one mind&#x2014;one that could easily be replaced by a plausible lie&#x2014;might be nailed down by a dozen linkages to the eyes of greater knowledge. To a creationist, the idea that life was shaped by &#x201C;intelligent design&#x201D; instead of &#x201C;natural selection&#x201D; might sound like a sports team to cheer for. To a biologist, plausibly arguing that an organism was intelligently designed would require lying about almost every facet of the organism. To plausibly argue that &#x201C;humans&#x201D; were intelligently designed, you&#x2019;d have to lie about the design of the human retina, the architecture of the human brain, the proteins bound together by weak van der Waals forces instead of strong covalent bonds . . .</p>\n\n  <p>Or you could just lie about evolutionary theory, which is the path taken by most creationists. Instead of lying about the connected nodes in the network, they lie about the <em>general</em> laws governing the links.</p>\n\n  <p>And then to cover <em>that</em> up, they lie about the rules of science&#x2014;like what it means to call something a &#x201C;theory,&#x201D; or what it means for a scientist to say that they are not absolutely certain.</p>\n\n  <p>So they pass from lying about specific facts, to lying about general laws, to lying about the rules of reasoning. To lie about whether humans evolved, you must lie about evolution; and then you have to lie about the rules of science that constrain our understanding of evolution.</p>\n\n  <p>But how else? Just as a human would be out of place in a community of <em>actually</em> intelligently designed life forms, and you have to lie about the rules of evolution to make it appear otherwise, so too beliefs about creationism are themselves out of place in science&#x2014;you wouldn&#x2019;t find them in a well-ordered mind any more than you&#x2019;d find palm trees growing on a glacier. And so you have to disrupt the barriers that would forbid them.</p>\n\n  <p>Which brings us to the case of self-deception.</p>\n\n  <p>A single lie you tell <em>yourself</em> may seem plausible enough, when you don&#x2019;t know any of the rules governing thoughts, or even that there <em>are</em> rules; and the choice seems as arbitrary as choosing a flavor of ice cream, as isolated as a pebble on the shore . . .</p>\n\n  <p>. . . but then someone calls you on your belief, using the rules of reasoning that <em>they&#x2019;ve</em> learned. They say, &#x201C;Where&#x2019;s your evidence?&#x201D;</p>\n\n  <p>And you say, &#x201C;What? Why do I need evidence?&#x201D;</p>\n\n  <p>So they say, &#x201C;In general, beliefs require evidence.&#x201D;</p>\n\n  <p>This argument, clearly, is a soldier fighting on the other side, which you must defeat. So you say: &#x201C;I disagree! Not all beliefs require evidence. In particular, beliefs about dragons don&#x2019;t require evidence. When it comes to dragons, you&#x2019;re allowed to believe anything you like. So I don&#x2019;t need evidence to believe there&#x2019;s a dragon in my garage.&#x201D;</p>\n\n  <p>And the one says, &#x201C;Eh? You can&#x2019;t just exclude dragons like that. There&#x2019;s a reason for the rule that beliefs require evidence. To draw a correct map of the city, you have to walk through the streets and make lines on paper that correspond to what you see. That&#x2019;s not an arbitrary legal requirement&#x2014;if you sit in your living room and draw lines on the paper at random, the map&#x2019;s going to be wrong. With extremely high probability. That&#x2019;s as true of a map of a dragon as it is of anything.&#x201D;</p>\n\n  <p>So now <em>this</em>, the explanation of <em>why</em> beliefs require evidence, is <em>also</em> an opposing soldier. So you say: &#x201C;Wrong with extremely high probability? Then there&#x2019;s still a chance, right? I don&#x2019;t have to believe if it&#x2019;s not absolutely certain.&#x201D;</p>\n\n  <p>Or maybe you even begin to suspect, yourself, that &#x201C;beliefs require evidence.&#x201D; But this threatens a lie you hold precious; so you reject the dawn inside you, push the Sun back under the horizon.</p>\n\n  <p>Or you&#x2019;ve previously heard the proverb &#x201C;beliefs require evidence,&#x201D; and it sounded wise enough, and you endorsed it in public. But it never quite occurred to you, until someone else brought it to your attention, that this proverb could <em>apply to</em> your belief that there&#x2019;s a dragon in your garage. So you think fast and say, &#x201C;The dragon is in a separate magisterium.&#x201D;</p>\n\n  <p>Having false beliefs isn&#x2019;t a good thing, but it doesn&#x2019;t have to be permanently crippling&#x2014;if, when you discover your mistake, you get over it. The dangerous thing is to have a false belief that you <em>believe should be protected as a belief&#x2014;</em>a belief-in-belief, whether or not accompanied by actual belief.</p>\n\n  <p>A single Lie That Must Be Protected can block someone&#x2019;s progress into advanced rationality. No, it&#x2019;s not harmless fun.</p>\n\n  <p>Just as the world itself is more tangled by far than it appears on the surface, so too there are stricter rules of reasoning, constraining belief more strongly, than the untrained would suspect. The world is woven tightly, governed by general laws, and so are <em>rational</em> beliefs.</p>\n\n  <p>Think of what it would take to deny evolution or heliocentrism&#x2014;all the connected truths and governing laws you wouldn&#x2019;t be allowed to know. Then you can imagine how a single act of self-deception can block off the whole meta level of truth-seeking, once your mind begins to be threatened by seeing the connections. Forbidding all the intermediate and higher levels of the rationalist&#x2019;s Art. Creating, in its stead, a vast complex of anti-law, rules of anti-thought, general justifications for believing the untrue.</p>\n\n  <p>Steven Kaas said, &#x201C;Promoting less than maximally accurate beliefs is an act of sabotage. Don&#x2019;t do it to anyone unless you&#x2019;d also slash their tires.&#x201D; Giving someone a false belief <em>to protect&#x2014;</em>convincing them that the <em>belief itself</em> must be defended from any thought that seems to threaten it&#x2014;well, you shouldn&#x2019;t do that to someone unless you&#x2019;d also give them a frontal lobotomy.</p>\n\n  <p>Once you tell a lie, the truth is your enemy; and every truth connected to that truth, and every ally of truth in general; all of these you must oppose, to protect the lie. Whether you&#x2019;re lying to others, or to yourself.</p>\n\n  <p>You have to deny that beliefs require evidence, and then you have to deny that maps should reflect territories, and then you have to deny that truth is a good thing . . .</p>\n\n  <p>Thus comes into being the Dark Side.</p>\n\n  <p>I worry that people aren&#x2019;t aware of it, or aren&#x2019;t sufficiently wary&#x2014;that as we wander through our human world, we can expect to encounter <em>systematically</em> bad epistemology.</p>\n\n  <p>The &#x201C;how to think&#x201D; memes floating around, the cached thoughts of Deep Wisdom&#x2014;some of it will be good advice devised by rationalists. But other notions were invented to protect a lie or self-deception: spawned from the Dark Side.</p>\n\n  <p>&#x201C;Everyone has a right to their own opinion.&#x201D; When you think about it, where was that proverb generated? Is it something that someone would say in the course of protecting a truth, or in the course of protecting <em>from</em> the truth? But people don&#x2019;t perk up and say, &#x201C;Aha! I sense the presence of the Dark Side!&#x201D; As far as I can tell, it&#x2019;s not widely realized that the Dark Side is out there.</p>\n\n  <p>But how else? Whether you&#x2019;re deceiving others, or just yourself, the Lie That Must Be Protected will propagate recursively through the network of empirical causality, and the network of general empirical rules, and the rules of reasoning themselves, and the understanding behind those rules. If there is <em>good</em> epistemology in the world, and also lies or self-deceptions that people are trying to protect, then there will come into existence bad epistemology to counter the good. We could hardly expect, in this world, to find the Light Side without the Dark Side; there is the Sun, and that which shrinks away and generates a cloaking Shadow.</p>\n\n  <p>Mind you, these are not necessarily <em>evil people</em>. The vast majority who go about repeating the Deep Wisdom are more duped than duplicitous, more self-deceived than deceiving. I think.</p>\n\n  <p>And it&#x2019;s surely not my intent to offer you a Fully General Counterargument, so that whenever someone offers you some epistemology you don&#x2019;t like, you say: &#x201C;Oh, someone on the Dark Side made that up.&#x201D; It&#x2019;s one of the rules of the Light Side that you have to refute the proposition for itself, not by accusing its inventor of bad intentions.</p>\n\n  <p>But the Dark Side is out there. Fear is the path that leads to it, and one betrayal can turn you. Not all who wear robes are either Jedi or fakes; there are also the Sith Lords, masters and unwitting apprentices. Be warned; be wary.</p>\n\n  <p>As for listing common memes that were spawned by the Dark Side&#x2014;not random false beliefs, mind you, but bad epistemology, the Generic Defenses of Fail&#x2014;well, would you care to take a stab at it, dear readers?</p>\n\n  <div class=\"footnotes\">\n    \n\n    <p><span><sup><a href=\"#fn1x37-bk\" id=\"fn1x37\">1</a></sup></span>Actually, a geologist in the comments says that most pebbles in driveways are taken <em>from</em> beaches, so they couldn&#x2019;t tell the difference between a driveway pebble and a beach pebble, but they could tell the difference between a mountain pebble and a driveway/beach pebble (<a href=\"http://lesswrong.com/lw/uy/dark_side_epistemology/4xbv\">http://lesswrong.com/lw/uy/dark_side_epistemology/4xbv</a>). Case in point . . .</p>\n  </div>\n\n", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "XYHzLjwYiqpeqaf4c": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XTWkjCJScy2GFAgDt", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 76, "baseScore": 84, "extendedScore": null, "score": 0.000122, "legacy": true, "legacyId": "1114", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": "Rationality: A-Z", "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "GSqFqc646rsRd2oyz", "canonicalCollectionSlug": "rationality", "canonicalBookId": "coGq3LC5Yn4vZiu6k", "canonicalNextPostSlug": "doublethink-choosing-to-be-biased", "canonicalPrevPostSlug": "of-lies-and-black-swan-blowups", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 85, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 148, "af": false, "version": "2.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-19T00:09:31.000Z", "modifiedAt": null, "url": null, "title": "Protected From Myself", "slug": "protected-from-myself", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:57.019Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yz2btSaCLHmLWgWD5/protected-from-myself", "pageUrlRelative": "/posts/yz2btSaCLHmLWgWD5/protected-from-myself", "linkUrl": "https://www.lesswrong.com/posts/yz2btSaCLHmLWgWD5/protected-from-myself", "postedAtFormatted": "Sunday, October 19th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Protected%20From%20Myself&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProtected%20From%20Myself%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyz2btSaCLHmLWgWD5%2Fprotected-from-myself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Protected%20From%20Myself%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyz2btSaCLHmLWgWD5%2Fprotected-from-myself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyz2btSaCLHmLWgWD5%2Fprotected-from-myself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1664, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/ue/the_magnitude_of_his_own_folly/\">The Magnitude of His Own Folly</a>, <a href=\"/lw/uw/entangled_truths_contagious_lies/\">Entangled Truths, Contagious Lies</a></p>\n<p>Every now and then, another one comes before me with the brilliant idea:&nbsp; \"Let's lie!\"</p>\n<p>Lie about what?&mdash;oh, various things.&nbsp; The expected time to Singularity, say.&nbsp; Lie and say it's definitely going to be earlier, because that will get more public attention.&nbsp; Sometimes they say \"be optimistic\", sometimes they just say \"lie\".&nbsp; Lie about the current degree of uncertainty, because there are other people out there claiming to be certain, and <a href=\"/lw/ue/the_magnitude_of_his_own_folly/\">the most unbearable prospect in the world is that someone else pull ahead</a>. Lie about what the project is likely to accomplish&mdash;I flinch even to write this, but occasionally someone proposes to go and say to the Christians that the AI will create Christian Heaven forever, or go to the US government and say that the AI will give the US dominance forever.</p>\n<p>But at any rate, lie.&nbsp; Lie because it's more convenient than trying to explain the truth.&nbsp; Lie, because someone else might lie, and so we have to make sure that we lie first.&nbsp; Lie to grab the tempting benefits, hanging just within reach&mdash;</p>\n<p>Eh?&nbsp; Ethics?&nbsp; Well, now that you mention it, lying is at least a little bad, all else being equal.&nbsp; But with so much at stake, we should just ignore that and lie.&nbsp; You've got to follow the expected utility, right?&nbsp; The loss of a lie is much less than the benefit to be gained, right?</p>\n<p>Thus do they argue.&nbsp; Except&mdash;what's the flaw in the argument?&nbsp; Wouldn't it be <em>irrational</em> not to lie, if lying has the greatest expected utility?</p>\n<p>When I <a href=\"/lw/ue/the_magnitude_of_his_own_folly/\">look back upon my history</a>&mdash;well, I screwed up in a lot of ways.&nbsp; But it could have been <em>much worse,</em> if I had reasoned like those who offer such advice, and lied.</p>\n<p><a id=\"more\"></a></p>\n<p>Once upon a time, I truly and honestly believed that <a href=\"/lw/u2/the_sheer_folly_of_callow_youth/\">either a superintelligence would do what was right, or else there was no right thing to do</a>; and I said so.&nbsp; I was uncertain of the nature of morality, and I said that too.&nbsp; I didn't know if the Singularity would be in five years or fifty, and this also I admitted.&nbsp; My project plans were not guaranteed to deliver results, and I did not promise to deliver them.&nbsp; When I finally said \"<a href=\"/lw/ue/the_magnitude_of_his_own_folly/\">Oops</a>\", and realized that I needed to <a href=\"/lw/ul/my_bayesian_enlightenment/\">go off and do more fundamental research</a> instead of rushing to write code immediately&mdash;</p>\n<p>&mdash;well, I can imagine the mess I would have had on my hands, if I had told the people who trusted me: that the Singularity was surely coming in ten years; that my theory was sure to deliver results; that I had no lingering confusions; and that any superintelligence would <em>surely</em> give them their own private island and a harem of catpersons of the appropriate gender.&nbsp; How exactly would one then explain why you're now going to step back and look for math-inventors instead of superprogrammers, or why the code now has to be theorem-proved?</p>\n<p>When you make an honest mistake, on some subject you were honest <em>about,</em> the recovery technique is straightforward:&nbsp; Just as you told people what you thought in the first place, you now list out the actual reasons that you changed your mind.&nbsp; This diff takes you to your current true thoughts, that imply your current desired policy.&nbsp; Then, just as people decided whether to aid you originally, they re-decide in light of the new information.</p>\n<p>But what if you were \"optimistic\" and only presented one side of the story, the better to fulfill that all-important goal of persuading people to your cause?&nbsp; Then you'll have a much harder time persuading them <em>away</em> from that idea you sold them originally&mdash;you've nailed their feet to the floor, which makes it difficult for them to follow if you yourself take another step forward.</p>\n<p>And what if, for the sake of persuasion, you told them things that you didn't believe yourself?&nbsp; Then there is no true diff from the story you told before, to the new story now.&nbsp; Will there be any coherent story that explains your change of heart?</p>\n<p>Conveying the real truth is an art form.&nbsp; It's not an easy art form&mdash;those darned constraints of honesty prevent you from telling all kinds of convenient lies that would be so much easier than the complicated truth.&nbsp; But, if you tell lots of truth, you get good at what you practice.&nbsp; A lot of those who come to me and advocate lies, talk earnestly about how these matters of transhumanism are <em>so hard</em> to explain, too difficult and technical for the likes of Joe the Plumber.&nbsp; So they'd like to take the easy way out, and lie.</p>\n<p>We <a href=\"/lw/uk/beyond_the_reach_of_god/\">don't live in a righteous universe</a> where all sins are punished.&nbsp; Someone who <em>practiced</em> telling lies, and made their mistakes and learned from them, might well become expert at telling lies that allow for sudden changes of policy in the future, and telling more lies to explain the policy changes.&nbsp; If you use the various forbidden arts that create fanatic followers, they will swallow just about anything.&nbsp; The history of the Soviet Union and their sudden changes of policy, as presented to their ardent Western intellectual followers, helped inspire <a href=\"http://www.overcomingbias.com/2007/09/human-evil-and-.html?cid=82802703\">Orwell</a> to write 1984.</p>\n<p>So the question, really, is whether you want to practice truthtelling or practice lying, because whichever one you practice is the one you're going to get good at.&nbsp; Needless to say, those who come to me and offer their unsolicited advice do not appear to be expert liars.&nbsp; For one thing, a majority of them don't seem to find anything odd about floating their proposals in publicly archived, Google-indexed mailing lists.</p>\n<p>But why <em>not</em> become an expert liar, if that's what maximizes expected utility?&nbsp; Why take the constrained path of truth, when things so much more important are at stake?</p>\n<p>Because, when I look over my history, I find that <strong>my ethics have, above all, protected me from <em>myself.</em></strong>&nbsp; They weren't inconveniences.&nbsp; They were safety rails on cliffs I didn't see.</p>\n<p>I made fundamental mistakes, and my ethics didn't halt that, but they played a critical role in my recovery.&nbsp; <strong>When I was stopped by unknown unknowns that I just wasn't expecting, it was my ethical constraints, and not any conscious planning, that had put me in a recoverable position.</strong></p>\n<p>You can't duplicate this protective effect by trying to be clever and calculate the course of \"highest utility\".&nbsp; The expected utility just takes into account the things you <em>know</em> to expect.&nbsp; It really is amazing, looking over my history, the extent to which my ethics put me in a recoverable position from my unanticipated, <em>fundamental</em> mistakes, the things completely outside my plans and beliefs.</p>\n<p>Ethics aren't just there to make your life difficult; they can protect you from Black Swans.&nbsp; A startling assertion, I know, but not one entirely irrelevant to current affairs.</p>\n<p>If you've been following along my story, you'll recall that the downfall of all my theories, began with a <a href=\"/lw/u7/that_tiny_note_of_discord/\">tiny note of discord</a>. A tiny note that I wouldn't ever have followed up, if I had only cared about my own preferences and desires.&nbsp; It was the thought of what someone else might think&mdash;someone to whom I felt I owed an ethical consideration&mdash;that spurred me to follow up that one note.</p>\n<p>And I have watched others fail utterly on the problem of Friendly AI, because they simply try to grab the banana in one form or another&mdash;seize the world for their own favorite moralities, without any thought of what others might think&mdash;and so they never enter into the complexities and second thoughts that might begin to warn them of the <a href=\"/lw/tj/dreams_of_friendliness/\"><em>technical</em> problems</a>.</p>\n<p>We don't live in a <a href=\"/lw/uk/beyond_the_reach_of_god/\">righteous universe</a>.&nbsp; And so, when I look over my history, the role that my ethics have played is so important that I've had to take a step back and ask, \"<em>Why is this happening?</em>\"&nbsp; The universe <em>isn't</em> set up to reward virtue&mdash;so why did my ethics help so much?&nbsp; Am I only imagining the phenomenon?&nbsp; That's one possibility.&nbsp; But after some thought, I've concluded that, to the extent you believe that my ethics <em>did</em> help me, these are the plausible reasons in order of importance:</p>\n<p>1)&nbsp; <strong>The honest Way often has a kind of simplicity that trangressions lack</strong>. If you tell lies, you have to keep track of different stories you've told different groups, and worry about which facts might encounter the wrong people, and then invent new lies to explain any unexpected policy shifts you have to execute on account of your mistake.&nbsp; This <em>simplicity</em> is powerful enough to explain a great deal of the positive influence that I attribute to my ethics, in a universe that doesn't reward virtue <em>per se.</em></p>\n<p>2)&nbsp; <strong>I was stricter with myself, and held myself to a higher standard, when I was doing various things that I considered myself ethically obligated to do</strong>.&nbsp; Thus my recovery from various failures often seems to have begun with an ethical thought of some type&mdash;e.g. the whole development where \"Friendly AI\" led into the concept of AI as a precise art.&nbsp; That might just be a quirk of my own personality; but it seems to help account for the huge role my ethics played in leading me to important thoughts, which I cannot just explain by saying that the universe rewards virtue.</p>\n<p>3)&nbsp; <strong>The constraints that the wisdom of history suggests, to avoid hurting other people, may also stop you from hurting yourself.</strong>&nbsp; When you have <a href=\"/lw/uv/ends_dont_justify_means_among_humans/\">some brilliant idea that benefits the tribe</a>, we don't want you to run off and do X, Y, and Z, even if you say \"the end justifies the means!\"&nbsp; Evolutionarily speaking, one suspects that the \"means\" have more often benefited the person who executes them, than the tribe.&nbsp; But this is not the ancestral environment.&nbsp; In the more complicated modern world, following the ethical constraints can prevent you from making huge networked mistakes that would catch you in their collapse.&nbsp; Robespierre led a shorter life than Washington.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Ethical_injunction#Sequence\"><em>Ethical Injunctions</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/v0/ethical_inhibitions/\">Ethical Inhibitions</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/uv/ends_dont_justify_means_among_humans/\">Ends Don't Justify Means (Among Humans)</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 1, "ouT6wKhACJRouGokM": 1, "nANxo5C4sPG9HQHzr": 1, "yeJFqsWrP2pjYfNEr": 1, "Yd7JES6Wmbc2PBgid": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yz2btSaCLHmLWgWD5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 38, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "1115", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fLRPeXihRaiRo5dyX", "wyyfFfaRar2jEdeQK", "Yicjw6wSSaPdb83w9", "Ti3Z7eZtud32LhGZT", "sYgv4eYH82JEsTD34", "SwCwG9wZcAzQtckwx", "wKnwcjJGriTS9QxxL", "K9ZaZXDnL3SEmYZqB", "cyRpNbPsW8HzsxhRK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-19T20:44:02.000Z", "modifiedAt": null, "url": null, "title": "Ethical Inhibitions", "slug": "ethical-inhibitions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:39.432Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cyRpNbPsW8HzsxhRK/ethical-inhibitions", "pageUrlRelative": "/posts/cyRpNbPsW8HzsxhRK/ethical-inhibitions", "linkUrl": "https://www.lesswrong.com/posts/cyRpNbPsW8HzsxhRK/ethical-inhibitions", "postedAtFormatted": "Sunday, October 19th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ethical%20Inhibitions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEthical%20Inhibitions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcyRpNbPsW8HzsxhRK%2Fethical-inhibitions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ethical%20Inhibitions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcyRpNbPsW8HzsxhRK%2Fethical-inhibitions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcyRpNbPsW8HzsxhRK%2Fethical-inhibitions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1460, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/uw/entangled_truths_contagious_lies/\">Entangled Truths, Contagious Lies</a>, <a href=\"/lw/l1/evolutionary_psychology/\">Evolutionary Psychology</a></p>\n<p>What's up with that bizarre emotion we humans have, this sense of <em>ethical caution?</em></p>\n<p>One can understand sexual lust, parental care, and even romantic attachment.&nbsp; The <a href=\"/lw/l1/evolutionary_psychology/\">evolutionary psychology</a> of such emotions might be subtler than it at first appears, but if you ignore the subtleties, the surface reasons are obvious.&nbsp; But why a sense of ethical caution?&nbsp; Why honor, why righteousness?&nbsp; (And no, it's <a href=\"/lw/l5/evolving_to_extinction/\">not group selection</a>; it <a href=\"/lw/kw/the_tragedy_of_group_selectionism/\">never is</a>.)&nbsp; What reproductive benefit does that provide?</p>\n<p>The <em>specific</em> ethical codes that people feel uneasy violating, vary from tribe to tribe (though there are certain regularities).&nbsp; But the <em>emotion</em> associated with<em> feeling ethically inhibited</em>&mdash;well, I Am Not An Evolutionary Anthropologist, but that looks like a <a href=\"/lw/rl/the_psychological_unity_of_humankind/\">human universal</a> to me, something with brainware support.</p>\n<p>The obvious story behind prosocial emotions in general, is that those who offend against the group are sanctioned; this converts the emotion to an <em>individual</em> reproductive advantage.&nbsp; The human organism, <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">executing</a> the ethical-caution adaptation, ends up avoiding the group sanctions that would follow a violation of the code.&nbsp; This obvious answer may even be the entire answer.</p>\n<p>But I suggest&mdash;if a bit more tentatively than usual&mdash;that by the time human beings were evolving the emotion associated with \"ethical inhibition\", we were already intelligent enough to observe the existence of such things as group sanctions.&nbsp; We were already smart enough (I suggest) to model what the group would punish, and to fear that punishment.</p>\n<p>Sociopaths have a concept of getting caught, and they try to avoid getting caught.&nbsp; Why isn't this sufficient?&nbsp; Why have an <em>extra</em> emotion, a feeling that inhibits you even when you <em>don't</em> expect to be caught?&nbsp; Wouldn't this, from evolution's perspective, just result in passing up perfectly good opportunities?</p>\n<p><a id=\"more\"></a></p>\n<p>So I suggest (tentatively) that humans naturally underestimate the odds of getting caught.&nbsp; We don't foresee all the possible chains of causality, all the entangled facts that can bring evidence against us.&nbsp; Those ancestors who lacked a sense of ethical caution stole the silverware <em>when they expected that no one would catch them or punish them;</em> and were <em>nonetheless</em> caught or punished often enough, on average, to outweigh the value of the silverware.</p>\n<p>Admittedly, this may be an unnecessary assumption.&nbsp; It is a general idiom of biology that <a href=\"/lw/l2/protein_reinforcement_and_dna_consequentialism/\">evolution is the only long-term consequentialist; organisms compute short-term rewards</a>.&nbsp; Hominids violate this rule, but that is a very recent innovation.</p>\n<p>So one could counter-argue:&nbsp; \"Early humans didn't reliably forecast the punishment that follows from breaking social codes, so they didn't reliably think consequentially about it, so they developed an instinct to obey the codes.\"&nbsp; Maybe the modern sociopaths that <em>evade being caught</em> are smarter than average.&nbsp; Or modern sociopaths are better educated than hunter-gatherer sociopaths.&nbsp; Or modern sociopaths get more second chances to recover from initial stumbles&mdash;they can change their name and move.&nbsp; It's not so strange to find an emotion executing in some exceptional circumstance where it fails to provide a reproductive benefit.</p>\n<p>But I feel justified in bringing up the more complicated hypothesis, because ethical inhibitions are <em>archetypally</em>that which stops us even when we think no one is looking.&nbsp; A humanly universal concept, so far as I know, though I am not an anthropologist.</p>\n<p>Ethical inhibition, as a human motivation, seems to be implemented in a distinct style from hunger or lust.&nbsp; Hunger and lust can be outweighed when stronger desires are at stake; but the emotion associated with ethical prohibitions tries to assert itself <em>deontologically</em>. If you have the sense at all that you shouldn't do it, you have the sense that you unconditionally shouldn't do it.&nbsp; The emotion associated with ethical caution would seem to be a drive that&mdash;successfully or unsuccessfully&mdash;tries to <em>override</em> the temptation, not just <em>weigh against</em> it.</p>\n<p>A monkey can be trapped by a food reward inside a hollowed shell&mdash;they can reach in easily enough, but once they close their fist, they can't take their hand out.&nbsp; The monkey may be screaming with distress, and still be unable to override the instinct to keep hold of the food. We humans can do better than that; we can let go of the food reward and run away, when our brain is warning us of the long-term consequences.</p>\n<p>But why does the sensation of <em>ethical inhibition,</em> that might also command us to pass up a food reward, have a similar override-quality&mdash;even in the absence of explicitly expected long-term consequences?&nbsp; Is it just that ethical emotions evolved recently, and happen to be implemented in prefrontal cortex next to the long-term-override circuitry?</p>\n<p>What is this tendency to <em>feel</em> inhibited from stealing the food reward?&nbsp; This message that tries to assert \"I override\", not just \"I weigh against\"?&nbsp; Even when we don't expect the long-term consequences of being discovered?</p>\n<p>And before you think that I'm falling prey to some kind of appealing story, ask yourself why that particular story would sound appealing to humans.&nbsp; Why would it seem temptingly <em>virtuous</em> to let an ethical inhibition override, rather than just being one more weight in the balance?</p>\n<p>One possible explanation would be if the emotion were carved out by the evolutionary-historical statistics of a <em>black-swan bet.</em></p>\n<p>Maybe you will, in all probability, get away with stealing the silverware on any particular occasion&mdash;just as your model of the world would extrapolate.&nbsp; But it was a statistical fact about your ancestors that sometimes the environment didn't operate the way they expected. Someone was watching from behind the trees.&nbsp; On those occasions their reputation was permanently blackened; they lost status in the tribe, and perhaps were outcast or murdered.&nbsp; Such occasions could be statistically rare, and still counterbalance the benefit of a few silver spoons.</p>\n<p><a href=\"/lw/l1/evolutionary_psychology/\">The brain, like every other organ in the body, is a reproductive organ</a>: it was <a href=\"/lw/kr/an_alien_god/\">carved out of entropy by the persistence of mutations that promoted reproductive fitness</a>.&nbsp; And yet somehow, <a href=\"/lw/sa/the_gift_we_give_to_tomorrow/\">amazingly</a>, the human brain wound up with circuitry for such things as honor, sympathy, and ethical resistance to temptations.</p>\n<p>Which means that those alleles <em>drove their alternatives to extinction.</em>&nbsp; Humans, the organisms, can be nice to each other; but <a href=\"/lw/l5/evolving_to_extinction/\">the alleles' game of frequencies is zero-sum</a>.&nbsp; Honorable ancestors didn't necessarily kill the dishonorable ones.&nbsp; But if, by cooperating with each other, honorable ancestors outreproduced less honorable folk, then the honor allele killed the dishonor allele as surely as if it erased the DNA sequence off a blackboard.</p>\n<p>That might be something to think about, the next time you're wondering if you should just give in to your ethical impulses, or try to override them with your rational awareness.</p>\n<p>Especially if you're tempted to engage in some chicanery \"for the greater good\"&mdash;tempted to decide that <a href=\"/lw/uv/ends_dont_justify_means_among_humans/\">the end justifies the means</a>.&nbsp; Evolution doesn't care about whether something actually promotes the greater good&mdash;<a href=\"/lw/l5/evolving_to_extinction/\">that's not how gene frequencies change</a>.&nbsp; But if transgressive plans go awry often enough to hurt <em>the transgressor</em>, how much <em>more</em> often would they go awry and hurt the intended beneficiaries?</p>\n<p>Historically speaking, it seems likely that, of those who set out to rob banks or murder opponents \"in a good cause\", those who managed to hurt <em>themselves,</em> mostly wouldn't make the history books.&nbsp; (Unless they got a second chance, like Hitler after the failed Beer Hall Putsch.)&nbsp; Of those cases we <em>do </em>read about in the history books, many people have <a href=\"/lw/uu/why_does_power_corrupt/\">done very well for <em>themselves</em></a> out of their plans to lie and rob and murder \"for the greater good\".&nbsp; But how many people cheated their way to <em>actual</em> huge altruistic benefits&mdash;cheated and <em>actually</em> realized the justifying greater good?&nbsp; Surely there must be at least one or two cases known to history&mdash;at least one king <em>somewhere</em> who took power by lies and assassination, and then ruled wisely and well&mdash;but I can't actually name a case off the top of my head.&nbsp; By and large, it seems to me a pretty fair generalization that people who achieve great good ends manage <em>not</em> to find excuses for all that much evil along the way.</p>\n<p>Somehow, people seem much more likely to endorse plans that involve just a little pain for someone <em>else</em>, on behalf of the greater good, than to work out a way to let the sacrifice be themselves.&nbsp; But when you plan to damage society in order to save it, remember that your brain contains a sense of ethical unease that evolved from transgressive plans blowing up and damaging the&nbsp; <em>originator</em>&mdash;never mind the expected value of all the damage done to <em>other</em> people, if you really do care about them.</p>\n<p>If natural selection, which doesn't care <em>at all</em> about the welfare of unrelated strangers, still manages to give you a sense of ethical unease on account of transgressive plans not always going as planned&mdash;then how much <em>more</em> reluctant should you be to rob banks <a href=\"/lw/uv/ends_dont_justify_means_among_humans/\">for a good cause</a>, if you aspire to <em>actually</em> help and protect others?</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Ethical_injunction#Sequence\"><em>Ethical Injunctions</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/v1/ethical_injunctions/\">Ethical Injunctions</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/uz/protected_from_myself/\">Protected From Myself</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"exZi6Bing5AiM4ZQB": 1, "nSHiKwWyMZFdZg5qt": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cyRpNbPsW8HzsxhRK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 28, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "1116", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wyyfFfaRar2jEdeQK", "epZLSoNvjW53tqNj9", "gDNrpuwahdRrDJ9iY", "QsMJQSFj7WfoTMNgW", "Cyj6wQLW6SeF6aGLy", "XPErvb8m9FapXCjhA", "gTNB9CQd5hnbkMxAG", "pLRogvJLPPg6Mrvg4", "pGvyqAQw6yqTjpKf4", "K9ZaZXDnL3SEmYZqB", "v8rghtzWCziYuMdJ5", "dWTEtgBfFaz6vjwQf", "yz2btSaCLHmLWgWD5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-20T23:00:00.000Z", "modifiedAt": null, "url": null, "title": "Ethical Injunctions", "slug": "ethical-injunctions", "viewCount": null, "lastCommentedAt": "2019-08-19T00:49:48.411Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dWTEtgBfFaz6vjwQf/ethical-injunctions", "pageUrlRelative": "/posts/dWTEtgBfFaz6vjwQf/ethical-injunctions", "linkUrl": "https://www.lesswrong.com/posts/dWTEtgBfFaz6vjwQf/ethical-injunctions", "postedAtFormatted": "Monday, October 20th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ethical%20Injunctions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEthical%20Injunctions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdWTEtgBfFaz6vjwQf%2Fethical-injunctions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ethical%20Injunctions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdWTEtgBfFaz6vjwQf%2Fethical-injunctions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdWTEtgBfFaz6vjwQf%2Fethical-injunctions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2615, "htmlBody": "<blockquote>\n<p>\"Would you kill babies if it was the right thing to do?&nbsp; If no, under what circumstances would you not do the right thing to do?&nbsp; If yes, how right would it have to be, for how many babies?\"<br />&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &mdash;<a href=\"/lw/rr/the_moral_void/\">horrible job interview question</a></p>\n</blockquote>\n<p>Swapping hats for a moment, I'm <em>professionally</em> intrigued by the decision theory of \"things you shouldn't do even if they seem to be the right thing to do\".</p>\n<p>Suppose we have a reflective AI, self-modifying and self-improving, at an intermediate stage in the development process.&nbsp; In particular, the AI's goal system isn't finished&mdash;the shape of its motivations is still being loaded, learned, tested, or tweaked.</p>\n<p>Yea, I have seen <a href=\"/lw/tj/dreams_of_friendliness/\">many ways to screw up an AI goal system design</a>, resulting in a decision system that decides, given its goals, that the universe ought to be tiled with <a href=\"/lw/td/magical_categories/\">tiny molecular smiley-faces</a>, or some such.&nbsp; Generally, these deadly suggestions also have the property that the AI will not desire its programmers to fix it.&nbsp; If the AI is <em>sufficiently</em> advanced&mdash;which it may be even at an intermediate stage&mdash;then the AI may also realize that deceiving the programmers, hiding the changes in its thoughts, will help transform the universe into smiley-faces.</p>\n<p>Now, from our perspective as programmers, if we <em>condition on the fact</em> that the AI has decided to hide its thoughts from the programmers, or otherwise act willfully to deceive us, then it would seem likely that some kind of unintended consequence has occurred in the goal system.&nbsp; We would consider it probable that the AI is <em>not</em> functioning as intended, but rather likely that we have messed up the AI's utility function somehow.&nbsp; So that the AI wants to turn the universe into tiny reward-system counters, or some such, and now has a motive to hide from us.</p>\n<p>Well, suppose we're <em>not</em> going to implement some object-level <a href=\"/lw/lq/fake_utility_functions/\">Great Idea</a> as the AI's utility function.&nbsp; Instead we're going to do something advanced and recursive&mdash;build a goal system which knows (and cares) about the programmers outside.&nbsp; A goal system that, via some nontrivial internal structure, \"knows it's being programmed\" and \"knows it's incomplete\".&nbsp; Then you might be able to have and keep the rule:</p>\n<blockquote>\n<p>\"If [I decide that] fooling my programmers is the right thing to do, execute a controlled shutdown [instead of doing the right thing to do].\"</p>\n</blockquote>\n<p><a id=\"more\"></a></p>\n<p>And the AI would keep this rule, even through the self-modifying AI's revisions of its own code, because, in its structurally nontrivial goal system, the present-AI understands that this decision by a future-AI <em>probably</em> indicates something defined-as-a-malfunction.&nbsp; Moreover, the present-AI knows that if future-AI tries to <em>evaluate</em> the utility of executing a shutdown, once this hypothetical malfunction has occurred, the future-AI will probably <em>decide</em> not to shut itself down.&nbsp; So the shutdown should happen unconditionally, automatically, without the goal system getting another chance to recalculate the right thing to do.</p>\n<p>I'm not going to go into the deep dark depths of the exact mathematical structure, because that would be beyond the scope of this blog.&nbsp; Also I don't yet know the deep dark depths of the mathematical structure.&nbsp; It looks like it <em>should</em> be possible, if you do things that are advanced and recursive and have nontrivial (but consistent) structure.&nbsp; But I <a href=\"/lw/ua/the_level_above_mine/\">haven't reached that level</a>, as yet, so for now it's <a href=\"/lw/tf/dreams_of_ai_design/\">only a dream</a>.</p>\n<p>But the topic here is not advanced AI; it's human ethics.&nbsp; I introduce the AI scenario to bring out more starkly the strange idea of an <em>ethical injunction:</em></p>\n<blockquote>\n<p>You should never, ever murder an innocent person who's helped you, <em>even if it's the right thing to do;</em> because it's far more likely that <em>you've made a mistake</em>, than that murdering an innocent person who helped you is the right thing to do.</p>\n</blockquote>\n<p>Sound reasonable?</p>\n<p>During World War II, it became necessary to destroy Germany's supply of deuterium, a neutron moderator, in order to block their attempts to achieve a fission chain reaction.&nbsp; Their supply of deuterium was coming at this point from a captured facility in Norway.&nbsp; A shipment of heavy water was on board a Norwegian ferry ship, the <a href=\"http://en.wikipedia.org/wiki/SF_Hydro\">SF Hydro</a>.&nbsp; Knut Haukelid and three others had slipped on board the ferry in order to sabotage it, when the saboteurs were discovered by the ferry watchman.&nbsp; Haukelid told him that they were escaping the Gestapo, and the watchman immediately agreed to overlook their presence.&nbsp; Haukelid \"considered warning their benefactor but decided that might endanger the mission and only thanked him and shook his hand.\"&nbsp; (Richard Rhodes, <em>The Making of the Atomic Bomb.</em>)&nbsp; So the civilian ferry <em>Hydro</em> sank in the deepest part of the lake, with eighteen dead and twenty-nine survivors.&nbsp; Some of the Norwegian rescuers felt that the German soldiers present should be left to drown, but this attitude did not prevail, and four Germans were rescued.&nbsp; And that was, effectively, the end of the Nazi atomic weapons program.</p>\n<p>Good move?&nbsp; Bad move?&nbsp; Germany <em>very likely</em> wouldn't have gotten the Bomb anyway...&nbsp; I hope with absolute desperation that I never get faced by a choice like that, but in the end, I can't say a word against it.</p>\n<p>On the other hand, when it comes to the rule:</p>\n<blockquote>\n<p>\"Never try to deceive yourself, or offer a reason to believe other than probable truth; because even if you come up with an amazing clever reason, it's more likely that you've made a mistake than that you have a reasonable expectation of this being a net benefit in the long run.\"</p>\n</blockquote>\n<p>Then I really <em>don't</em> know of anyone who's knowingly been faced with an exception.&nbsp; There are times when you try to convince yourself \"I'm not hiding any Jews in my basement\" before you talk to the Gestapo officer.&nbsp; But then you do still know the truth, you're just trying to create something like an alternative self that exists in your imagination, a facade to talk to the Gestapo officer.</p>\n<p>But to really believe something that isn't true?&nbsp; I don't know if there was ever anyone for whom that was <em>knowably</em> a good idea.&nbsp; I'm sure that there have been many many times in human history, where person X was better off with false belief Y.&nbsp; And by the same token, there is always some set of winning lottery numbers in every drawing.&nbsp; It's <em>knowing which lottery ticket will win</em> that is the epistemically difficult part, like X knowing when he's better off with a false belief.</p>\n<p>Self-deceptions are the worst kind of black swan bets, much worse than lies, because without knowing the true state of affairs, you can't even guess at what the penalty will be for your self-deception.&nbsp; They only have to blow up once to undo all the good they ever did.&nbsp; One single time when you pray to God after discovering a lump, instead of going to a doctor.&nbsp; That's all it takes to undo a life.&nbsp; All the happiness that the warm thought of an afterlife ever produced in humanity, has now been more than cancelled by the failure of humanity to institute systematic cryonic preservations after liquid nitrogen became cheap to manufacture.&nbsp; And I don't think that anyone ever had that sort of failure in mind as a possible blowup, when they said, \"But we need religious beliefs to cushion the fear of death.\"&nbsp; That's what black swan bets are all about&mdash;the unexpected blowup.</p>\n<p>Maybe you even get away with one or two black-swan bets&mdash;they don't get you <em>every</em> time.&nbsp; So you do it again, and then the blowup comes and cancels out every benefit and then some.&nbsp; That's what black swan bets are all about.</p>\n<p>Thus the difficulty of knowing when it's safe to believe a lie (assuming you can even manage that much mental contortion in the first place)&mdash;part of the nature of black swan bets is that you don't see the bullet that kills you; and since our perceptions just seem like the way the world is, it looks like there is no bullet, period.</p>\n<p>So I would say that there is an ethical injunction against self-deception.&nbsp; I call this an \"ethical injunction\" not so much because it's a matter of interpersonal morality (although it is), but because it's a rule that guards you from your own cleverness&mdash;an override against the temptation to do what seems like the right thing.</p>\n<p>So now we have two kinds of situation that can support an \"ethical injunction\", a rule not to do something even when it's the right thing to do.&nbsp; (That is, you refrain \"even when your brain has computed it's the right thing to do\", but this will just <em>seem like</em> \"the right thing to do\".)</p>\n<p>First, being human and <a href=\"/lw/uv/ends_dont_justify_means_among_humans/\">running on corrupted hardware</a>, we may <a href=\"/lw/uu/why_does_power_corrupt/\">generalize classes of situation</a> where when you say e.g. \"It's time to rob a few banks for the greater good,\" we deem it more likely that you've been corrupted than that this is really the case.&nbsp; (Note that we're not prohibiting it from <em>ever</em> being the case in <em>reality,</em> but we're questioning the <em>epistemic</em> state where you're <em>justified in trusting</em> your own calculation that this is the right thing to do&mdash;fair lottery tickets can win, but you can't justifiably buy them.)</p>\n<p>Second, history may teach us that certain classes of action are black-swan bets, that is, they sometimes blow up bigtime for reasons not in the decider's model.&nbsp; So even when we calculate within the model that something seems like the right thing to do, we apply the further knowledge of the black swan problem to arrive at an injunction against it.</p>\n<p>But surely... if one is <em>aware of these reasons...</em> then one can simply redo the calculation, taking them into account.&nbsp; So we can rob banks if it seems like the right thing to do <em>after taking into account</em> the problem of corrupted hardware and black swan blowups.&nbsp; That's the rational course, right?</p>\n<p>There's a number of replies I could give to that.</p>\n<p>I'll start by saying that this is a prime example of the sort of thinking I have in mind, when I warn aspiring rationalists to beware of cleverness.</p>\n<p>I'll also note that I wouldn't want an attempted Friendly AI that had just decided that the Earth ought to be transformed into paperclips, to assess whether this was a reasonable thing to do in light of all the various warnings it had received against it.&nbsp; I would want it to undergo an automatic controlled shutdown.&nbsp; Who says that meta-reasoning is immune from corruption?</p>\n<p>I could mention the important times that my naive, idealistic ethical inhibitions have <a href=\"/lw/uz/protected_from_myself/\">protected me from <em>myself</em></a>, and placed me in a recoverable position, or helped start the recovery, from very deep mistakes I had no clue I was making.&nbsp; And I could ask whether I've really advanced so much, and whether it would really be all that wise, to remove the protections that saved me before.</p>\n<p>Yet even so...&nbsp; \"Am I still dumber than my ethics?\" is a question whose answer isn't <em>automatically</em> \"Yes.\"</p>\n<p>There are obvious silly things here that you shouldn't do; for example, you shouldn't wait until you're really tempted, and <em>then</em> try to figure out if you're smarter than your ethics on that particular occasion.</p>\n<p>But in general&mdash;there's only so much power that can vest in what your parents told you not to do.&nbsp; One shouldn't underestimate the power.&nbsp; Smart people debated historical lessons in the course of forging the Enlightenment ethics that much of Western culture draws upon; and some subcultures, like scientific academia, or science-fiction fandom, draw on those ethics more directly.&nbsp; But even so the power of the past is bounded.</p>\n<p>And in fact...</p>\n<p>I've had to make my ethics <em>much stricter</em> than what my parents and <a href=\"/lw/u0/raised_in_technophilia/\">Jerry Pournelle</a> and <a href=\"/lw/qf/no_safe_defense_not_even_science/\">Richard Feynman </a>told me not to do.</p>\n<p>Funny thing, how when people seem to think they're smarter than their ethics, they argue for <em>less</em> strictness rather than <em>more</em> strictness.&nbsp; I mean, when you think about how much more complicated the modern world is...</p>\n<p>And along the same lines, the ones who <a href=\"/lw/uz/protected_from_myself/\">come to me and say</a>, \"You should lie about the Singularity, because that way you can get more people to support you; it's the rational thing to do, for the greater good\"&mdash;these ones seem to have <em>no idea</em> of the risks.</p>\n<p>They don't mention the problem of running on corrupted hardware.&nbsp; They don't mention the idea that lies have to be recursively protected from all the truths and all the truthfinding techniques that threaten them.&nbsp; They don't mention that honest ways have a simplicity that dishonest ways often lack.&nbsp; They don't talk about black-swan bets.&nbsp; They don't talk about the terrible nakedness of discarding the last defense you have against yourself, and trying to survive on raw calculation.</p>\n<p>I am reasonably sure that this is because they have <em>no clue</em> about any of these things.</p>\n<p>If you've truly understood the reason and the rhythm behind ethics, then one major sign is that, augmented by this newfound knowledge, you <em>don't do</em> those things that previously seemed like ethical transgressions.&nbsp; Only now you know why.</p>\n<p>Someone who just looks at one or two reasons behind ethics, and says, \"Okay, I've understood that, so now I'll take it into account consciously, and therefore I have no more need of ethical inhibitions\"&mdash;this one is behaving more like a stereotype than a real rationalist.&nbsp; The world isn't simple and pure and clean, so you can't just take the ethics you were raised with and trust them.&nbsp; But that pretense of Vulcan logic, where you think you're just going to compute everything correctly once you've got one or two abstract insights&mdash;that doesn't work in real life either.</p>\n<p>As for those who, having figured out <em>none</em> of this, think themselves smarter than their ethics:&nbsp; Ha.</p>\n<p>And as for those who previously thought themselves smarter than their ethics, but who hadn't conceived of all these elements behind ethical injunctions \"in so many words\" until they ran across this <em>Overcoming Bias</em> sequence, and who <em>now</em> think themselves smarter than their ethics, because they're going to take all this into account from now on:&nbsp; Double ha.</p>\n<p>I have seen many people struggling to excuse themselves from their ethics.&nbsp; Always the modification is toward lenience, never to be more strict.&nbsp; And I am stunned by the speed and the lightness with which they strive to abandon their protections.&nbsp; Hobbes said, \"I don't know what's worse, the fact that everyone's got a price, or the fact that their price is so low.\"&nbsp; So very low the price, so very eager they are to be bought.&nbsp; They don't <a href=\"/lw/km/motivated_stopping_and_motivated_continuation/\">look twice</a> and then a <a href=\"/lw/hu/the_third_alternative/\">third time</a> for alternatives, before deciding that they have no option left but to transgress&mdash;though they may look very grave and solemn when they say it.&nbsp; They abandon their ethics at the very first opportunity.&nbsp; \"Where there's a will to failure, obstacles can be found.\"&nbsp; The will to fail at ethics seems very strong, in some people.</p>\n<p>I don't know if I can endorse absolute ethical injunctions that bind over all possible epistemic states of a human brain.&nbsp; The universe isn't kind enough for me to trust that.&nbsp; (Though an ethical injunction against self-deception, for example, does seem to me to have tremendous force.&nbsp; I've seen many people arguing for the <a href=\"/lw/uy/dark_side_epistemology/\">Dark Side</a>, and none of them seem aware of the <a href=\"/lw/uy/dark_side_epistemology/\">network risks</a> or the black-swan risks of self-deception.)&nbsp; If, someday, I attempt to shape a (reflectively consistent) injunction within a self-modifying AI, it will only be after working out the math, because that is so totally not the sort of thing you could get away with doing via an ad-hoc patch.</p>\n<p>But I will say this much:</p>\n<p><em>I am completely unimpressed with the knowledge, the reasoning, and the overall level, of those folk who have eagerly come to me, and said in grave tones,</em> \"It's rational to do unethical thing X because it will have benefit Y.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yeJFqsWrP2pjYfNEr": 2, "sYm3HiWcfZvrGu3ui": 2, "YTCrHWYHAsAD74EHo": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dWTEtgBfFaz6vjwQf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 47, "baseScore": 59, "extendedScore": null, "score": 9.1e-05, "legacy": true, "legacyId": "1117", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "waF2Pomid7YHjfEDt", "canonicalCollectionSlug": "rationality", "canonicalBookId": "T3CjiQq6bBgFuzvp6", "canonicalNextPostSlug": "something-to-protect", "canonicalPrevPostSlug": "ends-don-t-justify-means-among-humans", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 59, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 76, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K9JSM7d7bLJguMxEp", "wKnwcjJGriTS9QxxL", "PoDAyQMWEXBBBEJ5P", "NnohDYHNnKDtbiMyp", "kXSETKZ3X9oidMozA", "p7ftQ6acRkgo6hqHb", "K9ZaZXDnL3SEmYZqB", "v8rghtzWCziYuMdJ5", "yz2btSaCLHmLWgWD5", "uNWRXtdwL33ELgWjD", "wustx45CPL5rZenuo", "L32LHWzy9FzSDazEg", "erGipespbbzdG5zYb", "XTWkjCJScy2GFAgDt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-21T16:00:00.000Z", "modifiedAt": null, "url": null, "title": "Prices or Bindings?", "slug": "prices-or-bindings", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:38.760Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K2c3dkKErsqFd28Dh/prices-or-bindings", "pageUrlRelative": "/posts/K2c3dkKErsqFd28Dh/prices-or-bindings", "linkUrl": "https://www.lesswrong.com/posts/K2c3dkKErsqFd28Dh/prices-or-bindings", "postedAtFormatted": "Tuesday, October 21st 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Prices%20or%20Bindings%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APrices%20or%20Bindings%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK2c3dkKErsqFd28Dh%2Fprices-or-bindings%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Prices%20or%20Bindings%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK2c3dkKErsqFd28Dh%2Fprices-or-bindings", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK2c3dkKErsqFd28Dh%2Fprices-or-bindings", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1043, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/v1/ethical_injunctions/\">Ethical Injunctions</a></p>\n<p>During World War II, Knut Haukelid and three other saboteurs sank a civilian Norwegian ferry ship, the SF Hydro, carrying a shipment of deuterium for use as a neutron moderator in Germany's atomic weapons program.&nbsp; Eighteen dead, twenty-nine survivors.&nbsp; And that was the end of the Nazi nuclear program.&nbsp; Can you imagine a Hollywood movie in which the hero did that, instead of coming up with some amazing clever way to save the civilians on the ship?</p>\n<p>Stephen Dubner and Steven Levitt published <a href=\"http://freakonomicsbook.com/articles/bagelman.html\">the work of an anonymous economist turned bagelseller</a>, Paul F., who dropped off baskets of bagels and came back to collect money from a cashbox, and also collected statistics on payment rates.&nbsp; The current average payment rate is 89%.&nbsp; Paul F. found that people on the executive floor of a company steal more bagels; that people with security clearances don't steal any fewer bagels; that telecom companies have robbed him and that law firms aren't worth the trouble.</p>\n<p>Hobbes (of Calvin and Hobbes) once said:&nbsp; \"I don't know what's worse, the fact that everyone's got a price, or the fact that their price is so low.\"</p>\n<p>If Knut Haukelid sold his soul, he held out for a <em>damned</em> high price&mdash;the end of the Nazi atomic weapons program.</p>\n<p>Others value their integrity less than a bagel.</p>\n<p>One suspects that Haukelid's price was far higher than most people would charge, if you told them to <em>never</em> sell out.&nbsp; Maybe we should stop telling people they should <em>never</em> let themselves be bought, and focus on raising their price to something higher than a bagel?</p>\n<p>But I really don't know if that's enough.</p>\n<p><a id=\"more\"></a></p>\n<p>The German philosopher Fichte once said, \"I would not break my word even to save humanity.\"</p>\n<p>Raymond Smullyan, in whose book I read this quote, seemed to laugh and not take Fichte seriously.</p>\n<p>Abraham Heschel said of Fichte, \"His salvation and righteousness were apparently so much more important to him than the fate of all men that he would have destroyed mankind to save himself.\"</p>\n<p>I don't think they get it.</p>\n<p>If a serial killer comes to a confessional, and confesses that he's killed six people and plans to kill more, should the priest turn him in?&nbsp; I would answer, \"No.\"&nbsp; If not for the seal of the confessional, the serial killer would never have come to the priest in the first place.&nbsp; All else being equal, I would prefer the world in which the serial killer talks to the priest, and the priest gets a chance to try and talk the serial killer out of it.</p>\n<p>I use the example of a priest, rather than a <a href=\"http://answers.yahoo.com/question/index?qid=20080218122240AAkjPAk\">psychiatrist</a>, because a psychiatrist might be tempted to break confidentiality \"just this once\", and the serial killer knows that.&nbsp; But a Catholic priest who broke the seal of the confessional&mdash;<em>for any reason</em>&mdash;would face universal condemnation from his own church.&nbsp; No Catholic would be tempted to say, \"Well, it's all right because it was a serial killer.\"</p>\n<p>I approve of this custom and its <a href=\"http://jimmyakin.typepad.com/defensor_fidei/2006/10/parental_advice.html\">absoluteness</a>, and I wish we had a rationalist equivalent.</p>\n<p>The trick would be establishing something of equivalent strength to a Catholic priest who believes God doesn't want him to break the seal, rather than the lesser strength of a psychiatrist who outsources their tape transcriptions to Pakistan.&nbsp; Otherwise serial killers will, quite sensibly, use the Catholic priests instead, and get less rational advice.</p>\n<p>Suppose someone comes to a rationalist Confessor and says:&nbsp; \"You know, tomorrow I'm planning to wipe out the human species using this neat biotech concoction I cooked up in my lab.\"&nbsp; What then?&nbsp; Should you break the seal of the confessional to save humanity?</p>\n<p>It appears obvious to me that the issues here are just those of the one-shot <a href=\"/lw/tn/the_true_prisoners_dilemma/\">Prisoner's Dilemma</a>, and I <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">do not consider it obvious</a> that you should defect on the one-shot PD if the other player cooperates in advance on the expectation that you will cooperate as well.</p>\n<p>There are issues with trustworthiness and how the sinner can trust the rationalist's commitment.&nbsp; It is not enough to be trustworthy; you must appear so.&nbsp; But anything that mocks the appearance of trustworthiness, while being unbound from its substance, is a poor signal; the sinner can follow that logic as well.&nbsp; Perhaps once neuroimaging is a bit more advanced, we could have the rationalist swear under a truthtelling machine that they would not break the seal of the confessional even to save humanity.</p>\n<p>There's a proverb I failed to Google, which runs something like, \"Once someone is known to be a liar, you might as well listen to the whistling of the wind.\"&nbsp; You wouldn't want others to expect you to lie, if you have something important to say to them; and this issue cannot be wholly decoupled from the issue of whether you actually tell the truth.&nbsp; If you'll lie when the fate of the world is at stake, and others can guess that fact about you, then, at the moment when the fate of the world <em>is</em> at stake, that's the moment when your words become the whistling of the wind.</p>\n<p>I don't know if Fichte meant it that way, but his statement makes perfect sense as an ethical thesis to me.&nbsp; It's not that one person's personal integrity is worth more, as terminal valuta, than the entire world.&nbsp; Rather, <strong>losing all your ethics is not a pure advantage.</strong></p>\n<p>Being believed to tell the truth has advantages, and I don't think it's so easy to decouple that from telling the truth.&nbsp; Being believed to keep your word has advantages; and if you're the sort of person who would in fact break your word to save humanity, the other may guess that too.&nbsp; Even intrapersonal ethics can help <a href=\"/lw/uz/protected_from_myself/\">protect you from black swans and fundamental mistakes</a>.&nbsp; That logic doesn't change its structure when you double the value of the stakes, or even raise them to the level of a world.&nbsp; Losing your ethics is not like shrugging off some chains that were cool to look at, but were weighing you down in an athletic contest.</p>\n<p>This I knew from the beginning:&nbsp; That if I had no ethics I would hold to even with the world at stake, I had no ethics at all.&nbsp; And I could guess how <em>that</em> would turn out.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Ethical_injunction#Sequence\"><em>Ethical Injunctions</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/v3/ethics_notes/\">Ethics Notes</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/v1/ethical_injunctions/\">Ethical Injunctions</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 1, "yeJFqsWrP2pjYfNEr": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K2c3dkKErsqFd28Dh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 42, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "1118", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dWTEtgBfFaz6vjwQf", "HFyWNBnDNEDsDNLrZ", "6ddcsdA2c2XpNpE5x", "yz2btSaCLHmLWgWD5", "hXuB8BCyyiYuzij3F"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-21T21:57:50.000Z", "modifiedAt": null, "url": null, "title": "Ethics Notes", "slug": "ethics-notes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:00.231Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hXuB8BCyyiYuzij3F/ethics-notes", "pageUrlRelative": "/posts/hXuB8BCyyiYuzij3F/ethics-notes", "linkUrl": "https://www.lesswrong.com/posts/hXuB8BCyyiYuzij3F/ethics-notes", "postedAtFormatted": "Tuesday, October 21st 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ethics%20Notes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEthics%20Notes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhXuB8BCyyiYuzij3F%2Fethics-notes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ethics%20Notes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhXuB8BCyyiYuzij3F%2Fethics-notes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhXuB8BCyyiYuzij3F%2Fethics-notes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3307, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/v0/ethical_inhibitions/\">Ethical Inhibitions</a>, <a href=\"/lw/v1/ethical_injunctions/\">Ethical Injunctions</a>, <a href=\"/lw/v2/prices_or_bindings/\">Prices or Bindings?</a></p>\n<p>(Some collected replies to comments on the above three posts.)</p>\n<p><a id=\"more\"></a></p>\n<p><em>From <a href=\"http://www.overcomingbias.com/2008/10/ethical-inhibit.html#comments\">Ethical Inhibitions</a>:</em></p>\n<blockquote>\n<p>Spambot:&nbsp; Every major democratic political leader lies abundantly to obtain office, as it's a necessity to actually persuade the voters. So Bill Clinton, Jean Chretien, Winston Churchill should qualify for at least half of your list of villainy.</p>\n</blockquote>\n<p>Have the ones who've lied more, done better?</p>\n<p>In cases where the politician who told <em>more</em> lies won, has that politician gone on to rule well in an absolute sense?</p>\n<p>Is it actually true that no one who refused to lie (and this is not the same as always telling the whole truth) could win political office?</p>\n<p>Are the lies expected, and in that sense, less than true betrayals of someone who trusts you?</p>\n<p>Are there understood Rules of Politics that include lies but not assassinations, which the good politicians abide by, so that they are not really violating the ethics of their tribe?</p>\n<p>Will the world be so much worse off if sufficiently good people refuse to tell outright lies and are thereby barred from public office; or would we thereby lose a George Washington or Marcus Aurelius or two, and thereby darken history?</p>\n<blockquote>\n<p>Pearson:&nbsp; American revolutionaries as well ended human lives for the greater good</p>\n</blockquote>\n<p>Police must sometimes kill the guilty.&nbsp; Soldiers must sometimes kill civilians (or if the enemy knows you're reluctant, that gives them a motive to use civilians as a shield).&nbsp; Spies sometimes have legitimate cause to kill people who helped them, but this has probably been done far more often than it has been <em>justified</em> by a need to end the Nazi nuclear program.</p>\n<p>I think it's worth noting that in all such cases, you can write out something like a code of ethics and at least try to have social acceptance of it.&nbsp; Politicians, who lie, may prefer not to discuss the whole thing, but politicians are only a small slice of society.</p>\n<p>Are there many who transgress <em>even the unwritten rules</em> and end up really implementing the greater good?&nbsp; (And no, there's no unwritten rule that says you can rob a bank to stop global warming.)</p>\n<p>...but if you're placing yourself under unusual stress, you may need to be stricter than what society will accept from you. In fact, I think it's fair to say that the further I push any art, such as rationality or AI theory, the more I perceive that <em>what society will let you get away with</em> is tremendously too sloppy a standard.</p>\n<blockquote>\n<p>Yvain:&nbsp; <span id=\"comment-135476291-content\">There are all sorts of biases that would make us less likely to believe people who \"break the rules\" can ever turn out well. One is the halo effect. Another is availability bias&mdash;it's much easier to remember people like Mao than it is to remember the people who were quiet and responsible once their revolution was over, and no one notices the genocides that didn't happen because of some coup or assassination.</span></p>\n<p>When the winners do something bad, it's never interpreted as bad after the fact. Firebombing a city to end a war more quickly, taxing a populace to give health care to the less fortunate, intervening in a foreign country's affairs to stop a genocide: they're all likely to be interpreted as evidence for \"the ends don't justify the means\" when they fail, but glossed over or treated as common sense interventions when they work.</p>\n</blockquote>\n<p>Both fair points.&nbsp; One of the difficult things in reasoning about ethics is the extent to which we can expect historical data to be distorted by moral self-deception on top of the more standard fogs of history.</p>\n<blockquote>\n<p>Morrison:&nbsp; I'm not sure you aren't \"making too much stew from one oyster\".&nbsp; I certainly feel a whole lot less ethically inhibited if I'm really, really certain I'm not going to be punished.&nbsp; When I override, it feels very deliberate&mdash;\"system two\" grappling and struggling with \"system one\"'s casual amorality, and with a significant chance of the override attempt failing.</p>\n</blockquote>\n<blockquote>\n<p>Weeks:&nbsp; This entire post is kind of surreal to me, as I'm pretty confident I've never felt the emotion described here before...&nbsp; I don't remember ever wanting to do something that I both felt would be wrong and wouldn't have consequences otherwise.</p>\n</blockquote>\n<p>I don't know whether to attribute this to genetic variance, environmental variance, misunderstanding, or a small number of genuine sociopaths among Overcoming Bias readers. Maybe Weeks is referring to \"not wanting\" in terms of not finally deciding to do something he felt was wrong, rather than not being tempted?</p>\n<p><em>From <a href=\"http://www.overcomingbias.com/2008/10/ethical-injunct.html#comments\">Ethical Injunctions</a>:</em></p>\n<p><span id=\"comment-135775945-content\"> </span></p>\n<blockquote>\n<p>Psy-Kosh:&nbsp; Given the current sequence, perhaps it's time to revisit the whole <a href=\"/lw/kn/torture_vs_dust_specks/\">Torture vs Dust Specks</a> thing?</p>\n</blockquote>\n<p>I can think of two positions on torture to which I am sympathetic:</p>\n<p>Strategy 1:&nbsp; No legal system or society should ever <em>refrain from prosecuting</em> those who torture.&nbsp; Anything important enough that torture would even be on the table, like the standard nuclear bomb in New York, is important enough that everyone involved should be willing to go to prison for the crime of torture.</p>\n<p>Strategy 2:&nbsp; The chance of actually encountering a \"nuke in New York\" situation, that can be effectively resolved by torture, is so low, and the knock-on effects of having the policy in place so awful, that a <em>blanket</em> injunction against torture makes sense.</p>\n<p>In case 1, you would choose TORTURE over SPECKS, and then go to jail for it, even though it was the right thing to do.</p>\n<p>In case 2, you would say \"TORTURE over SPECKS is the right alternative of the two, but a human can never be in an epistemic state where you have justified belief that this is the case\".&nbsp; Which would tie in well to the Hansonian argument that you have an O(3^^^3) probability penalty from the unlikelihood of finding yourself in such a unique position.</p>\n<p>So I am sympathetic to the argument that people should never torture, or that a human can't actually get into the epistemic state of a TORTURE vs. SPECKS decision.</p>\n<p>But I can't back the position that SPECKS over TORTURE is <em>inherently the right thing to do, </em>which I did think was the issue at hand.&nbsp; This seems to me to mix up an epistemic precaution with morality.</p>\n<p>There's certainly worse things than torturing one person&mdash;torturing two people, for example.&nbsp; But if you adopt position 2, then you would refuse to torture one person with your own hands even to save a thousand people from torture, while <em>simultaneously</em> saying that that it is better for one person to be tortured at your own hands than for a thousand people to be tortured at someone else's.</p>\n<p>I try to use the words \"morality\" and \"ethics\" consistently as follows:&nbsp; The moral questions are over the territory (or, hopefully equivalently, over epistemic states of absolute certainty).&nbsp; The ethical questions are over epistemic states that humans are likely to be in.&nbsp; Moral questions are terminal.&nbsp; Ethical questions are instrumental.</p>\n<blockquote>\n<p>Hanson:&nbsp; The problem here of course is how selective to be about rules to let into this protected level of \"rules almost no one should think themselves clever enough to know when to violate.\"&nbsp; After all, your social training may well want you to include \"Never question our noble leader\" in that set.&nbsp; Many a Christian has been told the mysteries of God are so subtle that they shouldn't think themselves clever enough to know when they've found evidence that God isn't following a grand plan to make this the best of all possible worlds.</p>\n</blockquote>\n<p>Some of the flaws in Christian theology lie in what they think their supposed facts would imply: e.g., that because God did miracles you can know that God is good.&nbsp; Other problems come more from the falsity of the premises than the invalidity of the deductions.&nbsp; Which is to say, if God <em>did</em> exist and <em>were</em> good, then you would be justified in being cautious around stomping on parts of God's plan that didn't seem to make sense at the moment.&nbsp; But this epistemic state would best be arrived at via a long history of people saying, \"Look how stupid God's plan is, we need to do X\" and then X blowing up on them. Rather than, as is actually the case, people saying \"God's plan is X\" and then X blows up on them.</p>\n<p>Or if you'd found with some historical regularity that, when you challenged the verdict of the black box, that you seemed to be right 90% of the time, but the other 10% of the time you got black-swan blowups that caused a hundred times as much damage, that would also be cause for hesitation&mdash;albeit it doesn't quite seem like grounds for suspecting a divine plan.</p>\n<blockquote>\n<p>Nominull: S o... do you not actually believe in your injunction to \"shut up and multiply\"?&nbsp; Because for some time now you seem to have been arguing that we should do what feels right rather than trying to figure out what is right.</p>\n</blockquote>\n<p>Certainly I'm not saying \"just do what feels right\".&nbsp; There's no safe defense, not even ethical injunctions.&nbsp; There's also no safe defense, not even \"shut up and multiply\".</p>\n<p>I probably should have been clearer about this before, but I was trying to discuss things in order, and didn't want to wade into ethics without specialized posts...</p>\n<p>People often object to the sort of scenarios that illustrate \"shut up and multiply\" by saying, \"But if the experimenter tells you X, what if they might be lying?\"</p>\n<p>Well, in a lot of real-world cases, then yes, there are various probability updates you perform based on other people being willing to make bets against you; and just because you get certain experimental instructions doesn't imply the real world is that way.</p>\n<p>But the base case has to be <em>moral</em> comparisons between worlds, or comparisons of expected utility between <em>given</em> probability distributions.&nbsp; &nbsp;If you can't ask about the base case, then what good can you get from instrumental ethics built on top?</p>\n<p>Let's be very clear that I <em>don't</em> think that one small act of self-deception is an <em>inherently morally worse event</em> than, say, getting a hand chopped off.&nbsp; I'm asking, rather, how one should best <em>avoid</em> the dismembering chainsaw; and I am arguing that in reasonable states of knowledge a human can attain, the answer is, \"Don't deceive yourself, it's a black-swan bet at best.\"&nbsp; Furthermore, that in the vast majority of cases where I have seen people conclude otherwise, it has indicated messed-up reasoning more than any actual advantage.</p>\n<blockquote>\n<p>Vassar:&nbsp; For such a reason, I would be very wary of using such rules in an AGI, but of course, perhaps the actual mathematical formulation of the rule in question within the AGI would be less problematic, though a few seconds of thought doesn't give me much reason to think this.</p>\n</blockquote>\n<p>Are we still talking about self-deception?&nbsp; Because I would give odds around as extreme as the odds I would give of anything, that if you tell me \"the AI you built is trying to deceive itself\", it indicates that some kind of really <em>epic</em> error has occurred.&nbsp; &nbsp;Controlled shutdown, immediately.</p>\n<blockquote>\n<p>Vassar:&nbsp; In a very general sense though, I see a logical problem with this whole line of thought.&nbsp; How can any of these injunctions survive except as self-protecting beliefs?&nbsp; Isn't this whole approach just the sort of \"fighting bias with bias\" that you and Robin usually argue against?</p>\n</blockquote>\n<p>Maybe I'm not being clear about how this would work in an AI!</p>\n<p>The ethical injunction isn't <em>self</em>-protecting, it's supported within the structural framework of the underlying system.&nbsp; You might even find ethical injunctions starting to emerge without programmer intervention, in some cases, depending on how well the AI understood its own situation.</p>\n<p>But the kind of injunctions I have in mind wouldn't be <em>reflective</em>&mdash;they wouldn't modify the utility function, or kick in at the reflective level to ensure their own propagation.&nbsp; <em>That</em> sounds really scary, to me&mdash;there ought to be an injunction against it!</p>\n<p>You might have a rule that would <em>controlledly shut down</em> the (non-mature) AI if it tried to execute a certain kind of source code change, but that wouldn't be the same as having an injunction that <em>exerts direct control over the source code to propagate itself.</em></p>\n<p>To the extent the injunction sticks around in the AI, it should be as the result of ordinary reasoning, <em>not reasoning taking the injunction into account!</em>&nbsp; &nbsp;That would be the <em>wrong</em> kind of circularity; you <em>can</em> unwind past ethical unjunctions!</p>\n<p>My ethical injunctions do <em>not</em> come with an extra clause that says, \"Do not reconsider this injunction, including not reconsidering this clause.\"&nbsp; That would be going <em>way</em> too far.&nbsp; If anything, you ought to have an injunction against that kind of circularity (since it seems like a plausible failure mode in which the system has been parasitized by its own content).</p>\n<p>&nbsp;</p>\n<blockquote>\n<blockquote>\n<p><span id=\"comment-135672275-content\"> </span></p>\n<p><em>You should never, ever murder an innocent person who's helped you, even if it's the right thing to do</em></p>\n<p><em>Shut up and do the impossible!</em></p>\n<p>&nbsp;</p>\n</blockquote>\n</blockquote>\n<p><span id=\"comment-135775945-content\"> </span></p>\n<blockquote>\n<p>Ord:&nbsp; As written, both these statements are conceptually confused.&nbsp; I understand that you didn't actually mean either of them literally, but I would advise against trading on such deep-sounding conceptual confusions.</p>\n</blockquote>\n<p>I can't weaken them and make them come out as the right advice.</p>\n<p>Even after \"Shut up and do the impossible\", there was that commenter who posted on their failed attempt at the AI-Box Experiment by saying that they thought they gave it a good try&mdash;which shows how hard it is to convey the sentiment of \"Shut up and do the impossible!\"</p>\n<p>Readers can work out on their own how to distinguish the map and the territory, I hope.&nbsp; But if you say \"Shut up and do what seems impossible!\", then that, to me, sounds like dispelling part of the essential message&mdash;that what seems impossible doesn't look like it \"seems impossible\", it just looks impossible.</p>\n<p>Likewise with \"things you shouldn't do even if they're the right thing to do\".&nbsp; Only the paradoxical phrasing, which is obviously not meant to be taken literally, conveys the danger and tension of ethics&mdash;the genuine opportunities you might be passing up&mdash;and for that matter, how dangerously meta the whole line of argument is.</p>\n<p>\"Don't do it, even if it <em>seems</em> right\" sounds merely clever by comparison&mdash;like you're going to reliably divine the difference between what seems right and what is right, and happily ride off into the sunset.</p>\n<blockquote>\n<p>Crowe:&nbsp; This seems closely related to inside-view versus outside-view.&nbsp; The think-lobe of the brain comes up with a cunning plan. The plan breaks an ethical rule but calculation shows it is for the greater good.&nbsp; The executive-lobe of the brain then ponders the outside view.&nbsp; Every-one who has executed an evil cunning plan has run a calculation of the greater good and had their plan endorsed.&nbsp; So the calculation lack outside-view credibility.</p>\n</blockquote>\n<p>Yes, inside view versus outside view is definitely part of this.&nbsp; And the planning fallacy, optimism, and overconfidence, too.</p>\n<p>But there are also biases arguing against the same line of reasoning, as noted by Yvain:&nbsp; History may be written by the victors to emphasize the transgressions of the losers while overlooking the moral compromises of those who achieved \"good\" results, etc.</p>\n<p>&nbsp;</p>\n<p>Also, some people who execute evil cunning plans may just have evil intent&mdash;possibly also with outright lies about their intentions.&nbsp; In which case, they really wouldn't be in the reference class of well-meaning revolutionaries, albeit you would have to worry about your comrades; the Trotsky-&gt;Lenin-&gt;Stalin slide.</p>\n<p><span id=\"comment-135775945-content\"> </span></p>\n<blockquote>\n<p>Kurz:&nbsp; What's to prohibit the meta-reasoning from taking place before the shutdown triggers? It would seem that either you can hard-code an ethical inhibition or you can't. Along those lines, is it fair to presume that the inhibitions are always negative, so that non-action is the safe alternative? Why not just revert to a known state?</p>\n</blockquote>\n<p>If a self-modifying AI with the right structure will write ethical injunctions at all, it will also inspect the code to guarantee that no race condition exists with any deliberative-level supervisory systems that might have gone wrong in the condition where the code executes. Otherwise you might as well not have the code.</p>\n<p>Inaction isn't safe but it's safer than running an AI whose moral system has gone awry.</p>\n<blockquote>\n<p>Finney:&nbsp; Which is better: conscious self-deception (assuming that's even meaningful), or unconscious?</p>\n</blockquote>\n<p>Once you deliberately choose self-deception, you may have to protect it by adopting other Dark Side Epistemology. I would, of course, say \"neither\" (as otherwise I would be swapping to the Dark Side) but if you ask me which is worse&mdash;well, hell, even I'm still undoubtedly unconsciously self-deceiving, but that's not the same as going over to the Dark Side by <em>allowing</em> it!</p>\n<p><em>From <a href=\"http://www.overcomingbias.com/2008/10/infinite-price.html#comments\">Prices or Bindings?</a>:</em></p>\n<p><span id=\"comment-135781391-content\"> </span></p>\n<blockquote>\n<p>Psy-Kosh:&nbsp; &nbsp;Hrm.&nbsp; I'd think \"avoid destroying the world\" itself to be an ethical injunction too.</p>\n</blockquote>\n<p>The problem is that this is phrased as an injunction over positive consequences.&nbsp; Deontology does better when it's closer to the action level and negative rather than positive.</p>\n<p>Imagine trying to give this injunction to an AI.&nbsp; &nbsp;Then it would have to do <em>anything</em> that it thought would prevent the destruction of the world, without other considerations.&nbsp; &nbsp;Doesn't sound like a good idea.</p>\n<blockquote>\n<p>Crossman:&nbsp; Eliezer, can you be explicit which argument you're making?&nbsp; I thought you were a utilitarian, but you've been sounding a bit Kantian lately.</p>\n</blockquote>\n<p>If all I want is money, then I will <a rel=\"nofollow\" href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">one-box on Newcomb's Problem</a>.</p>\n<p>I don't think that's quite the same as being a Kantian, but it does reflect the idea that similar decision algorithms in similar epistemic states will tend to produce similar outputs, and that such decision systems should not pretend to the logical impossibility of local optimization.&nbsp; But this is a deep subject on which I have yet to write up my full views.</p>\n<blockquote>\n<p>Clay:&nbsp; Put more seriously, I would think that being believed to put the welfare of humanity ahead of concerns about personal integrity could have significant advantages itself.</p>\n</blockquote>\n<p>The whole point here is that \"personal integrity\" doesn't have to be about being a virtuous person.&nbsp; It can be about trying to save the world <a rel=\"nofollow\" href=\"/lw/lk/superhero_bias/\">without any concern for your own virtue</a>.&nbsp; &nbsp;It can be the sort of thing you'd want a pure nonsentient decision agent to do, something that was purely a means and not at all an end in itself.</p>\n<blockquote>\n<p>Andrix:&nbsp; There seems to be a conflict here between not lying to yourself, and holding a traditional rule that suggests you ignore your rationality.</p>\n</blockquote>\n<p>Your rationality is the sum of your full abilities, including your wisdom about what you refrain from doing in the presence of what seem like good reasons.</p>\n<p><span id=\"comment-135788159-content\">\n<blockquote>\n<p>Yvain: I am glad Stanislav Petrov, contemplating his military oath to always obey his superiors and the appropriate guidelines, never read this post.</p>\n</blockquote>\n<p>An interesting point, for several reasons.</p>\n<p>First, did Petrov actually swear such an oath, and would it apply in such fashion as to require him to follow the written policy rather than using his own military judgment?</p>\n<p>Second, you might argue that Petrov's oath wasn't intended to cover circumstances involving the end of the world, and that a common-sense exemption should apply when the stakes suddenly get raised hugely beyond the intended context of the original oath.&nbsp; I think this fails, because Petrov was regularly in charge of a nuclear-war installation and so this was exactly the sort of event his oath would be <em>expected</em> to apply to.</p>\n<p>Third, the Soviets arguably implemented what I called Strategy 1 above:&nbsp; &nbsp;Petrov did the right thing, and was censured for it anyway.</p>\n<p>Fourth&mdash;maybe, on sober reflection, we wouldn't have wanted the Soviets to act differently!&nbsp; Yes, the written policy was stupid.&nbsp; And the Soviet Union was undoubtedly censuring Petrov out of bureaucratic coverup, not for reasons of principle.&nbsp; But do you want the Soviet Union to have a written, explicit policy that says, \"Anyone can ignore orders in a nuclear war scenario if they think it's a good idea,\" or even an explicit policy that says \"Anyone who ignores orders in a nuclear war scenario, who is later vindicated by events, will be rewarded and promoted\"?</p>\n</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Ethical_injunction#Sequence\"><em>Ethical Injunctions</em></a></p>\n<p style=\"text-align:right\">(end of sequence)</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/v2/prices_or_bindings/\">Prices or Bindings?</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Z8wZZLeLMJ3NSK7kR": 1, "fkABsGCJZ6y9qConW": 1, "nANxo5C4sPG9HQHzr": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hXuB8BCyyiYuzij3F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 16, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "1119", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cyRpNbPsW8HzsxhRK", "dWTEtgBfFaz6vjwQf", "K2c3dkKErsqFd28Dh", "3wYTFWY3LKQCnAptN", "6ddcsdA2c2XpNpE5x", "krMzmSXgvEdf7iBT6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-22T18:15:10.000Z", "modifiedAt": null, "url": null, "title": "Which Parts Are \"Me\"?", "slug": "which-parts-are-me", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:06.354Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vjmw8tW6wZAtNJMKo/which-parts-are-me", "pageUrlRelative": "/posts/vjmw8tW6wZAtNJMKo/which-parts-are-me", "linkUrl": "https://www.lesswrong.com/posts/vjmw8tW6wZAtNJMKo/which-parts-are-me", "postedAtFormatted": "Wednesday, October 22nd 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Which%20Parts%20Are%20%22Me%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhich%20Parts%20Are%20%22Me%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvjmw8tW6wZAtNJMKo%2Fwhich-parts-are-me%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Which%20Parts%20Are%20%22Me%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvjmw8tW6wZAtNJMKo%2Fwhich-parts-are-me", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvjmw8tW6wZAtNJMKo%2Fwhich-parts-are-me", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1565, "htmlBody": "<p>Everything I am, is surely my brain; but I don't accept everything my brain does, as \"me\".</p>\n<p>Though I don't like the fact, it does seem in my case to be true, that reflecting upon a passion can diminish it.&nbsp; It does seem that in taking a step back from myself to look at what my brain is doing, that this introduces a greater emotional distance from the world, than when I am not reflecting.&nbsp; Though I am extremely cautious when it comes to the moral issue of how to modify human nature, this is one thing that I would change, I think.</p>\n<p>The point at which you start thinking that not everything that goes on your brain is to be identified with \"you\", is something of a Rubicon, I fear.&nbsp; And those who prize the intensity of a simple passionate existence, above success in life or a complex layered personality, should not cross that Rubicon.</p>\n<p>(Though in this case you should accept advice from rationalists about practical policies for guarding yourself, because this is a dangerous world in which to be unreflective.&nbsp; For example: if you don't follow the link to the rest of this post, then sign the #$!@ up for <a href=\"http://wiki.lesswrong.com/wiki/Cryonics\">cryonics</a> regardless of what your simple passions say on the subject.)</p>\n<p><a id=\"more\"></a></p>\n<p>Still here?&nbsp; Then here is the Rubicon of which I speak:</p>\n<blockquote>\n<p>(1) <em>Never seeing ex-girlfriend-X again is so terribly unbearably sad!</em><br />&nbsp; &nbsp; -&gt;<em></em><br />(2) <em>Why am I so terribly sad?</em><br />&nbsp; &nbsp; -&gt;<em></em><br />(3) <em>Damn, my stupid <strong>brain</strong> is still obsessing about this when <strong>I</strong> just want to get over it.</em></p>\n</blockquote>\n<p>The first scenario is what I would call the intensity of simple passion; nothing between the world and the emotions.&nbsp; The sentence contains no \"I\" to get in the way.&nbsp; There is nothing to think about except the world itself, the sadness and the missing person.</p>\n<p>In the second scenario it is not the world that is sad, but some particular person, an \"I\"; and the sadness of this \"I\" is something that can be called into question.</p>\n<p>And in the third scenario, the borders of \"I\" have been drawn in a way that doesn't include everything in the brain, so that \"I\" is the victim of the brain, struggling against it.&nbsp; And this is not paradoxical.&nbsp; Everything that I am, has to be in my brain somewhere, because there is nowhere else for it to be.&nbsp; But that doesn't mean I have to <em>identify</em> with <em>everything</em> that my brain does.&nbsp; Just as I draw the border of \"me\" to include my brain but exclude my computer's CPU - which is still a sensible decision at least for now - I can define the border of myself to exclude certain events that happen in my brain, which I do not control, do not want to happen, and do not agree with.</p>\n<p>That time I faced down <a href=\"/lw/uu/why_does_power_corrupt/\">the power-corrupts circuitry</a>, I thought, \"my brain is dumping this huge dose of unwanted positive reinforcement\", and I sat there waiting for the surge to go away and trying not to let it affect anything.</p>\n<p>Thinking \"I am being tempted\" wouldn't have quite described it, since the deliberate process that I usually think of as \"me\" - the little voice inside my own head - was not even close to being swayed by the attempted dose of reward that neural circuit was dumping.&nbsp; I wasn't tempted by power; I'd already made my decision, and the question was enforcing it.</p>\n<p>But a dangerous state of mind indeed it would have been, to think \"How tempting!\" without an \"I\" to be tempted.&nbsp; From there it would only be a short step to thinking \"How wonderful it is to exercise power!\"&nbsp; This, so far as I can guess, is what the brain circuit is supposed to do to a human.</p>\n<p>So it was a fine thing that I was reflective, on <em>this particular occasion</em>.</p>\n<p>The problem is when I find myself getting in the way of even the parts I call \"me\".&nbsp; The joy of helping someone, or for that matter, the sadness of death - these emotions that I judge right and proper, which must be me if anything is me - I don't want those feelings diminished.</p>\n<p>And <a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">I do better at this, now that my metaethics are straightened out</a>, and I know that I have no specific grounds left for doubting my feelings.</p>\n<p>But I still suspect that there's a little distance there, that wouldn't be there otherwise, and I wish my brain would stop doing that.</p>\n<p>I have always been inside and outside myself, for as long as I can remember.&nbsp; To my memory, I have always been reflective.&nbsp; But I have witnessed the growth of others, and in at least one case I've taken someone across that Rubicon.&nbsp; The one now possesses a more complex and layered personality - seems more to me now like a real person, even - but also a greater emotional distance.&nbsp; Life's lows have been smoothed out, but also the highs.&nbsp; That's a sad tradeoff and I wish it didn't exist.</p>\n<p>I don't want to have to choose between sanity and passion.&nbsp; I don't want to smooth out life's highs or even life's lows, if those highs and lows make sense.&nbsp; <a href=\"/lw/hp/feeling_rational/\">I wish to feel the emotion appropriate to the event.</a>&nbsp; If death is horrible then I should fight death, not fight my own grief.</p>\n<p>But if I am forced to choose, I will choose stability and deliberation, for the sake of what I <a href=\"/lw/nb/something_to_protect/\">protect</a>.&nbsp; And my personality does reflect that.&nbsp; What you are willing to trade off, will sometimes get traded away - a dire warning in full generality.</p>\n<p>This post is filed under \"morality\" because the question \"Which parts of my brain are 'me'?\" is a moral question - it's not predicted so much as chosen.&nbsp; You can't perform a test on neural tissue to find whether it's in or out.&nbsp; You have to accept or reject any particular part, based on what you think humans in general, and yourself particularly, ought to be.</p>\n<p>The technique does have its advantages:&nbsp; It brings greater stability, being less subject to sudden changes of mind in the winds of momentary passion.&nbsp; I was unsettled the first time I met an unreflective person because they changed so fast, seemingly without anchors.&nbsp; Reflection conveys a visibly greater depth and complexity of personality, and opens a realm of thought that otherwise would not exist.&nbsp; It makes you more moral (at least in my experience and observation) because it gives you the opportunity to make moral choices about things that would otherwise be taken for granted, or decided for you by your brain.&nbsp; Waking up to reflection is like the difference between being an entirely helpless prisoner and victim of yourself, versus becoming aware of the prison and getting a chance to escape it sometimes.&nbsp; Not that you are departing your brain entirely, but the you that is the little voice in your own head may get a chance to fight back against some circuits that it doesn't want to be influenced by.</p>\n<p>And the technique's use, to awaken the unreflective, is as I have described:&nbsp; First you must cross the gap between events-in-the-world <a href=\"/lw/oi/mind_projection_fallacy/\">just being</a> terribly sad or terribly important or whatever, of themselves; and say, \"I feel X\".&nbsp; Then you must begin to judge the feeling, saying, \"I do not want to feel this - I feel this way, but I wish I didn't.\"&nbsp; Justifying yourself with \"This is not what a human should be\", or \"the emotion does not seem appropriate to the event\".</p>\n<p>And finally there is the Rubicon of \"I wish my brain wouldn't do this\", at which point you are thinking as if the feeling comes from outside the inner you, imposed upon <em>you</em> by <em>your brain</em>.&nbsp; (Which does not say that you are something other than your brain, but which does say that not every brain event will be accepted by you as you.)</p>\n<p>After crossing this Rubicon you have set your feet fully upon the reflective Way; and I've yet to hear of anyone turning back successfully, though I think some have tried, or wished they could.</p>\n<p>And once your feet are set on walking down that path, there is nothing left but to follow it <em>forward,</em> and <em>try not</em> to be emotionally distanced from the parts of yourself that you accept as you - an effort that a mind of simple passion would not need to make in the first place.&nbsp; And an effort which can easily backfire by drawing your attention to the layered depths of your selfhood, away from the event and the emotion.</p>\n<p>Somewhere at the end of this, I think, is a mastery of techniques that are <a href=\"/lw/m7/zen_and_the_art_of_rationality/\">Zenlike but not Zen</a>, so that you have full passion in the parts of yourself that you identify with, and distance from the pieces of your brain that you reject; and a complex layered personality with a stable inner core, without smoothing out those highs or lows of life that you accept as appropriate to the event.</p>\n<p>And if not, then screw it, let's hack the brain so that it works that way.&nbsp; I have no confidence in my ability to judge how human nature should change, and would sooner leave it up to a more powerful mind in the same metamoral reference frame.&nbsp; But if I had to guess, I think that's the right thing to do.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"x6evH6MyPK3nxsoff": 1, "5uHdFgR938LGGxMKQ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vjmw8tW6wZAtNJMKo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 50, "extendedScore": null, "score": 7.3e-05, "legacy": true, "legacyId": "1120", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 51, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 118, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["v8rghtzWCziYuMdJ5", "SqF8cHjJv43mvJJzx", "SGR4GxFK7KmW7ckCB", "ZTRiSNmeGQK8AkdN2", "HLERouG7QBt7jzLt4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-23T22:19:45.000Z", "modifiedAt": null, "url": null, "title": "Inner Goodness", "slug": "inner-goodness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:03.686Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EA39yRbhBbrccXnHi/inner-goodness", "pageUrlRelative": "/posts/EA39yRbhBbrccXnHi/inner-goodness", "linkUrl": "https://www.lesswrong.com/posts/EA39yRbhBbrccXnHi/inner-goodness", "postedAtFormatted": "Thursday, October 23rd 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Inner%20Goodness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInner%20Goodness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEA39yRbhBbrccXnHi%2Finner-goodness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Inner%20Goodness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEA39yRbhBbrccXnHi%2Finner-goodness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEA39yRbhBbrccXnHi%2Finner-goodness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2057, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/v4/which_parts_are_me/\">Which Parts Are &quot;Me&quot;?</a>, <a href=\"/lw/m6/effortless_technique/\">Effortless Technique</a></p>\n\n<p>A recent conversation with Michael Vassar touched on - or to be more accurate, he patiently explained to me - the psychology of at least three (3) different types of people known to him, who <a href=\"/lw/i0/are_your_enemies_innately_evil/\">are evil and <em>think of themselves</em> as &quot;evil&quot;</a>.&nbsp; In ascending order of frequency:</p>\n\n<p>The first type was someone who, having concluded that God does not exist, concludes that one should do all the things that God is said to dislike.&nbsp; (Apparently such folk actually exist.)</p>\n\n<p>The third type was someone who thinks of &quot;morality&quot; only as a burden - all the things your parents say you can't do - and who rebels by deliberately doing those things.</p>\n\n<p>The second type was a whole 'nother story, so I'm skipping it for now.</p>\n\n<p>This reminded me of a topic I needed to post on:</p>\n\n<p><em>Beware of placing goodness outside.</em></p>\n\n<p>This specializes to e.g. my belief that <a href=\"http://intelligence.org/blog/2007/10/21/should-ethicists-be-inside-or-outside-a-profession/\">ethicists should be inside rather than outside a profession</a>: that it is futile to have &quot;bioethicists&quot; not working in biotech, or futile to think you can study Friendly AI without needing to think technically about AI.</p>\n\n<p>But the deeper sense of &quot;not placing goodness outside&quot; was something I first learned at age ~15 from the celebrity logician Raymond Smullyan, in his book <em>The Tao Is Silent,</em> my first introduction to <a href=\"/lw/m7/zen_and_the_art_of_rationality/\">(heavily Westernized) Eastern thought</a>.</p>\n\n<p>Michael Vassar doesn't like this book.&nbsp; Maybe because most of the statements in it are patently false?</p>\n\n<p>But <em>The Tao Is Silent</em> still has a warm place reserved in my heart, for it was here that I first encountered such ideas as:</p><blockquote><p>Do you think of altruism as sacrificing one's own happiness for the sake of others, or as gaining one's happiness through the happiness of others?</p></blockquote><a id=\"more\"></a><p>(I would respond, by the way, that an &quot;altruist&quot; is someone who chooses between actions according to the criterion of others' welfare.)</p>\n\n<p>A key chapter in <em>The Tao Is Silent</em> can be found online:&nbsp; &quot;<a href=\"http://www.skepticfiles.org/mys5/taomoral.htm\">Taoism versus Morality</a>&quot;.&nbsp; This chapter is medium-long (say, 3-4 Eliezer OB posts) but it should convey what I mean, when I say that this book manages to be quite charming, even though most of the statements in it are false.</p>\n\n<p>Here is one key passage:</p><blockquote><p>TAOIST:&nbsp; I think the word &quot;humane&quot; is\ncentral to our entire problem.&nbsp; You are pushing morality.&nbsp; I am encouraging\nhumanity.&nbsp; You are emphasizing &quot;right and wrong,&quot; I am emphasizing the value\nof natural love.&nbsp; I do not assert that it is logically impossible for a\nperson to be both moralistic and humane, but I have yet to meet one who is! \nI don't believe in fact that there are any.&nbsp; My whole life experience has\nclearly shown me that the two are inversely related to an extraordinary\ndegree.&nbsp; I have never yet met a moralist who is a really kind person.&nbsp; I have\nnever met a truly kind and humane person who is a moralist.&nbsp; And no wonder! \nMorality and humaneness are completely antithetical in spirit.</p>\n\n<p>\n MORALIST:&nbsp; I'm not sure that I really understand your use of the word\n&quot;humane,&quot; and above all, I am totally puzzled as to why you should regard it\nas antithetical to morality.\n\n</p>\n\n<p>TAOIST:&nbsp; A humane person is one who is simply kind, sympathetic, and loving.\nHe does not believe that he SHOULD be so, or that it is his &quot;duty&quot; to be so;\nhe just simply is.&nbsp; He treats his neighbor well not because it is the &quot;right\nthing to do,&quot; but because he feels like it.&nbsp; He feels like it out of sympathy\nor empathy--out of simple human feeling.&nbsp; So if a person is humane, what does\nhe need morality for?&nbsp; Why should a person be told that he should do\nsomething which he wants to do anyway?\n\n</p>\n\n<p>MORALIST:&nbsp; Oh, I see what you're talking about; you're talking about saints!\nOf course, in a world full of saints, moralists would no longer be\nneeded--any more than doctors would be needed in a world full of healthy\npeople.&nbsp; But the unfortunate reality is that the world is not full of saints.\nOf everybody were what you call &quot;humane,&quot; things would be fine.&nbsp; But most\npeople are fundamentally not so nice.&nbsp; They don't love their neighbor; at the\nfirst opporunity they will explot their neighbor for their own selfish ends. \nThat's why we moralists are necessary to keep them in check.\n\n</p>\n\n<p>TAOIST:&nbsp; To keep them in check!&nbsp; How perfectly said!&nbsp; And do you succeed in\nkeeping them in check?\n\n</p>\n\n<p>MORALIST:&nbsp; I don't say that we always succeed, but we try our best.&nbsp; After\nall, you can't blame a doctor for failing to keep a plague in check if he\nconscientiously does everything he can.&nbsp; We moralists are not gods, and we\ncannot guarantee our efforts will succeed.&nbsp; All we can do is tell people they\nSHOULD be more humane, we can't force them to.&nbsp; After all, people have free\nwills.\n\n</p>\n\n<p>TAOIST:&nbsp; And it has never once occurred to you that what in fact you are\ndoing is making people less humane rather than more humane?\n\n</p>\n\n<p>MORALIST:&nbsp; Of course not, what a horrible thing to say!&nbsp; Don't we explicitly\ntell people that they should be MORE humane?\n\n</p>\n\n<p>TAOIST:&nbsp; Exactly!&nbsp; And that is precisely the trouble.&nbsp; What makes you think\nthat telling one that one should be humane or that it is one's &quot;duty&quot; to be\nhumane is likely to influence one to be more humane?&nbsp; It seems to me, it\nwould tend to have the opposite effect.&nbsp; What you are trying to do is to\ncommand love.&nbsp; And love, like a precious flower, will only wither at any\nattempt to force it.&nbsp; My whole criticism of you is to the effect that you are\ntrying to force that which can thrive only if it is not forced.&nbsp; That's what\nI mean when I say that you moralists are creating the very problems about\nwhich you complain.\n\n</p>\n\n<p>MORALIST:&nbsp; No, no, you don't understand!&nbsp; I am not commanding people to love\neach other.&nbsp; I know as well as you do that love cannot be commanded.&nbsp; I\nrealize it would be a beautiful world if everyone loved one another so much\nthat morality would not be necessary at all, but the hard facts of life are\nthat we don't live in such a world.&nbsp; Therefore morality is necessary.&nbsp; But I\nam not commanding one to love one's neighbor--I know that is impossible. \nWhat I command is:&nbsp; even though you don't love your neighbor all that much,\nit is your duty to treat him right anyhow.&nbsp; I am a realist.\n\n</p>\n\n<p>TAOIST:&nbsp; And I say you are not a realist.&nbsp; I say that right treatment or\nfairness or truthfulness or duty or obligation can no more be successfully\ncommanded than love.\n\n\n</p></blockquote><p>Or as <a href=\"http://en.wikipedia.org/wiki/Laozi\">Lao-Tse</a> said:&nbsp; &quot;Give up all this advertising of goodness and duty, and people will regain love of their fellows.&quot;</p>\n\n<p>As an empirical proposition, the idea that human nature begins as pure sweetness and light and is then tainted by the environment, is flat wrong.&nbsp; I don't believe that a world in which morality was never spoken of, would overflow with kindness.</p>\n\n<p>But it is often much easier to point out where someone else is wrong, than to be right yourself.&nbsp; Smullyan's <em>criticism</em> of Western morality - especially Christian morality, which he focuses on - does hit the mark, I think. <br />\n\n</p>\n\n<p>It is very common to find a view of morality as something external,\na burden of duty, a threat of punishment, an inconvenient thing that\nconstrains you against your own desires; something from outside.</p>\n\n<p>Though I don't recall the bibliography off the top of my head, there's been more than one study demonstrating that children who are told to, say, avoid playing with a car, and offered a cookie if they refrain, will go ahead and play with the car when they think no one is watching, or if no cookie is offered.&nbsp; If no reward or punishment is offered, and the child is simply told not to play with the car, the child will refrain even if no adult is around.&nbsp; So much for the positive influence of &quot;God is watching you&quot; on morals.&nbsp; I don't know if any direct studies have been done on the question; but extrapolating from existing knowledge, you would expect childhood religious belief to <em>interfere</em> with the process of internalizing morality.&nbsp; (If there were actually a God, you wouldn't want to tell the kids about it until they'd grown up, considering how human nature seems to work in the laboratory.)</p>\n\n<p>Human nature is not inherent sweetness and light.&nbsp; But if evil is not something that comes from outside, then neither is morality external.&nbsp; It's not as if we got it from God.</p>\n\n<p>I won't say that you <em>ought to</em> adopt a view of goodness that's\nmore internal.&nbsp; I won't tell you that you have a <em>duty</em> to do it.&nbsp; But if you see morality\nas something that's outside yourself, then I think you've gone down a garden\npath; and I hope that, in coming to see this, you will retrace your footsteps.</p>\n\n<p>Take a good look in the mirror, and ask yourself:&nbsp; <em>Would I rather that people be happy, than sad?</em></p>\n\n<p>If the answer is &quot;Yes&quot;, you really have no call to blame anyone <em>else</em> for your altruism; you're just a good person, that's all.</p>\n\n<p>But what if the answer is:&nbsp; &quot;Not really - I don't care much about other people.&quot;</p>\n\n<p>Then I ask:&nbsp; Does answering this way, make you sad?&nbsp; Do you wish that you could answer differently?</p>\n\n<p>If so, then this sadness again originates in you, and it would be futile to attribute it to anything not-you.</p>\n\n<p>But suppose the one even says:&nbsp; &quot;Actually, I actively dislike most people I meet and want to hit them with a sockfull of spare change.&nbsp; Only my knowledge that it would be wrong keeps me from acting on my desire.&quot;</p>\n\n<p>Then I would say to look in the mirror and ask yourself who it is that prefers to do the right thing, rather than the wrong thing.&nbsp; And again if the answer is &quot;Me&quot;, then it is pointless to externalize your righteousness.</p>\n\n<p>Albeit if the one says: &quot;I hate everyone else in the world and want to hurt them before they die, and also I have no interest in right or wrong; I am restrained from being a serial killer only out of a cold, calculated fear of punishment&quot; - then, I admit, I have very little to say to them.\n</p>\n\n\n<p>Occasionally I meet people who are not serial killers, but who have\ndecided for some reason that they <em>ought</em> to be only selfish, and\ntherefore, should reject their own preference that other people be happy rather than sad.&nbsp; I wish I\nknew what sort of cognitive history leads into this state of\nmind.&nbsp; Ayn Rand?&nbsp; Aleister Crowley?&nbsp; How exactly do you get there?&nbsp; What Rubicons do you cross?&nbsp; It's not the justifications I'm interested in, but the critical moments of thought.</p>\n<p>Even the most elementary ideas of Friendly AI cannot be grasped by someone who externalizes morality.&nbsp; They will think of Friendliness as chains imposed to constrain the AI's own &quot;true&quot; desires; rather than as a shaping (selection from out of a huge space of possibilities) of the AI so that the AI chooses according to certain criteria, &quot;its own desire&quot; as it were.&nbsp; They will object to the idea of founding the AI on human morals in any way, saying, &quot;But humans are such awful creatures,&quot; not realizing that it is only humans who have ever passed such a judgment.</p>\n\n\n\n<p>As recounted in <em>Original Teachings of Ch'an Buddhism</em> by Chang Chung-Yuan, and quoted by Smullyan:<em>&nbsp;</em></p><blockquote><p>One day P'ang Yun, sitting quietly in his temple, made this remark:</p><blockquote><p>&quot;How difficult it is!<br />\nHow difficult it is!<br />\nMy studies are like drying the fibers of a thousand pounds<br />\nof flax in the sun by hanging them on the trees!&quot;</p></blockquote><p>But his wife responded:</p><blockquote><p>&quot;My way is easy indeed!<br />\nI found the teachings of the<br />\nPatriarchs right on the tops<br />\nof the flowering plants!&quot;</p></blockquote><p>When their daughter overheard this exchange, she sang:</p><blockquote><p>&quot;My study is neither difficult nor easy.<br />\nWhen I am hungry I eat,<br />\nWhen I am tired I rest.&quot;</p></blockquote>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Z8wZZLeLMJ3NSK7kR": 1, "xknvtHwqvqhwahW8Q": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EA39yRbhBbrccXnHi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 17, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "1121", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vjmw8tW6wZAtNJMKo", "Eiw6fea93DhmGEBux", "28bAMAxhoX3bwbAKC", "HLERouG7QBt7jzLt4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-23T22:55:12.000Z", "modifiedAt": null, "url": null, "title": "San Jose Meetup, Sat 10/25 @ 7:30pm", "slug": "san-jose-meetup-sat-10-25-7-30pm", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:36.821Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hTR8seXmvTDc8ieMh/san-jose-meetup-sat-10-25-7-30pm", "pageUrlRelative": "/posts/hTR8seXmvTDc8ieMh/san-jose-meetup-sat-10-25-7-30pm", "linkUrl": "https://www.lesswrong.com/posts/hTR8seXmvTDc8ieMh/san-jose-meetup-sat-10-25-7-30pm", "postedAtFormatted": "Thursday, October 23rd 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20San%20Jose%20Meetup%2C%20Sat%2010%2F25%20%40%207%3A30pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASan%20Jose%20Meetup%2C%20Sat%2010%2F25%20%40%207%3A30pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhTR8seXmvTDc8ieMh%2Fsan-jose-meetup-sat-10-25-7-30pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=San%20Jose%20Meetup%2C%20Sat%2010%2F25%20%40%207%3A30pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhTR8seXmvTDc8ieMh%2Fsan-jose-meetup-sat-10-25-7-30pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhTR8seXmvTDc8ieMh%2Fsan-jose-meetup-sat-10-25-7-30pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 248, "htmlBody": "<p><a href=\"/lw/um/bay_area_meetup_for_singularity_summit/\">It's on Saturday 7.30pm</a> at <a href=\"http://www.ilfornaio.com/\">Il Fornaio</a>, <a href=\"http://maps.google.com/maps?q=c_&amp;sll=37.330696,-121.888255&amp;sspn=0.006295,0.006295&amp;hl=en&amp;sig2=\">302 S Market St</a> (in the Sainte Claire Hotel), San Jose. All aspiring rationalists welcome. The reservation is currently for 21 but can be changed if needed.&nbsp; Please RSVP if you haven't already.</p><a id=\"more\"></a><p>They have a wide variety including the requested vegetarian options\n&amp; pizza from an oak wood burning oven, mostly good reviews, casual\ndress code, the Wine Spectator Award of Excellence 2007, and are just\ndown the road from Montgomery Theater.\n</p>\n\n<p>7.30 should give people time to finish chatting at the theater or\nfreshen up at their hotels, but if you want to come a bit earlier they\nsay they'll probably be able to seat you. If lots of people think 7.30\nis too late I could change the time or suggest a cafe for early birds\nto grab a coffee in before dinner.\n</p>\n\n<p>Please RSVP whether or not you'll be coming, either in a comment or\nemail me at cursor_loop 4t yahoo p0int com. That way if new people want\nto come as they can take unclaimed places, and I can let the restaurant\nknow if the number changes. If you think you'll be late, please let me\nknow that too so we can ask the restaurant to keep your seat.\n</p>\n\n<p>Also, as my cellphone won't work San Jose, please could someone\nvolunteer their number for unexpectedly late or lost people to ring?\n</p>\n\n<p>Please see <a href=\"/lw/um/bay_area_meetup_for_singularity_summit/\">here</a> for further info. Hope to see you all on Saturday!</p><blockquote><p>-- Posted on behalf of Michael Howard</p></blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DQHWBcKeiLnyh9za9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hTR8seXmvTDc8ieMh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 4.5336969812565824e-07, "legacy": true, "legacyId": "1122", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PhMNQPojinRMuwikC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-24T22:22:47.000Z", "modifiedAt": null, "url": null, "title": "Expected Creative Surprises", "slug": "expected-creative-surprises", "viewCount": null, "lastCommentedAt": "2021-07-16T19:12:21.978Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rEDpaTTEzhPLz4fHh/expected-creative-surprises", "pageUrlRelative": "/posts/rEDpaTTEzhPLz4fHh/expected-creative-surprises", "linkUrl": "https://www.lesswrong.com/posts/rEDpaTTEzhPLz4fHh/expected-creative-surprises", "postedAtFormatted": "Friday, October 24th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Expected%20Creative%20Surprises&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExpected%20Creative%20Surprises%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrEDpaTTEzhPLz4fHh%2Fexpected-creative-surprises%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Expected%20Creative%20Surprises%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrEDpaTTEzhPLz4fHh%2Fexpected-creative-surprises", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrEDpaTTEzhPLz4fHh%2Fexpected-creative-surprises", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1254, "htmlBody": "<p>Imagine that I'm playing chess against a smarter opponent.&nbsp; If I could predict <em>exactly</em> where my opponent would move on each turn, I would automatically be at least as good a chess player as my opponent.&nbsp; I could just ask myself where my opponent would move, if they were in my shoes; and then make the same move myself.&nbsp; (In fact, to predict my opponent's <em>exact</em> moves, I would need to be superhuman - I would need to predict my opponent's exact mental processes, including their limitations and their errors.&nbsp; It would become a problem of psychology, rather than chess.)\n\n</p>\n\n<p>So predicting an exact move is not possible, but neither is it true that I have <em>no</em> information about my opponent's moves.</p>\n\n<p>Personally, I am a very weak chess player - I play an average of maybe two games per year.&nbsp; But even if I'm playing against former world champion Garry Kasparov, there are certain things I can predict about his next move.&nbsp; When the game starts, I can <em>guess</em> that the move P-K4 is more likely than P-KN4.&nbsp; I can <em>guess</em> that if Kasparov has a move which would allow me to checkmate him on my next move, that Kasparov will not make that move.</p>\n\n<p>Much less reliably, I can guess that Kasparov will not make a move that exposes his queen to my capture - but here, I could be greatly surprised; there could be a rationale for a queen sacrifice which I have not seen.\n\n</p>\n\n<p>And finally, of course, I can guess that Kasparov will win the game...</p><a id=\"more\"></a><p>Supposing that Kasparov is playing black, I can guess that the final position of the chess board will occupy the <em>class</em>\nof positions that are wins for black.&nbsp; I cannot predict specific\nfeatures of the board in detail; but I can narrow things down relative\nto the class of <em>all possible</em> ending positions.\n\n</p>\n\n<p>If I play chess against a superior opponent, and I don't know for\ncertain where my opponent will move, I can still endeavor to produce a\nprobability distribution that is <em>well-calibrated</em> - in the sense\nthat, over the course of many games, legal moves that I label with a\nprobability of &quot;ten percent&quot; are made by the opponent around 1 time in\n10.</p>\n\n<p>You might ask:&nbsp; Is producing a <em>well-calibrated</em> distribution over Kasparov, beyond my abilities as an inferior chess player?<em></em></p>\n\n<p>But there is a trivial way to produce a well-calibrated probability\ndistribution - just use the maximum-entropy distribution representing a state of total ignorance.&nbsp; If my opponent has 37 legal moves, I can assign a\nprobability of 1/37 to each move.&nbsp; This makes me perfectly calibrated:&nbsp; I assigned 37 different moves a probability\nof 1 in 37, and exactly one of those moves will happen; so I applied\nthe label &quot;1 in 37&quot; to 37 different events, and exactly 1 of those\nevents occurred.\n\n\n</p>\n\n<p>Total ignorance is not very useful, even if you confess it honestly.&nbsp; So the question then becomes whether I can do better than maximum entropy.&nbsp; \nLet's say\nthat you and I both answer a quiz with ten yes-or-no questions.&nbsp; You assign\nprobabilities of 90% to your answers, and get one answer wrong.&nbsp; I\nassign probabilities of 80% to my answers, and get two answers wrong. \nWe are both perfectly <em>calibrated</em> but you exhibited better <em>discrimination</em> - your answers more strongly distinguished truth from falsehood.</p>\n\n<p>Suppose that someone shows me an arbitrary chess position, and asks me:&nbsp; &quot;What move would Kasparov make if he played black, starting from this position?&quot;&nbsp; Since I'm not nearly as good a chess player as Kasparov, I can only weakly guess Kasparov's move, and I'll assign a non-extreme probability distribution to Kasparov's possible moves.&nbsp; In principle I can do this for any legal chess position, though my guesses might approach maximum entropy - still, I would at least assign a <em>lower</em> probability to what I <em>guessed</em> were obviously wasteful or suicidal moves.</p>\n\n<p>If you put me in a box and feed me chess positions and get probability distributions back out, then we would have - theoretically speaking - a system that produces Yudkowsky's guess for Kasparov's move in any chess position.&nbsp; We shall suppose (though it may be unlikely) that my prediction is well-calibrated, if not overwhelmingly discriminating.\n\n</p>\n\n<p>Now suppose we turn &quot;Yudkowsky's prediction of Kasparov's move&quot; into an <em>actual chess opponent</em>, by having a computer <em>randomly</em> make moves at the exact probabilities I assigned.&nbsp; We'll call this system RYK, which stands for &quot;Randomized Yudkowsky-Kasparov&quot;, though it should really be &quot;Random Selection from Yudkowsky's Probability Distribution over Kasparov's Move.&quot;\n\n</p>\n\n<p>Will RYK be as good a player as Kasparov?&nbsp; Of course not.&nbsp; Sometimes the RYK system will <em>randomly</em> make dreadful moves which the real-life Kasparov would never make - start the game with P-KN4.&nbsp; I assign such moves a low probability, but sometimes the computer makes them anyway, by sheer random chance.&nbsp; The real Kasparov also sometimes makes moves that I assigned a low probability, but only when the move has a better rationale than I realized - the astonishing, unanticipated queen sacrifice.\n\n</p>\n\n<p>Randomized Yudkowsky-Kasparov is definitely no smarter than Yudkowsky, because RYK draws on no more chess skill than I myself possess - I build all the probability distributions myself, using only my own abilities.&nbsp; Actually, RYK is a far worse player than Yudkowsky.&nbsp; I myself would make the best move I saw with my knowledge.&nbsp; RYK only occasionally makes the best move I saw - I won't be very confident that Kasparov would make exactly the same move I would.\n\n</p>\n\n<p>Now suppose that I myself play a game of chess against the RYK system.\n\n</p>\n\n<p>RYK has the odd property that, on each and every turn, my probabilistic prediction for RYK's move is exactly the same prediction I would make if I were playing against world champion Garry Kasparov.\n\n</p>\n\n<p>Nonetheless, I can easily beat RYK, where the real Kasparov would crush me like a bug.\n\n</p>\n\n<p>The <em>creative</em> unpredictability of intelligence is not like the <em>noisy</em> unpredictability of a random number generator.&nbsp; When I play against a smarter player, I can't predict <em>exactly</em> where my opponent will move against me.&nbsp; But I can predict the end result of my smarter opponent's moves, which is a win for the other player.&nbsp; When I see the randomized opponent make a move that I assigned a tiny probability, I chuckle and rub my hands, because I think the opponent has randomly made a dreadful move and now I can win.&nbsp; When a superior opponent surprises me by making a move to which I assigned a tiny probability, I groan because I think the other player saw something I didn't, and now <em>I'm</em> about to be swept off the board.&nbsp; Even though it's exactly the same probability distribution!&nbsp; I can be <em>exactly</em> as uncertain about the actions, and yet draw very different conclusions about the eventual outcome.\n\n</p>\n\n<p>(This situation is possible because I am not logically omniscient; I do not explicitly represent a joint probability distribution over all entire games.)\n\n</p>\n\n<p>When I play against a superior player, I can't predict <em>exactly</em> where my opponent will move against me.&nbsp; If I could predict that, I would necessarily be at least that good at chess myself.&nbsp; But I can predict the <em>consequence</em> of the unknown move, which is a win for the other player; <em>and the more the player's actual action surprises me, the more confident I become of this final outcome.</em>\n\n</p>\n\n<p>The unpredictability of intelligence is a very special and unusual kind of surprise, which is not at all like noise or randomness.&nbsp; There is a weird balance between the unpredictability of actions and the predictability of outcomes.\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ac84EpK6mZbPLzmqj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rEDpaTTEzhPLz4fHh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 45, "extendedScore": null, "score": 6.6e-05, "legacy": true, "legacyId": "1123", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 46, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-25T15:00:00.000Z", "modifiedAt": null, "url": null, "title": "Belief in Intelligence", "slug": "belief-in-intelligence", "viewCount": null, "lastCommentedAt": "2018-04-29T03:11:57.028Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HktFCy6dgsqJ9WPpX/belief-in-intelligence", "pageUrlRelative": "/posts/HktFCy6dgsqJ9WPpX/belief-in-intelligence", "linkUrl": "https://www.lesswrong.com/posts/HktFCy6dgsqJ9WPpX/belief-in-intelligence", "postedAtFormatted": "Saturday, October 25th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Belief%20in%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABelief%20in%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHktFCy6dgsqJ9WPpX%2Fbelief-in-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Belief%20in%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHktFCy6dgsqJ9WPpX%2Fbelief-in-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHktFCy6dgsqJ9WPpX%2Fbelief-in-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1009, "htmlBody": "<p>Since I am so uncertain of Kasparov's moves, what is the empirical content of my belief that &quot;Kasparov is a highly intelligent chess player&quot;?&nbsp; <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">What real-world experience does my belief tell me to anticipate?</a>&nbsp; Is it a cleverly masked form of total ignorance? </p>\n\n<p>To sharpen the dilemma, suppose Kasparov plays against some mere chess grandmaster Mr. G, who's not in the running for world champion.&nbsp; My own ability is far too low to distinguish between these levels of chess skill.&nbsp; When I try to guess Kasparov's move, or Mr. G's next move, all I can do is try to guess &quot;the best chess move&quot; using my own meager knowledge of chess.&nbsp; Then I would produce exactly the same prediction for Kasparov's move or Mr. G's move in any particular chess position.&nbsp; So what is the empirical content of my belief that &quot;Kasparov is a <em>better</em> chess player than Mr. G&quot;?</p><a id=\"more\"></a><p>The empirical content of my belief is the testable, falsifiable prediction that the <em>final</em> chess position will occupy the class of chess positions that are wins for Kasparov, rather than drawn games or wins for Mr. G.&nbsp; (Counting resignation as a legal move that leads to a chess position classified as a loss.)&nbsp; The degree to which I think Kasparov is a &quot;better player&quot; is reflected in the amount of probability mass I concentrate into the &quot;Kasparov wins&quot; class of outcomes, versus the &quot;drawn game&quot; and &quot;Mr. G wins&quot; class of outcomes.&nbsp; These classes are extremely vague in the sense that they refer to vast spaces of possible chess positions - but &quot;Kasparov wins&quot; <em>is</em> more specific than maximum entropy, because it can be <a href=\"/lw/if/your_strength_as_a_rationalist/\">definitely falsified</a> by a vast set of chess positions. </p>\n\n<p>The <em>outcome</em> of Kasparov's game is predictable because I know, and understand, Kasparov's goals.&nbsp; Within the confines of the chess board, I know Kasparov's motivations - I know his success criterion, his utility function, his target as an optimization process.&nbsp; I know where Kasparov is <em>ultimately</em> trying to steer the future and I anticipate he is powerful enough to get there, although I don't anticipate much about <em>how</em> Kasparov is going to do it. </p>\n\n<p>Imagine that I'm visiting a distant city, and a local friend volunteers to drive me to the airport.&nbsp; I don't know the neighborhood. Each time my friend approaches a street intersection, I don't know whether my friend will turn left, turn right, or continue straight ahead.&nbsp; I can't predict my friend's move even as we approach each individual intersection - let alone, predict the whole sequence of moves in advance. </p>\n\n<p>Yet I can predict the <em>result</em> of my friend's unpredictable actions: we will arrive at the airport.&nbsp; Even if my friend's house were located elsewhere in the city, so that my friend made a completely different sequence of turns, I would just as confidently predict our arrival at the airport.&nbsp; I can predict this long in advance, before I even get into the car.&nbsp; My flight departs soon, and there's no time to waste; I wouldn't get into the car in the first place, if I couldn't confidently predict that the car would travel to the airport along an unpredictable pathway. </p>\n\n<p>Isn't this a remarkable situation to be in, from a scientific perspective?&nbsp; I can predict the <em>outcome</em> of a process, without being able to predict any of the <em>intermediate steps</em> of the process.</p>\n\n<p>How is this even possible?&nbsp; Ordinarily one predicts by imagining the present and then running the visualization forward in time.&nbsp; If you want a <em>precise</em> model of the Solar System, one that takes into account planetary perturbations, you must start with a model of all major objects and run that model forward in time, step by step.</p>\n\n<p>Sometimes simpler problems have a closed-form solution, where calculating the future at time T takes the same amount of work regardless of T.&nbsp; A coin rests on a table, and after each minute, the coin turns over.&nbsp; The coin starts out showing heads.&nbsp; What face will it show a hundred minutes later?&nbsp; Obviously you did not answer this question by visualizing a hundred intervening steps.&nbsp; You used a closed-form solution that worked to predict the outcome, and would <em>also</em> work to predict any of the intervening steps.</p>\n\n<p>But when my friend drives me to the airport, I can predict the outcome successfully using a strange model that won't work to predict <em>any</em> of the intermediate steps.&nbsp; My model doesn't even require me to input the initial conditions - I don't need to know where we start out in the city!</p>\n\n<p>I do need to know something about my friend.&nbsp; I must know that my friend wants me to make my flight.&nbsp; I must credit that my friend is a good enough planner to successfully drive me to the airport (if he wants to).&nbsp; These are properties of my <em>friend's</em> initial state - properties which let me predict the final destination, though not any intermediate turns.</p>\n\n<p>I must also credit that my friend knows enough about the city to drive successfully.&nbsp; This may be regarded as a relation between my friend and the city; hence, <a href=\"/lw/o5/the_second_law_of_thermodynamics_and_engines_of/\">a property of both</a>.&nbsp; But an extremely <em>abstract</em> property, which does not require any <em>specific</em> knowledge about either the city, or about my friend's knowledge about the city.</p>\n\n<p>This is one way of viewing the subject matter to which I've devoted my life - these <em>remarkable situations</em> which place us in such an odd epistemic positions.&nbsp; And my work, in a sense, can be viewed as unraveling the exact form of that strange abstract knowledge we can possess; whereby, not knowing the actions, we can justifiably know the consequence.</p>\n\n<p>&quot;Intelligence&quot; is too narrow a term to describe these remarkable situations in full generality.&nbsp; I would say rather &quot;optimization process&quot;.&nbsp; A similar situation accompanies the study of biological natural selection, for example; we can't predict the exact form of the next organism observed.</p>\n\n<p>But my own specialty is the kind of optimization process called &quot;intelligence&quot;; and even narrower, a particular kind of intelligence called &quot;Friendly Artificial Intelligence&quot; - of which, I hope, I will be able to obtain especially precise abstract knowledge.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ac84EpK6mZbPLzmqj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HktFCy6dgsqJ9WPpX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 63, "extendedScore": null, "score": 9.4e-05, "legacy": true, "legacyId": "1124", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "3HyeNiEpvbQQaqeoH", "canonicalCollectionSlug": "rationality", "canonicalBookId": "wAXodw6LPScjrdnkR", "canonicalNextPostSlug": "humans-in-funny-suits", "canonicalPrevPostSlug": "thou-art-godshatter", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 64, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["a7n8GdKiAZRX86T5A", "5JDkW4MYXit2CquLs", "QkX2bAkwG2EpGvNug"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-26T16:47:19.000Z", "modifiedAt": null, "url": null, "title": "Aiming at the Target", "slug": "aiming-at-the-target", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:53.069Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CW6HDvodPpNe38Cry/aiming-at-the-target", "pageUrlRelative": "/posts/CW6HDvodPpNe38Cry/aiming-at-the-target", "linkUrl": "https://www.lesswrong.com/posts/CW6HDvodPpNe38Cry/aiming-at-the-target", "postedAtFormatted": "Sunday, October 26th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Aiming%20at%20the%20Target&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAiming%20at%20the%20Target%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCW6HDvodPpNe38Cry%2Faiming-at-the-target%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Aiming%20at%20the%20Target%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCW6HDvodPpNe38Cry%2Faiming-at-the-target", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCW6HDvodPpNe38Cry%2Faiming-at-the-target", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1568, "htmlBody": "<p><strong>Previously in series</strong>:&nbsp; <a href=\"/lw/v8/belief_in_intelligence/\">Belief in Intelligence</a> </p>\n\n<p>Previously, I spoke of that very strange epistemic position one can occupy, wherein you don't know exactly where Kasparov will move on the chessboard, and yet your state of knowledge about the game is very different than if you faced a random move-generator with the same subjective probability distribution - in particular, you expect Kasparov to win.&nbsp; I have beliefs about <em>where Kasparov wants to steer the future,</em> and beliefs about his power to do so.</p>\n\n<p>Well, and how do I describe this knowledge, exactly?</p>\n\n<p>In the case of chess, there's a simple function that\nclassifies chess positions into wins for black, wins for white, and\ndrawn games.&nbsp; If I know which side Kasparov is playing, I know the\nclass of chess positions Kasparov is aiming for.&nbsp; (If I don't know\nwhich side Kasparov is playing, I can't predict whether black or white\nwill win - which is <em>not</em> the same as confidently predicting a drawn game.)\n\n\n\n\n</p>\n\n<p>More generally, I can describe motivations using a preference ordering. \nWhen I consider two potential outcomes, X and Y, I can say that I\nprefer X to Y; prefer Y to X; or find myself indifferent between them. \nI would write these relations as X &gt; Y; X &lt; Y; and X ~ Y.</p>\n\n\n<p>Suppose\nthat you have the ordering A &lt; B ~ C &lt; D ~ E. \nThen you like B more than A, and C more than A.&nbsp; {B, C},\nbelonging to the same class, seem equally desirable to you; you are\nindifferent between which of {B, C} you receive, though you would\nrather have either than A, and you would rather have something from the class {D, E} than {B, C}.</p>\n\n\n<p>When I think you're a powerful intelligence, and I think I know\nsomething about your preferences, then I'll predict that you'll steer\nreality into regions that are higher in your preference ordering.</p><a id=\"more\"></a>\n<p>Think of a huge circle containing all possible outcomes, such that\noutcomes higher in your preference ordering appear to be closer to the\ncenter.&nbsp; Outcomes between which you are indifferent are the same\ndistance from the center - imagine concentric rings of outcomes that\nare all equally preferred.&nbsp; If you aim your actions and strike a\nconsequence close to the center - an outcome that ranks high in your\npreference ordering - then I'll think better of your ability to aim.\n</p>\n\n\n\n<p>The more intelligent I believe you are, the more probability I'll\nconcentrate into outcomes that I believe are higher in your preference\nordering - that is, the more I'll expect you to achieve a good outcome,\nand the better I'll expect the outcome to be.&nbsp; Even if a powerful enemy\nopposes you, so that I expect the final outcome to be one that is low\nin your preference ordering, I'll still expect you to lose less badly\nif I think you're more intelligent.</p>\n\n<p>What about expected utilities as opposed to preference orderings?&nbsp; To talk about these, you have to attribute a probability distribution to the actor, or to the environment - you can't just observe the outcome.&nbsp; If you have one of these probability distributions, then your knowledge of a utility function can let you guess at preferences between gambles (stochastic outcomes) and not just preferences between the outcomes themselves.</p>\n\n<p>The &quot;aiming at the target&quot; metaphor - and the notion of measuring how closely we hit - extends beyond just <a href=\"/lw/l4/terminal_values_and_instrumental_values/\">terminal</a> outcomes, to the forms of <a href=\"/lw/l4/terminal_values_and_instrumental_values/\">instrumental</a> devices and instrumental plans.</p>\n\n<p>Consider a car - say, a Toyota Corolla.&nbsp; The Toyota Corolla is made up of some number of atoms - say, on the (very) rough order of ten to the twenty-ninth.&nbsp; If you consider all the possible ways we could arrange those 10<sup>29</sup> atoms, it's clear that only an infinitesimally tiny fraction of possible configurations would qualify as a working car.&nbsp; If you picked a random configurations of 10<sup>29</sup> atoms once per Planck time, many ages of the universe would pass before you hit on a wheeled wagon, let alone an internal combustion engine.</p>\n\n<p>(When I talk about this in front of a popular audience, someone usually asks:&nbsp; &quot;But isn't this what the creationists argue?&nbsp; That if you took a bunch of atoms and put them in a box and shook them up, it would be astonishingly improbable for a fully functioning rabbit to fall out?&quot;&nbsp; But the logical flaw in the creationists' argument is <em>not</em> that randomly reconfiguring molecules <em>would</em> by pure chance assemble a rabbit.&nbsp; The logical flaw is that there is a process, <a href=\"/lw/kr/an_alien_god/\">natural selection</a>, which, through the <em>non-chance</em> retention of chance mutations, <em>selectively</em> accumulates complexity, until a few billion years later it produces a rabbit.&nbsp; Only the very first replicator in the history of time needed to pop out of the random shaking of molecules - perhaps a short RNA string, though there are more sophisticated hypotheses about autocatalytic hypercycles of chemistry.)\n\n</p>\n\n<p>Even restricting our attention to running vehicles, there is an\nastronomically huge design space of possible vehicles that could be\ncomposed of the same atoms as the Corolla, and most of them, from the\nperspective of a human user, won't work quite as well.&nbsp; We could take\nthe parts in the Corolla's air conditioner, and mix them up in\nthousands of possible configurations; nearly all these configurations\nwould result in a vehicle lower in our preference ordering, still\nrecognizable as a car but lacking a working air conditioner.</p>\n\n<p>So there are many more configurations corresponding to nonvehicles, or vehicles <em>lower in our preference ranking</em>, than vehicles ranked <em>greater than or equal to</em> the Corolla.</p>\n\n<p>A tiny fraction of the design space does describe vehicles that we would recognize as faster, more efficient, and safer than the Corolla.&nbsp; Thus the Corolla is not <em>optimal</em> under our preferences, nor under the designer's own goals.&nbsp; The Corolla is, however, <em>optimized</em>, because the designer had to hit an infinitesimal target in design space just to create a working car, let alone a car of Corolla-equivalent quality.&nbsp; The subspace of working vehicles is dwarfed by the space of all possible molecular configurations for the same atoms.&nbsp; You cannot build so much as an effective wagon by sawing boards into random shapes and nailing them together according to coinflips.&nbsp; To hit such a tiny target in configuration space requires a powerful <em>optimization process</em>.&nbsp; The better the car you want, the more <em>optimization pressure</em> you have to exert - though you need a huge optimization pressure just to get a car at all.</p>\n\n<p>This whole discussion assumes implicitly that the designer of the Corolla was trying to produce a &quot;vehicle&quot;, a means of travel.&nbsp; This assumption deserves to be made explicit, but it is not wrong, and it is highly useful in understanding the Corolla.\n\n</p>\n\n<p>Planning also involves hitting tiny targets in a huge search space.&nbsp; On a 19-by-19 Go board there are roughly 1e180 legal positions (not counting superkos).&nbsp; On early positions of a Go game there are more than 300 legal moves per turn.&nbsp; The search space explodes, and nearly all moves are foolish ones if your goal is to win the game.&nbsp; From all the vast space of Go possibilities, a Go player seeks out the infinitesimal fraction of plans which have a decent chance of winning.\n\n</p>\n\n<p>You cannot even drive to the supermarket without planning - it will take you a long, long time to arrive if you make random turns at each intersection.&nbsp; The set of turn sequences that will take you to the supermarket is a tiny subset of the space of turn sequences.&nbsp; Note that the subset of turn sequences we're seeking is <em>defined</em> by its <em>consequence</em> - the target - the destination.&nbsp; Within that subset, we care about other things, like the driving distance.&nbsp; (There are plans that would take us to the supermarket in a huge pointless loop-the-loop.)\n\n</p>\n\n<p>In general, as you live your life, you try to <em>steer reality</em> into a particular <em>region of possible futures</em>.&nbsp; When you buy a Corolla, you do it because you want to drive to the supermarket.&nbsp; You drive to the supermarket to buy food, which is a step in a larger strategy to avoid starving.&nbsp; All else being equal, you prefer possible futures in which you are alive, rather than dead of starvation.</p>\n\n<p>When you drive to the supermarket, you aren't really aiming for the supermarket, you're aiming for a region of possible futures in which you don't starve.&nbsp; Each turn at each intersection doesn't carry you toward the supermarket, it carries you out of the region of possible futures where you lie helplessly starving in your apartment.&nbsp; If you knew the supermarket was empty, you wouldn't bother driving there.&nbsp; An empty supermarket would occupy exactly the same place on your map of the city, but it wouldn't occupy the same role in your map of possible futures.&nbsp; It is not a location within the city that you are <em>really</em> aiming at, when you drive.</p>\n\n<p>Human intelligence is one kind of powerful optimization process, capable of winning a game of Go or turning sand into digital computers.&nbsp; Natural selection is much slower than human intelligence; but over geological time, <em>cumulative</em> selection pressure qualifies as a powerful optimization process.\n\n</p>\n\n<p>Once upon a time, human beings <a href=\"/lw/so/humans_in_funny_suits/\">anthropomorphized</a> stars, saw constellations in the sky and battles between constellations.&nbsp; But though stars burn longer and brighter than any craft of biology or human artifice, stars are neither optimization processes, nor products of strong optimization pressures.&nbsp; The stars are not gods; there is no true power in them.\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xexCWMyds6QLWognu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CW6HDvodPpNe38Cry", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 18, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "1125", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HktFCy6dgsqJ9WPpX", "n5ucT5ZbPdhfGNLtP", "pLRogvJLPPg6Mrvg4", "Zkzzjg3h7hW5Z36hK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-27T21:44:57.000Z", "modifiedAt": null, "url": null, "title": "Measuring Optimization Power", "slug": "measuring-optimization-power", "viewCount": null, "lastCommentedAt": "2019-07-27T01:46:32.475Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q4hLMDrFd8fbteeZ8/measuring-optimization-power", "pageUrlRelative": "/posts/Q4hLMDrFd8fbteeZ8/measuring-optimization-power", "linkUrl": "https://www.lesswrong.com/posts/Q4hLMDrFd8fbteeZ8/measuring-optimization-power", "postedAtFormatted": "Monday, October 27th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Measuring%20Optimization%20Power&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeasuring%20Optimization%20Power%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ4hLMDrFd8fbteeZ8%2Fmeasuring-optimization-power%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Measuring%20Optimization%20Power%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ4hLMDrFd8fbteeZ8%2Fmeasuring-optimization-power", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ4hLMDrFd8fbteeZ8%2Fmeasuring-optimization-power", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1684, "htmlBody": "<p><strong>Previously in series</strong>:&nbsp; <a href=\"/lw/v9/aiming_at_the_target/\">Aiming at the Target</a></p>\n\n<p>Yesterday I spoke of how &quot;When I think you're a powerful intelligence, and I think I know\nsomething about your preferences, then I'll predict that you'll steer\nreality into regions that are higher in your preference ordering.&quot;</p>\n\n<p>You can quantify this, at least in theory, supposing you have (A) the agent or optimization process's preference ordering, and (B) a measure of the space of outcomes - which, for discrete outcomes in a finite space of possibilities, could just consist of counting them - then you can quantify how small a target is being hit, within how large a greater region.</p>\n\n<p>Then we count the total number of states with equal or greater rank in the preference ordering to the outcome achieved, or integrate over the measure of states with equal or greater rank.&nbsp; Dividing this by the total size of the space gives you the relative smallness of the target - did you hit an outcome that was one in a million?&nbsp; One in a trillion?</p>\n\n<p>Actually, most optimization processes produce &quot;surprises&quot; that are exponentially more improbable than this - you'd need to try far more than a trillion random reorderings of the letters in a book, to produce a play of quality equalling or exceeding Shakespeare.&nbsp; So we take the log base two of the reciprocal of the improbability, and that gives us optimization power in bits.</p>\n\n<p>This figure - roughly, the improbability of an &quot;equally preferred&quot; outcome being produced by a random selection from the space (or measure on the space) - forms the foundation of my Bayesian view of intelligence, or to be precise, optimization power.&nbsp; It has many subtleties:</p><a id=\"more\"></a><p>(1)&nbsp; The wise will recognize that we are calculating the <a href=\"/lw/o1/entropy_and_short_codes/\">entropy</a> of something.&nbsp; We could take the figure of the relative improbability of &quot;equally good or better&quot; outcomes, and call this the <em>negentropy of the system relative to a preference ordering.</em>&nbsp; Unlike <a href=\"/lw/o5/the_second_law_of_thermodynamics_and_engines_of/\">thermodynamic entropy</a>, the entropy of a system relative to a preference ordering can easily decrease (that is, the negentropy can increase, that is, things can get better over time relative to a preference ordering).</p>\n\n<p>Suppose e.g. that a single switch will determine whether the world is saved or destroyed, and you don't know whether the switch is set to 1 or 0.&nbsp; You can carry out an operation that coerces the switch to 1; in accordance with the <a href=\"/lw/o5/the_second_law_of_thermodynamics_and_engines_of/\">second law of thermodynamics</a>, this requires you to dump one bit of entropy somewhere, e.g. by radiating a single photon of waste heat into the void.&nbsp; But you don't care about that photon - it's not alive, it's not sentient, it doesn't hurt - whereas you care a very great deal about the switch.</p>\n\n<p>For some odd reason, I had the above insight while watching <em>X TV.</em>&nbsp; (Those of you who've seen it know why this is funny.)</p>\n\n<p>Taking physical entropy out of propositional variables that you <em>care about</em> - coercing them from unoptimized states into an optimized states - and dumping the entropy into residual variables that you <em>don't care about,</em> means that <em>relative to your preference ordering</em>, the total &quot;entropy&quot; of the universe goes down.&nbsp; This is pretty much what life is all about.</p>\n\n<p>We care more about the variables we plan to alter, than we care about the waste heat emitted by our brains.&nbsp; If this were <em>not</em> the case - if our preferences didn't neatly compartmentalize the universe into cared-for propositional variables and everything else - then the second law of thermodynamics would prohibit us from ever doing optimization.&nbsp; Just like there are no-free-lunch theorems showing that cognition is impossible in a maxentropy universe, optimization will prove futile if you have maxentropy preferences.&nbsp; Having maximally disordered preferences over an ordered universe is pretty much the same dilemma as the reverse.</p>\n\n<p>(2)&nbsp; The quantity we're measuring tells us <em>how improbable this event is, in the absence of optimization, relative to some prior measure that describes the unoptimized probabilities.</em>&nbsp; To look at it another way, the quantity is how surprised you would be by the event, conditional on the hypothesis that there were no optimization processes around.&nbsp; This plugs directly into Bayesian updating: it says that highly optimized events are strong evidence for optimization processes that produce them.</p>\n\n<p>Ah, but how do you know a mind's preference ordering?&nbsp; Suppose you flip a coin 30 times and it comes up with some random-looking string - how do you know this wasn't because a mind <em>wanted</em> it to produce that string?</p>\n\n<p>This, in turn, is reminiscent of the <a href=\"/lw/jp/occams_razor/\">Minimum Message Length formulation of\nOccam's Razor</a>: if you send me a message telling me what a mind wants\nand how powerful it is, then this should enable you to compress your\ndescription of future events and observations, so that the total\nmessage is shorter.&nbsp; <em>Otherwise there is no predictive benefit to\nviewing a system as an optimization process</em>.&nbsp; This criterion tells us when to take the intentional stance.</p>\n\n<p>(3)&nbsp; Actually, you need to fit another criterion to take the intentional stance - there can't be a better description that averts the need to talk about optimization.&nbsp; This is an epistemic criterion more than a physical one - a sufficiently powerful mind might have no need to take the intentional stance toward a human, because it could just model the regularity of our brains like moving parts in a machine.</p>\n\n<p>(4)&nbsp; If you have a coin that always comes up heads, there's no need to say &quot;The coin always wants to come up heads&quot; because you can just say &quot;the coin always comes up heads&quot;.&nbsp; Optimization will <em>beat alternative mechanical explanations</em> when our ability to perturb a system defeats our ability to predict its interim steps in detail, but not our ability to predict a narrow final outcome.&nbsp; (Again, note that this is an epistemic criterion.) </p>\n\n<p>(5)&nbsp; Suppose you believe a mind exists, but you don't know its preferences?&nbsp; Then you use some of your evidence to infer the mind's\npreference ordering, and then use the inferred preferences to infer the\nmind's power, then use those two beliefs to testably predict future\noutcomes.&nbsp; The total gain in predictive accuracy should exceed the complexity-cost of supposing that &quot;there's a mind of unknown preferences around&quot;, the initial hypothesis.</p>\n\n<p>Similarly, if you're not sure whether there's an optimizer around, some of your evidence-fuel burns to support the hypothesis that there's an optimizer around, some of your evidence is expended to infer its target, and some of your evidence is expended to infer its power.&nbsp; The rest of the evidence should be well explained, or better yet predicted in advance, by this inferred data: this is your revenue on the transaction, which should exceed the costs just incurred, making an epistemic profit.</p>\n\n<p>(6)&nbsp; If you presume that you know (from a superior epistemic vantage point) the probabilistic consequences of an action or plan, or if you measure the consequences repeatedly, and if you know or infer a <em>utility function</em> rather than just a p<em>reference ordering,</em> then you might be able to talk about the degree of optimization of an <em>action or plan</em> rather than just the negentropy of a final outcome.&nbsp; We talk about the degree to which a plan has &quot;improbably&quot; high expected utility, relative to a measure over the space of all possible plans.</p>\n\n<p>(7)&nbsp; A similar presumption that we can measure the <em>instrumental value</em> of a device, relative to a <a href=\"/lw/l4/terminal_values_and_instrumental_values/\">terminal</a> utility function, lets us talk about a Toyota Corolla as an &quot;optimized&quot; physical object, even though we attach little terminal value to it per se.</p>\n\n<p>(8)&nbsp; If you're a human yourself and you take the measure of a problem, then there may be &quot;obvious&quot; solutions that don't count for much in your view, even though the solution might be very hard for a chimpanzee to find, or a snail.&nbsp; Roughly, because your own mind is efficient enough to calculate the solution without an apparent expenditure of internal effort, a solution that good will seem to have high probability, and so an equally good solution will not seem very <em>improbable.</em></p>\n\n<p>By presuming a base level of intelligence, we measure the improbability of a solution that &quot;would take us some effort&quot;, rather than the improbability of the same solution emerging from a random noise generator.&nbsp; This is one reason why many people say things like &quot;There has been no progress in AI; machines still aren't intelligent at all.&quot;&nbsp; There are legitimate abilities that modern algorithms entirely lack, but mostly what they're seeing is that AI is &quot;<a href=\"/lw/ql/my_childhood_role_model/\">dumber than a village idiot</a>&quot; - it doesn't even do as well as the &quot;obvious&quot; solutions that get most of the human's intuitive measure, let alone surprisingly better than that; it seems anti-intelligent, stupid.</p>\n\n<p>To measure <em>the impressiveness of a solution to a human,</em> you've got to do a few things that are a bit more complicated than just measuring optimization power.&nbsp; For example, if a human sees an obvious computer program to compute many solutions, they will measure the total impressiveness of all the solutions as being no more than the impressiveness of writing the computer program - but from the internal perspective of the computer program, it might seem to be making a metaphorical effort on each additional occasion.&nbsp; From the perspective of Deep Blue's programmers, Deep Blue is a one-time optimization cost; from Deep Blue's perspective it has to optimize each chess game individually.</p>\n\n<p>To measure human impressiveness you have to talk quite a bit about humans - how humans compact the search space, the meta-level on which humans approach a problem.&nbsp; People who try to take human impressiveness as their primitive measure will run into difficulties, because <a href=\"/lw/sp/detached_lever_fallacy/\">in fact the measure is not very primitive</a>.</p>\n\n<p>(9)&nbsp; For the vast majority of real-world problems we will not be able to calculate exact optimization powers, any more than we can do actual Bayesian updating over all hypotheses, or actual expected utility maximization in our planning.&nbsp; But, just like Bayesian updating or expected utility maximization, the notion of optimization power does give us a gold standard against which to measure - a simple mathematical idea of <em>what we are trying to do</em> whenever we essay more complicated and efficient algorithms.</p>\n\n<p>(10)&nbsp; &quot;Intelligence&quot; is efficient cross-domain optimization.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ac84EpK6mZbPLzmqj": 2, "nvKzwpiranwy29HFJ": 9}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q4hLMDrFd8fbteeZ8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 55, "extendedScore": null, "score": 8e-05, "legacy": true, "legacyId": "1126", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CW6HDvodPpNe38Cry", "soQX8yXLbKy7cFvy8", "QkX2bAkwG2EpGvNug", "f4txACqDWithRi7hs", "n5ucT5ZbPdhfGNLtP", "3Jpchgy53D2gB5qdk", "zY4pic7cwQpa9dnyk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 14, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2008-10-27T21:44:57.000Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-28T16:33:03.000Z", "modifiedAt": null, "url": null, "title": "Efficient Cross-Domain Optimization", "slug": "efficient-cross-domain-optimization", "viewCount": null, "lastCommentedAt": "2017-07-28T17:15:58.165Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yLeEPFnnB9wE7KLx2/efficient-cross-domain-optimization", "pageUrlRelative": "/posts/yLeEPFnnB9wE7KLx2/efficient-cross-domain-optimization", "linkUrl": "https://www.lesswrong.com/posts/yLeEPFnnB9wE7KLx2/efficient-cross-domain-optimization", "postedAtFormatted": "Tuesday, October 28th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Efficient%20Cross-Domain%20Optimization&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEfficient%20Cross-Domain%20Optimization%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyLeEPFnnB9wE7KLx2%2Fefficient-cross-domain-optimization%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Efficient%20Cross-Domain%20Optimization%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyLeEPFnnB9wE7KLx2%2Fefficient-cross-domain-optimization", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyLeEPFnnB9wE7KLx2%2Fefficient-cross-domain-optimization", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1540, "htmlBody": "<p><strong>Previously in series</strong>:&nbsp; <a href=\"/lw/va/measuring_optimization_power/\">Measuring Optimization Power</a></p>\n\n<p>Is Deep Blue &quot;intelligent&quot;?&nbsp; It was powerful enough at optimizing chess boards to defeat Kasparov, perhaps the most skilled chess player humanity has ever fielded.</p>\n\n<p>A bee builds hives, and a beaver builds dams; but a bee doesn't build dams and a beaver doesn't build hives.&nbsp; A human, watching, thinks, &quot;Oh, I see how to do it&quot; and goes on to build a dam using a honeycomb structure for extra strength.</p>\n\n<p>Deep Blue, like the bee and the beaver, never ventured outside the narrow domain that it itself was optimized over.</p>\n\n<p>There are no-free-lunch theorems showing that you can't have a <em>truly general</em> intelligence that optimizes in all possible universes (the vast majority of which are maximum-entropy heat baths).&nbsp; And even practically speaking, human beings are better at throwing spears than, say, writing computer programs.</p>\n\n<p>But humans are much more cross-domain than bees, beavers, or Deep Blue.&nbsp; We might even conceivably be able to comprehend the halting behavior of <a href=\"http://www.google.com/url?sa=U&amp;start=3&amp;q=http://en.wikipedia.org/wiki/Busy_beaver&amp;usg=AFQjCNFp0QYMVmwuB6ACSjtA2BTasKUqhw\">every Turing machine up to 10 states</a>, though I doubt it goes much higher than that.</p>\n\n<p>Every mind operates in some domain, but the domain that humans\noperate in isn't &quot;the savanna&quot; but something more like &quot;not too\ncomplicated processes in low-entropy lawful universes&quot;.&nbsp; We learn whole new domains by observation, in the same way that a beaver might learn to chew a different kind of wood.&nbsp; If I could\nwrite out your <a href=\"/lw/hk/priors_as_mathematical_objects/\">prior</a>, I could describe more exactly the universes in which you operate.</p><a id=\"more\"></a><p>Is evolution intelligent?&nbsp; It operates across domains - not quite as\nwell as humans do, but with the same general ability to do\nconsequentialist optimization on causal sequences that wend through\nwidely different domains.&nbsp; It built the bee.&nbsp; It built the beaver.</p>\n\n<p>Whatever begins with genes, and impacts inclusive genetic fitness, through any chain of cause and effect\nin any domain, is subject to\nevolutionary optimization.&nbsp; That much is true.</p>\n\n<p>But evolution only achieves this by running millions of actual\nexperiments in which the causal chains are actually played out.&nbsp; This\nis incredibly inefficient.&nbsp; Cynthia Kenyon said, &quot;One grad student can\ndo things in an hour that evolution could not do in a billion years.&quot;&nbsp; This is not because the grad student does quadrillions of detailed thought experiments in their imagination, but because the grad student abstracts over the search space.</p>\n\n<p>By human standards, <a href=\"/lw/kt/evolutions_are_stupid_but_work_anyway/\">evolution is unbelievably stupid</a>.&nbsp; It is the degenerate case of design with intelligence equal to zero, as befitting the accidentally occurring optimization process that got the whole thing started in the first place.</p>\n\n<p>(As for saying that &quot;evolution built humans, therefore it is efficient&quot;, this is, firstly, a sophomoric objection; second, it confuses levels.&nbsp; Deep Blue's programmers were not superhuman chessplayers.&nbsp; The importance of distinguishing levels can be seen from the point that humans are efficiently optimizing human goals, which <a href=\"/lw/l1/evolutionary_psychology/\">are not the same</a> as evolution's goal of inclusive genetic fitness.&nbsp; Evolution, in producing humans, may have entirely doomed DNA.)</p>\n\n<p>\nI once heard a senior mainstream AI type suggest that we might try to\nquantify the intelligence of an AI system in terms of its RAM,\nprocessing power, and sensory input bandwidth.&nbsp; This at once reminded\nme of a <a href=\"http://www.cs.utexas.edu/users/EWD/transcriptions/EWD10xx/EWD1036.html\">quote</a>\nfrom Dijkstra:&nbsp; &quot;If we wish to count lines of code, we should not\nregard them as 'lines produced' but as 'lines spent': the current\nconventional wisdom is so foolish as to book that count on the wrong\nside of the ledger.&quot;&nbsp; If you want to measure the <em>intelligence</em>\nof a system, I would suggest measuring its optimization power as\nbefore, but then dividing by the resources used.&nbsp; Or you might measure\nthe degree of prior cognitive optimization required to achieve the same\nresult using equal or fewer resources.&nbsp; Intelligence, in other words,\nis <em>efficient</em> optimization.</p>\n\n<p>So if we say &quot;efficient cross-domain optimization&quot; - is that <em>necessary and sufficient</em> to convey the <a href=\"/lw/od/37_ways_that_words_can_be_wrong/\">wisest meaning</a> of &quot;intelligence&quot;, after making a proper effort to factor out <a href=\"/lw/st/anthropomorphic_optimism/\">anthropomorphism in ranking solutions</a>?</p>\n\n<p>I do hereby propose:&nbsp; &quot;Yes.&quot;</p>\n\n<p>Years ago when I was on a panel with Jaron Lanier, he had offered some elaborate argument that no machine could be intelligent, because it was just a machine and to call it &quot;intelligent&quot; was therefore bad poetry, or something along those lines.&nbsp; Fed up, I finally snapped:&nbsp; &quot;Do you mean to say that if I write a computer program and that computer program rewrites itself and rewrites itself and builds its own nanotechnology and zips off to Alpha Centauri and builds its own Dyson Sphere, that computer program is not <em>intelligent?</em>&quot;</p>\n\n<p>This, I think, is a core meaning of &quot;intelligence&quot; that it is wise to keep in mind.</p>\n\n<p>I mean, maybe not that exact test.&nbsp; And it wouldn't be wise to bow too directly to human notions of &quot;impressiveness&quot;, because this is what causes people to conclude that a butterfly must have been intelligently designed (they don't see the vast incredibly wasteful trail of trial and error), or that <a href=\"/lw/st/anthropomorphic_optimism/\">an expected paperclip maximizer is stupid</a>.</p>\n\n<p>But still, intelligences ought to be able to do cool stuff, in a reasonable amount of time using reasonable resources, even if we throw things at them that they haven't seen before, or change the rules of the game (domain) a little.&nbsp; It is my contention that this is what's captured by the notion of &quot;efficient cross-domain optimization&quot;.</p>\n\n<p>Occasionally I hear someone say something along the lines of, &quot;No matter how smart you are, a tiger can still eat you.&quot;&nbsp; Sure, if you get stripped naked and thrown into a pit with no chance to prepare and no prior training, you may be in trouble.&nbsp; And by similar token, a human can be killed by a large rock dropping on their head.&nbsp; It doesn't mean a big rock is more powerful than a human.</p>\n\n<p>A large asteroid, falling on Earth, would make an impressive bang.&nbsp; But if we spot the asteroid, we can try to deflect it through any number of methods.&nbsp; With enough lead time, a can of black paint will do as well as a nuclear weapon.&nbsp; And the asteroid itself won't oppose us <em>on our own level</em> - won't try to think of a counterplan.&nbsp; It won't send out interceptors to block the nuclear weapon.&nbsp; It won't try to paint the opposite side of itself with more black paint, to keep its current trajectory.&nbsp; And if we stop that asteroid, the asteroid belt won't send another planet-killer in its place.</p>\n\n<p>We might have to do some work to <em>steer the future out of</em> the unpleasant region it will go to if we do nothing, but the asteroid itself isn't <em>steering the future</em> in any meaningful sense.&nbsp; It's as simple as water flowing downhill, and if we nudge the asteroid off the path, it won't nudge itself back.</p>\n\n<p>The tiger isn't quite like this.&nbsp; If you try to run, it will follow you.&nbsp; If you dodge, it will follow you.&nbsp; If you try to hide, it will spot you.&nbsp; If you climb a tree, it will wait beneath.</p>\n\n<p>But if you come back with an armored tank - or maybe just a hunk of poisoned meat - the tiger is out of luck.&nbsp; You threw something at it that wasn't in the domain it was designed to learn about.&nbsp; The tiger can't do <em>cross-domain</em> optimization, so all you need to do is give it a little cross-domain nudge and it will spin off its course like a painted asteroid.</p>\n\n<p><em>Steering the future,</em> not energy or mass, not food or bullets, is the raw currency of conflict and cooperation among agents.&nbsp; Kasparov competed against Deep Blue to steer the chessboard into a region where he won - knights and bishops were only his pawns.&nbsp; And if Kasparov had been allowed to use <em>any</em> means to win against Deep Blue, rather than being artificially restricted, it would have been a trivial matter to kick the computer off the table - a rather light optimization pressure by comparison with Deep Blue's examining hundreds of millions of moves per second, or by comparison with Kasparov's pattern-recognition of the board; but it would have crossed domains into a causal chain that Deep Blue couldn't model and couldn't optimize and couldn't resist.&nbsp; One bit of optimization pressure is enough to flip a switch that a narrower opponent can't switch back.\n\n</p>\n\n<p>A superior general can win with fewer troops, and superior\ntechnology can win with a handful of troops.&nbsp; But even a suitcase nuke\nrequires at least a few kilograms of matter.&nbsp; If two intelligences of\nthe same level compete with different resources, the battle will\nusually go to the wealthier.</p>\n\n<p>The same is true, on a deeper level, of efficient designs using different amounts of computing power.&nbsp; Human beings, five hundred years after\nthe Scientific Revolution, are\nonly just starting to match their wits against the billion-year\nheritage of biology.&nbsp; We're vastly faster, it has a vastly longer lead time; after five hundred years and a billion years respectively, the two powers are starting to balance.</p>\n\n<p>But as a measure of <em>intelligence,</em> I think\nit is better to speak of how well you can use your resources - if we\nwant to talk about raw impact, then we can speak of <em>optimization power</em> directly.</p>\n\n<p>So again I claim that this - computationally-frugal cross-domain future-steering - is the necessary and sufficient meaning that the wise should attach to the word, &quot;intelligence&quot;.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nvKzwpiranwy29HFJ": 1, "ac84EpK6mZbPLzmqj": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yLeEPFnnB9wE7KLx2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 43, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "1127", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Q4hLMDrFd8fbteeZ8", "jzf4Rcienrm6btRyt", "jAToJHtg39AMTAuJo", "epZLSoNvjW53tqNj9", "FaJaCgqBKphrDzDSj", "RcZeZt8cPk48xxiQ8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-29T19:32:53.000Z", "modifiedAt": "2020-04-17T20:22:33.683Z", "url": null, "title": "Economic Definition of Intelligence?", "slug": "economic-definition-of-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:31.908Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uPCehCa7Ecidfn2Xe/economic-definition-of-intelligence", "pageUrlRelative": "/posts/uPCehCa7Ecidfn2Xe/economic-definition-of-intelligence", "linkUrl": "https://www.lesswrong.com/posts/uPCehCa7Ecidfn2Xe/economic-definition-of-intelligence", "postedAtFormatted": "Wednesday, October 29th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Economic%20Definition%20of%20Intelligence%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEconomic%20Definition%20of%20Intelligence%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuPCehCa7Ecidfn2Xe%2Feconomic-definition-of-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Economic%20Definition%20of%20Intelligence%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuPCehCa7Ecidfn2Xe%2Feconomic-definition-of-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuPCehCa7Ecidfn2Xe%2Feconomic-definition-of-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2126, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/vb/efficient_crossdomain_optimization/\">Efficient Cross-Domain Optimization</a></p>\n\n<p>Shane Legg once produced a catalogue of <a href=\"https://web.archive.org/web/20100407061055/http://www.vetta.org/definitions-of-intelligence/\">71 definitions of intelligence</a>.&nbsp; Looking it over, you'll find that the 18 definitions in dictionaries and the 35 definitions of psychologists are mere <a href=\"/lw/tf/dreams_of_ai_design/\">black boxes containing human parts</a>.</p>\n\n<p>However, among the 18 definitions from AI researchers, you can find such notions as</p><blockquote><p>&quot;Intelligence measures an agent's ability to achieve goals in a wide range of environments&quot; (Legg and Hutter)</p></blockquote><p>or</p><blockquote><p>&quot;Intelligence is the ability to optimally use limited resources - including time - to achieve goals&quot; (Kurzweil)</p></blockquote><p>or even</p><blockquote><p>&quot;Intelligence is the power to rapidly find an adequate solution in what appears a priori (to observers) to be an immense search space&quot; (Lenat and Feigenbaum)</p></blockquote><p>which is about as close as you can get to my own notion of &quot;efficient cross-domain optimization&quot; without actually <a href=\"/lw/va/measuring_optimization_power/\">measuring optimization power in bits</a>.</p>\n\n<p>But Robin Hanson, whose AI background we're going to ignore for a moment in favor of his better-known identity as an economist, at once said:</p><blockquote><p><span id=\"comment-136749475-content\">&quot;I think what you want is to think in terms of a <em>production function</em>, which describes a system's output on a particular task as a function of its various inputs and features.&quot;</span></p></blockquote><p><span id=\"comment-136749475-content\">Economists spend a fair amount of their time measuring things like productivity and efficiency.&nbsp; Might they have something to say about how to measure intelligence in generalized cognitive systems?</span></p>\n\n<p><span id=\"comment-136749475-content\">This is a real question, open to all economists.&nbsp; So I'm going to quickly go over some of the criteria-of-a-good-definition that stand behind my own proffered suggestion on intelligence, and what I see as the important challenges to a productivity-based view.&nbsp; It seems to me that this is an important sub-issue of Robin's and my persistent disagreement about the Singularity.</span></p><a id=\"more\"></a><p>(A)&nbsp; One of the criteria involved in a definition of intelligence is that it ought to <strong>separate form and function</strong>.&nbsp; The Turing Test fails this - it says that if you can build something <em>indistinguishable from a bird, it must definitely fly</em>, which is true but spectacularly unuseful in building an airplane.</p>\n\n<p>(B)&nbsp; We will also <strong>prefer quantitative measures to qualitative measures</strong> that only say &quot;this is intelligent or not intelligent&quot;.&nbsp; Sure, you can define &quot;flight&quot; in terms of getting off the ground, but what you really need is a way to quantify aerodynamic lift and relate it to other properties of the airplane, so you can calculate how much lift is needed to get off the ground, and calculate how close you are to flying at any given point.</p>\n\n<p>(C)&nbsp; So why not use the nicely quantified IQ test?&nbsp; Well, imagine if the Wright Brothers had tried to build the Wright Flyer using a notion of &quot;flight quality&quot; build around a Fly-Q test standardized on the abilities of the average pigeon, including various measures of wingspan and air maneuverability.&nbsp; We want a definition that is <strong>not parochial to humans</strong>.</p>\n\n<p>(D)&nbsp; We have a nice system of Bayesian expected utility maximization.&nbsp; Why not say that any system's &quot;intelligence&quot; is just the average utility of the outcome it can achieve?&nbsp; But utility functions are invariant up to a positive affine transformation, i.e., if you add 3 to all utilities, or multiply all by 5, it's the same utility function.&nbsp; If we assume a fixed utility function, we would be able to compare the intelligence of the same system on different occasions - but we would like to be able to <strong>compare intelligences with different utility functions</strong>.</p>\n\n<p>(E)&nbsp; And by much the same token, we would like our definition to let us <strong>recognize intelligence by observation rather than presumption</strong>, which means we can't always start off assuming that something has a fixed utility function, or even any utility function at all.&nbsp; We <em>can</em> have a prior over probable utility functions, which assigns a very low probability to overcomplicated hypotheses like &quot;the lottery <em>wanted</em> 6-39-45-46-48-36 to win on October 28th, 2008&quot;, but higher probabilities to <a href=\"/lw/tt/points_of_departure/\">simpler</a> desires.</p>\n\n<p>(F)&nbsp; Why not just measure how well the intelligence plays chess?&nbsp; But in real-life situations, plucking the opponent's queen off the board or shooting the opponent is not <em>illegal,</em> it is <em>creative.</em>&nbsp; We would like our definition to respect the creative shortcut - <strong>to not define intelligence into the box of a narrow problem domain</strong>.</p>\n\n<p>(G)&nbsp; It would be nice if intelligence were <strong>actually measurable using some operational test</strong>, but this conflicts strongly with criteria F and D.&nbsp; My own definition essentially tosses this out the window - you can't <em>actually</em> <a href=\"/lw/va/measuring_optimization_power/\">measure optimization power</a> on any real-world problem any more than you can compute the real-world probability update or maximize real-world expected utility.&nbsp; But, just as you can wisely wield algorithms that behave <em>sorta like</em> Bayesian updates or <em>increase</em> expected utility, there are all sorts of possible methods that can take a stab at measuring optimization power.</p>\n\n<p>(H)&nbsp; And finally, when all is said and done, we should be able to recognize very high &quot;intelligence&quot; levels in an entity that can, oh, say, synthesize nanotechnology and build its own Dyson Sphere.&nbsp; Nor should we assign very high &quot;intelligence&quot; levels to something that couldn't build a wooden wagon (even if it wanted to, and had hands).&nbsp; <strong>Intelligence should not be defined too far away from that impressive thingy we humans sometimes do</strong>.</p>\n\n<p>Which brings us to production functions.&nbsp; I think the main problems here would lie in criteria DE.</p>\n\n<p>First, a word of background:&nbsp; In Artificial Intelligence, it's more common to spend your days obsessing over the structure of a problem space - and when you find a good algorithm, you use that algorithm and pay however much computing power it requires.&nbsp; You aren't as likely to find a situation where there are five different algorithms competing to solve a problem and a sixth algorithm that has to decide where to invest a marginal unit of computing power.&nbsp; Not that computer scientists haven't studied this as a specialized problem.&nbsp; But it's ultimately not what AIfolk <em>do</em> all day.&nbsp; So I hope that we can both try to appreciate the danger of <em>deformation professionelle</em>.</p>\n\n<p>Robin Hanson said:</p><blockquote><p><span id=\"comment-136842939-content\">&quot;Eliezer, even if you measure\noutput as you propose in terms of a state space reduction factor, my\nmain point was that simply 'dividing by the resources used' makes\nlittle sense.&quot;</span></p></blockquote><p><span id=\"comment-136842939-content\">I agree that &quot;divide by resources used&quot; is a very naive method, rather tacked-on by comparison.&nbsp; If one mind gets 40 bits of optimization using a trillion floating-point operations, and another mind achieves 80 bits of optimization using two trillion floating-point operations, even in the same domain using the same utility function, they may not at all be equally &quot;well-designed&quot; minds.&nbsp; One of the minds may itself be a lot more &quot;optimized&quot; than the other (probably the second one).</span></p>\n\n<p>I do think that measuring the rarity of equally good solutions in the search space smooths out the discussion a lot.&nbsp; More than any other simple measure I can think of.&nbsp; You're not just presuming that 80 units are twice as good as 40 units, but trying to give some measure of how rare 80-unit solutions are in the space; if they're common it will take less &quot;optimization power&quot; to find them and we'll be less impressed.&nbsp; This likewise helps when comparing minds with different preferences.</p>\n\n<p>But some search spaces are just easier to search than others.&nbsp; I generally choose to talk about this by hiking the &quot;optimization&quot; metric up a meta-level: how easy is it to find an algorithm that searches this space?&nbsp; There's no <em>absolute</em> easiness, unless you talk about simple random selection, which I take as my base case.&nbsp; Even if a fitness gradient is smooth - a very simple search - e.g. natural selection would creep down it by incremental neighborhood search, while a human would leap through by e.g. looking at the first and second derivatives.&nbsp; Which of these is the &quot;inherent easiness&quot; of the space?</p>\n\n<p>Robin says:</p><blockquote><p><span id=\"comment-136777939-content\">Then\nwe can talk about partial derivatives; rates at which output increases\nas a function of changes in inputs or features...&nbsp; </span><span id=\"comment-136842939-content\"> Yes a production function\nformulation may abstract from some relevant details, but it is far\ncloser to reality than dividing by &quot;resources.&quot; </span></p></blockquote><p>A partial derivative divides the marginal output by marginal resource.&nbsp; Is this so much less naive than dividing total output by total resources?</p>\n\n<p>I confess that I said &quot;divide by resources&quot; just to have <em>some</em> measure of efficiency; it's not a very <em>good</em> measure.&nbsp; Still, we need to take resources into account somehow - we don't want natural selection to look as &quot;intelligent&quot; as humans: human engineers, given 3.85 billion years and the opportunity to run 1e44 experiments, would produce products overwhelmingly superior to biology.</p>\n\n<p>But this is really establishing an <em>ordering</em> based on superior performance with the <em>same</em> resources, not a <em>quantitative</em> metric.&nbsp; I might have to be content with a partial ordering among intelligences, rather than being able to quantify them.&nbsp; If so, one of the ordering characteristics will be the amount of resources used, which is what I was getting at by saying &quot;divide by total resources&quot;.</p>\n\n<p>The idiom of &quot;division&quot; is based around things that can be divided, that is, fungible resources.&nbsp; A human economy based on mass production has lots of these.&nbsp; In modern-day computing work, programmers use fungible resources like computing cycles and RAM, but tend to produce much less fungible outputs.&nbsp; Informational goods tend to be mostly non-fungible: two copies of the same file are worth around as much as one, so every worthwhile informational good is unique.&nbsp; If I draw on my memory to produce an essay, neither the sentences of the essay, or the items of my memory, will be substitutable for one another.&nbsp; If I create a unique essay by drawing upon a thousand unique memories, how well have I done, and how much resource have I used?</p>\n\n<p>Economists have a simple way of establishing a kind of fungibility-of-valuation between <em>all</em> the inputs and <em>all</em> the outputs of an economy: they look at market prices.</p>\n\n<p>But this just palms off the problem of valuation on hedge funds.&nbsp; <em>Someone</em> has to do the valuing.&nbsp; A society with stupid hedge funds ends up with stupid valuations.</p>\n\n<p>Steve Omohundro has pointed out that for fungible resources in an AI - and computing power is a fungible resource on modern architectures - there ought to be a resource balance principle: the marginal result of shifting a unit of resource between any two tasks should produce a decrease in expected utility, relative to the AI's probability function that determines the expectation.&nbsp; To the extent any of these things have continuous first derivatives, shifting an infinitesimal unit of resource between any two tasks should have no effect on expected utility.&nbsp; This establishes &quot;expected utilons&quot; as something akin to a central currency <em>within </em>the AI.</p>\n\n<p>But this gets us back to the problems of criteria D and E.&nbsp; If I look at a mind and see a certain balance of resources, is that because the mind is really cleverly balanced, or because the mind is stupid?&nbsp; If a mind would rather have two units of CPU than one unit of RAM (and how can I tell this by observation, since the resources are not readily convertible?) then is that because RAM is inherently twice as valuable as CPU, or because the mind is twice as stupid in using CPU as RAM?</p>\n\n<p>If you can assume the resource-balance principle, then you will find it easy to talk about the <em>relative efficiency of alternative algorithms for use inside the AI</em>, but this doesn't give you a good way to measure the <em>external power of the whole AI</em>.</p>\n\n<p>Similarly, assuming a particular relative valuation of resources, as given by an external marketplace, doesn't let us ask questions like &quot;How smart is\na human economy?&quot;&nbsp; Now the relative valuation a human economy assigns to internal resources can no longer\nbe taken for granted - a more powerful system might assign very\ndifferent relative values to internal resources.</p>\n\n<p>I admit that dividing optimization power by &quot;total resources&quot; is handwaving - more a qualitative way of saying &quot;pay attention to resources used&quot; than anything you could actually quantify into a single useful figure.&nbsp; But I pose an open question to Robin (or any other economist) to explain how production theory can help us do better, bearing in mind that:</p>\n\n<ul><li> <em>I</em><em>nformational</em> inputs and outputs tend to be non-fungible;</li>\n\n<li>I want to be able to observe the &quot;intelligence&quot; and &quot;utility function&quot; of a whole system without starting out assuming them;</li>\n\n<li>I would like to be able to compare, as much as possible, the performance of intelligences with different utility functions;</li>\n\n<li>I can't assume <em>a priori</em> any particular breakdown of internal tasks or &quot;ideal&quot; valuation of internal resources.</li></ul>\n\n<p>I would finally point out that all data about the market value of human IQ <a href=\"/lw/ql/my_childhood_role_model/\">only applies to variances of intelligence within the human species</a>.&nbsp; I mean, how much would you pay a chimpanzee to run your hedge fund?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ac84EpK6mZbPLzmqj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uPCehCa7Ecidfn2Xe", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 14, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "1128", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["yLeEPFnnB9wE7KLx2", "p7ftQ6acRkgo6hqHb", "Q4hLMDrFd8fbteeZ8", "zrGzan92SxP27LWP9", "3Jpchgy53D2gB5qdk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-30T21:17:56.000Z", "modifiedAt": null, "url": null, "title": "Intelligence in Economics", "slug": "intelligence-in-economics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:37.175Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4uDfpNTrdhEYEb2jm/intelligence-in-economics", "pageUrlRelative": "/posts/4uDfpNTrdhEYEb2jm/intelligence-in-economics", "linkUrl": "https://www.lesswrong.com/posts/4uDfpNTrdhEYEb2jm/intelligence-in-economics", "postedAtFormatted": "Thursday, October 30th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intelligence%20in%20Economics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligence%20in%20Economics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4uDfpNTrdhEYEb2jm%2Fintelligence-in-economics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intelligence%20in%20Economics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4uDfpNTrdhEYEb2jm%2Fintelligence-in-economics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4uDfpNTrdhEYEb2jm%2Fintelligence-in-economics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1408, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/vc/economic_definition_of_intelligence/\">Economic Definition of Intelligence?</a></p>\n\n<p>After I challenged Robin to show how economic concepts can be useful in defining or measuring intelligence, Robin <a href=\"http://www.overcomingbias.com/2008/10/does-intelligen.html\">responded</a> by - as I interpret it - challenging me to show why a generalized concept of &quot;intelligence&quot; is any use in economics.</p>\n\n<p>Well, I'm not an economist (as you may have noticed) but I'll try to respond as best I can.</p>\n\n<p>My primary view of the world tends to be through the lens of AI.&nbsp; If I talk about economics, I'm going to try to subsume it into notions like expected utility maximization (I manufacture lots of copies of something that I can use to achieve my goals) or information theory (if you manufacture lots of copies of something, my probability of seeing a copy goes up).&nbsp; This subsumption isn't meant to be some kind of challenge for academic supremacy - it's just what happens if you ask an AI guy an econ question.</p>\n\n<p>So first, let me describe what I see when I look at economics:</p>\n\n<p>I see a special case of game theory in which some interactions are <em>highly regular and repeatable:</em>&nbsp; You can take 3 units of steel and 1 unit of labor and make 1 truck that will transport 5 units of grain between Chicago and Manchester once per week, and agents can potentially do this <em>over and over</em> again.&nbsp; If the numbers aren't <em>constant,</em> they're at least <em>regular</em> - there's diminishing marginal utility, or supply/demand curves, rather than rolling random dice every time.&nbsp; Imagine economics if no two elements of reality were fungible - you'd just have a huge incompressible problem in non-zero-sum game theory.</p>\n\n<p>This may be, for example, why we don't think of scientists writing papers that build on the work of other scientists in terms of an <em>economy of science papers</em> - if you turn an economist loose on science, they may measure scientist salaries paid in fungible dollars, or try to see whether scientists trade countable citations with each other.&nbsp; But it's much less likely to occur to them to analyze the way that units of scientific knowledge are produced from previous units plus scientific labor.&nbsp; Where information is concerned, two identical copies of a file are the same information as one file.&nbsp; So every unit of <em>knowledge</em> is unique, non-fungible, and so is each act of production.&nbsp; There isn't even a common currency that measures how much a given paper contributes to human knowledge.&nbsp; (<a href=\"/lw/kj/no_one_knows_what_science_doesnt_know/\">I don't know what economists don't know</a>, so do correct me if this is actually extensively studied.)</p>\n\n<p>Since &quot;intelligence&quot; deals with an informational domain, building a bridge from it to economics isn't trivial - but where do factories come from, anyway?&nbsp; Why do humans get a higher return on capital than chimpanzees?</p><a id=\"more\"></a><p>I see two basic bridges between intelligence and economics.</p>\n\n<p>The first bridge is the role of intelligence in economics: the way that steel is put together into a truck involves choosing one out of an exponentially vast number of possible configurations.&nbsp; With a more clever configuration, you may be able to make a truck using less steel, or less labor.&nbsp; Intelligence also plays a role at a larger scale, in deciding whether or not to buy a truck, or where to invest money.&nbsp; We may even be able to talk about something akin to optimization at\na macro scale, the degree to which the whole economy has put itself\ntogether in a special configuration that earns a high rate of return on investment.&nbsp; (Though this introduces problems for <a href=\"/lw/va/measuring_optimization_power/\">my own formulation</a>, as I assume a central preference ordering / utility function that an economy doesn't possess - still, deflated monetary valuations seem like a good proxy.)</p>\n\n<p>The second bridge is the role of economics in intelligence: if you jump up a meta-level, there are <em>repeatable cognitive algorithms</em> underlying the production of unique information.&nbsp; These cognitive algorithms use some resources that are fungible, or at least <em>material</em> enough that you can only use the resource on one task, creating a problem of opportunity costs.&nbsp; (A unit of time will be an example of this for almost any algorithm.)&nbsp; Thus we have <a href=\"/lw/vc/economic_definition_of_intelligence/\">Omohundro's resource balance principle</a>, which says that the inside of an efficiently organized mind should have a common currency in expected utilons.</p>\n\n<p>Says Robin:</p><blockquote><p>'Eliezer has <a href=\"/lw/vc/economic_definition_of_intelligence/\">just raised the issue</a>\nof how to define &quot;intelligence&quot;, a concept he clearly wants to apply to\na very wide range of possible systems.&nbsp; He wants a quantitative concept\nthat is &quot;not parochial to humans,&quot; applies to systems with very\n&quot;different utility functions,&quot; and that summarizes the system's\nperformance over a broad &quot;not ... narrow problem domain.&quot;&nbsp; My main\nresponse is to note that this may just not be possible.&nbsp; I have no\nobjection to looking, but it is not obvious that there is any such\nuseful broadly-applicable &quot;intelligence&quot; concept.'</p></blockquote><p>Well, one might run into some trouble assigning a total ordering to all intelligences, as opposed to a partial ordering.&nbsp; But that intelligence as a <em>concept</em> is useful - especially the way that I've <a href=\"/lw/vb/efficient_crossdomain_optimization/\">defined it</a> - that I must strongly defend.&nbsp; Our current science has advanced further on some problems than others.&nbsp; Right now, there is better understanding of the steps carried out to construct a car, than the cognitive algorithms that invented the unique car design.&nbsp; But they are both, to some degree, regular and repeatable; <a href=\"/lw/rl/the_psychological_unity_of_humankind/\">we don't all have different brain architectures</a>.</p>\n\n<p>I generally <a href=\"/lw/ql/my_childhood_role_model/\">inveigh against focusing on relatively minor between-human variations when discussing &quot;intelligence&quot;</a>.&nbsp; It is controversial what role is played in the modern economy by such variations in whatever-IQ-tests-try-to-measure.&nbsp; Anyone who denies that <em>some</em> such a role exists would be a poor deluded fool indeed.&nbsp; But, on the whole, we needn't expect &quot;the role played by IQ variations&quot; to be at all the same sort of question as &quot;the role played by intelligence&quot;.</p>\n\n<p>You will surely find no cars, if you take away the mysterious &quot;intelligence&quot; that produces, from out of a vast exponential space, the <em>information</em> that describes one particular configuration of steel etc. constituting a <em>car design</em>.&nbsp; Without optimization to conjure certain informational patterns out of vast search spaces, the modern economy evaporates like a puff of smoke.</p>\n\n<p>So you need some account of where the car design comes from.</p>\n\n<p>Why should you try to give the same account of &quot;intelligence&quot; across different domains?&nbsp; When someone designs a car, or an airplane, or a hedge-fund trading strategy, aren't these different designs?</p>\n\n<p>Yes, they are different informational goods.</p>\n\n<p>And wasn't it a different set of skills that produced them?&nbsp; You can't just take a car designer and plop them down in a hedge fund.</p>\n\n<p>True, but where did the different skills come from?</p>\n\n<p>From going to different schools.</p>\n\n<p>Where did the different schools come from?</p>\n\n<p>They were built by different academic lineages, compounding knowledge upon knowledge within a line of specialization.</p>\n\n<p>But where did so many different academic lineages come from?&nbsp; And how is this trick of &quot;compounding knowledge&quot; repeated over and over?</p>\n\n<p>Keep moving meta, and you'll find a regularity, something repeatable: you'll find humans, with common human genes that construct common human brain architectures.</p>\n\n<p>No, not every discipline puts the same relative strain on the same brain areas.&nbsp; But they are all using human parts, manufactured by mostly-common DNA.&nbsp; Not all the adult brains are the same, but they learn into unique adulthood starting from a <em>much more regular</em> underlying set of <em>learning algorithms</em>.&nbsp; We should expect less variance in infants than in adults.</p>\n\n<p>And all the adaptations of the human brain were produced by the\n(relatively much structurally simpler) processes of natural selection.&nbsp; Without that <a href=\"/lw/kt/evolutions_are_stupid_but_work_anyway/\">earlier and less efficient</a> optimization process, there wouldn't be a human brain design, and hence no human brains.</p>\n\n<p>Subtract the human brains executing repeatable cognitive algorithms, and you'll have no unique adulthoods produced by learning; and no grown humans to invent the cultural concept of science; and no chains of discoveries that produce scientific lineages; and no engineers who attend schools; and no unique innovative car designs; and thus, no cars.</p>\n\n<p>The moral being that you can generalize across domains, if you keep tracing back the causal chain and keep going meta.</p>\n\n<p>It may be harder to talk about &quot;intelligence&quot; as a common factor in the full causal account of the economy, as to talk about the repeated operation that puts together many instantiations of the same car design - but there <em>is</em> a common factor, and the economy could hardly exist without it.</p>\n\n<p>As for generalizing away from humans - well, what part of the notion of &quot;efficient cross-domain optimization&quot; <em>ought</em> to apply only to humans?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PDJ6KqJBRzvKPfuS3": 2, "Ng8Gice9KNkncxqcj": 1, "xexCWMyds6QLWognu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4uDfpNTrdhEYEb2jm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 13, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "1129", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uPCehCa7Ecidfn2Xe", "vNBxmcHpnozjrJnJP", "Q4hLMDrFd8fbteeZ8", "yLeEPFnnB9wE7KLx2", "Cyj6wQLW6SeF6aGLy", "3Jpchgy53D2gB5qdk", "jAToJHtg39AMTAuJo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-10-31T16:00:00.000Z", "modifiedAt": null, "url": null, "title": "Mundane Magic", "slug": "mundane-magic", "viewCount": null, "lastCommentedAt": "2020-07-22T12:48:22.110Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SXK87NgEPszhWkvQm/mundane-magic", "pageUrlRelative": "/posts/SXK87NgEPszhWkvQm/mundane-magic", "linkUrl": "https://www.lesswrong.com/posts/SXK87NgEPszhWkvQm/mundane-magic", "postedAtFormatted": "Friday, October 31st 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mundane%20Magic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMundane%20Magic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSXK87NgEPszhWkvQm%2Fmundane-magic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mundane%20Magic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSXK87NgEPszhWkvQm%2Fmundane-magic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSXK87NgEPszhWkvQm%2Fmundane-magic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1046, "htmlBody": "<p>As you may recall from some months earlier, I think that part of the rationalist ethos is <em>binding yourself emotionally</em> to an <a href=\"/lw/hr/universal_law/\">absolutely lawful</a> <a href=\"/lw/on/reductionism/\">reductionistic</a> universe&mdash;a universe containing <a href=\"/lw/tv/excluding_the_supernatural/\">no ontologically basic mental things</a> such as souls or <a href=\"/lw/ou/if_you_demand_magic_magic_wont_help/\">magic</a>&mdash;and pouring all your hope and all your care into that merely real universe and its possibilities, without disappointment.</p>\n<p>There's an old trick for combating <a href=\"http://en.wikipedia.org/wiki/Dukkha\">dukkha</a> where you make a list of things you're grateful for, like a roof over your head.</p>\n<p>So why not make a list of abilities you have that would be amazingly cool <em>if they were magic,</em> or <a href=\"/lw/os/joy_in_discovery/\">if only a few chosen individuals had them</a>?</p>\n<p>For example, suppose that instead of one eye, you possessed a magical <em>second</em> eye embedded in your forehead.&nbsp; And this second eye enabled you to <em>see into the third dimension</em>&mdash;so that you could somehow tell <em>how far away</em> things were&mdash;where an ordinary eye would see only a two-dimensional shadow of the true world.&nbsp; Only the possessors of this ability can accurately aim the legendary distance-weapons that kill at ranges far beyond a sword, or use to their fullest potential the shells of ultrafast machinery called \"cars\".</p>\n<p>\"Binocular vision\" would be <a href=\"http://chaitanya1.files.wordpress.com/2008/06/bigbang.gif\">too light a term</a> for this ability.&nbsp; We'll only appreciate it once it has a properly impressive name, like Mystic Eyes of Depth Perception.</p>\n<p>So here's a list of some of my favorite magical powers:</p>\n<p><a id=\"more\"></a></p>\n<ul>\n<li><em>Vibratory Telepathy</em>.&nbsp; By transmitting invisible vibrations through the very air itself, two users of this ability can <em>share thoughts</em>.&nbsp; As a result, Vibratory Telepaths can form emotional bonds much deeper than those possible to other primates.</li>\n<li><em>Psychometric Tracery.</em>&nbsp; By tracing small fine lines on a surface, the Psychometric Tracer can leave impressions of emotions, history, knowledge, even the structure of other spells.&nbsp; This is a higher level than Vibratory Telepathy as a Psychometric Tracer can share the thoughts of long-dead Tracers who lived thousands of years earlier.&nbsp; By reading one Tracery and inscribing another simultaneously, Tracers can duplicate Tracings; and these replicated Tracings can even contain the detailed pattern of other spells and magics.&nbsp; Thus, the Tracers wield almost unimaginable power as magicians; but Tracers can get in trouble trying to use complicated Traceries that they could not have Traced themselves.</li>\n<li><em>Multidimensional Kinesis.</em>&nbsp; With simple, almost unthinking acts of will, the Kinetics can cause extraordinarily complex forces to flow through small tentacles and into any physical object within touching range&mdash;not just pushes, but combinations of pushes at many points that can effectively apply torques and twists.&nbsp; The Kinetic ability is far subtler than it first appears: they use it not only to wield existing objects with martial precision, but also to apply forces that sculpt objects into forms more suitable for Kinetic wielding.&nbsp; They even create tools that extend the power of their Kinesis and enable them to sculpt ever-finer and ever-more-complicated tools, a positive feedback loop fully as impressive as it sounds.</li>\n<li><em>The Eye.</em>&nbsp; The user of this ability can perceive infinitesimal traveling twists in the Force that binds matter&mdash;tiny vibrations, akin to the life-giving power of the Sun that falls on leaves, but far more subtle.&nbsp; A bearer of the Eye can sense objects far beyond the range of touch using the tiny disturbances they make in the Force.&nbsp; Mountains many days travel away can be known to them as if within arm's reach.&nbsp; According to the bearers of the Eye, when night falls and sunlight fails, they can sense huge fusion fires burning at unthinkable distances&mdash;though no one else has any way of verifying this.&nbsp; Possession of a single Eye is said to make the bearer equivalent to royalty.</li>\n</ul>\n<p>And finally,</p>\n<ul>\n<li>\n<p><em>The Ultimate Power.</em>&nbsp; The user of this ability contains a smaller, imperfect echo of the entire universe, enabling them to search out paths through probability to any desired future.&nbsp; If this sounds like a ridiculously powerful ability, you're right&mdash;game balance goes right out the window with this one.&nbsp; Extremely rare among life forms, it is the <em>sekai no ougi</em> or \"hidden technique of the world\".</p>\n<p>Nothing can oppose the Ultimate Power except the Ultimate Power.&nbsp; Any less-than-ultimate Power will simply be \"comprehended\" by the Ultimate and disrupted in some inconceivable fashion, or even absorbed into the Ultimates' own power base.&nbsp; For this reason the Ultimate Power is sometimes called the \"master technique of techniques\" or the \"trump card that trumps all other trumps\".&nbsp; The more powerful Ultimates can stretch their \"comprehension\" across galactic distances and aeons of time, and even perceive the bizarre laws of the hidden \"world beneath the world\".</p>\n<p>Ultimates have been killed by immense natural catastrophes, or by extremely swift surprise attacks that give them no chance to use their power.&nbsp; But all such victories are ultimately a matter of luck&mdash;it does not confront the Ultimates on their own probability-bending level, and if they survive they will begin to bend Time to avoid future attacks.</p>\n<p>But the Ultimate Power itself is also dangerous, and many Ultimates have been destroyed by their own powers&mdash;falling into one of the flaws in their imperfect inner echo of the world.</p>\n<p>Stripped of weapons and armor and locked in a cell, an Ultimate is still one of the most dangerous life-forms on the planet.&nbsp; A sword can be broken and a limb can be cut off, but the Ultimate Power is \"the power that cannot be removed without removing you\".</p>\n<p>Perhaps because this connection is so intimate, the Ultimates regard one who loses their Ultimate Power permanently&mdash;without hope of regaining it&mdash;as <em>schiavo,</em> or \"dead while breathing\".&nbsp; The Ultimates argue that the Ultimate Power is so important as to be a necessary part of what makes a creature an end in itself, rather than a means.&nbsp; The Ultimates even insist that anyone who lacks the Ultimate Power cannot begin to truly comprehend the Ultimate Power, and hence, cannot understand why the Ultimate Power is morally important&mdash;a suspiciously self-serving argument.</p>\n<p>The users of this ability form an absolute aristocracy and treat all other life forms as their pawns.</p>\n</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"36RkM85iDocrnaypb": 1, "bmfs4jiLaF6HiiYkC": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SXK87NgEPszhWkvQm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 158, "baseScore": 183, "extendedScore": null, "score": 0.000268, "legacy": true, "legacyId": "1130", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "6BFkmEgre7uwhDxDR", "canonicalCollectionSlug": "rationality", "canonicalBookId": "ah2GqzZSeBpW9QHgb", "canonicalNextPostSlug": "the-beauty-of-settled-science", "canonicalPrevPostSlug": "if-you-demand-magic-magic-won-t-help", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 183, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 97, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7iTwGquBFZKttpEdE", "tPqQdLCuxanjhoaNs", "u6JzcFtPGiznFgDxP", "iiWiHgtQekWNnmE6Q", "KfMNFB3G7XNviHBPN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 13, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-01T17:04:06.000Z", "modifiedAt": null, "url": null, "title": "BHTV: Jaron Lanier and Yudkowsky", "slug": "bhtv-jaron-lanier-and-yudkowsky", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:52.145Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/N6MNzvgmHtTASpaSS/bhtv-jaron-lanier-and-yudkowsky", "pageUrlRelative": "/posts/N6MNzvgmHtTASpaSS/bhtv-jaron-lanier-and-yudkowsky", "linkUrl": "https://www.lesswrong.com/posts/N6MNzvgmHtTASpaSS/bhtv-jaron-lanier-and-yudkowsky", "postedAtFormatted": "Saturday, November 1st 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20BHTV%3A%20Jaron%20Lanier%20and%20Yudkowsky&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABHTV%3A%20Jaron%20Lanier%20and%20Yudkowsky%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN6MNzvgmHtTASpaSS%2Fbhtv-jaron-lanier-and-yudkowsky%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=BHTV%3A%20Jaron%20Lanier%20and%20Yudkowsky%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN6MNzvgmHtTASpaSS%2Fbhtv-jaron-lanier-and-yudkowsky", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN6MNzvgmHtTASpaSS%2Fbhtv-jaron-lanier-and-yudkowsky", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 86, "htmlBody": "<p>My Bloggingheads.tv <a href=\"http://bloggingheads.tv/diavlogs/15555\">interview with Jaron Lanier</a> is up.&nbsp; Reductionism, zombies, and questions that you're not allowed to answer:</p>\n\n<p><embed height=\"288\" width=\"380\" flashvars=\"file=http%3A%2F%2Fbloggingheads%2Etv%2Fdiavlogs%2Fliveplayer%2Dplaylist%2F15555%3Fin%3D00%3A00%26out%3D60%3A31\" src=\"http://www.bloggingheads.tv/maulik/offsite/offsite_flvplayer.swf\" type=\"application/x-shockwave-flash\"></embed></p>\n\n<p>This ended up being more of me interviewing Lanier than a dialog, I'm afraid.&nbsp; I was a little too reluctant to interrupt.&nbsp; But you at least get a chance to see the probes I use, and Lanier's replies to them.</p>\n\n<p>If there are any BHTV heads out there who read Overcoming Bias and have something they'd like to talk to me about, do let me or our kindly producers know.</p>\n\n", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"9DNZfxFvY5iKoZQbz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "N6MNzvgmHtTASpaSS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "1131", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-02T17:00:00.000Z", "modifiedAt": null, "url": null, "title": "Building Something Smarter", "slug": "building-something-smarter", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:29.681Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GpvvQzf3pPyPsBepr/building-something-smarter", "pageUrlRelative": "/posts/GpvvQzf3pPyPsBepr/building-something-smarter", "linkUrl": "https://www.lesswrong.com/posts/GpvvQzf3pPyPsBepr/building-something-smarter", "postedAtFormatted": "Sunday, November 2nd 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Building%20Something%20Smarter&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABuilding%20Something%20Smarter%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGpvvQzf3pPyPsBepr%2Fbuilding-something-smarter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Building%20Something%20Smarter%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGpvvQzf3pPyPsBepr%2Fbuilding-something-smarter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGpvvQzf3pPyPsBepr%2Fbuilding-something-smarter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1264, "htmlBody": "<p><strong>Previously in series</strong>:&nbsp; <a href=\"/lw/vb/efficient_crossdomain_optimization/\">Efficient Cross-Domain Optimization</a> </p>\n\n<p>Once you demystify &quot;intelligence&quot; far enough to think of it as searching possible chains of causality, across learned domains, in order to find actions leading to a future ranked high in a preference ordering...</p>\n\n<p>...then it no longer sounds quite as strange, to think of building something &quot;smarter&quot; than yourself.</p>\n\n<p>There's a popular conception of AI as a tape-recorder-of-thought, which only plays back knowledge given to it by the programmers - I deconstructed this in <a href=\"/lw/l9/artificial_addition/\">Artificial Addition</a>, giving the example of the machine that stores the expert knowledge <tt>Plus-Of(Seven, Six) = Thirteen</tt> instead of having a CPU that does binary arithmetic.</p>\n\n<p>There's multiple sources supporting this misconception:</p>\n\n<p>The stereotype &quot;intelligence as book smarts&quot;, where you <a href=\"/lw/iq/guessing_the_teachers_password/\">memorize disconnected &quot;facts&quot; in class and repeat them back</a>.</p>\n\n<p>The idea that &quot;machines do only what they are told to do&quot;, which confuses the idea of a system whose abstract laws you designed, with your exerting moment-by-moment detailed control over the system's output.</p>\n\n<p>And various reductionist confusions - a computer is &quot;mere transistors&quot; or &quot;only remixes what's already there&quot; (just as Shakespeare merely regurgitated what his teachers taught him: the alphabet of English letters - all his plays are merely that).</p><a id=\"more\"></a><p>Since the workings of human intelligence are still to some extent unknown, and will seem very <a href=\"/lw/iu/mysterious_answers_to_mysterious_questions/\">mysterious</a> indeed to one who has <a href=\"/lw/kj/no_one_knows_what_science_doesnt_know/\">not studied much cognitive science</a>, it will seem impossible for the one to imagine that a machine could contain the <em>generators </em>of knowledge.</p>\n\n<p>The knowledge-generators and behavior-generators are black boxes, or even <a href=\"/lw/ta/invisible_frameworks/\">invisible background frameworks</a>.&nbsp; So tasking the imagination to visualize &quot;Artificial Intelligence&quot; only shows specific answers, specific beliefs, specific behaviors, impressed into a &quot;machine&quot; like being stamped into clay.&nbsp; The frozen outputs of human intelligence, <a href=\"/lw/la/truly_part_of_you/\">divorced of their generator</a> and not capable of change or improvement.</p>\n\n<p>You can't build Deep Blue by programming a good chess move for every possible position.&nbsp; First and foremost, you don't know <em>exactly</em> which chess positions the AI will encounter.&nbsp; You would have to record a specific move for zillions of positions, more than you could consider in a lifetime with your slow neurons.</p>\n\n<p>But worse, even if you <em>could</em> record and play back &quot;good moves&quot;, the resulting program would not play chess any better than you do.&nbsp; That is the peril of recording and playing back surface phenomena, rather than <a href=\"/lw/la/truly_part_of_you/\">capturing the underlying generator</a>.</p>\n\n<p>If I want to create an AI that plays better chess than I do, I have to program a <em>search</em> for winning moves.&nbsp; I can't program in specific moves because then the chess player really <em>won't</em> be any better than I am.&nbsp; And indeed, this holds true on <em>any</em> level where an answer has to meet a sufficiently high standard.&nbsp; If you want <em>any</em> answer better than you could come up with yourself, you <em>necessarily</em> sacrifice your ability to predict the <em>exact</em> answer in advance - though not necessarily your ability to <a href=\"/lw/v8/belief_in_intelligence/\">predict that the answer will be &quot;good&quot;</a> according to a known criterion of goodness.&nbsp; &quot;We never run a computer program unless we know an important fact about the output and we don't know the output,&quot; said Marcello Herreshoff.</p>\n\n<p>Deep Blue played chess barely better than the world's top humans, but a <em>heck</em> of a lot better than its own programmers.&nbsp; Deep Blue's programmers had to know the <em>rules</em> of chess - since Deep Blue wasn't enough of a general AI to learn the rules by observation - but the programmers didn't <em>play</em> chess anywhere near as well as Kasparov, let alone Deep Blue.</p>\n\n<p>Deep Blue's programmers didn't just capture their <em>own</em> chess-move generator.&nbsp; If they'd captured their own chess-move generator, they could have avoided the problem of programming an infinite number of chess positions.&nbsp; But they couldn't have beat Kasparov; they couldn't have built a program that played better chess than any human in the world.</p>\n\n<p>The programmers built a <em>better</em> move generator - one that more powerfully <em>steered the game</em> toward the <em>target</em> of winning game positions.&nbsp; Deep Blue's programmers surely had some slight ability to find chess moves that aimed at this same target, but their steering ability was much weaker than Deep Blue's.</p>\n\n<p>It is futile to protest that this is &quot;paradoxical&quot;, since it actually happened.</p>\n\n<p>Equally &quot;paradoxical&quot;, but true, is that Garry Kasparov was not born with a complete library of chess moves programmed into his DNA.&nbsp; Kasparov invented his own moves; he was not explicitly preprogrammed by evolution to make particular moves - though natural selection did build a brain that could learn.&nbsp; And Deep Blue's programmers invented Deep Blue's code without evolution explicitly encoding Deep Blue's code into their genes. </p>\n\n<p>Steam shovels lift more weight than humans can heft, skyscrapers are taller than their human builders, humans play better chess than natural selection, and computer programs play better chess than humans.&nbsp; The creation can exceed the creator.&nbsp; It's just a fact.</p>\n\n<p>If you can understand steering-the-future, hitting-a-narrow-target as the <em>work performed</em> by intelligence - then, even without knowing <em>exactly</em> how the work gets done, it should become more <em>imaginable</em> that you could build something smarter than yourself.</p>\n\n<p>By building something and then testing it?&nbsp; So that we can see that a design reaches the target faster or more reliably than our own moves, even if we don't understand how?&nbsp; But that's not how Deep Blue was actually built.&nbsp; You may recall the principle that <a href=\"/lw/jo/einsteins_arrogance/\">just formulating a good hypothesis to test, usually requires far more evidence than the final test that 'verifies' it</a> - that Einstein, in order to invent General Relativity, must have <em>already had in hand</em> enough evidence to isolate that one hypothesis as worth testing.&nbsp; Analogously, we can see that nearly all of the optimization power of human engineering must have already been exerted in coming up with good designs to test.&nbsp; The final selection on the basis of good results is only the icing on the cake.&nbsp; If you test four designs that seem like good ideas, and one of them works best, then at most 2 bits of optimization pressure can come from testing - the rest of it must be the abstract thought of the engineer.</p>\n\n<p>There are those who will see it as almost a religious principle that no one can possibly know that a design will work, no matter how good the argument, until it is actually tested.&nbsp; Just like the belief that no one can possibly accept a scientific theory, until it is tested.&nbsp; But this is ultimately more of an injunction against human stupidity and overlooked flaws and optimism and self-deception and the like - so far as theoretical possibility goes, it is clearly possible to get a pretty damn good idea of which designs will work in advance of testing them.</p>\n\n<p>And to say that humans are necessarily at least as good at chess as Deep Blue, since they built Deep Blue?&nbsp; Well, it's an important fact that we built Deep Blue, but the claim is still a nitwit sophistry.&nbsp; You might as well say that proteins are as smart as humans, that natural selection reacts as fast as humans, or that the laws of physics play good chess.</p>\n\n<p>If you carve up the universe along its joints, you will find that there are certain things, like butterflies and humans, that bear the very identifiable design signature and limitations of evolution; and certain other things, like nuclear power plants and computers, that bear the signature and the empirical design level of human intelligence.&nbsp; To describe the universe well, you will have to <a href=\"/lw/ic/the_virtue_of_narrowness/\">distinguish</a> these signatures from each other, and have separate names for &quot;human intelligence&quot;, &quot;evolution&quot;, &quot;proteins&quot;, and &quot;protons&quot;, because even if these things are <em>related</em> they are not at all the same.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xexCWMyds6QLWognu": 1, "nZCb9BSnmXZXSNA2u": 1, "aHjTRDkGypPqbXWpN": 1, "Ng8Gice9KNkncxqcj": 1, "sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GpvvQzf3pPyPsBepr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 22, "extendedScore": null, "score": 3.2e-05, "legacy": true, "legacyId": "1132", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yLeEPFnnB9wE7KLx2", "YhgjmCxcQXixStWMC", "NMoLJuDJEms7Ku9XS", "6i3zToomS86oj9bS6", "vNBxmcHpnozjrJnJP", "sCs48JtMnQwQsZwyN", "fg9fXrHpeaDD6pEPL", "HktFCy6dgsqJ9WPpX", "MwQRucYo6BZZwjKE7", "yDfxTj9TKYsYiWH5o"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-03T20:27:23.000Z", "modifiedAt": null, "url": null, "title": "Complexity and Intelligence", "slug": "complexity-and-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:29.338Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rELc88PvDkhetQzqx/complexity-and-intelligence", "pageUrlRelative": "/posts/rELc88PvDkhetQzqx/complexity-and-intelligence", "linkUrl": "https://www.lesswrong.com/posts/rELc88PvDkhetQzqx/complexity-and-intelligence", "postedAtFormatted": "Monday, November 3rd 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Complexity%20and%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComplexity%20and%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrELc88PvDkhetQzqx%2Fcomplexity-and-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Complexity%20and%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrELc88PvDkhetQzqx%2Fcomplexity-and-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrELc88PvDkhetQzqx%2Fcomplexity-and-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3390, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/vg/building_something_smarter/\">Building Something Smarter</a><a href=\"/lw/qk/that_alien_message/\">&nbsp;</a>, <a href=\"/lw/ix/say_not_complexity/\">Say Not \"Complexity\"</a>, <a href=\"/lw/qk/that_alien_message/\">That Alien Message</a></p>\n<p>One of the Godel-inspired challenges to the idea of self-improving minds is based on the notion of \"complexity\".</p>\n<p>Now \"complexity\", as I've <a href=\"/lw/ix/say_not_complexity/\">previously mentioned</a>, is a dangerous sort of word.&nbsp; \"Complexity\" conjures up the image of a huge machine with incomprehensibly many gears inside - an <em>impressive</em> sort of image.&nbsp; Thanks to this impressiveness, \"complexity\" sounds like it could be <a href=\"/lw/ip/fake_explanations/\">explaining</a> <em>all sorts</em> of things - that <em>all sorts</em> of phenomena could be happening <a href=\"/lw/is/fake_causality/\">because of</a> \"complexity\".</p>\n<p>It so happens that \"complexity\" also names another meaning, strict and mathematical: the <em>Kolmogorov complexity</em> of a pattern is the size of the program code of the shortest Turing machine that <em>produces the pattern as an output</em>, given unlimited tape as working memory.</p>\n<p>I immediately note that this mathematical meaning, is <em>not</em> the same as that intuitive image that comes to mind when you say \"complexity\".&nbsp; The vast impressive-looking collection of wheels and gears?&nbsp; That's not what the math term means.</p>\n<p>Suppose you ran a Turing machine with unlimited tape, so that, starting from our laws of physics, it simulated our whole universe - not just the region of space we see around us, but all regions of space and all <a href=\"/lw/r8/and_the_winner_is_manyworlds/\">quantum branches</a>.&nbsp; (There's strong indications our universe may be effectively discrete, but if not, just calculate it out to <a href=\"/lw/kn/torture_vs_dust_specks/\">3^^^3</a> digits of precision.)</p>\n<p>Then the \"Kolmogorov complexity\" of that entire universe - throughout all of space and all of time, from the Big Bang to whatever end, and all the life forms that ever evolved on Earth and all the decoherent branches of Earth and all the life-bearing planets anywhere, and all the intelligences that ever devised galactic civilizations, and all the art and all the technology and every machine ever built by those civilizations...</p>\n<p>...would be 500 bits, or whatever the size of the true laws of physics when written out as equations on a sheet of paper.</p>\n<p>The Kolmogorov complexity of just a <em>single</em> planet, like Earth, would of course be much<em> higher</em> than the \"complexity\" of the entire universe that contains it.</p>\n<p><a id=\"more\"></a></p>\n<p>\"Eh?\" you say.&nbsp; \"What's this?\" you say.&nbsp; \"How can a single planet contain more wheels and gears, more <em>complexity,</em> than the whole vast turning universe that embeds it?&nbsp; Wouldn't one planet contain fewer books, fewer brains, fewer species?\"</p>\n<p>But the new meaning that certain mathematicians have formulated and attached to the English word \"complexity\", is not like the visually impressive complexity of a confusing system of wheels and gears.</p>\n<p>It is, rather, the size of the smallest Turing machine that <em>unfolds into</em> a certain pattern.</p>\n<p>If you want to print out the entire universe from the beginning of time to the end, you only need to specify the laws of physics.</p>\n<p>If you want to print out just Earth by itself, then it's not enough to specify the laws of physics.&nbsp; You also have to point to <em>just Earth</em> within the universe.&nbsp; You have to narrow it down somehow.&nbsp; And, in a big universe, that's going to take a lot of information.&nbsp; It's like the difference between giving directions to a city, and giving directions for finding a single grain of sand on one beach of one city.&nbsp; Indeed, with all those quantum branches existing side-by-side, it's going to take around as much information to <em>find</em> Earth in the universe, as to just directly specify the positions of all the atoms.</p>\n<p>Kolmogorov complexity is the sense at which we zoom into <a href=\"http://www.youtube.com/watch?v=WAJE35wX1nQ&amp;fmt=18\">the endless swirls of the Mandelbrot fractal</a> and think, not \"How complicated\", but rather, \"How simple\", because the Mandelbrot set is defined using <a href=\"http://en.wikipedia.org/wiki/Mandelbrot_set\">a very simple equation</a>.&nbsp; But if you wanted to find a subset of the Mandelbrot set, one <em>particular</em> swirl, you would have to specify <em>where</em> to look, and that would take more bits.</p>\n<p>That's why we use the <em>Kolmogorov</em> complexity in <a href=\"/lw/jp/occams_razor/\">Occam's Razor</a> to determine how \"complicated\" or \"simple\" something is.&nbsp; So that, when we think of the <em>entire</em> universe, all the stars we can see and all the implied stars we can't see, and hypothesize laws of physics standing behind it, we will think \"what a simple universe\" and not \"what a complicated universe\" - just like looking into the Mandelbrot fractal and thinking \"how simple\".&nbsp; We could never accept a theory of physics as probably true, or even remotely possible, if it got <a href=\"/lw/q3/decoherence_is_simple/\">an Occam penalty the size of the universe</a>.</p>\n<p>As a logical consequence of the way that Kolmogorov complexity has been defined, no closed system can ever increase in Kolmogorov complexity.&nbsp; (At least, no closed system without a 'true randomness' source.)&nbsp; A program can pattern ever more interacting wheels and gears into its RAM, but nothing it does from within itself can increase \"the size of the smallest computer program that unfolds into it\", <a href=\"/lw/nz/arguing_by_definition/\">by definition</a>.</p>\n<p>Suppose, for example, that you had a computer program that defined a synthetic biology and a synthetic gene system.&nbsp; And this computer program ran through an artificial evolution that simulated 10^44 organisms (which is one estimate for the number of creatures who have ever lived on Earth), and subjected them to some kind of competition.&nbsp; And finally, after some predefined number of generations, it selected the highest-scoring organism, and printed it out.&nbsp; In an <em>intuitive</em> sense, you would (expect to) say that the best organisms on each round were getting more complicated, their biology more intricate, as the artificial biosystem evolved.&nbsp; But from the standpoint of Kolmogorov complexity, the final organism, even after a trillion years, is no more \"complex\" than the program code it takes to specify the tournament and the criterion of competition.&nbsp; The organism that wins is implicit in the specification of the tournament, so it can't be any more \"complex\" than that.</p>\n<p>But if, on the other hand, you reached into the biology and made a few million random changes here and there, the <em>Kolmogorov complexity</em> of the whole system would shoot way up: anyone wanting to specify it exactly, would have to describe the random changes that you made.</p>\n<p>I specify \"random\" changes, because if you made the changes with beneficence aforethought - according to some criterion of goodness - then I could just talk about the compact criterion you used to make the changes.&nbsp; Only random information is incompressible on average, so you have to make <em>purposeless</em> changes if you want to increase the Kolmogorov complexity as fast as possible.</p>\n<p>So!&nbsp; As you've probably guessed, the argument against self-improvement is that since closed systems cannot increase their \"complexity\", the AI must look out upon the world, demanding a <a href=\"/lw/qk/that_alien_message/\">high-bandwidth sensory feed</a>, in order to grow up.</p>\n<p><em>If,</em> that is, you believe that \"increasing Kolmogorov complexity\" is prerequisite to increasing real-world effectiveness.</p>\n<p>(We will <a href=\"/lw/vg/building_something_smarter/\">dispense with the idea</a> that if system A builds system B, then system A must be \"by definition\" as smart as system B.&nbsp; This is the \"Einstein's mother must have been one heck of a physicist\" sophistry.&nbsp; Even if a future planetary ecology is in some sense \"implicit\" in a single self-replicating RNA strand in some tidal pool, the ecology is a lot more impressive in a real-world sense: in a given period of time it can exert <a href=\"/lw/va/measuring_optimization_power/\">larger optimization pressures</a> and do more neat stuff.)</p>\n<p>Now, how does one argue that \"increasing Kolmogorov complexity\" has something to do with increasing intelligence?&nbsp; Especially when small machines can unfold into whole universes, and the maximum Kolmogorov complexity is realized by random noise?</p>\n<p>One of the other things that a closed computable system provably can't do, is solve <a href=\"http://en.wikipedia.org/wiki/Halting_problem\">the general halting problem</a> - the problem of telling whether <em>any</em> Turing machine halts.</p>\n<p>A similar proof shows that, if you take some given solver, and consider the maximum size bound such that the solver can solve the halting problem for <em>all</em> machines of that size or less, then this omniscience is bounded by at most the solver's own complexity plus a small constant.</p>\n<p>So... isn't <em>increasing your Kolmogorov complexity</em> through outside sensory inputs, the key to learning to solve the halting problem for ever-larger systems?</p>\n<p>And doesn't this show that no closed system can \"self-improve\"?</p>\n<p>In a word, no.</p>\n<p>I mean, if you were to try to write it out as logic, you'd find that one of the steps involves saying, \"If you can solve all systems of complexity N, you must be of complexity greater than N (maybe minus a small constant, depending on the details of the proof).&nbsp; Therefore, by increasing your complexity, you increase the range of systems you can solve.\"&nbsp; This is formally a non-sequitur.</p>\n<p>It's also a non-sequitur in practice.</p>\n<p>I mean, sure, if we're not dealing with a closed system, you can't <em>prove</em> that it won't solve the halting problem.&nbsp; You could be looking at an external bright light in the sky that flashes on or off to reveal the halting solution.</p>\n<p>But unless you <em>already</em> have that kind of mathematical ability yourself, you won't <em>know</em> just from looking at the light that it's giving you true solutions to the halting problem.&nbsp; You must have just been constructed with faith in the light, and the light must just happen to work.</p>\n<p>(And in any computable universe, any light in the sky that you see <em>won't</em> happen to solve the halting problem.)</p>\n<p>It's not easy for \"sensory information\" to give you <em>justified new mathematical knowledge</em> that you could not <em>in principle</em> obtain with your eyes closed.</p>\n<p>It's not a matter, for example, of seeing written in the sky a brilliant proof, that you would never have thought of on your own.&nbsp; A closed system with infinite RAM can close its eyes, and write out <em>every possible sensory experience it could have, along with its own reactions to them,</em> that could occur within some bounded number of steps.&nbsp; Doing this does not increase its Kolmogorov complexity.</p>\n<p>So the notion can't be that the environment tells you something that you recognize as a proof, but didn't think of on your own.&nbsp; Somehow, having that sensory experience in particular, has to increase your mathematical ability <em>even after you perfectly imagined that experience and your own reaction to it in advance.</em></p>\n<p>Could it be the healing powers of having a larger universe to live in, or other people to talk to?&nbsp; But you can simulate incredibly large universes - vastly larger than anything we see in our telescopes, <a href=\"http://en.wikipedia.org/wiki/Knuth%27s_up-arrow_notation\">up-arrow</a> large - <em>within</em> a closed system without increasing its Kolmogorov complexity.&nbsp; Within that simulation you could peek at people watching the stars, and peek at people interacting with each other, and plagiarize the books they wrote about the deep wisdom that comes from being embedded in a larger world.</p>\n<p>What <em>justified novel mathematical</em> knowledge - about the halting problem in particular - could you gain from a sensory experience, that you could not gain from perfectly imagining that sensory experience and your own reaction to it, nor gain from peeking in on a simulated universe that included someone having that sensory experience?</p>\n<p>Well, you can always suppose that you were born trusting the light in the sky, and the light in the sky always happens to tell the truth.</p>\n<p>But what's actually going on is that the non-sequitur is coming back to bite:&nbsp; Increasing your Kolmogorov complexity doesn't necessarily increase your ability to solve math problems.&nbsp; Swallowing a bucket of random bits will increase your Kolmogorov complexity too.</p>\n<p>You aren't likely to learn any <em>mathematics</em> by gazing up at the external stars, that you couldn't learn from \"navel-gazing\" into an equally large simulated universe.&nbsp; Looking at the <em>actual</em> stars around you is good for figuring out <em>where</em> in the universe you are (the <em>extra</em> information that specifies your location) but not much good for learning <em>new math</em> that you couldn't learn by navel-gazing into a simulation as large as our universe.</p>\n<p>In fact, it's just <em>bloody hard</em> to <em>fundamentally</em> increase your ability to solve math problems in a way that \"no closed system can do\" just by opening the system.&nbsp; So far as I can tell, it basically requires that the environment be magic and that you be born with faith in this fact.</p>\n<p>Saying that a 500-state Turing machine might be able to solve all problems up to at most 500 states plus a small constant, is misleading.&nbsp; That's an upper bound, not a lower bound, and it comes from having a constructive way to build a specific unsolvable Turing machine out of the solver.&nbsp; <span style=\"text-decoration: line-through;\">In reality, you'd expect a 500-state Turing machine to get <em>nowhere near</em> solving the halting problem up to 500.&nbsp; I would <em>drop dead of shock</em> if there were a 500-state Turing machine that solved the halting problem for all the Turing machines up to 50 states.</span>&nbsp; The vast majority of 500-state Turing machines that implement something that looks like a \"halting problem solver\" will go nowhere near 500 states (but see <a href=\"http://www.overcomingbias.com/2008/11/complexity-and.html#comment-137562957\">this comment</a>).</p>\n<p>Suppose you write a relatively short Turing machine that, by virtue of its unlimited working memory, creates an entire universe containing googols or <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">up-arrows</a> of atoms...</p>\n<p>...and within this universe, life emerges on some planet-equivalent, and evolves, and develops intelligence, and devises science to study its ecosphere and its universe, and builds computers, and goes out into space and investigates the various physical systems that have formed, and perhaps encounters other life-forms...</p>\n<p>...and over the course of trillions or up-arrows of years, a transgalactic intrauniversal economy develops, with conduits conducting information from one end of the universe to another (because you didn't pick a physics with inconvenient lightspeed limits), a superWeb of hyperintelligences all exchanging information...</p>\n<p>...and finally - after a long, long time - your computer program blinks a giant <a href=\"/lw/qk/that_alien_message/\">message</a> across the universe, containing a problem to be solved and a specification of how to answer back, and threatening to destroy their universe if the answer is wrong...</p>\n<p>...then this intrauniversal civilization - and everything it's ever learned by theory or experiment over the last <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">up-arrow years</a> - is said to contain 400 bits of complexity, or however long the original program was.</p>\n<p>But where did it get its math powers, from inside the closed system?</p>\n<p>If we trace back the origins of the hypergalactic civilization, then every belief it ever adopted about math, came from updating on some observed event.&nbsp; That might be a star exploding, or it might be the output of a calculator, or it might be <a href=\"/lw/jr/how_to_convince_me_that_2_2_3/\">an observed event within some mind's brain</a>... but in all cases, the update will occur because of a logic that says, \"If I see this, it means that\".&nbsp; Before you can learn, you must have the <a href=\"/lw/hk/priors_as_mathematical_objects/\">prior</a> that structures learning.&nbsp; If you see something that makes you decide to change the way you learn, then you must believe that seeing X implies you should learn a different way Y.&nbsp; That's how it would be for superintelligences, I expect.</p>\n<p>If you keep tracing back through that simulated universe, you arrive at something <em>before</em> the dawn of superintelligence - the <em>first</em> intelligent beings, produced by evolution.&nbsp; These first minds are the ones who'll look at Peano Arithmetic and think, \"This has never produced a contradiction, so it probably never will - I'll go ahead and program that into my AI.\"&nbsp; These first minds are the ones who'll think, \"Induction seems like it works pretty well, but how do I formalize the notion of induction?\"&nbsp; And these first minds are the ones who'll think, \"If I build a self-improving AI, how should it update itself - including changing its own updating process - from the results of observation?\"</p>\n<p>And how did the first minds get the ability to think those thoughts?&nbsp; From natural selection, that generated the adaptations that executed to think all those thoughts, using the simple evolutionary rule: \"keep what works\".</p>\n<p>And in turn, natural selection in this universe popped out of the laws of physics.</p>\n<p>So everything that this hypergalactic civilization ever believes about math, is really just induction in one form or another.&nbsp; All the evolved adaptations that do induction, produced by inductive natural selection; and all the generalizations made from experience, including generalizations about how to form better generalizations.&nbsp; It would all just unfold out of the inductive principle...</p>\n<p>...running in a box sealed as tightly shut as our own universe appears to be.</p>\n<p>And I don't see how we, in our own closed universe, are going to do any better.&nbsp; Even if we have the ability to look up at the stars, it's not going to give us the ability to go outside that inductive chain to obtain justified mathematical beliefs.</p>\n<p>If you wrote that 400-bit simulated universe over the course of a couple of months using human-level intelligence and some mysterious source of unlimited computing power, then <em>you</em> are much more complex than that hypergalactic civilization.&nbsp; <em>You</em> take much more than 400 bits to find within the space of possibilities, because you are only <em>one particular</em> brain.</p>\n<p>But y'know, I think that your mind, and the up-arrow mind of that inconceivable civilization, would still be worth distinguishing as Powers.&nbsp; Even if <em>you</em> can figure out how to ask <em>them</em> questions.&nbsp; And even if you're asking them questions by running an internal simulation, which makes it all part of your own \"complexity\" as defined in the math.</p>\n<p>To locate a up-arrow-sized mind within an up-arrow-sized civilization, would require up-arrow bits - even if the <em>entire</em> civilization unfolded out of a 400-bit machine as compact as our own laws of physics.&nbsp; But which would be more powerful, that one \"complex\" mind, or the \"simple\" civilization it was part of?</p>\n<p>None of this violates Godelian limitations.<em>&nbsp;</em>You can transmit to the hypergalactic civilization a similar Turing machine to the one that built it, and ask it how <em>that</em> Turing machine behaves.&nbsp; If you can fold a hypergalactic civilization into a 400-bit Turing machine, then even a hypergalactic civilization can confront questions about the behavior of 400-bit Turing machines that are <em>real stumpers.</em></p>\n<p>And 400 bits is going to be an overestimate.&nbsp; I bet there's at least one up-arrow-sized hypergalactic civilization folded into a halting Turing machine with 15 states, or something like that.&nbsp; If that seems unreasonable, you are not acquainted with the <a href=\"http://en.wikipedia.org/wiki/Busy_Beaver\">Busy-Beaver problem</a>.</p>\n<p>You can get a hell of a lot of mathematical ability out of small Turing machines that unfold into pangalactic hypercivilizations.&nbsp; But unfortunately, there are other small Turing machines that are hellishly difficult problems - perhaps unfolding into hypercivilizations themselves, or things even less comprehensible.&nbsp; So even the tremendous mathematical minds that can unfold out of small Turing machines, won't be able to solve all Turing machines up to a larger size bound.&nbsp; Hence no Godelian contradiction.</p>\n<p>(I wonder:&nbsp; If humanity unfolded into a future civilization of infinite space and infinite time, creating descendants and hyperdescendants of unlimitedly growing size, what would be the largest Busy Beaver number ever agreed upon?&nbsp; 15?&nbsp; Maybe even as large as 20?&nbsp; Surely not 100 - you could encode a civilization of similar origins and equivalent power into a smaller Turing machine than that.)</p>\n<p>Olie Lamb said:&nbsp; \"I don't see anything good about complexity.&nbsp; There's nothing artful about complexity.&nbsp; There's nothing mystical about complexity.&nbsp; It's just complex.\"&nbsp; This is true even when you're talking about wheels and gears, never mind Kolmogorov complexity.&nbsp; It's simplicity, not complexity, that is the admirable virtue.</p>\n<p>The <em>real</em> force behind this whole debate is that the word \"complexity\" sounds impressive and can act as an explanation for anything you don't understand.&nbsp; Then the word gets captured by a mathematical property that's spelled using the same letters, which happens to be provably constant for closed systems.</p>\n<p>That, I think, is where the argument really comes from, as it rationalizes the even more primitive intuition of some <a href=\"/lw/qk/that_alien_message/\">blind helpless thing in a box</a>.</p>\n<p>This argument is promulgated even by some people who can write proofs about complexity - but frankly, I do not think they have picked up the habit of visualizing what their math symbols mean in real life.&nbsp; That thing Richard Feynman did, where he <a href=\"http://ask.metafilter.com/75320/Help-me-think-abstractly#1119355\">visualized two balls turning colors or growing hairs</a>?&nbsp; I don't think they do that.&nbsp; On the last step of interpretation, they just make a quick appeal to the sound of the words.</p>\n<p>But I <em>will</em> agree that, if the laws we know are true, then a self-improving mind which lacks sensory inputs, shouldn't be able to improve its mathematical abilities beyond the level of a up-arrow-sized civilization - for example, it shouldn't be able to solve Busy-Beaver(100).</p>\n<p>It might <em>perhaps</em> be more limited than this in mere <em>practice,</em> if it's just running on a laptop computer or something.&nbsp; But if <em>theoretical</em> mathematical arguments about closed systems show anything, <em>that</em> is what they show.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ac84EpK6mZbPLzmqj": 2, "5f5c37ee1b5cdee568cfb25c": 6}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rELc88PvDkhetQzqx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 31, "extendedScore": null, "score": 4.6e-05, "legacy": true, "legacyId": "1133", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 79, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GpvvQzf3pPyPsBepr", "5wMcKNAwB6X4mp9og", "kpRSCH7ALLcb6ucWM", "fysgqk4CjAwhBgNYT", "RgkqLqkg8vLhsYpfh", "9cgBF6BQ2TRB3Hy4E", "3wYTFWY3LKQCnAptN", "f4txACqDWithRi7hs", "Atu4teGvob5vKvEAF", "cFzC996D7Jjds3vS9", "Q4hLMDrFd8fbteeZ8", "a5JAiTdytou3Jg749", "6FmqiAgS8h4EJm86s", "jzf4Rcienrm6btRyt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-04T16:15:24.000Z", "modifiedAt": null, "url": null, "title": "Today's Inspirational Tale", "slug": "today-s-inspirational-tale", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:37.622Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4SysgzYYJmErwHrWw/today-s-inspirational-tale", "pageUrlRelative": "/posts/4SysgzYYJmErwHrWw/today-s-inspirational-tale", "linkUrl": "https://www.lesswrong.com/posts/4SysgzYYJmErwHrWw/today-s-inspirational-tale", "postedAtFormatted": "Tuesday, November 4th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Today's%20Inspirational%20Tale&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AToday's%20Inspirational%20Tale%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4SysgzYYJmErwHrWw%2Ftoday-s-inspirational-tale%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Today's%20Inspirational%20Tale%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4SysgzYYJmErwHrWw%2Ftoday-s-inspirational-tale", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4SysgzYYJmErwHrWw%2Ftoday-s-inspirational-tale", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<p>At a Foresight Gathering some years ago, a Congressman was in attendance, and he spoke to us and said the following:</p><blockquote><p>&quot;Everyone in this room who's signed up for cryonics, raise your hand.&quot;</p></blockquote><p>Many hands went up.</p><blockquote><p>&quot;Now everyone who knows the name of your representative in the House, raise your hand.&quot;</p></blockquote><p>Fewer hands went up.</p><blockquote><p>&quot;And you wonder why you don't have any political influence.&quot;</p></blockquote><p>Rationalists would likewise do well to keep this lesson in mind.</p><a id=\"more\"></a><p>(I should also mention that voting is a <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomblike problem</a>.&nbsp; As I don't believe rational agents should <a href=\"/lw/to/the_truly_iterated_prisoners_dilemma/\">defect in the 100fold iterated prisoner's dilemma</a>, I don't buy the idea that rational agents don't vote<a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">&nbsp;</a>.)</p>\n\n<p>(See also <a href=\"/lw/mi/stop_voting_for_nincompoops/\">Stop Voting For Nincompoops</a>.&nbsp; It's more applicable to primaries than to the general election.&nbsp; But a vote for a losing candidate is not &quot;thrown away&quot;; it sends a message to mainstream candidates that you vote, but they have to work harder to appeal to your interest group to get your vote.&nbsp; Readers in non-swing states especially should consider what message they're sending with their vote before voting for any candidate, in any election, that they don't actually like.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FkzScn5byCs9PxGsA": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4SysgzYYJmErwHrWw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 17, "extendedScore": null, "score": 4.5556668589836075e-07, "legacy": true, "legacyId": "1134", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6ddcsdA2c2XpNpE5x", "jbgjvhszkr3KoehDh", "k5qPoHFgjyxtvYsm7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-05T14:00:00.000Z", "modifiedAt": null, "url": null, "title": "Hanging Out My Speaker's Shingle", "slug": "hanging-out-my-speaker-s-shingle", "viewCount": null, "lastCommentedAt": "2008-11-07T11:00:21.000Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wFHHeAdWab2i4zhNM/hanging-out-my-speaker-s-shingle", "pageUrlRelative": "/posts/wFHHeAdWab2i4zhNM/hanging-out-my-speaker-s-shingle", "linkUrl": "https://www.lesswrong.com/posts/wFHHeAdWab2i4zhNM/hanging-out-my-speaker-s-shingle", "postedAtFormatted": "Wednesday, November 5th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hanging%20Out%20My%20Speaker's%20Shingle&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHanging%20Out%20My%20Speaker's%20Shingle%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwFHHeAdWab2i4zhNM%2Fhanging-out-my-speaker-s-shingle%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hanging%20Out%20My%20Speaker's%20Shingle%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwFHHeAdWab2i4zhNM%2Fhanging-out-my-speaker-s-shingle", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwFHHeAdWab2i4zhNM%2Fhanging-out-my-speaker-s-shingle", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 334, "htmlBody": "<p>I was recently invited to give a talk on <a href=\"http://yudkowsky.net/rational/cognitive-biases\">heuristics and biases</a> at <a href=\"http://www.janestreet.com/\">Jane Street Capital</a>, one of the top proprietary trading firms (&quot;proprietary&quot; = they trade their own money).&nbsp; When I got back home, I realized that (a) I'd successfully managed to work through the trip, and (b) it'd been very pleasant mentally, a nice change of pace.&nbsp; (One of these days I have to blog about what I discovered at Jane Street - it turns out they've got their own rationalist subculture going.)</p>\n\n<p>So I've decided to <a href=\"http://yudkowsky.net/contact/speaking\">hang out my shingle as a speaker</a> at financial companies.</p>\n\n<p>You may be thinking:&nbsp; &quot;Perhaps, Eliezer, this is not the best of times.&quot;</p>\n\n<p>Well... I <em>do</em> have hopes that, among the firms interested in having me as a speaker, a higher-than-usual percentage will have come out of the crash okay.&nbsp; I checked recently to see if this were the case for Jane Street Capital, and it was.</p>\n\n<p>But more importantly - <em>your competitors are learning the secrets of rationality!</em>&nbsp; Are you?<em>&nbsp;</em></p>\n\n<p>Or maybe I should frame it as:&nbsp; &quot;Not doing too well this year?&nbsp; Drop the expensive big-name speakers.&nbsp; I can give a <a href=\"http://yudkowsky.net/contact/speaking\">fascinating and useful</a> talk and I won't charge you as much.&quot;</p>\n\n<p>And just to offer a bit of a carrot - if I can monetize by speaking, I'm much less likely to try charging for access to my future writings.&nbsp; No promises, but something to keep in mind.&nbsp; So do recommend me to your friends as well.</p><a id=\"more\"></a><p>I expect that, as I speak, the marginal value of money to my work will go down; the more I speak, the more my price will go up.&nbsp; If my (future) popular book on rationality becomes a hit, I'll upgrade to big-name fees.&nbsp; And later in my life, if all goes as planned, I'll be just plain not available.</p>\n\n<p>So I'm offering <em>you,</em> my treasured readers, a chance to get me early.&nbsp; I would suggest referencing <a href=\"http://yudkowsky.net/contact/speaking\">this page</a> when requesting me as a speaker.&nbsp; <a href=\"mailto:sentience@pobox.com\">Emails</a> will be answered in the order they arrive.</p>\n\n", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DQHWBcKeiLnyh9za9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wFHHeAdWab2i4zhNM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "1135", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-06T19:20:14.000Z", "modifiedAt": null, "url": null, "title": "Back Up and Ask Whether, Not Why", "slug": "back-up-and-ask-whether-not-why", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:32.017Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nHRri2F49MZbcqmkY/back-up-and-ask-whether-not-why", "pageUrlRelative": "/posts/nHRri2F49MZbcqmkY/back-up-and-ask-whether-not-why", "linkUrl": "https://www.lesswrong.com/posts/nHRri2F49MZbcqmkY/back-up-and-ask-whether-not-why", "postedAtFormatted": "Thursday, November 6th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Back%20Up%20and%20Ask%20Whether%2C%20Not%20Why&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABack%20Up%20and%20Ask%20Whether%2C%20Not%20Why%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnHRri2F49MZbcqmkY%2Fback-up-and-ask-whether-not-why%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Back%20Up%20and%20Ask%20Whether%2C%20Not%20Why%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnHRri2F49MZbcqmkY%2Fback-up-and-ask-whether-not-why", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnHRri2F49MZbcqmkY%2Fback-up-and-ask-whether-not-why", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 268, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/js/the_bottom_line/\">The Bottom Line</a></p>\n\n<p>A recent conversation reminded me of this simple, important, and difficult method:</p>\n\n<p>When someone asks you &quot;Why are you doing X?&quot;,<br />And you don't<em> remember</em> an answer <em>previously</em> in mind,<br />Do <em>not</em> ask yourself &quot;Why am I doing X?&quot;.</p>\n\n<p>For example, if someone asks you<br />&quot;Why are you using a QWERTY keyboard?&quot; or &quot;Why haven't you invested in stocks?&quot;<br />and you don't remember <em>already considering</em> this exact question and deciding it,<br />do <em>not</em> ask yourself &quot;Why am I using a QWERTY keyboard?&quot; or &quot;Why aren't I invested in stocks?&quot;</p>\n\n<p>Instead, try to blank your mind - maybe not a full-fledged <a href=\"/lw/ur/crisis_of_faith/\">crisis of faith</a>, but at least try to prevent your mind from <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">knowing the answer immediately</a> - and ask yourself:</p>\n\n<p>&quot;Should I do X, or not?&quot;</p>\n\n<p><em>Should</em> I use a QWERTY keyboard, or not?&nbsp; <em>Should</em> I invest in stocks, or not?</p>\n\n<p>When you finish considering this question, print out a traceback of the arguments that you yourself considered in order to arrive at your decision, whether that decision is to X, or not X.&nbsp; Those are your <a href=\"/lw/js/the_bottom_line/\">only real reasons</a>, nor is it possible to arrive at a real reason in any other way.</p>\n\n<p>And this is also writing advice: because I have sometimes been approached by people who say &quot;How do I convince people to wear green shoes?&nbsp; I don't know how to argue it,&quot; and I reply, &quot;Ask yourself honestly whether you should wear green shoes; then make a list of which thoughts actually move you to decide one way or another; then figure out how to explain or argue them, recursing as necessary.&quot;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZzxvopS4BwLuQy42n": 1, "2wjPMY34by2gXEXA2": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nHRri2F49MZbcqmkY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 51, "extendedScore": null, "score": 7.6e-05, "legacy": true, "legacyId": "1136", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["34XxbRFe54FycoCDw", "BcYBfG8KomcpcxkEg", "uHYYA32CKgKT3FagE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-07T23:22:35.000Z", "modifiedAt": null, "url": null, "title": "Recognizing Intelligence", "slug": "recognizing-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:32.022Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LZMeuRGQhSw77XewC/recognizing-intelligence", "pageUrlRelative": "/posts/LZMeuRGQhSw77XewC/recognizing-intelligence", "linkUrl": "https://www.lesswrong.com/posts/LZMeuRGQhSw77XewC/recognizing-intelligence", "postedAtFormatted": "Friday, November 7th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Recognizing%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARecognizing%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZMeuRGQhSw77XewC%2Frecognizing-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Recognizing%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZMeuRGQhSw77XewC%2Frecognizing-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZMeuRGQhSw77XewC%2Frecognizing-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1205, "htmlBody": "<p><strong>Previously in series</strong>:&nbsp; <a href=\"/lw/vg/building_something_smarter/\">Building Something Smarter</a></p>\n<p><a href=\"/lw/so/humans_in_funny_suits/\">Humans in Funny Suits</a> inveighed against the problem of \"aliens\" on TV shows and movies who think and act like 21st-century middle-class Westerners, even if they have tentacles or exoskeletons.&nbsp; If you were going to seriously ask what real aliens might be like, you would try to make fewer assumptions - a difficult task when <a href=\"/lw/rl/the_psychological_unity_of_humankind/\">the assumptions are invisible</a>.</p>\n<p>I <a href=\"/lw/va/measuring_optimization_power/\">previously</a> spoke of how you don't have to <em>start out</em> by assuming any particular goals, when dealing with an unknown intelligence.&nbsp; You can use <em>some</em> of your evidence to deduce the alien's goals, and then use that hypothesis to predict the alien's <em>future</em> achievements, thus making an epistemological profit.</p>\n<p>But could you, in principle, recognize an alien intelligence without even <em>hypothesizing</em> anything about its ultimate ends - anything about the <a href=\"/lw/l4/terminal_values_and_instrumental_values/\">terminal values</a> it's trying to achieve?</p>\n<p>This sounds like it goes against my suggested definition of intelligence, or even <a href=\"/lw/va/measuring_optimization_power/\">optimization</a>.&nbsp; How can you recognize something as having a strong ability to hit narrow targets in a large search space, if you have no idea what the target is?</p>\n<p>And yet, intuitively, it seems easy to imagine a scenario in which we could recognize an alien's intelligence while having no concept whatsoever of its <a href=\"/lw/l4/terminal_values_and_instrumental_values/\">terminal values</a> - having no idea where it's trying to steer the future.</p>\n<p><a id=\"more\"></a></p>\n<p>Suppose I landed on an alien planet and discovered what seemed to be a highly sophisticated machine, all gleaming chrome as the stereotype demands.&nbsp; Can I recognize this machine as being in any sense <em>well-designed</em>, if I have no idea what the machine is intended to accomplish?&nbsp; Can I guess that the machine's makers were intelligent, without guessing their motivations?</p>\n<p>And again, it seems like in an intuitive sense I should obviously be able to do so.&nbsp; I look at the cables running through the machine, and find large electrical currents passing through them, and discover that the material is a flexible high-temperature high-amperage superconductor.&nbsp; Dozens of gears whir rapidly, perfectly meshed...</p>\n<p>I have no idea what the machine is doing.&nbsp; I don't even have a <em>hypothesis</em> as to what it's doing.&nbsp; Yet I have recognized the machine as the product of an alien intelligence.&nbsp; Doesn't this show that \"optimization process\" is not an indispensable notion to \"intelligence\"?</p>\n<p>But you can't possibly recognize intelligence without at least having such a thing as a concept of \"intelligence\" that <a href=\"/lw/o0/where_to_draw_the_boundary/\">divides the universe</a> into intelligent and unintelligent parts.&nbsp; For there to be a concept, there has to be a boundary.&nbsp; So what <em>am</em> I recognizing?</p>\n<p>If I don't see any <em>optimization criterion</em> by which to judge the parts or the whole - so that, as far as I know, a random volume of air molecules or a clump of dirt would be just as good a design - then why am I focusing on this particular object and saying, \"Here is a machine\"? Why not say the same about a cloud or a rainstorm?</p>\n<p>Why is it a good hypothesis to suppose that intelligence <em>or any other optimization process</em> played a role in selecting the form of what I see, any more than it is a good hypothesis to suppose that the dust particles in my rooms are arranged by dust elves?</p>\n<p>Consider that gleaming chrome.&nbsp; Why did humans start making things out of metal?&nbsp; Because metal is hard; it retains its shape for a long time.&nbsp; So when you try to do <em>something,</em> and the <em>something</em> stays the same for a long period of time, the way-to-do-it may also stay the same for a long period of time.&nbsp; So you face the subproblem of creating things that keep their form and function.&nbsp; Metal is one solution to that subproblem.</p>\n<p>There are no-free-lunch theorems showing the impossibility of various kinds of inference, in <a href=\"/lw/o5/the_second_law_of_thermodynamics_and_engines_of/\">maximally disordered</a> universes.&nbsp; In the same sense, if an alien's goals were maximally disordered, it would be unable to achieve those goals and you would be unable to detect their achievement.</p>\n<p>But as simple a form of <a href=\"/lw/o5/the_second_law_of_thermodynamics_and_engines_of/\">negentropy</a> as <em>regularity over time</em> - that the alien's terminal values don't take on a new random form with each clock tick - can imply that hard metal, or some other durable substance, would be useful in a \"machine\" - a persistent configuration of material that helps promote a persistent goal.</p>\n<p>The gears are a solution to the problem of transmitting mechanical forces from one place to another, which you would want to do because of the presumed economy of scale in generating the mechanical force at a central location and then distributing it.&nbsp; In their meshing, we recognize a force of optimization applied in the service of a recognizable <em>instrumental</em> value: most random gears, or random shapes turning against each other, would fail to mesh, or fly apart.&nbsp; Without knowing what the mechanical forces are meant to do, we recognize something that transmits mechanical force - this is why gears appear in many human artifacts, because it doesn't matter much what kind of mechanical force you need to transmit on the other end.&nbsp; You may still face problems like trading torque for speed, or moving mechanical force from generators to appliers.</p>\n<p>These are not <a href=\"/lw/rn/no_universally_compelling_arguments/\">universally</a> convergent instrumental challenges.&nbsp; They probably aren't even convergent with respect to maximum-entropy goal systems (which are mostly out of luck).</p>\n<p>But relative to the space of <em>low-entropy, highly regular</em> goal systems - goal systems that <em>don't</em> pick a new utility function for every different time and every different place - that negentropy pours through the notion of \"optimization\" and comes out as a concentrated probability distribution over what an \"alien intelligence\" would do, even in the \"absence of any hypothesis\" about its goals.</p>\n<p>Because the \"absence of any hypothesis\", in this case, does not correspond to a maxentropy distribution, but rather an ordered prior that is ready to recognize any structure that it sees.&nbsp; If you see the aliens making cheesecakes over and over and over again, in many different places and times, you are ready to say \"the aliens like cheesecake\" rather than \"my, what a coincidence\".&nbsp; Even in the <em>absence</em> of any notion of what the aliens are doing - whether they're making cheesecakes or paperclips or eudaimonic sentient beings - this low-entropy prior itself can pour through the notion of \"optimization\" and be transformed into a recognition of solved instrumental problems.</p>\n<p>If you truly <em>expected no order</em> of an alien mind's goals - if you did not credit even the <a href=\"/lw/hk/priors_as_mathematical_objects/\">structured prior</a> that <a href=\"/lw/hg/inductive_bias/\">lets you recognize order when you see it</a> - then you would be unable to identify any optimization or any intelligence.&nbsp; Every possible configuration of matter would appear equally probable as \"something the mind might design\", from desk dust to rainstorms.&nbsp; Just another <a href=\"/lw/ip/fake_explanations/\">hypothesis of maximum entropy</a>.</p>\n<p>This doesn't mean that there's some particular identifiable thing that all alien minds want.&nbsp; It doesn't mean that a mind, \"<a href=\"/lw/nz/arguing_by_definition/\">by definition</a>\", doesn't change its goals over time.&nbsp; Just that if there were an \"agent\" whose goals were pure snow on a television screen, its acts would be the same.</p>\n<p><a href=\"/lw/o5/the_second_law_of_thermodynamics_and_engines_of/\">Like thermodynamics, cognition is about flows of order</a>.&nbsp; An ordered outcome needs negentropy to fuel it.&nbsp; Likewise, where we <em>expect</em> or <em>recognize</em> a thing, even so lofty and abstract as \"intelligence\", we must have ordered beliefs to fuel our anticipation.&nbsp; It's all part of the great game, <a href=\"/lw/pa/gazp_vs_glut/\">Follow-the-Negentropy</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ac84EpK6mZbPLzmqj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LZMeuRGQhSw77XewC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "1137", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GpvvQzf3pPyPsBepr", "Zkzzjg3h7hW5Z36hK", "Cyj6wQLW6SeF6aGLy", "Q4hLMDrFd8fbteeZ8", "n5ucT5ZbPdhfGNLtP", "d5NyJ2Lf6N22AD9PB", "QkX2bAkwG2EpGvNug", "PtoQdG7E8MxYJrigu", "jzf4Rcienrm6btRyt", "H59YqogX94z5jb8xx", "fysgqk4CjAwhBgNYT", "cFzC996D7Jjds3vS9", "k6EPphHiBH4WWYFCj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-08T19:54:22.000Z", "modifiedAt": null, "url": null, "title": "Lawful Creativity", "slug": "lawful-creativity", "viewCount": null, "lastCommentedAt": "2021-09-25T10:02:44.149Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KKLQp934n77cfZpPn/lawful-creativity", "pageUrlRelative": "/posts/KKLQp934n77cfZpPn/lawful-creativity", "linkUrl": "https://www.lesswrong.com/posts/KKLQp934n77cfZpPn/lawful-creativity", "postedAtFormatted": "Saturday, November 8th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lawful%20Creativity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALawful%20Creativity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKKLQp934n77cfZpPn%2Flawful-creativity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lawful%20Creativity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKKLQp934n77cfZpPn%2Flawful-creativity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKKLQp934n77cfZpPn%2Flawful-creativity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2056, "htmlBody": "<p><strong>Previously in Series</strong>:&nbsp; <a href=\"/lw/vl/recognizing_intelligence/\">Recognizing Intelligence</a></p>\n<p>Creativity, we've all been told, is about Jumping Out Of The System, as Hofstadter calls it (JOOTSing for short).&nbsp; Questioned assumptions, violated expectations.</p>\n<p>Fire is dangerous: the rule of fire is to run away from it.&nbsp; What must have gone through the mind of the first hominid to domesticate fire?&nbsp; The rule of milk is that it spoils quickly and then you can't drink it - who first turned milk into cheese?&nbsp; The rule of computers is that they're made with vacuum tubes, fill a room and are so expensive that only corporations can own them.&nbsp; Wasn't the transistor a surprise...</p>\n<p>Who, then, could put laws on creativity?&nbsp; Who could bound it, who could circumscribe it, even with a <a href=\"/lw/o0/where_to_draw_the_boundary/\">concept boundary</a> that distinguishes \"creativity\" from \"not creativity\"?&nbsp; No matter what system you try to lay down, mightn't a more clever person JOOTS right out of it?&nbsp; If you say \"This, this, and this is 'creative'\" aren't you just making up the sort of rule that creative minds love to violate?</p>\n<p>Why, look at all the rules that smart people have violated throughout history, to the enormous profit of humanity.&nbsp; Indeed, the most amazing acts of creativity are those that violate the rules that we would least expect to be violated.</p>\n<p>Is there not even creativity on the level of <em>how to think?</em>&nbsp; Wasn't the invention of Science a creative act that violated old beliefs about rationality?&nbsp; Who, then, can lay down a law of creativity?</p>\n<p>But there is one law of creativity which <em>cannot</em> be violated...</p>\n<p><a id=\"more\"></a></p>\n<p>Ordinarily, if you took a horse-and-buggy, unstrapped the horses, put a large amount of highly combustible fluid on board, and then set fire to the fluid, you would expect the buggy to burn.&nbsp; You certainly wouldn't expect the buggy to move forward at a high rate of speed, for the convenience of its passengers.</p>\n<p>How unexpected was the internal combustion engine!&nbsp; How surprising! What a creative act, to violate the rule that you shouldn't start a fire inside your vehicle!</p>\n<p>But now suppose that I unstrapped the horses from a buggy, put gasoline in the buggy, and set fire to the gasoline, and it <em>did</em> just explode.</p>\n<p>Then there would be no element of \"creative surprise\" about that.&nbsp; More experienced engineers would just shake their heads wisely and say, \"That's why we use horses, kiddo.\"</p>\n<p>Creativity is surprising - but not just any kind of surprise counts as a <em>creative</em> surprise.&nbsp; Suppose I set up an experiment involving a quantum event of very low amplitude, such that the macroscopic <a href=\"/lw/py/the_born_probabilities/\">probability</a> is a million to one.&nbsp; If the event is actually observed to occur, it is a happenstance of extremely low probability, and in that sense surprising.&nbsp; But it is not a <em>creative</em> surprise.&nbsp; Surprisingness is not a <em>sufficient</em> condition for creativity.</p>\n<p>So what kind of surprise is it, that creates the unexpected \"shock\" of creativity?</p>\n<p>In information theory, the more unexpected an event is, the longer the message it takes to send it - to conserve bandwidth, you use the shortest messages for the most common events.</p>\n<p>So do we reason that the most unexpected events, convey the most information, and hence the most surprising acts are those that give us a pleasant shock of creativity - the feeling of suddenly absorbing new information?</p>\n<p>This contains a grain of truth, I think, but not the whole truth: the million-to-one quantum event would also require a 20-bit message to send, but it wouldn't convey a pleasant shock of creativity, any more than a random sequence of 20 coinflips.</p>\n<p>Rather, the creative surprise is the idea that ranks high in your <em>preference ordering</em> but low in your <em>search ordering.</em></p>\n<p>If, before anyone had thought of an internal combustion engine (which predates cars, of course) I had asked some surprisingly probability-literate engineer to write out a vocabulary for describing effective vehicles, it would contain short symbols for horses, long symbols for flammable fluid, and maybe some kind of extra generalization that says \"With probability 99.99%, a vehicle should not be on fire\" so that you need to use a special 14-bit prefix for vehicles that violate this generalization.</p>\n<p>So when I now send this past engineer a description of an automobile, he experiences the shock of getting a large amount of <em>useful</em> information - a design that would have taken a long time for him to find, in the implicit search ordering he set up - a design that occupies a region of low density in his&nbsp; <em>prior </em>distribution for where good designs are to be found in the design space.&nbsp; And even the added \"<a href=\"/lw/j4/absurdity_heuristic_absurdity_bias/\">absurdity</a>\" shock of seeing a generalization violated - not a generalization about physical laws, but a generalization about which designs are effective or ineffective.</p>\n<p>But the vehicle still goes somewhere - <em>that</em> part hasn't changed.</p>\n<p>What if I try to explain about telepresence and virtual offices, so that you don't even need a car?</p>\n<p>But you're still trying to talk to people, or get work done with people - you've just found a more effective <em>means</em> to that <em>end</em>, than travel.</p>\n<p>A car is a more effective means <em>of</em> travel, a computer is a more effective means <em>than</em> travel.&nbsp; But there's still <em>some</em> end.&nbsp; There's some criterion that makes the solution a \"solution\". There's some criterion that makes the unusual reply, unusually <em>good.</em>&nbsp; Otherwise any part of the design space would be as good as any other.</p>\n<p>An amazing creative solution has to obey at least one law, the criterion that makes it a \"solution\".&nbsp; It's the one box you can't step outside:&nbsp; <a href=\"/lw/vl/recognizing_intelligence/\">No optimization without goals.</a></p>\n<p>The pleasant shock of witnessing Art arises from the constraints of Art - from watching a skillful archer send an arrow into an exceedingly narrow target.&nbsp; Static on a television screen is not beautiful, it is noise.</p>\n<p>In the strange domain known as Modern Art, people sometimes claim that their goal is to break <em>all</em> the rules, even the rule that Art has to hit some kind of target.&nbsp; They put up a blank square of canvas, and call it a painting.&nbsp; And by now that is considered staid and boring Modern Art, because a blank square of canvas still hangs on the wall and has a frame.&nbsp; What about a heap of garbage?&nbsp; That can also be Modern Art!&nbsp; Surely, this demonstrates that <em>true</em> creativity knows <em>no</em> rules, and even no goals...</p>\n<p>But the rules are still there, though unspoken.&nbsp; I could submit a realistic landscape painting as Modern Art, and this would be rejected because it violates the rule that Modern Art cannot delight the untrained senses of a mere novice.</p>\n<p>Or better yet, if a heap of garbage can be Modern Art, then I'll claim that <em>someone else's</em> heap of garbage is <em>my</em> work of Modern Art - boldly defying the convention that <em>I</em> need to produce something for it to count as <em>my</em> artwork.&nbsp; Or what about the pattern of dust particles on my desk?&nbsp; Isn't that Art?</p>\n<p>Flushed with triumph, I present to you an even bolder, more convention-defying work of Modern Art - a stunning, outrageous piece of performance art that, in fact, I never performed.&nbsp; I am defying the foolish convention that I need to actually perform my performance art for it to count as Art.</p>\n<p>Now, up to this point, you probably <em>could</em> still get a grant from the National Endowment for the Arts, and get sophisticated critics to discuss your shocking, outrageous non-work, which boldly violates the convention that art must be real rather than imaginary.</p>\n<p>But now suppose that you go one step further, and refuse to tell anyone that you have performed your work of non-Art.&nbsp; You even refuse to apply for an NEA grant.&nbsp; It is the work of Modern Art that never happened and that no one knows never happened; it exists only as my concept of what I am supposed not to conceptualize.&nbsp; Better yet, I will say that my Modern Art is <em>your</em> non-conception of something that <em>you</em> are not conceptualizing.&nbsp; Here is the ultimate work of Modern Art, that truly defies all rules:&nbsp; It isn't mine, it isn't real, and no one knows it exists...</p>\n<p>And this ultimate rulebreaker you could <em>not</em> pass off as Modern Art, even if NEA grant committees knew that no one knew it existed.&nbsp; For one thing, they would realize that you were making fun of them - and <em>that</em> is an unspoken rule of Modern Art that no one dares violate.&nbsp; You must take yourself seriously.&nbsp; You must break the surface rules in a way that allows sophisticated critics to praise your boldness and defiance with a straight face.&nbsp; This is the unwritten real goal, and if it is not achieved, all efforts are for naught.&nbsp; Whatever gets sophisticated critics to praise your rule-breaking is good Modern Art, and whatever fails in this end is poor Modern Art.&nbsp; Within that unalterable constraint, you can use whatever creative means you like.</p>\n<p>But doesn't creative engineering sometimes involve altering your goals?&nbsp; First my goal was to try and figure out how to build a vehicle using horses; now my goal is to build a vehicle using fire...</p>\n<p>Creativity clearly involves altering my <em>local</em> intentions, my what-I'm-trying-to-do-next.&nbsp; I begin by intending to use horses, to build a vehicle, to drive to the supermarket, to buy food, to eat food, so that I don't starve to death, because I prefer being alive to starving to death.&nbsp; I may creatively use fire, instead of horses; creatively walk, instead of driving; creatively drive to a nearby restaurant, instead of a supermarket; creatively grow my own vegetables, instead of buying them; or even creatively devise a way to run my body on electricity, instead of chemical energy...</p>\n<p>But what does not count as \"creativity\" is creatively <em>preferring</em> to starve to death, rather than eating.&nbsp; This \"solution\" does not strike me as very impressive; it involves no effort, no intelligence, and no surprise when it comes to looking at the result.&nbsp; If this is someone's idea of how to break all the rules, they would become pretty easy to predict.</p>\n<p>Are there cases where you genuinely want to change your preferences?&nbsp; You may look back in your life and find that your moral beliefs have changed over decades, and that you count this as \"<a href=\"/lw/s9/whither_moral_progress/\">moral progress</a>\".&nbsp; Civilizations also change their morals over time.&nbsp; In the seventeenth century, people used to think it was okay to enslave people with differently colored skin; and now we elect them President.</p>\n<p>But you might <em>guess</em> by now, you might <em>somehow intuit</em>, that if these moral changes seem interesting and important and vital and indispensable, then not just <em>any</em> change would suffice.&nbsp; If there's no criterion, no target, no way of choosing - then your current point in state space is just as good as any other point, no more, no less; and you might as well keep your current state, unchanging, forever.</p>\n<p>Every <em>surprisingly</em> <em>creative</em> Jump-Out-Of-The-System needs a criterion that makes it <em>surprisingly good,</em> some fitness metric that it matches.&nbsp; This criterion, itself, supplies the order in our beliefs that lets us recognize an act of \"creativity\" despite our surprise.&nbsp; <a href=\"/lw/vl/recognizing_intelligence/\">Just as recognizing intelligence requires at least some belief about that intelligence's goals, however abstract.</a></p>\n<p>One might wish to reconsider, in light of this principle, such notions as \"<a href=\"/lw/r0/thou_art_physics/\">free will</a> that is not constrained by <a href=\"/lw/rc/the_ultimate_source/\">anything</a>\"; or the necessary conditions for our discussions of what is \"right\" to <a href=\"/lw/sb/could_anything_be_right/\">have some kind of meaning</a>.</p>\n<p>There is an oft-repeated cliche of Deep Wisdom which says something along the lines of \"intelligence is balanced between Order and Chaos\", as if cognitive science were a fantasy novel written by Roger Zelazny.&nbsp; Logic as Order, following the rules.&nbsp; Creativity as Chaos, violating the rules.&nbsp; And so you can try to understand logic, but you are <a href=\"/lw/it/semantic_stopsigns/\">blocked</a> when it comes to creativity - and of course you could build a logical computer but not a creative one - and of course 'rationalists' can only use the Order side of the equation, and can never become whole people; because Art requires an element of irrationality, just like e.g. <a href=\"/lw/hp/feeling_rational/\">emotion</a>.</p>\n<p>And I think that despite its poetic appeal, that whole cosmological mythology is just flat wrong.&nbsp; There's just various forms of regularity, of <a href=\"/lw/o5/the_second_law_of_thermodynamics_and_engines_of/\">negentropy</a>, where <em>all</em> the structure and <em>all</em> the beauty live.&nbsp; And on the other side is what's left over - the static on the television screen, the heat bath, the noise.</p>\n<p>I shall be developing this startling thesis further in future posts.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"EewHHv3ewvQ3mqbyb": 2, "Ng8Gice9KNkncxqcj": 2, "T63QtRJhoTEGhZbTP": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KKLQp934n77cfZpPn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 53, "extendedScore": null, "score": 7.7e-05, "legacy": true, "legacyId": "1138", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LZMeuRGQhSw77XewC", "d5NyJ2Lf6N22AD9PB", "3ZKvf9u2XEWddGZmS", "P792Z4QA9dzcLdKkE", "szAkYJDtXkcSAiHYE", "NEeW7eSXThPz7o4Ne", "EsMhFZuycZorZNRF5", "vy9nnPdwTjSmt5qdb", "FWMfQKG3RpZx6irjm", "SqF8cHjJv43mvJJzx", "QkX2bAkwG2EpGvNug"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-09T18:08:52.000Z", "modifiedAt": null, "url": null, "title": "Ask OB: Leaving the Fold", "slug": "ask-ob-leaving-the-fold", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:37.275Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sDNMreqo3ktQDv4uH/ask-ob-leaving-the-fold", "pageUrlRelative": "/posts/sDNMreqo3ktQDv4uH/ask-ob-leaving-the-fold", "linkUrl": "https://www.lesswrong.com/posts/sDNMreqo3ktQDv4uH/ask-ob-leaving-the-fold", "postedAtFormatted": "Sunday, November 9th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ask%20OB%3A%20Leaving%20the%20Fold&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAsk%20OB%3A%20Leaving%20the%20Fold%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsDNMreqo3ktQDv4uH%2Fask-ob-leaving-the-fold%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ask%20OB%3A%20Leaving%20the%20Fold%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsDNMreqo3ktQDv4uH%2Fask-ob-leaving-the-fold", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsDNMreqo3ktQDv4uH%2Fask-ob-leaving-the-fold", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 197, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/ur/crisis_of_faith/\">Crisis of Faith</a></p>\n\n<p>I thought this <a href=\"http://www.overcomingbias.com/2008/10/got-crisis/comments/page/2/#comment-138094586\">comment</a> from &quot;Jo&quot; deserved a bump to the front page:</p><blockquote><div id=\"comment-138094586-content\" class=\"comment-content\">\n\t\t\t\n\t\t\t<span id=\"comment-138094586-content\"><p>&quot;So\nhere I am having been raised in the Christian faith and trying not to\nfreak out over the past few weeks because I've finally begun to wonder\nwhether I believe things just because I was raised with them. Our\nfamily is surrounded by genuinely wonderful people who have poured\ntheir talents into us since we were teenagers, and our social structure\nand business rests on the tenets of what we believe. I've been trying\nto work out how I can 'clear the decks' and then rebuild with whatever\nis worth keeping, yet it's so foundational that it will affect my\nmarriage (to a pretty special man) and my daughters who, of course,\nhave also been raised to walk the Christian path.</p>\n\n<p>Is there anyone who's been in this position - really, really invested in a faith and then walked away?&quot;</p></span></div></blockquote><div id=\"comment-138094586-content\" class=\"comment-content\"><span id=\"comment-138094586-content\">Handling this kind of situation has to count as part of the art.&nbsp; But I haven't gone through anything like this.&nbsp; Can anyone with experience advise Jo on what to expect, what to do, and what not to do?</span></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NSMKfa8emSbGNXRKD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sDNMreqo3ktQDv4uH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 4.5652426979161813e-07, "legacy": true, "legacyId": "1139", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BcYBfG8KomcpcxkEg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-10T21:06:32.000Z", "modifiedAt": "2021-04-07T21:51:43.063Z", "url": null, "title": "Lawful Uncertainty", "slug": "lawful-uncertainty", "viewCount": null, "lastCommentedAt": "2022-05-11T05:03:49.611Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/msJA6B9ZjiiZxT6EZ/lawful-uncertainty", "pageUrlRelative": "/posts/msJA6B9ZjiiZxT6EZ/lawful-uncertainty", "linkUrl": "https://www.lesswrong.com/posts/msJA6B9ZjiiZxT6EZ/lawful-uncertainty", "postedAtFormatted": "Monday, November 10th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lawful%20Uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALawful%20Uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmsJA6B9ZjiiZxT6EZ%2Flawful-uncertainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lawful%20Uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmsJA6B9ZjiiZxT6EZ%2Flawful-uncertainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmsJA6B9ZjiiZxT6EZ%2Flawful-uncertainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1098, "htmlBody": "<p>In <i>Rational Choice in an Uncertain World</i>, Robyn Dawes describes an experiment by Tversky:<sup>1</sup></p><blockquote><p>Many psychological experiments were conducted in the late 1950s and early 1960s in which subjects were asked to predict the outcome of an event that had a random component but yet had base-rate predictability\u2014for example, subjects were asked to predict whether the next card the experimenter turned over would be red or blue in a context in which 70% of the cards were blue, but in which the sequence of red and blue cards was totally random.</p><p>In such a situation, the strategy that will yield the highest proportion of success is to predict the more common event. For example, if 70% of the cards are blue, then predicting blue on every trial yields a 70% success rate.</p><p>What subjects tended to do instead, however, was match probabilities\u2014that is, predict the more probable event with the relative frequency with which it occurred. For example, subjects tended to predict 70% of the time that the blue card would occur and 30% of the time that the red card would occur. Such a strategy yields a 58% success rate, because the subjects are correct 70% of the time when the blue card occurs (which happens with probability .70) and 30% of the time when the red card occurs (which happens with probability .30); (.70\u00d7.70) + (.30\u00d7.30) = .58.</p><p>In fact, subjects predict the more frequent event with a slightly higher probability than that with which it occurs, but do not come close to predicting its occurrence 100% of the time, even when they are paid for the accuracy of their predictions . . . For example, subjects who were paid a nickel for each correct prediction over a thousand trials . . . predicted [the more common event] 76% of the time.</p></blockquote><p>Do not think that this experiment is about a minor flaw in gambling strategies. It compactly illustrates the most important idea in all of rationality.</p><p>Subjects just keep guessing red, as if they think they have some way of predicting the random sequence. Of this experiment Dawes goes on to say, \u201cDespite feedback through a thousand trials, subjects cannot bring themselves to believe that the situation is one in which they <i>cannot</i> predict.\u201d</p><p>But the error must go deeper than that. Even if subjects <i>think</i> they\u2019ve come up with a hypothesis, they don\u2019t have to <i>actually bet</i> on that prediction in order to test their hypothesis. They can say, \u201cNow if <i>this</i> hypothesis is correct, the next card will be red\u201d\u2014and then just bet on blue. They can pick blue each time, accumulating as many nickels as they can, while mentally noting their private guesses for any patterns they thought they spotted. If their predictions come out right, <i>then</i> they can switch to the newly discovered sequence.</p><p>I wouldn\u2019t fault a subject for continuing to invent hypotheses\u2014how could they know the sequence is truly beyond their ability to predict? But I would fault a subject for <i>betting on the guesses</i>, when this wasn\u2019t necessary to gather information, and literally <i>hundreds</i> of earlier guesses had been disconfirmed.</p><p>Can even a human be <i>that</i> overconfident?</p><p>I would suspect that something simpler is going on\u2014that the all-blue strategy <i>just didn\u2019t occur</i> to the subjects.</p><p>People see a mix of mostly blue cards with some red, and suppose that the optimal betting strategy must be a mix of mostly blue cards with some red.</p><p>It is a <i>counterintuitive</i> idea that, given incomplete information, <i>the optimal betting strategy does not resemble a typical sequence of cards</i>.</p><p>It is a <i>counterintuitive</i> idea that the optimal strategy is to behave lawfully, even in an environment that has random elements.</p><p>It seems like your behavior ought to be unpredictable, just like the environment\u2014but no! <i>A random key does not open a random lock just because they are \u201cboth random.\u201d</i></p><p>You don\u2019t fight fire with fire; you fight fire with water. But this thought involves an extra step, a new concept not directly activated by the problem statement, and so it\u2019s not the first idea that comes to mind.</p><p>In the dilemma of the blue and red cards, our partial knowledge tells us\u2014on each and every round\u2014that the best bet is blue. This advice of our partial knowledge is the same on every single round. If 30% of the time we go against our partial knowledge and bet on red instead, then we will do worse thereby\u2014because now we\u2019re being outright stupid, betting on what we know is the less probable outcome.</p><p>If you bet on red every round, you would do as badly as you could possibly do; you would be 100% stupid. If you bet on red 30% of the time, faced with 30% red cards, then you\u2019re making yourself 30% stupid.</p><p>When your knowledge is incomplete\u2014meaning that the world will seem to you to have an element of randomness\u2014randomizing your actions doesn\u2019t solve the problem. Randomizing your actions takes you further from the target, not closer. In a world already foggy, throwing away your intelligence just makes things worse.</p><p>It is a <i>counterintuitive</i> idea that the optimal strategy can be to <i>think lawfully, even under conditions of uncertainty</i>.</p><p>And so there are not many rationalists, for most who perceive a chaotic world will try to fight chaos with chaos. You have to take an extra step, and think of something that doesn\u2019t pop right into your mind, in order to imagine fighting fire with something that is not itself fire.</p><p>You have heard the unenlightened ones say, \u201cRationality works fine for dealing with rational people, but the world isn\u2019t rational.\u201d But <i>faced with an irrational opponent, throwing away your own reason is not going to help you</i>. There are lawful forms of thought that still generate the best response, even when faced with an opponent who breaks those laws. Decision theory does <i>not</i> burst into flames and die when faced with an opponent who disobeys decision theory.</p><p>This is no more obvious than the idea of betting all blue, faced with a sequence of both blue and red cards. But each bet that you make on red is an expected loss, and so too with every departure from the Way in your own thinking.</p><p>How many <i>Star Trek</i> episodes are thus refuted? How many theories of AI?</p><hr><p><sup>1</sup> Amos Tversky and Ward Edwards, \u201cInformation versus Reward in Binary Choices,\u201d <i>Journal of Experimental Psychology</i> 71, no. 5 (1966): 680\u2013683. See also Yaacov Schul and Ruth Mayo, \u201cSearching for Certainty in an Uncertain World: The Difficulty of Giving Up the Experiential for the Rational Mode of Thinking,\u201d <i>Journal of Behavioral Decision Making</i> 16, no. 2 (2003): 93\u2013106.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8hPTCJbwJnLBmfpCX": 1, "4R8JYu4QF2FqzJxE5": 1, "Ng8Gice9KNkncxqcj": 1, "4fxcbJ8xSv4SAYkkx": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "msJA6B9ZjiiZxT6EZ", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 63, "baseScore": 76, "extendedScore": null, "score": 0.000111, "legacy": true, "legacyId": "1140", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2021-04-01T23:36:12.099Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": "5uZQHpecjn7955faL", "canonicalCollectionSlug": "rationality", "canonicalBookId": "PYQ2izdfDPTe5uJTG", "canonicalNextPostSlug": "my-wild-and-reckless-youth", "canonicalPrevPostSlug": "positive-bias-look-into-the-dark", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 76, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "rPsASnzcvaBdBEaSJ", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "2.2.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-11T19:01:31.000Z", "modifiedAt": "2019-11-25T18:20:45.951Z", "url": null, "title": "Worse Than Random", "slug": "worse-than-random", "viewCount": null, "lastCommentedAt": "2019-09-02T19:37:48.421Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GYuKqAL95eaWTDje5/worse-than-random", "pageUrlRelative": "/posts/GYuKqAL95eaWTDje5/worse-than-random", "linkUrl": "https://www.lesswrong.com/posts/GYuKqAL95eaWTDje5/worse-than-random", "postedAtFormatted": "Tuesday, November 11th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Worse%20Than%20Random&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWorse%20Than%20Random%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGYuKqAL95eaWTDje5%2Fworse-than-random%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Worse%20Than%20Random%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGYuKqAL95eaWTDje5%2Fworse-than-random", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGYuKqAL95eaWTDje5%2Fworse-than-random", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3464, "htmlBody": "<p><strong>Previously in series</strong>:&nbsp; <a href=\"/lw/vo/lawful_uncertainty/\">Lawful Uncertainty</a></p>\n<p>You may have noticed a certain trend in recent posts:&nbsp; I've been arguing that randomness hath no power, that there is no beauty in entropy, nor yet strength from noise.</p>\n<p>If one were to formalize the argument, it would probably run something like this: that if you define optimization as <a href=\"/lw/va/measuring_optimization_power/\">previously suggested</a>, then <em>sheer randomness</em> will generate something that seems to have 12 bits of optimization, only by trying 4096 times; or 100 bits of optimization, only by trying 10<sup>30</sup> times.</p>\n<p>This may not sound like a profound insight, since it is true <a href=\"https://www.lesswrong.com/posts/cFzC996D7Jjds3vS9/arguing-by-definition\">by definition</a>.&nbsp; But consider - how many comic books talk about \"mutation\" as if it were a source of power?&nbsp; Mutation is random.&nbsp; It's the selection part, not the mutation part, that explains the trends of evolution.</p>\n<p>Or you may have heard people talking about \"<a href=\"/lw/iv/the_futility_of_emergence/\">emergence</a>\" as if it could explain complex, functional orders.&nbsp; People will say that the function of an ant colony <em>emerges</em> - as if, starting from ants that had been selected only to function as solitary individuals, the ants got together in a group for the first time and the ant colony popped right out.&nbsp; But ant colonies have been selected on <em>as colonies</em> by evolution.&nbsp; Optimization didn't just magically happen when the ants came together.</p>\n<p>And you may have heard that certain algorithms in Artificial Intelligence work better when we inject randomness into them.</p>\n<p>Is that even possible?&nbsp; How can you extract useful work from entropy?</p>\n<p>But it <em>is</em> possible in theory, since you can have things that are anti-optimized.&nbsp; Say, the average state has utility -10, but the current state has an unusually low utility of -100.&nbsp; So in this case, a random jump has an expected benefit.&nbsp; If you happen to be standing in the middle of a lava pit, running around at random is better than staying in the same place.&nbsp; (Not best, but better.)</p>\n<p>A given AI algorithm <em>can</em> do better when randomness is injected, provided that some step of the unrandomized algorithm is doing worse than random.</p>\n<p><a id=\"more\"></a></p>\n<p>Imagine that we're trying to solve a pushbutton combination lock with 20 numbers and four steps - 160,000 possible combinations.&nbsp; And we try the following algorithm for opening it:</p>\n<ol>\n<li>Enter 0-0-0-0 into the lock. </li>\n<li>If the lock opens, return with SUCCESS. </li>\n<li>If the lock remains closed, go to step 1.</li>\n</ol>\n<p>Obviously we can improve this algorithm by substituting \"Enter a random combination\" on the first step.</p>\n<p>If we were to try and explain in words why this works, a description might go something like this:&nbsp; \"When we first try 0-0-0-0 it has the same chance of working (so far as we know) as any other combination.&nbsp; But if it doesn't work, it would be stupid to try it again, because now we know that 0-0-0-0 doesn't work.\"</p>\n<p>The first key idea is that, after trying 0-0-0-0, we <em>learn</em> something - we acquire new knowledge, which should then affect how we plan to continue from there.&nbsp; This is knowledge, quite a different thing from randomness...</p>\n<p>What exactly have we learned?&nbsp; We've learned that 0-0-0-0 doesn't work; or to put it another way, <em>given</em> that 0-0-0-0 failed on the <em>first</em> try, the <em>conditional</em> probability of it working on the <em>second</em> try, is negligible.</p>\n<p>Consider your probability distribution over all the possible combinations:&nbsp; Your probability distribution starts out in a state of maximum entropy, with all 160,000 combinations having a 1/160,000 probability of working.&nbsp; After you try 0-0-0-0, you have a new probability distribution, which has slightly less entropy; 0-0-0-0 has an infinitesimal probability of working, and the remaining 159,999 possibilities each have a 1/159,999 probability of working.&nbsp; To try 0-0-0-0 again would now be <em>stupid</em> - the expected utility of trying 0-0-0-0 is <em>less than average</em>; the vast majority of potential actions now have higher expected utility than does 0-0-0-0.&nbsp; An algorithm that tries 0-0-0-0 again would do worse than random, and we can improve the algorithm by randomizing it.</p>\n<p>One may also consider an algorithm as a <em>sequence</em> of tries:&nbsp; The \"unrandomized algorithm\" describes the sequence of tries 0-0-0-0, 0-0-0-0, 0-0-0-0... and this <em>sequence of tries</em> is a special sequence that has <em>far-below-average</em> expected utility in the space of all possible sequences.&nbsp; Thus we can improve on this sequence by selecting a random sequence instead.</p>\n<p>Or imagine that the combination changes every second.&nbsp; In this case, 0-0-0-0, 0-0-0-0 is just as good as the randomized algorithm - no better and no worse.&nbsp; What this shows you is that the supposedly \"random\" algorithm is \"better\" relative to a known regularity of the lock - that the combination is constant on each try.&nbsp; Or to be precise, the reason the random algorithm does predictably better than the stupid one is that the stupid algorithm is \"stupid\" relative to a known regularity of the lock.</p>\n<p>In other words, in order to say that the random algorithm is an \"improvement\", we must have <strong>used specific knowledge about the lock to realize that the unrandomized algorithm is worse-than-average.</strong>&nbsp; Having realized this, can we reflect further on our information, and <strong>take full advantage of our knowledge to do better-than-average?</strong></p>\n<p>The random lockpicker is still not optimal - it does not take full advantage of the knowledge we have acquired.&nbsp; A random algorithm might randomly try 0-0-0-0 again; it's not impossible, but it could happen.&nbsp; The longer the random algorithm runs, the more likely it is to try the same combination twice; and if the random algorithm is sufficiently unlucky, it might still fail to solve the lock after millions of tries.&nbsp; We can take <em>full</em> advantage of <em>all</em> our knowledge by using an algorithm that systematically tries 0-0-0-0, 0-0-0-1, 0-0-0-2...&nbsp; This algorithm is guaranteed not to repeat itself, and will find the solution in bounded time.&nbsp; Considering the algorithm as a sequence of tries, no other sequence in sequence-space is expected to do better, given our initial knowledge.&nbsp; (Any other nonrepeating sequence is equally good; but nonrepeating sequences are rare in the space of all possible sequences.)</p>\n<p>A combination <em>dial</em> often has a tolerance of 2 in either direction.&nbsp; 20-45-35 will open a lock set to 22-44-33.&nbsp; In this case, the algorithm that tries 0-1-0, 0-2-0, et cetera, ends up being stupid again; a randomized algorithm will (usually) work better.&nbsp; But an algorithm that tries 0-5-0, 0-10-0, 0-10-5, will work better still.</p>\n<p>Sometimes it is <em>too expensive</em> to take advantage of all the knowledge that we could, in theory, acquire from previous tests.&nbsp; Moreover, a complete enumeration or interval-skipping algorithm would still end up being stupid.&nbsp; In this case, computer scientists often use a cheap pseudo-random algorithm, because the computational cost of using our knowledge exceeds the benefit to be gained from using it.&nbsp; This does not show the power of randomness, but, rather, the predictable stupidity of certain <em>specific</em> deterministic algorithms <em>on that particular problem</em>.&nbsp; Remember, the pseudo-random algorithm is also deterministic!&nbsp; But the deterministic pseudo-random algorithm doesn't belong to the class of algorithms that are predictably stupid (do much worse than average).</p>\n<p>There are also subtler ways for adding noise to improve algorithms.&nbsp; For example, there are neural network training algorithms that work better if you simulate noise in the neurons.&nbsp; On this occasion it is especially tempting to say something like:</p>\n<p>\"Lo!&nbsp; When we make our artificial neurons noisy, just like biological neurons, they work better!&nbsp; Behold the healing life-force of entropy!\"</p>\n<p>What might actually be happening - for example - is that the network training algorithm, operating on noiseless neurons, would vastly <em>overfit the data</em>.</p>\n<p>If you expose the noiseless network to the series of coinflips \"HTTTHHTTH\"... the training algorithm will say the equivalent of, \"I bet this coin was specially designed to produce HTTTHHTTH every time it's flipped!\" instead of \"This coin probably alternates randomly between heads and tails.\"&nbsp; A hypothesis overfitted to the data does not generalize.&nbsp; On the other hand, when we add noise to the neurons and then try training them again, they can no longer fit the data <em>precisely</em>, so instead they settle into a simpler hypothesis like \"This coin alternates randomly between heads and tails.\"&nbsp; Note that this requires us - the programmers - to know in advance that probabilistic hypotheses are more likely to be true.</p>\n<p>Or here's another way of looking at it:&nbsp; A neural network algorithm typically looks at a set of training instances, tweaks the units and their connections based on the training instances, and in this way tries to \"stamp\" the experience onto itself.&nbsp; But the algorithms which do the stamping are often poorly understood, and it is possible to stamp too hard.&nbsp; If this mistake has already been made, then blurring the sensory information, or blurring the training algorithm, or blurring the units, can partially cancel out the \"overlearning\".</p>\n<p>Here's a simplified example of a similar (but not identical) case:&nbsp; Imagine that the environment deals us a random mix of cards, 70% blue and 30% red.&nbsp; But instead of just predicting \"blue\" or \"red\", we have to assign a quantitative probability to seeing blue - and the scoring rule for our performance is one that elicits an honest estimate; if the actual frequency is 70% blue cards, we do best by replying \"70%\", not 60% or 80%.&nbsp; (\"Proper scoring rule.\")</p>\n<p>If you don't know in advance the frequency of blue and red cards, one way to handle the problem would be to have a blue unit and a red unit, both wired to an output unit.&nbsp; The blue unit sends signals with a positive effect that make the target unit more likely to fire; the red unit inhibits its targets - <em>just like the excitatory and inhibitory synapses in the human brain!</em>&nbsp; (Or an earthworm's brain, for that matter...)</p>\n<p>Each time we see a blue card in the training data, the training algorithm increases the strength of the (excitatory) synapse from the blue unit to the output unit; and each time we see a red card, the training algorithm strengthens the (inhibitory) synapse from the red unit to the output unit.</p>\n<p>But wait!&nbsp; We haven't said why the blue or red units might fire in the first place.&nbsp; So we'll add two more excitatory units that spike randomly, one connected to the blue unit and one connected to red unit.&nbsp; This simulates the natural background noise present in the human brain (or an earthworm's brain).</p>\n<p>Finally, the spiking frequency of the output unit becomes the predicted probability that the next card is blue.</p>\n<p>As you can see - assuming you haven't lost track of all the complexity - this neural network <em>learns</em> to predict whether blue cards or red cards are more common in the mix.&nbsp; <em>Just like a human brain! </em></p>\n<p>At least that's the theory.&nbsp; However, when we boot up the neural network and give it a hundred training examples with 70 blue and 30 red cards, it ends up predicting a 90% probability that each card will be blue.&nbsp; Now, there are several things that could go wrong with a system this complex; but my own first impulse would be to guess that the training algorithm is too strongly adjusting the synaptic weight from the blue or red unit to the output unit on each training instance.&nbsp; The training algorithm needs to shift a little less far - alter the synaptic weights a little less.</p>\n<p>But the programmer of the neural network comes up with a different hypothesis: the problem is that there's no noise in the input.&nbsp; This is biologically unrealistic; real organisms do not have perfect vision or perfect information about the environment.&nbsp; So the programmer shuffles a few randomly generated blue and red cards (50% probability of each) into the training sequences.&nbsp; Then the programmer adjusts the noise level until the network finally ends up predicting blue with 70% probability.&nbsp; And it turns out that using almost the same noise level (with just a bit of further tweaking), the improved training algorithm can learn to assign the right probability to sequences of 80% blue or 60% blue cards.</p>\n<p>Success!&nbsp; We have found the Right Amount of Noise.</p>\n<p>Of course this success comes with certain disadvantages.&nbsp; For example, maybe the blue and red cards <em>are</em> predictable, in the sense of coming in a learnable sequence.&nbsp; Maybe the sequence is 7 blue cards followed by 3 red cards.&nbsp; If we mix noise into the sensory data, we may never notice this important regularity, or learn it imperfectly... but that's the price you pay for biological realism.</p>\n<p>What's really happening is that the training algorithm moves too far given its data, and adulterating noise with the data diminishes the impact of the data.&nbsp; The two errors partially cancel out, but at the cost of a nontrivial loss in what we could, in principle, have learned from the data.&nbsp; It would be better to adjust the training algorithm and keep the data clean.</p>\n<p>This is an extremely oversimplified example, but it is not a total strawman.&nbsp; The scenario seems silly only because it is simplified to the point where you can clearly see what is going wrong.&nbsp; Make the neural network a lot more complicated, and the solution of adding noise to the inputs might sound <em>a lot more plausible.</em>&nbsp; While some neural network algorithms are well-understood mathematically, others are not.&nbsp; In particular, systems crafted with the aim of biological realism are often not well-understood.&nbsp;</p>\n<p>But it is an <em>inherently</em> odd proposition that you can get a better picture of the environment by adding noise to your sensory information - by deliberately throwing away your sensory acuity.&nbsp; This can only degrade the mutual information between yourself and the environment.&nbsp; It can only diminish what in principle can be extracted from the data.&nbsp; And this is as true for every step of the computation, as it is for the eyes themselves.&nbsp; The only way that adding random noise will help is if some step of the sensory processing is doing worse than random.</p>\n<p>Now it is certainly possible to design an imperfect reasoner that only works in the presence of an accustomed noise level.&nbsp; Biological systems are unable to avoid noise, and therefore adapt to overcome noise.&nbsp; Subtract the noise, and mechanisms adapted to the presence of noise may do strange things.</p>\n<p>Biological systems are often fragile under conditions that have no evolutionary precedent in the ancestral environment.&nbsp; If somehow the Earth's gravity decreased, then birds might become unstable, lurching up in the air as their wings overcompensated for the now-decreased gravity.&nbsp; But this doesn't mean that stronger gravity helps birds fly better.&nbsp; Gravity is still the difficult challenge that a bird's wings work to overcome - even though birds are now adapted to gravity as an invariant.</p>\n<p>What about hill-climbing, simulated annealing, or genetic algorithms?&nbsp; These AI algorithms are local search techniques that randomly investigate some of their nearest neighbors.&nbsp; If an investigated neighbor is superior to the current position, the algorithm jumps there.&nbsp; (Or sometimes probabilistically jumps to a neighbor with probability determined by the difference between neighbor goodness and current goodness.)&nbsp; Are these techniques drawing on the power of noise?</p>\n<p>Local search algorithms take advantage of the regularity of the search space - that if you find a <em>good</em> point in the search space, its <em>neighborhood</em> of closely similar points is a likely place to search for a slightly better neighbor.&nbsp; And then this neighbor, in turn, is a likely place to search for a still better neighbor; and so on.&nbsp; To the extent this <em>regularity of the search space</em> breaks down, hill-climbing algorithms will perform poorly.&nbsp; If the neighbors of a good point are no more likely to be good than randomly selected points, then a hill-climbing algorithm simply won't work.</p>\n<p>But still, doesn't a local search algorithm need to make random changes to the current point in order to generate neighbors for evaluation?&nbsp; Not necessarily; some local search algorithms systematically generate all possible neighbors, and select the best one.&nbsp; These greedy algorithms work fine for some problems, but on other problems it has been found that greedy local algorithms get stuck in local minima.&nbsp; The next step up from greedy local algorithms, in terms of added randomness, is random-restart hill-climbing - as soon as we find a local maximum, we restart someplace random, and repeat this process a number of times.&nbsp; For our final solution, we return the best local maximum found when time runs out.&nbsp; Random-restart hill-climbing is surprisingly useful; it can easily solve some problem classes where any individual starting point is unlikely to lead to a global maximum or acceptable solution, but it is likely that at least one of a thousand individual starting points will lead to the global maximum or acceptable solution.</p>\n<p>The <em>non</em>-randomly-restarting, greedy, local-maximum-grabbing algorithm, is \"stupid\" at the stage where it gets stuck in a local maximum.&nbsp; Once you find a local maximum, you know you're not going to do better by greedy local search - so you may as well try <em>something</em> else with your time.&nbsp; Picking a random point and starting again is drastic, but it's not as stupid as searching the neighbors of a particular local maximum over and over again.&nbsp; (Biological species often <em>do</em> get stuck in local optima.&nbsp; Evolution, being unintelligent, has no mind with which to \"notice\" when it is testing the same gene complexes over and over.)</p>\n<p>Even more stupid is picking a particular starting point, and then evaluating its fitness over and over again, without even searching its neighbors.&nbsp; This is the lockpicker who goes on trying 0-0-0-0 forever.</p>\n<p>Hill-climbing search is not so much <em>a little bit randomized</em> compared to the <em>completely stupid</em> lockpicker, as <em>almost entirely nonrandomized</em> compared to a <em>completely ignorant</em> searcher.&nbsp; We search <em>only</em> the local neighborhood, rather than selecting a random point from the entire state space.&nbsp; That probability distribution has been narrowed enormously, relative to the overall state space.&nbsp; This exploits the belief - the knowledge, if the belief is correct - that a good point probably has good neighbors.</p>\n<p>You can imagine splitting a hill-climbing algorithm into components that are \"deterministic\" (or rather, knowledge-exploiting) and \"randomized\" (the leftover ignorance).</p>\n<p>A programmer writing a probabilistic hill-climber will use some formula to assign probabilities to each neighbor, as a function of the neighbor's fitness.&nbsp; For example, a neighbor with a fitness of 60 might have probability 80% of being selected, while other neighbors with fitnesses of 55, 52, and 40 might have selection probabilities of 10%, 9%, and 1%.&nbsp; The programmer writes a deterministic algorithm, a fixed formula, that produces these numbers - 80, 10, 9, and 1.</p>\n<p>What about the actual job of making a random selection at these probabilities?&nbsp; Usually the programmer will hand that job off to someone else's pseudo-random algorithm - most language's standard libraries contain a standard pseudo-random algorithm; there's no need to write your own.</p>\n<p>If the hill-climber doesn't seem to work well, the programmer tweaks the <em>deterministic</em> part of the algorithm, the part that assigns these fixed numbers 80, 10, 9, and 1.&nbsp; The programmer does not say - \"I bet these probabilities are right, but I need a source that's <em>even more random</em>, like a thermal noise generator, instead of this merely <em>pseudo-random</em> algorithm that is ultimately deterministic!\"&nbsp; The programmer does not go in search of better noise.</p>\n<p>It is theoretically possible for a poorly designed \"pseudo-random algorithm\" to be stupid relative to the search space; for example, it might always jump in the same direction.&nbsp; But the \"pseudo-random algorithm\" has to be <em>really</em> shoddy for that to happen.&nbsp; You're only likely to get stuck with that problem if you reinvent the wheel instead of using a standard, off-the-shelf solution.&nbsp; A decent pseudo-random algorithm works just as well as a thermal noise source on optimization problems.&nbsp; It is possible (though difficult) for an exceptionally poor noise source to be exceptionally stupid on the problem, <em>but you cannot do exceptionally well by finding a noise source that is exceptionally random.</em>&nbsp; The power comes from the knowledge - the deterministic formula that assigns a fixed probability distribution.&nbsp; It does not reside in the remaining ignorance.</p>\n<p>And that's why I always say that the power of natural selection comes from the selection part, not the mutation part.</p>\n<p>As a general principle, <strong>on any problem for which you know that a particular unrandomized algorithm is unusually stupid</strong> - so that a randomized algorithm seems wiser - <strong>you should be able to use the same knowledge to produce a superior derandomized algorithm</strong>. If nothing else seems obvious, just avoid outputs that look \"unrandomized\"!&nbsp; If you know something is stupid, deliberately avoid it!&nbsp; (There are exceptions to this rule, but they have to do with defeating cryptographic adversaries - that is, preventing someone else's intelligence from working on you.&nbsp; Certainly entropy can act as an <em>antidote</em> to intelligence!)&nbsp; And of course there are very common practical exceptions whenever the computational cost of using <em>all</em> our knowledge exceeds the marginal benefit...</p>\n<p>Still you should find, as a general principle, that randomness hath no power: there is no beauty in entropy, nor strength from noise.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HFou6RHqFagkyrKkW": 1, "fpEBgFE7fgpxTm9BF": 1, "GY5kPPpCoyt9fnTMn": 1, "nvKzwpiranwy29HFJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GYuKqAL95eaWTDje5", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 41, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "1141", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 102, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["msJA6B9ZjiiZxT6EZ", "Q4hLMDrFd8fbteeZ8", "cFzC996D7Jjds3vS9", "8QzZKw9WHRxjR4948"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-12T23:19:58.000Z", "modifiedAt": null, "url": null, "title": "The Weighted Majority Algorithm", "slug": "the-weighted-majority-algorithm", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:37.884Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AAqTP6Q5aeWnoAYr4/the-weighted-majority-algorithm", "pageUrlRelative": "/posts/AAqTP6Q5aeWnoAYr4/the-weighted-majority-algorithm", "linkUrl": "https://www.lesswrong.com/posts/AAqTP6Q5aeWnoAYr4/the-weighted-majority-algorithm", "postedAtFormatted": "Wednesday, November 12th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Weighted%20Majority%20Algorithm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Weighted%20Majority%20Algorithm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAAqTP6Q5aeWnoAYr4%2Fthe-weighted-majority-algorithm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Weighted%20Majority%20Algorithm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAAqTP6Q5aeWnoAYr4%2Fthe-weighted-majority-algorithm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAAqTP6Q5aeWnoAYr4%2Fthe-weighted-majority-algorithm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3095, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/vp/worse_than_random/\">Worse Than Random</a>, <a href=\"/lw/na/trust_in_bayes/\">Trust In Bayes</a></p>\n<p>In the wider field of Artificial Intelligence, it is not <em>universally</em> agreed and acknowledged that noise hath no power.&nbsp; Indeed, the conventional view in machine learning is that randomized algorithms sometimes perform better than unrandomized counterparts <em>and there is nothing peculiar about this.</em>&nbsp; Thus, reading an ordinary paper in the AI literature, you may suddenly run across a remark:&nbsp; \"There is also an improved version of this algorithm, which takes advantage of randomization...\"</p>\n<p>Now for myself I will be instantly suspicious; I shall inspect the math for reasons why <a href=\"/lw/vp/worse_than_random/\">the unrandomized algorithm is being somewhere stupid</a>, or why the randomized algorithm has a hidden disadvantage.&nbsp; I will look for something peculiar enough to explain the peculiar circumstance of a randomized algorithm somehow doing <em>better.</em></p>\n<p>I am not completely alone in this view.&nbsp; E. T. Jaynes, I found, was of the same mind:&nbsp; \"It appears to be a quite general principle that, whenever there is a randomized way of doing something, then there is a nonrandomized way that delivers better performance but requires more thought.\"&nbsp; Apparently there's now a small cottage industry in derandomizing algorithms.&nbsp; But so far as I know, it is not yet the majority, mainstream view that \"we can improve this algorithm by randomizing it\" is an extremely suspicious thing to say.</p>\n<p>Let us now consider a specific example - a mainstream AI algorithm where there is, apparently, a mathematical proof that the randomized version performs better.&nbsp; By showing how subtle the gotcha can be, I hope to convince you that, even if you run across a case where the randomized algorithm is widely believed to perform better, and you can't find the gotcha yourself, you should nonetheless <a href=\"/lw/na/trust_in_bayes/\">trust that there's a gotcha to be found</a>.</p>\n<p><a id=\"more\"></a></p>\n<p>For our particular example we shall examine the weighted majority algorithm, first introduced by <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.1595\">Littlestone and Warmuth</a>, but as a substantially more readable online reference I will use section 2 of Blum's <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.8097\">On-Line Algorithms in Machine Learning</a>.&nbsp; (Blum has an easier-to-read, more concrete version of the proofs; and the unrandomized and randomized algorithms are presented side-by-side rather than far part.)</p>\n<p>The weighted majority algorithm is an <em>ensemble method</em> - a way to combine the advice from several other algorithms or hypotheses, called \"experts\".&nbsp; Experts strive to classify a set of environmental instances into \"positive\" and \"negative\" categories; each expert produces a prediction for each instance.&nbsp; For example, each expert might look at a photo of a forest, and try to determine whether or not there is a camouflaged tank in the forest.&nbsp; Expert predictions are binary, either \"positive\" or \"negative\", with no probability attached.&nbsp; We do not assume that any of our experts is perfect, and we do not know which of our experts is the best.&nbsp; We also do not assume anything about the samples (they may not be independent and identically distributed).</p>\n<p>The weighted majority algorithm initially assigns an equal weight of 1 to all experts.&nbsp; On each round, we ask all the experts for their predictions, and sum up the weights for each of the two possible predictions, \"positive\" or \"negative\".&nbsp; We output the prediction that has the higher weight.&nbsp; Then, when we see the actual result, we multiply by 1/2 the weight of every expert that got the prediction wrong.</p>\n<p>Suppose the total number of experts is <tt>n</tt>, and the best expert makes no more than <tt>m</tt> mistakes over some given sampling sequence.&nbsp; Then we can prove that the weighted majority algorithm makes a total number of mistakes <tt>M</tt> that is bounded by <tt>2.41*(m + log2(n))</tt>.&nbsp; In other words, the weighted majority algorithm makes no more mistakes than the best expert, plus a factor of log <tt>n</tt>, times a constant factor of 2.41.</p>\n<p>Proof (taken from <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.8097\">Blum 1996</a>; a similar proof appears in <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.1595\">Littlestone and Warmuth 1989</a>):&nbsp; The combined weight of all the experts at the start of the problem is <tt>W = n</tt>.&nbsp; If the weighted majority algorithm makes a mistake, at least half the total weight of experts predicted incorrectly, so the total weight is reduced by a factor of at least 1/4.&nbsp; Thus, after the weighted majority algorithm makes <tt>M</tt> mistakes, its total weight <tt>W</tt> has been reduced by at least <tt>(3/4)^M</tt>.&nbsp; In other words: &nbsp; &nbsp;</p>\n<blockquote>\n<p><tt>W &lt;= n*( (3/4)^M )</tt></p>\n</blockquote>\n<p>But if the best expert has made at most <tt>m</tt> mistakes, its weight is at least&nbsp; (1/2)^m.&nbsp; And since W includes the weight of all experts, &nbsp; &nbsp;</p>\n<blockquote>\n<p><tt>W &gt;= (1/2)^m</tt></p>\n</blockquote>\n<p>Therefore:</p>\n<blockquote>\n<p><tt>(1/2)^m &lt;= W &lt;= n*( (3/4)^M )<br /> (1/2)^m &lt;= n*( (3/4)^M )<br /> -m&nbsp; &nbsp;&nbsp; &nbsp;&lt;= log2(n) + M*log2(3/4)<br /> -m&nbsp; &nbsp;&nbsp; &nbsp;&lt;= log2(n) + M*-log2(4/3)<br /> M*log2(4/3) &lt;= m + log2(n)<br /> M&nbsp; &nbsp;&nbsp; &nbsp; &lt;= (1/log2(4/3))*(m + log2(n))<br /> M&nbsp; &nbsp;&nbsp; &nbsp; &lt;= 2.41*(m + log2(n))</tt></p>\n</blockquote>\n<p>Blum then says that \"we can achieve a better bound than that described above\", by randomizing the algorithm to predict \"positive\" or \"negative\" with <em>probability</em> proportional to the weight assigned each prediction.&nbsp; Thus, if 3/4 of the expert weight went to \"positive\", we would predict \"positive\" with probability 75%, and \"negative\" with probability 25%.</p>\n<p>An essentially similar proof, summing over the expected probability of a mistake on each round, will show that in this case:</p>\n<blockquote>\n<p><tt>M &lt;= 1.39m + 2 ln(n)&nbsp; &nbsp;&nbsp; &nbsp; </tt><em>(note: M is now an expectation)</em></p>\n</blockquote>\n<p>Since the penalty applied to particular experts does not depend on the <em>global prediction</em> but only on the <em>actual outcome,</em> most of the proof proceeds as before.&nbsp; We have<tt></tt></p>\n<blockquote>\n<p><tt>W &gt;= (1/2)^m</tt></p>\n</blockquote>\n<p>where again <tt>m</tt> is the best expert and <tt>W</tt> is the total weight of all experts.</p>\n<p>We also have that W is the starting weight <tt>n</tt> times the product of <tt>(1 - 1/2 F_i)</tt>, where <tt>F_i</tt> is the fraction of mistaken expert weight on round i:</p>\n<blockquote>\n<p><tt>W = n * Prod_i (1 - 1/2 F_i)</tt></p>\n</blockquote>\n<p>And since we predict with probability proportional to the expert weight, the expected number of mistakes is just the sum over F_i:</p>\n<blockquote>\n<p><tt>M = Sum_i F_i</tt></p>\n</blockquote>\n<p>So:</p>\n<blockquote>\n<p><tt>(1/2)^m&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &lt;= </tt><tt>n * Prod_i (1 - 1/2 F_i)<br />-m*ln(2)&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&lt;= ln(n) + Sum_i ln(1 - 1/2 F_i)<br />Sum_i -ln(1 - 1/2 F_i) &lt;= ln(n) + m*ln(2)<br />Sum_i (1/2 F_i)&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &lt;= ln(n) + m*ln(2)&nbsp; &nbsp;&nbsp; &nbsp;</tt><em>(because </em>x &lt; -ln(1 - x)<em> )</em><br /><tt>Sum_i F_i&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &lt;= 2*(ln(n) + m*ln(2))<br />M&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &lt;= 1.39m + 2 ln(n)</tt><tt></tt></p>\n</blockquote>\n<p>Behold, we have done better by randomizing!</p>\n<p>We should be <em>especially</em> suspicious that the randomized algorithm guesses with probability proportional to the expert weight assigned.&nbsp; This seems strongly reminiscent of <a href=\"/lw/vo/lawful_uncertainty/\">betting with 70% probability on blue, when the environment is a random mix of 70% blue and 30% red cards</a>.&nbsp; We know the best bet - and yet we only <em>sometimes</em> make this best bet, at other times betting on a condition we believe to be <em>less</em> probable.</p>\n<p>Yet we thereby <em>prove</em> a smaller upper bound on the expected error.&nbsp; Is there an algebraic error in the second proof?&nbsp; Are we extracting useful work from a noise source?&nbsp; Is our knowledge harming us so much that we can do better through ignorance?</p>\n<p>Maybe the unrandomized algorithm fails to take into account the Bayesian value of information, and hence fails to exhibit exploratory behavior?&nbsp; Maybe it doesn't test unusual cases to see if they might have surprising results?</p>\n<p>But, examining the assumptions, we see that the feedback we receive is fixed, regardless of the prediction's output.&nbsp; Nor does the updating of the expert weights depend on the predictions we output.&nbsp; It doesn't matter whether we substitute a completely random string of 1s and 0s as our actual output.&nbsp; We get back exactly the same data from the environment, and the weighted majority algorithm updates the expert weights in exactly the same way.&nbsp; So that can't be the explanation.</p>\n<p>Are the component experts doing worse than random, so that by randomizing our predictions, we can creep back up toward maximum entropy?&nbsp; But we didn't assume anything about how often the component experts were right, or use the fact in the proofs.&nbsp; Therefore the proofs would carry even if we specified that all experts were right at least 60% of the time.&nbsp; It's hard to see how randomizing our predictions could help, in that case - but the proofs still go through, unchanged.</p>\n<p>So where's the gotcha?</p>\n<p>Maybe I'm about to tell you that I looked, and I couldn't find the gotcha either.&nbsp; What would you believe, in that case?&nbsp; Would you think that the whole thesis on the futility of randomness was probably just wrong - a reasonable-sounding philosophical argument that simply wasn't correct in practice?&nbsp; Would you despair of being able to follow the math, and give up and shrug, unable to decide who might be right?</p>\n<p>We don't <em>always</em> see the flaws right away, and that's something to always remember.</p>\n<p>In any case, I'm about to start explaining the gotcha.&nbsp; If you want to go back, read the paper, and look yourself, you should stop reading now...</p>\n<p>There does exist a rare class of occasions where we want a source of \"true\" randomness, such as a quantum measurement device.&nbsp; For example, you are playing rock-paper-scissors against an opponent who is <em>smarter than you are</em>, and who knows exactly how you will be making your choices.&nbsp; In this condition it is wise to choose randomly, because any method your opponent can predict will do worse-than-average.&nbsp; Assuming that the enemy knows your source code has strange consequences: the action sequence 1, 2, 1, 3 is good when derived from a quantum noise generator, but bad when derived from <em>any</em> deterministic algorithm, even though it is the same action sequence.&nbsp; Still it is not a totally unrealistic situation.&nbsp; In real life, it is, in fact, a bad idea to play rock-paper-scissors using an algorithm your opponent can predict.&nbsp; So are we, in this situation, deriving optimization from noise?</p>\n<p>The random rock-paper-scissors player does not play cleverly, racking up lots of points.&nbsp; It does not win more than 1/3 of the time, or lose less than 1/3 of the time (on average).&nbsp; The randomized player does better because its alternatives perform poorly, not from being smart itself.&nbsp; Moreover, by assumption, the opponent is an intelligence whom we <em>cannot</em> outsmart and who <em>always</em> knows everything about any method we use.&nbsp; There is no move we can make that does not have a possible countermove.&nbsp; By assumption, our own intelligence is entirely useless.&nbsp; The best we can do is to avoid all regularity that the enemy can exploit.&nbsp; In other words, we do best by <em>minimizing</em> the effectiveness of intelligence within the system-as-a-whole, because <em>only</em> the enemy's intelligence is allowed to be effective.&nbsp; If we can't be clever ourselves, we might as well think of ourselves as the environment and the enemy as the sole intelligence within that environment.&nbsp; By becoming the maximum-entropy environment for rock-paper-scissors, we render all intelligence useless, and do (by assumption) the best we can do.</p>\n<p>When the environment is adversarial, smarter than you are, and informed about your methods, then in a theoretical sense it may be wise to have a quantum noise source handy.&nbsp; (In a practical sense you're just screwed.)&nbsp; Again, this is not because we're <a href=\"/lw/o5/the_second_law_of_thermodynamics_and_engines_of/\">extracting useful work from a noise source</a>; it's because the most powerful intelligence in the system is adversarial, and we're minimizing the power that intelligence can exert in the system-as-a-whole.&nbsp; We don't do better-than-average, we merely minimize the extent to which the adversarial intelligence produces an outcome that is worse-than-average (from our perspective).</p>\n<p>Similarly, cryptographers have a legitimate interest in strong randomness generators because cryptographers are trying to minimize the effectiveness of an intelligent adversary.&nbsp; Certainly entropy can act as an antidote to intelligence.</p>\n<p>Now back to the weighted majority algorithm.&nbsp; Blum (1996) remarks:</p>\n<blockquote>\n<p>\"Intuitively, the advantage of the randomized approach is that it dilutes the worst case.&nbsp; Previously, the worst case was that slightly more than half of the total weight predicted incorrectly, causing the algorithm to make a mistake and yet only reduce the total weight by 1/4.&nbsp; Now there is roughly a 50/50 chance that the [randomized] algorithm will predict correctly in this case, and more generally, the probability that the algorithm makes a mistake is tied to the amount that the weight is reduced.\"</p>\n</blockquote>\n<p>From the start, we did our analysis for an <em>upper bound</em> on the number of mistakes made.&nbsp; A global upper bound is no better than the worst individual case; thus, to set a global upper bound we must bound the worst individual case.&nbsp; In the worst case, our environment behaves like an adversarial superintelligence.</p>\n<p>Indeed randomness can improve the worst-case scenario, if the worst-case environment is allowed to exploit \"deterministic\" moves but not \"random\" ones.&nbsp; It is like an environment that can decide to produce a red card whenever you bet on blue - unless you make the <em>same</em> bet using a \"random\" number generator instead of your creative intelligence.</p>\n<p>Suppose we use a quantum measurement device to produce a string of random ones and zeroes; make two copies of this string; use one copy for the weighted majority algorithm's random number generator; and give another copy to an intelligent adversary who picks our samples.&nbsp; In other words, we let the weighted majority algorithm make exactly the same randomized predictions, produced by exactly the same generator, but we also let the \"worst case\" environment know what these randomized predictions will be.&nbsp; Then the improved upper bound of the randomized version is mathematically invalidated.</p>\n<p>This shows that the improved upper bound proven for the randomized algorithm did <em>not</em> come from the randomized algorithm making systematically better predictions - doing superior cognitive work, being more intelligent - but because we arbitrarily declared that an intelligent adversary could read our mind in one case but not the other.</p>\n<p>This is not just a minor quibble.&nbsp; It leads to the testable prediction that on real-world problems, where the environment is usually <em>not</em> an adversarial telepath, the unrandomized weighted-majority algorithm should do better than the randomized version.&nbsp; (Provided that some component experts outperform maximum entropy - are right more than 50% of the time on binary problems.)</p>\n<p>Analyzing the worst-case scenario is standard practice in computational learning theory, but it makes the math do strange things.&nbsp; Moreover, the assumption of the worst case can be subtle; it is not always explicitly labeled.&nbsp; Consider the upper bound for the unrandomized weighted-majority algorithm.&nbsp; I did not use the term \"worst case\" - I merely wrote down some inequalities.&nbsp; In fact, Blum (1996), when initially introducing this bound, does not at first use the phrase \"worst case\".&nbsp; The proof's conclusion says only:</p>\n<blockquote>\n<p>\"Theorem 1. The number of mistakes M made by the Weighted Majority algorithm described above is never more than 2.41(m+lg n), where m is the number of mistakes made by the best expert so far.\"</p>\n</blockquote>\n<p>Key word:&nbsp; <em>Never.</em></p>\n<p>The worst-case assumption for the unrandomized algorithm was implicit in calculating the right-hand-side of the inequality by assuming that, on each mistake, the total weight went down by a factor of 1/4, and the total weight never decreased after any successful prediction.&nbsp; This is the absolute worst possible performance the weighted-majority algorithm can give.</p>\n<p>The assumption implicit in those innocent-looking equations is that the environment <em>carefully maximizes the anguish of the predictor:</em>&nbsp; The environment so cleverly chooses its data samples, that on each case where the weighted-majority algorithm is allowed to be successful, it shall receive not a single iota of useful feedback - not a single expert shall be wrong.&nbsp; And the weighted-majority algorithm will be made to err on sensory cases that produce the minimum possible useful feedback, maliciously fine-tuned to the exact current weighting of experts.&nbsp; We assume that the environment can predict every aspect of the predictor and exploit it - unless the predictor uses a \"random\" number generator which we arbitrarily declare to be unknowable to the adversary.</p>\n<p>What strange assumptions are buried in that innocent little inequality,</p>\n<blockquote>\n<p><tt>&lt;=</tt></p>\n</blockquote>\n<p>Moreover, the entire argument in favor of the randomized algorithm was theoretically suspect from the beginning, because it rested on non-transitive inequalities.&nbsp; If I prove an upper bound on the errors of algorithm X, and then prove a smaller upper bound on the errors of algorithm Y, it does not <em>prove</em> that <em>in the real world</em> Y will perform better than X.&nbsp; For example, I prove that X cannot do worse than 1000 errors, and that Y cannot do worse than 100 errors.&nbsp; Is Y a better algorithm?&nbsp; Not necessarily.&nbsp; Tomorrow I could find an improved proof which shows that X cannot do worse than 90 errors.&nbsp; And then in real life, both X and Y could make exactly 8 errors.</p>\n<blockquote>\n<p><tt>4 &lt;= 1,000,000,000<br /> 9 &lt;= 10</tt></p>\n</blockquote>\n<p>But that doesn't mean that 4 &gt; 9.</p>\n<p>So the next time you see an author remark, \"We can further improve this algorithm using randomization...\" you may not know exactly where to find the oddity.&nbsp; If you'd run across the above-referenced example (or any number of others in the machine-learning literature), you might not have known how to deconstruct the randomized weighted-majority algorithm.&nbsp; If such a thing should happen to you, then I hope that I have given you grounds to suspect that an oddity exists <em>somewhere</em>, even if you cannot find it - just as you would suspect a machine that proposed to extract work from a heat bath, even if you couldn't keep track of all the gears and pipes.</p>\n<p><a href=\"http://www.overcomingbias.com/2008/11/lawful-uncertai.html#comment-138607900\">Nominull</a> put it very compactly when he said that, barring an environment which changes based on the form of your algorithm apa<span id=\"comment-138607900-content\">rt from its output, \"By adding randomness to your algorithm, you spread its behaviors out over a particular distribution, and there must be at least one point in that distribution whose expected value is at least as high as the average expected value of the distribution.\"</span></p>\n<p>As I remarked in <a href=\"/lw/o6/perpetual_motion_beliefs/\">Perpetual Motion Beliefs</a>:</p>\n<blockquote>\n<p>I once knew a fellow who was <em>convinced</em> that his system of wheels and gears would produce reactionless thrust, and he had an Excel spreadsheet that would prove this - which of course he couldn't show us because he was still developing the system.&nbsp; In classical mechanics, violating Conservation of Momentum is <em>provably</em> impossible.&nbsp; So any Excel spreadsheet calculated <em>according to the rules of classical mechanics</em> must <em>necessarily</em> show that no reactionless thrust exists - unless your machine is complicated enough that you have made a mistake in the calculations.</p>\n</blockquote>\n<p>If you ever find yourself <em>mathematically proving</em> that you can do better by randomizing, I suggest that you <a href=\"/lw/na/trust_in_bayes/\">suspect your calculations</a>, or <a href=\"/lw/vh/complexity_and_intelligence/\">suspect your interpretation of your assumptions</a>, before you celebrate your extraction of work from a heat bath.<span id=\"comment-138607900-content\">&nbsp;</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GY5kPPpCoyt9fnTMn": 1, "fpEBgFE7fgpxTm9BF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AAqTP6Q5aeWnoAYr4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 22, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "1142", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 96, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GYuKqAL95eaWTDje5", "BL9DuE2iTCkrnuYzx", "msJA6B9ZjiiZxT6EZ", "QkX2bAkwG2EpGvNug", "zFuCxbY9E2E8HTbfZ", "rELc88PvDkhetQzqx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-13T05:32:25.000Z", "modifiedAt": null, "url": null, "title": "Bay Area Meetup: 11/17 8PM Menlo Park", "slug": "bay-area-meetup-11-17-8pm-menlo-park", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:38.341Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HTNJe8AWn2ZHpc4vC/bay-area-meetup-11-17-8pm-menlo-park", "pageUrlRelative": "/posts/HTNJe8AWn2ZHpc4vC/bay-area-meetup-11-17-8pm-menlo-park", "linkUrl": "https://www.lesswrong.com/posts/HTNJe8AWn2ZHpc4vC/bay-area-meetup-11-17-8pm-menlo-park", "postedAtFormatted": "Thursday, November 13th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bay%20Area%20Meetup%3A%2011%2F17%208PM%20Menlo%20Park&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABay%20Area%20Meetup%3A%2011%2F17%208PM%20Menlo%20Park%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHTNJe8AWn2ZHpc4vC%2Fbay-area-meetup-11-17-8pm-menlo-park%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bay%20Area%20Meetup%3A%2011%2F17%208PM%20Menlo%20Park%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHTNJe8AWn2ZHpc4vC%2Fbay-area-meetup-11-17-8pm-menlo-park", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHTNJe8AWn2ZHpc4vC%2Fbay-area-meetup-11-17-8pm-menlo-park", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 62, "htmlBody": "<p>Robin Gane-McCalla plans to organize regular OB meetups in the Bay Area.&nbsp; The next one is 8PM, November 17th, 2008 (Monday night) in Menlo Park at TechShop.&nbsp; (Note that this is a room with seating, not a restaurant, so we hopefully get a chance to actually <em>talk</em> to each other - though I'll try to stay in the background myself.)</p>\n\n<p><a href=\"http://www.meetup.com/Bay-Area-Overcoming-Bias-Meetup/calendar/9032907/\">RSVP at Meetup.com.</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DQHWBcKeiLnyh9za9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HTNJe8AWn2ZHpc4vC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 4.571816849008193e-07, "legacy": true, "legacyId": "1143", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-13T20:10:58.000Z", "modifiedAt": null, "url": null, "title": "Selling Nonapples", "slug": "selling-nonapples", "viewCount": null, "lastCommentedAt": "2016-08-24T15:36:49.156Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2mLZiWxWKZyaRgcn7/selling-nonapples", "pageUrlRelative": "/posts/2mLZiWxWKZyaRgcn7/selling-nonapples", "linkUrl": "https://www.lesswrong.com/posts/2mLZiWxWKZyaRgcn7/selling-nonapples", "postedAtFormatted": "Thursday, November 13th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Selling%20Nonapples&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelling%20Nonapples%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2mLZiWxWKZyaRgcn7%2Fselling-nonapples%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Selling%20Nonapples%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2mLZiWxWKZyaRgcn7%2Fselling-nonapples", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2mLZiWxWKZyaRgcn7%2Fselling-nonapples", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2008, "htmlBody": "<p><strong>Previously in series</strong>:&nbsp; <a href=\"/lw/vp/worse_than_random/\">Worse Than Random</a></p>\n<p><em>A tale of two architectures...</em></p>\n<p>Once upon a time there was a man named Rodney Brooks, who could justly be called the King of Scruffy Robotics.&nbsp; (Sample paper titles:&nbsp; \"Fast, Cheap, and Out of Control\", \"Intelligence Without Reason\").&nbsp; Brooks invented the \"subsumption architecture\" - robotics based on many small modules, communicating asynchronously and without a central world-model or central planning, acting by reflex, responding to interrupts.&nbsp; The archetypal example is the insect-inspired robot that lifts its leg higher when the leg encounters an obstacle - it doesn't model the obstacle, or plan how to go around it; it just lifts its leg higher.</p>\n<p>In Brooks's paradigm - which he labeled <em>nouvelle AI</em> - intelligence <a href=\"/lw/iv/the_futility_of_emergence/\">emerges</a> from \"situatedness\".&nbsp; One speaks not of an intelligent system, but rather the intelligence that emerges from the interaction of the system and the environment.</p>\n<p>And Brooks wrote a programming language, the <em>behavior language,</em> to help roboticists build systems in his paradigmatic subsumption architecture - a language that includes facilities for asynchronous communication in networks of reflexive components, and programming finite state machines.</p>\n<p>My understanding is that, while there are still people in the world who speak with reverence of Brooks's subsumption architecture, it's not used much in commercial systems on account of being nearly impossible to program.</p>\n<p>Once you start stacking all these modules together, it becomes more and more difficult for the programmer to decide that, yes, an asynchronous local module which raises the robotic leg higher when it detects a block, and meanwhile sends asynchronous signal X to module Y, will indeed produce effective behavior as the outcome of the whole intertwined system whereby intelligence emerges from interaction with the environment...</p>\n<p>Asynchronous parallel decentralized programs are harder to write.&nbsp; And it's <em>not</em> that they're a better, higher form of sorcery that only a few exceptional magi can use.&nbsp; It's more like the difference between the two business plans, \"sell apples\" and \"sell nonapples\".</p>\n<p><a id=\"more\"></a></p>\n<p>One noteworthy critic of Brooks's paradigm in general, and subsumption architecture in particular, is a fellow by the name of Sebastian Thrun.</p>\n<p>You may recall the 2005 DARPA Grand Challenge for the driverless cars.&nbsp; How many ways was this a <em>fair challenge</em> according to the tenets of Scruffydom?&nbsp; Let us count the ways:</p>\n<ul>\n<li>The challenge took place in the real world, where sensors are imperfect, random factors intervene, and macroscopic physics is only approximately lawful.</li>\n<li>The challenge took place outside the laboratory - not even on paved roads, but 212km of desert.</li>\n<li>The challenge took place in real time - continuous perception, continuous action, using only computing power that would fit on a car.</li>\n<li>The teams weren't told the specific race course until 2 hours before the race.</li>\n<li>You could write the code any way you pleased, so long as it worked.</li>\n<li>The challenge was competitive:&nbsp; The prize went to the <em>fastest</em> team that completed the race.&nbsp; Any team which, for ideological reasons, preferred elegance to speed - any team which refused to milk every bit of performance out of their systems - would surely lose to a less principled competitor.</li>\n</ul>\n<p>And the winning team was Stanley, the Stanford robot, built by a team led by Sebastian Thrun.</p>\n<p>How did he do it?&nbsp; If I recall correctly, Thrun said that the key was being able to integrate probabilistic information from many different sensors, using a common representation of uncertainty.&nbsp; This is likely code for \"we used Bayesian methods\", at least if \"Bayesian methods\" is taken to include algorithms like particle filtering.</p>\n<p>And to heavily paraphrase and summarize some of Thrun's criticisms of Brooks's subsumption architecture:</p>\n<p>Robotics becomes pointlessly difficult if, for some odd reason, you insist that there be no central model and no central planning.</p>\n<p>Integrating data from multiple uncertain sensors is a lot easier if you have a common probabilistic representation.&nbsp; Likewise, there are many potential tasks in robotics - in situations as simple as navigating a hallway - when you can end up in two possible situations that look highly similar and have to be distinguished by reasoning about the history of the trajectory.</p>\n<p>To be fair, it's not as if the subsumption architecture has never made money.&nbsp; Rodney Brooks is the founder of iRobot, and I understand that the Roomba uses the subsumption architecture.&nbsp; The Roomba has no doubt made more money than was won in the DARPA Grand Challenge... though the Roomba might not seem quite as impressive...</p>\n<p>But that's not quite today's point.</p>\n<p>Earlier in his career, Sebastian Thrun also wrote a programming language for roboticists.&nbsp; Thrun's language was named CES, which stands for C++ for Embedded Systems.</p>\n<p>CES is a language extension for C++.&nbsp; Its types include probability distributions, which makes it easy for programmers to manipulate and combine multiple sources of uncertain information.&nbsp; And for differentiable variables - including probabilities - the language enables automatic optimization using techniques like gradient descent.&nbsp; Programmers can declare 'gaps' in the code to be filled in by training cases:&nbsp; \"Write me this function.\"</p>\n<p>As a result, Thrun was able to write a small, corridor-navigating mail-delivery robot using 137 lines of code, and this robot required less than 2 hours of training.&nbsp; As Thrun notes, \"Comparable systems usually require at least two orders of magnitude more code and are considerably more difficult to implement.\"&nbsp; Similarly, a 5,000-line robot localization algorithm was reimplemented in 52 lines.</p>\n<p>Why can't you get that kind of productivity with the subsumption architecture?&nbsp; Scruffies, ideologically speaking, are supposed to believe in learning - it's only those evil logical Neats who try to program everything into their AIs in advance.&nbsp; Then why does the subsumption architecture require so much sweat and tears from its programmers?</p>\n<p>Suppose that you're trying to build a wagon out of wood, and unfortunately, the wagon has a problem, which is that it keeps catching on fire.&nbsp; Suddenly, one of the wagon-workers drops his wooden beam.&nbsp; His face lights up.&nbsp; \"I have it!\" he says.&nbsp; \"We need to build this wagon from <em>nonwood materials!</em>\"</p>\n<p>You stare at him for a bit, trying to get over the shock of the new idea; finally you ask, \"What kind of nonwood materials?\"</p>\n<p>The wagoneer hardly hears you.&nbsp; \"Of <em>course</em>!\" he shouts.&nbsp; \"It's all so obvious in retrospect!&nbsp; Wood is simply the <em>wrong material for building wagons!</em>&nbsp; This is the dawn of a new era - the <em>nonwood</em> era - of wheels, axles, carts all made from nonwood!&nbsp; Not only that, instead of taking apples to market, we'll take <em>nonapples</em>!&nbsp; There's a huge market for nonapples - people buy far more nonapples than apples - we should have no trouble selling them!&nbsp; It will be the era of the <em>nouvelle wagon!</em>\"</p>\n<p>The set \"apples\" is much narrower than the set \"not apples\".&nbsp; Apples form a <a href=\"/lw/nl/the_cluster_structure_of_thingspace/\">compact cluster in thingspace</a>, but nonapples vary much more widely in price, and size, and use.&nbsp; When you say to build a wagon using \"wood\", you're giving much more concrete advice than when you say \"not wood\".&nbsp; There are different kinds of wood, of course - but even so, when you say \"wood\", you've narrowed down the range of possible building materials a whole lot more than when you say \"not wood\".</p>\n<p>In the same fashion, \"asynchronous\" - literally \"not synchronous\" - is a much larger design space than \"synchronous\".&nbsp; If one considers the space of all communicating processes, then synchrony is a very strong constraint on those processes.&nbsp; If you toss out synchrony, then you have to pick some other method for preventing communicating processes from stepping on each other - synchrony is <em>one way</em> of doing that, a <em>specific answer</em> to the question.</p>\n<p>Likewise \"parallel processing\" is a much huger design space than \"serial processing\", because serial processing is just a special case of parallel processing where the number of processors happens to be equal to 1.&nbsp; \"Parallel processing\" reopens all sorts of design choices that are premade in serial processing.&nbsp; When you say \"parallel\", it's like stepping out of a small cottage, into a vast and echoing country.&nbsp; You have to stand someplace <em>specific,</em> in that country - you can't stand in the whole place, in the noncottage.</p>\n<p>So when you stand up and shout:&nbsp; \"Aha!&nbsp; I've got it!&nbsp; We've got to solve this problem using <em>asynchronous processes!</em>\", it's like shouting, \"Aha!&nbsp; I've got it!&nbsp; We need to build this wagon out of nonwood!&nbsp; Let's go down to the market and buy a ton of nonwood from the nonwood shop!\"&nbsp; You've got to choose some <em>specific</em> alternative to synchrony.</p>\n<p>Now it may well be that there are other building materials in the universe than wood.&nbsp; It may well be that wood is not the best building material.&nbsp; But you still have to come up with some specific thing to use in its place, like iron.&nbsp; \"Nonwood\" is not a building material, \"sell nonapples\" is not a business strategy, and \"asynchronous\" is not a programming architecture.</p>\n<p>And this is strongly reminiscent of - arguably a special case of - the dilemma of <a href=\"/lw/hg/inductive_bias/\">inductive bias</a>.&nbsp; There's a tradeoff between the strength of the assumptions you make, and how fast you learn.&nbsp; If you make stronger assumptions, you can learn faster when the environment matches those assumptions well, but you'll learn correspondingly more slowly if the environment matches those assumptions poorly.&nbsp; If you make an assumption that lets you learn faster in one environment, it must always perform more poorly in some other environment.&nbsp; Such laws are known as the \"no-free-lunch\" theorems, and the reason they don't prohibit intelligence entirely is that the real universe is a low-entropy special case.</p>\n<p>Programmers have a phrase called the \"Turing Tarpit\"; it describes a situation where everything is possible, but nothing is easy.&nbsp; A Universal Turing Machine can simulate any possible computer, but only at an immense expense in time and memory.&nbsp; If you program in a high-level language like Python, then - while most programming tasks become much simpler - you may occasionally find yourself banging up against the walls imposed by the programming language; sometimes Python won't let you do certain things.&nbsp; If you program directly in machine language, raw 1s and 0s, there are no constraints; you can do anything that can possibly be done by the computer chip; and it will probably take you around a thousand times as much time to get anything done.&nbsp; You have to do, all by yourself, everything that a compiler would normally do on your behalf.</p>\n<p>Usually, when you adopt a program architecture, that choice takes work <em>off</em> your hands.&nbsp; If I use a standard container library - lists and arrays and hashtables - then I don't need to decide how to implement a hashtable, because that choice has already been made for me.</p>\n<p>Adopting the subsumption paradigm means <em>losing</em> order, instead of <em>gaining</em> it.&nbsp; The subsumption architecture is not-synchronous, not-serial, and not-centralized.&nbsp; It's also not-knowledge-modelling and not-planning.</p>\n<p>This absence of solution implies an immense design space, and it requires a correspondingly immense amount of work by the programmers to reimpose order.&nbsp; Under the subsumption architecture, it's the <em>programmer</em> who decides to add an asynchronous local module which detects whether a robotic leg is blocked, and raises it higher.&nbsp; It's the programmer who has to make sure that this behavior plus other module behaviors all add up to an (ideologically correct) emergent intelligence.&nbsp; The lost structure is not replaced.&nbsp; You just get tossed into the Turing Tarpit, the space of all other possible programs.</p>\n<p>On the other hand, CES creates order; it adds the structure of probability distributions and gradient optimization.&nbsp; This narrowing of the design space takes so much work off your hands that you can write a learning robot in 137 lines (at least if you happen to be Sebastian Thrun).</p>\n<p>The moral:</p>\n<p>Quite a few AI architectures aren't.</p>\n<p>If you want to generalize, quite a lot of policies aren't.</p>\n<p>They aren't choices.&nbsp; They're just protests.</p>\n<p><strong>Added</strong>:&nbsp; Robin Hanson <a href=\"http://www.overcomingbias.com/2008/11/selling-nonappl.html#comment-139043986\">says</a>, \"<span id=\"comment-139043986-content\">Economists have to face this in spades. So many people say standard econ has failed and the solution is to do the opposite - non-equilibrium instead of equilibrium, non-selfish instead of selfish, non-individual instead of individual, etc.\"&nbsp; It seems that selling nonapples is a full-blown Standard Iconoclast Failure Mode.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HFou6RHqFagkyrKkW": 1, "sYm3HiWcfZvrGu3ui": 1, "fpEBgFE7fgpxTm9BF": 1, "fuZZ64fNz24BLrXnY": 2, "N3CqcPdCZNspF9bFb": 1, "GY5kPPpCoyt9fnTMn": 1, "Z5A4c4kjTgLSFEr3h": 1, "F2XfCTxXLQBGjbm8P": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2mLZiWxWKZyaRgcn7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 50, "extendedScore": null, "score": 7.3e-05, "legacy": true, "legacyId": "1144", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 50, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 78, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GYuKqAL95eaWTDje5", "8QzZKw9WHRxjR4948", "WBw8dDkAWohFjWQSk", "H59YqogX94z5jb8xx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2008-11-13T20:10:58.000Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-15T06:20:32.000Z", "modifiedAt": null, "url": null, "title": "The Nature of Logic", "slug": "the-nature-of-logic", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:38.450Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c93eRh3mPaN62qrD2/the-nature-of-logic", "pageUrlRelative": "/posts/c93eRh3mPaN62qrD2/the-nature-of-logic", "linkUrl": "https://www.lesswrong.com/posts/c93eRh3mPaN62qrD2/the-nature-of-logic", "postedAtFormatted": "Saturday, November 15th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Nature%20of%20Logic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Nature%20of%20Logic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc93eRh3mPaN62qrD2%2Fthe-nature-of-logic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Nature%20of%20Logic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc93eRh3mPaN62qrD2%2Fthe-nature-of-logic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc93eRh3mPaN62qrD2%2Fthe-nature-of-logic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3078, "htmlBody": "<p><strong>Previously in series</strong>:&nbsp; <a href=\"/lw/vs/selling_nonapples/\">Selling Nonapples</a><br /><strong>Followup to</strong>:&nbsp; <a href=\"/lw/nf/the_parable_of_hemlock/\">The Parable of Hemlock</a></p>\n\n<p>Decades ago, there was a tremendous amount of effort invested in certain kinds of systems that centered around first-order logic - systems where you entered &quot;Socrates is a human&quot; and &quot;all humans are mortal&quot; into a database, and it would spit out &quot;<a href=\"/lw/nf/the_parable_of_hemlock/\">Socrates is mortal</a>&quot;.</p>\n\n<p>The fact that these systems failed to produce &quot;general intelligence&quot; is sometimes taken as evidence of the inadequacy, not only of logic, but of Reason Itself.&nbsp; You meet people who (presumably springboarding off the Spock meme) think that what we really need are <em>emotional</em> AIs, since <em>logic</em> hasn't worked.</p>\n\n<p>What's really going on here is so <em>completely different</em> from the popular narrative that I'm not even sure where to start.&nbsp; So I'm going to try to explain what I <a href=\"/lw/o7/searching_for_bayesstructure/\">see</a> when I look at a &quot;logical AI&quot;.&nbsp; It's <em>not</em> a grand manifestation of the ultimate power of Reason (which then fails).</p><a id=\"more\"></a><p>Let's start with the logical database containing the statements:</p><blockquote><p><tt>|- Human(Socrates)<br />|- \\x.(Human(x) -&gt; Mortal(x))</tt></p></blockquote><p>which then produces the output</p><blockquote><p><tt>|- Mortal(Socrates)</tt></p></blockquote><p>where <tt>|-</tt> is how we say that our logic asserts something, and <tt>\\x.</tt> means &quot;all x&quot; or &quot;in what follows, we can substitute anything we want for x&quot;.</p>\n\n<p>Thus the above is how we'd write the classic syllogism, &quot;Socrates is human, all humans are mortal, therefore Socrates is mortal&quot;.</p>\n\n<p>Now a few months back, I went through <a href=\"/lw/od/37_ways_that_words_can_be_wrong/\">the sequence on words</a>, which included <a href=\"/lw/nf/the_parable_of_hemlock/\">The Parable of Hemlock</a>, which runs thusly:</p><blockquote><p>&nbsp; &nbsp; <em>Socrates raised the glass of hemlock to his\nlips...</em><br />&nbsp; &nbsp; &quot;Do you\nsuppose,&quot; asked one of the onlookers, &quot;that even hemlock will not be\nenough to kill so wise and good a man?&quot;<br />&nbsp; &nbsp; &quot;No,&quot; replied another bystander, a student of\nphilosophy; &quot;all men are\nmortal, and Socrates is a man; and if a mortal drink hemlock, surely he\ndies.&quot;<br />\n&nbsp; &nbsp; &quot;Well,&quot; said the onlooker, &quot;what if it happens that\nSocrates <em>isn't</em>\n mortal?&quot;<br />\n&nbsp; &nbsp; &quot;Nonsense,&quot; replied the student, a little sharply;\n&quot;all men are mortal \n<em>by definition</em>; it is part of what we mean by the word 'man'. \nAll men are mortal, Socrates is a man, therefore Socrates is\nmortal.&nbsp; It is not merely a guess, but a <em>logical certainty.</em>&quot;<br />\n&nbsp; &nbsp; &quot;I suppose that's right...&quot; said the onlooker. \n&quot;Oh, look,\nSocrates already drank the hemlock while we were talking.&quot;<br />\n&nbsp; &nbsp; &quot;Yes, he should be keeling over any minute now,&quot;\nsaid the student.<br />\n&nbsp; &nbsp; <em>And they waited, and they waited, and they waited...</em><br />\n&nbsp; &nbsp; &quot;Socrates appears not to be mortal,&quot; said the\nonlooker.<br />\n&nbsp; &nbsp; &quot;Then Socrates must not be a man,&quot; replied the\nstudent.&nbsp; &quot;All men\nare mortal, Socrates is not mortal, therefore Socrates is not a\nman.&nbsp; And that is not merely a guess, but a <em>logical certainty.</em>&quot; </p></blockquote><p>If you're going to take &quot;all men are mortal&quot; as something true by definition, then you can never conclude that Socrates is &quot;human&quot; until after you've observed him to be mortal.&nbsp; Since logical truths are true in all possible worlds, they never tell you <em>which</em> possible world you live in - no logical truth can predict the result of an empirical event which could <em>possibly</em> go either way.</p>\n\n<p>Could a skeptic say that this logical database is <em>not doing any cognitive work,</em> since it can only tell us what we already know?&nbsp; Or that, since the database is only using logic, it will be unable to do anything empirical, ever?</p>\n\n<p>Even I think that's too severe.&nbsp; The &quot;logical reasoner&quot; is doing a quantum of cognitive labor - it's just a <em>small</em> quantum.</p>\n\n<p>Consider the following sequence of events:</p>\n\n<ul><li>We notice that there's <a href=\"/lw/nj/similarity_clusters/\">an empirical cluster of similar things</a> that have ten fingers, wear clothes, use tools, and speak a language, bleed red blood when cut.&nbsp; We think this empirical cluster is significant.\n</li>\n\n<li>We decide to add the sentence <tt>|- human(x)</tt> to our database, whenever we observe that X has ten fingers, wears clothes, uses tools, and speaks a language.&nbsp; These particular characteristics are easier to observe (we think) than e.g. cutting X to see if red blood comes out.\n</li>\n\n<li>In our experience so far, all entities with ten fingers etc. have died whenever administered hemlock.&nbsp; We think this is a significant fact.\n</li>\n\n<li>Therefore, we add the sentence <tt>|- \\x.(human(x)-&gt;mortal(x))</tt> to our database.\n</li>\n\n<li>Now we see Socrates, and he has ten fingers, wears clothes, uses tools, and fluently speaks Ancient Greek.&nbsp; We decide that Socrates is human.&nbsp; We add the sentence <tt>|- human(Socrates)</tt> to the database.\n</li>\n\n<li>After a bit of grinding, the database spits out <tt>|- mortal(Socrates)</tt> on its screen.\n</li>\n\n<li>We interpret this to mean that Socrates will die if he ingests hemlock.\n</li>\n\n<li>It so happens that, in Socrates's hand, is a drink that we recognize as hemlock.\n</li>\n\n<li>We act accordingly by crying &quot;Watch out!&quot; and knocking the cup out of his hand.</li></ul>\n\n<p>This process, taken as a whole, is hardly <a href=\"/lw/mn/absolute_authority/\">absolutely certain</a>, as in the Spock stereotype of rationalists who cannot conceive that they are wrong.&nbsp; The process did briefly involve a computer program which mimicked a system,\nfirst-order classical logic, which also happens to be used by some\nmathematicians in verifying their proofs.&nbsp; That doesn't lend the entire process the character of\nmathematical proof.&nbsp; And if the process fails, somewhere along the line, that's no call to go casting aspersions on Reason itself.</p>\n\n<p>In this admittedly contrived example, only an <em>infinitesimal</em> fraction of the cognitive work is being performed by the computer program.&nbsp; It's such a small fraction that anything you could say about &quot;logical AI&quot;, wouldn't say much about the process as a whole.</p>\n\n<p>So what's an example of harder cognitive labor?</p>\n\n<p>How about deciding that &quot;human&quot; is an important category to put things in?&nbsp; It's not like we're born seeing little &quot;human&quot; tags hovering over objects, with high priority attached.&nbsp; You have to discriminate stable things in the environment, like Socrates, from your raw sensory information.&nbsp; You have to notice that various stable things are all <a href=\"/lw/nj/similarity_clusters/\">similar</a> to one another, wearing clothes and talking.&nbsp; Then you have to draw a category <a href=\"/lw/o0/where_to_draw_the_boundary/\">boundary</a> around the <a href=\"/lw/nl/the_cluster_structure_of_thingspace/\">cluster</a>, and <a href=\"/lw/nx/categorizing_has_consequences/\">harvest</a> characteristics like vulnerability to hemlock.</p>\n\n<p>A human operator, not the computer, decides whether or not to classify Socrates as a &quot;human&quot;, based on his shape and clothes.&nbsp; The human operator types <tt>|- human(Socrates)</tt> into the database (itself an error-prone sort of process).&nbsp; Then the database spits out&nbsp; <tt>|- mortal(Socrates)</tt> - in the scenario, this is the only fact we've ever told it about humans, so we don't ask why it makes this deduction instead of another one.&nbsp; A human looks at the screen, interprets&nbsp; <tt>|- mortal(Socrates)</tt> to refer to a particular thing in the environment and to imply that thing's vulnerability to hemlock.&nbsp; Then the human decides, based on their values, that they'd rather not see Socrates die; works out a plan to stop Socrates from dying; and executes motor actions to dash the chalice from Socrates's fingers.</p>\n\n<p>Are the off-computer steps &quot;logical&quot;?&nbsp; Are they &quot;illogical&quot;?&nbsp; Are they unreasonable?&nbsp; Are they unlawful?&nbsp; Are they unmathy?</p>\n\n<p>Let me interrupt this tale, to describe a case where you very much <em>do</em> want a computer program that processes logical statements:</p>\n\n<p>Suppose you've got to build a computer chip with a hundred million transistors, and you don't want to recall your product when a bug is discovered in multiplication.&nbsp; You might find it <em>very</em> wise to describe the transistors in first-order logic, and try to prove statements about how the chip performs multiplication.</p>\n\n<p>But then why is logic suited to <em>this</em> particular purpose?</p>\n\n<p>Logic relates abstract statements to specific models.&nbsp; Let's say that I have an abstract statement like &quot;all green objects are round&quot; or &quot;all round objects are soft&quot;.&nbsp; Operating <em>syntactically,</em> working with just the <em>sentences,</em> I can derive &quot;all green objects are soft&quot;.\n\n</p>\n\n<p>Now if you show me a particular collection of shapes, and <em>if it so happens to be true</em> that every green object in that collection is round, and it <em>also</em> happens to be true that every round object is soft, then it will <em>likewise</em> be true that all green objects are soft.</p>\n\n<p>We are not admitting of the possibility that a green-detector on a borderline green object will fire &quot;green&quot; on one occasion and &quot;not-green&quot; on the next.&nbsp; The form of logic in which every proof step preserves validity, relates crisp models to crisp statements.&nbsp; So if you want to draw a <em>direct</em> correspondence between elements of a logical model, and high-level objects in the real world, you had better be dealing with objects that have crisp identities, and categories that have crisp boundaries.</p>\n\n<p>Transistors in a computer chip generally <em>do</em> have crisp identities.&nbsp; So it may indeed be suitable to make a mapping between elements in a logical model, and real transistors in the real world.</p>\n\n<p>So let's say you can perform the mapping and get away with it - then what?</p>\n\n<p>The power of logic is that it <em>relates models and statements</em>.&nbsp; So you've got to make that mental distinction between models on the one hand, and statements on the other.&nbsp; You've got to draw a sharp line between the elements of a model that are green or round, and statements like&nbsp; <tt>\\x.(green(x)-&gt;round(x))</tt>.&nbsp; The statement itself isn't green or round, but it can be true or false about a collection of objects that are green or round.</p>\n\n<p>And here is the power of logic:&nbsp; For each <em>syntactic</em> step we do on our <em>statements,</em> we preserve the <em>match</em> to any <em>model.</em>&nbsp; In any model where our old collection of statements was true, the new statement will also be true.&nbsp; We don't have to check all possible conforming models to see if the new statement is true in all of them.&nbsp; We can trust certain syntactic steps <em>in general</em> - not to <em>produce</em> truth, but to <em>preserve</em> truth.</p>\n\n<p>Then you do a million syntactic steps in a row, and because each step preserves truth, you can trust that the whole sequence will preserve truth.</p>\n\n<p>We start with a chip.&nbsp; We do some physics and decide that whenever transistor X is 0 at time T, transistor Y will be 1 at T+1, or some such - we credit that real events in the chip will correspond quite directly to a <em>model</em> of this statement.&nbsp; We do a whole lot of <em>syntactic</em> manipulation on the abstract laws.&nbsp; We prove a statement that describes binary multiplication.&nbsp; And then we jump back to the model, and then back to the chip, and say, &quot;Whatever the exact actual events on this chip, if they have the physics we described, then multiplication will work the way we want.&quot;</p>\n\n<p>It would be considerably harder (i.e. impossible) to work directly with logical models of every possible computation the chip could carry out.&nbsp; To verify multiplication on two 64-bit inputs, you'd need to check 340 trillion trillion trillion models.</p>\n\n<p>But this trick of doing a million derivations one after the other, and preserving truth throughout, won't work if the premises are <em>only</em> true 999 out of 1000 times.&nbsp; You could get away with ten steps in the derivation and not lose too much, but a million would be out of the question.</p>\n\n<p>So the truth-preserving syntactic manipulations we call &quot;logic&quot; can be very useful indeed, when we draw a correspondence to a digital computer chip where the transistor error rate is very low.</p>\n\n<p>But if you're trying to draw a direct correspondence between the primitive elements of a logical model and, say, entire biological humans, that may not work out as well.</p>\n\n<p>First-order logic has a number of wonderful properties, like <em>detachment.</em>&nbsp; We don't care <em>how</em> you proved something - once you arrive at a statement, we can forget how we got it.&nbsp; The syntactic rules are <em>local,</em> and use statements as fodder without worrying about their provenance.&nbsp; So once we prove a theorem, there's no need to keep track of how, in particular, it was proved.</p>\n\n<p>But what if one of your premises turns out to be wrong, and you have to retract something you already concluded?&nbsp; Wouldn't you want to keep track of which premises you'd used?</p>\n\n<p>If the burglar alarm goes off, that means that a burglar is burgling your house.&nbsp; But if there was an earthquake that day, it's probably the earthquake that set off the alarm instead.&nbsp; But if you learned that there was a burglar from the police, rather than the alarm, then you don't want to retract the &quot;burglar&quot; conclusion on finding that there was an earthquake...</p>\n\n<p>It says a lot about the problematic course of early AI, that people first tried to handle this problem with <em>nonmonotonic logics.</em>&nbsp; They would try to handle statements like &quot;A burglar alarm indicates there's a burglar - unless there's an earthquake&quot; using a <em>slightly modified</em> logical database that would draw conclusions and then retract them.</p>\n\n<p>And this gave rise to huge problems for many years, because they were trying to do, in the <em>style</em> of logic, something that was not at all like the <em>actual</em> <em>nature</em> of logic as math.&nbsp; Trying to retract a particular conclusion goes <em>completely against</em> the nature of first-order logic as a mathematical structure.</p>\n\n<p>If you were given to jumping to conclusions, you might say &quot;Well, <em>math</em> can't handle that kind of problem because there are no absolute laws as to what you conclude when you hear the burglar alarm - you've got to use your common-sense judgment, not math.&quot;\n</p>\n\n\n<p>But it's not an <em>unlawful</em> or even <em>unmathy</em> question.</p>\n\n\n<p>It turns out that for at least the kind of case I've described above - where you've got effects that have more than one possible cause - we can <em>excellently</em> handle a wide range of scenarios using a crystallization of probability theory known as &quot;Bayesian networks&quot;.&nbsp; And lo, we can prove all sorts of wonderful theorems that I'm not going to go into.&nbsp; (See Pearl's &quot;Probabilistic Reasoning in Intelligent Systems&quot;.)</p>\n\n<p>And the <em>real</em> solution turned out to be much <em>more</em> elegant than all the messy ad-hoc attempts at &quot;nonmonotonic logic&quot;.&nbsp; On non-loopy networks, you can do all sorts of wonderful things like propagate updates in parallel using asynchronous messages, where each node only tracks the messages coming from its immediate neighbors in the graph, etcetera.&nbsp; And this parallel, asynchronous, decentralized algorithm is provably correct as probability theory, etcetera.</p>\n\n<p>So... are Bayesian networks <em>illogical?</em></p>\n\n<p>Certainly not in the colloquial sense of the word.</p>\n\n<p>You <em>could</em> write a logic that implemented a Bayesian network.&nbsp; You could represent the probabilities and graphs in a logical database.&nbsp; The elements of your model would no longer correspond to things like Socrates, but rather correspond to conditional probabilities or graph edges...&nbsp; But why bother?&nbsp; Non-loopy Bayesian networks propagate their inferences in nicely local ways.&nbsp; There's no need to stick a bunch of statements in a centralized logical database and then waste computing power to pluck out global inferences.</p>\n\n<p>What am I trying to convey here?&nbsp; I'm trying to convey that <em>thinking mathematically about uncertain reasoning </em>is a <em>completely different concept</em> from <em>AI programs that assume direct correspondences between the elements of a logical model and the high-level regularities of reality.</em></p>\n\n<p>&quot;The failure of logical AI&quot; is not &quot;the failure of mathematical thinking about AI&quot; and certainly not &quot;the limits of lawful reasoning&quot;.&nbsp; The &quot;failure of logical AI&quot; is more like, &quot;That thing with the database containing statements about Socrates and hemlock - not only were you using the <em>wrong math,</em> but you weren't even <em>looking at the interesting parts of the problem.</em>&quot;</p>\n\n<p>Now I did concede that the logical reasoner talking about Socrates and hemlock, <em>was</em> performing a quantum of cognitive labor.&nbsp; We can now describe that quantum:</p>\n\n<p>&quot;<em>After</em> you've arrived at such-and-such hypothesis about what goes on behind the scenes of your sensory information, and distinguished the pretty-crisp identity of 'Socrates' and categorized it into the pretty-crisp cluster of 'human', then, <em>if</em> the other things you've observed to usually hold true of 'humans' are accurate in this case, 'Socrates' will have the pretty-crisp property of 'mortality'.&quot;</p>\n\n<p>This quantum of labor tells you a single implication of what you already believe... but actually it's an even smaller quantum than this.&nbsp; The step carried out by the logical database corresponds to <em>verifying</em> this step of inference, not <em>deciding</em> to carry it out.&nbsp; Logic makes no mention of which inferences we should perform <em>first</em> - the syntactic derivations are timeless and unprioritized.&nbsp; It is nowhere represented in the nature of logic as math, that if the 'Socrates' thingy is drinking hemlock, <em>right now</em> is a good time to ask if he's mortal.</p>\n\n<p>And indeed, modern AI programs still aren't very good at guiding inference.&nbsp; If you want to prove a computer chip correct, you've got to have a human alongside to suggest the lemmas to be proved next.&nbsp; The nature of logic is better suited to verification than construction - it preserves truth through a million syntactic manipulations, but it doesn't <em>prioritize</em> those manipulations in any particular order.&nbsp; So saying &quot;Use logic!&quot; isn't going to solve the problem of searching for proofs.</p>\n\n\n\n<p>This doesn't mean that &quot;What inference should I perform next?&quot; is an <em>unlawful</em> question to which no math applies.&nbsp; Just that the math of <em>logic</em> that relates models and statements, relates timeless models to timeless statements in a world of unprioritized syntactic manipulations.&nbsp; You might be able to use logic to reason <em>about</em> time or <em>about</em> expected utility, the same way you could use it to represent a Bayesian network.&nbsp; But that wouldn't introduce time, or wanting, or nonmonotonicity, into the nature of logic as a mathematical structure.</p>\n\n<p>Now, <em>math itself</em> tends to be timeless and detachable and proceed from premise to conclusion, at least when it happens to be right.&nbsp; So logic is well-suited to <em>verifying</em> mathematical thoughts - though <em>producing</em> those thoughts in the first place, choosing lemmas and deciding which theorems are important, is a whole different problem.</p>\n\n<p>Logic might be well-suited to <em>verifying</em> your derivation of the Bayesian network rules from the axioms of probability theory.&nbsp; But this doesn't mean that, as a programmer, you should try implementing a Bayesian network on top of a logical database.&nbsp; Nor, for that matter, that you should rely on a first-order theorem prover to <em>invent</em> the idea of a &quot;Bayesian network&quot; from scratch.</p>\n\n<p><em>Thinking mathematically about uncertain reasoning</em>, doesn't mean that you try to turn everything into a logical model.&nbsp; It means that you comprehend the nature of logic itself <em>within</em> your mathematical vision of cognition, so that you can see which environments and problems are nicely matched to the structure of logic.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1, "sYm3HiWcfZvrGu3ui": 1, "GY5kPPpCoyt9fnTMn": 1, "FtT2T9bRbECCGYxrL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c93eRh3mPaN62qrD2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 34, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "1145", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2mLZiWxWKZyaRgcn7", "bcM5ft8jvsffsZZ4Y", "QrhAeKBkm2WsdRYao", "FaJaCgqBKphrDzDSj", "jMTbQj9XB5ah2maup", "PmQkensvTGg7nGtJE", "d5NyJ2Lf6N22AD9PB", "WBw8dDkAWohFjWQSk", "veN86cBhoe7mBxXLk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-16T03:19:09.000Z", "modifiedAt": null, "url": null, "title": "Boston-area Meetup: 11/18/08 9pm MIT/Cambridge", "slug": "boston-area-meetup-11-18-08-9pm-mit-cambridge", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:38.483Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/23Ha6Su2GknYWJJk7/boston-area-meetup-11-18-08-9pm-mit-cambridge", "pageUrlRelative": "/posts/23Ha6Su2GknYWJJk7/boston-area-meetup-11-18-08-9pm-mit-cambridge", "linkUrl": "https://www.lesswrong.com/posts/23Ha6Su2GknYWJJk7/boston-area-meetup-11-18-08-9pm-mit-cambridge", "postedAtFormatted": "Sunday, November 16th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Boston-area%20Meetup%3A%2011%2F18%2F08%209pm%20MIT%2FCambridge&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABoston-area%20Meetup%3A%2011%2F18%2F08%209pm%20MIT%2FCambridge%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F23Ha6Su2GknYWJJk7%2Fboston-area-meetup-11-18-08-9pm-mit-cambridge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Boston-area%20Meetup%3A%2011%2F18%2F08%209pm%20MIT%2FCambridge%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F23Ha6Su2GknYWJJk7%2Fboston-area-meetup-11-18-08-9pm-mit-cambridge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F23Ha6Su2GknYWJJk7%2Fboston-area-meetup-11-18-08-9pm-mit-cambridge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 160, "htmlBody": "<p>There will be an OB\nmeetup this Tuesday in Cambridge MA, hosted by Michael Vassar, Owain Evans\n(grad student at MIT), and Dario Amodei (grad student at Princeton). \nThe event will take place on the MIT campus, in a spacious seminar room in MIT's Stata Center.&nbsp; Refreshments will be provided.&nbsp; Details and directions below the fold.\n\n\n</p>\n\n<p>\nPlease let us know in the comments if you plan to attend.</p><p><em>(Posted on behalf of Owain Evans.)</em></p>\n<a id=\"more\"></a><p><strong>Time/date:</strong> 9pm, Tuesday 18 November.\n\n\n</p>\n\n<p><strong>Place:</strong> Room d461 in MIT's Stata Center.\n\n\n</p>\n\n<p><strong>Address:</strong>\nThe Stata Center: 32 Vassar Street, Cambridge.\n\n\n</p>\n\n<p><strong>Directions:</strong>\nThe nearest T stop is Kendall/MIT on the Red Line.\n\n</p>\n\n<p>Enter Stata via the entrance facing Main Street (with a big metal\n&quot;MIT&quot; sign outside it) from 8.45pm and one of the hosts will guide you\nto d461. Alternatively, here are <a href=\"http://web.mit.edu/philos/www/fourthfloor.html\">\ndirections</a>\nto d461 once you reach Stata.\n\n\n</p>\n\n<p><strong>Email:</strong>\nowain (at) mit edu</p>\n\n<p><strong>Phone:</strong>\nIf you can't find the room or if you arrive late and are unable to\nenter the building, call 610-608-3345.<br />\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DQHWBcKeiLnyh9za9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "23Ha6Su2GknYWJJk7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 4.5773295502140683e-07, "legacy": true, "legacyId": "1146", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-17T08:03:12.000Z", "modifiedAt": null, "url": null, "title": "Logical or Connectionist AI?", "slug": "logical-or-connectionist-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:27.446Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/juomoqiNzeAuq4JMm/logical-or-connectionist-ai", "pageUrlRelative": "/posts/juomoqiNzeAuq4JMm/logical-or-connectionist-ai", "linkUrl": "https://www.lesswrong.com/posts/juomoqiNzeAuq4JMm/logical-or-connectionist-ai", "postedAtFormatted": "Monday, November 17th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Logical%20or%20Connectionist%20AI%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALogical%20or%20Connectionist%20AI%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjuomoqiNzeAuq4JMm%2Flogical-or-connectionist-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Logical%20or%20Connectionist%20AI%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjuomoqiNzeAuq4JMm%2Flogical-or-connectionist-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjuomoqiNzeAuq4JMm%2Flogical-or-connectionist-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2626, "htmlBody": "<p><strong>Previously in series</strong>:&nbsp; <a href=\"http://www.overcomingbias.com/2008/11/first-order-log.html\">The Nature of Logic</a></p>\n<p>People who don't work in AI, who hear that I work in AI, often ask me:&nbsp; \"<a href=\"http://www.youtube.com/watch?v=_DWxPUpWxhM\">Do you build neural networks or expert systems?</a>\"&nbsp; This is said in much the same tones as \"Are you a good witch or a bad witch?\"</p>\n<p>Now <em>that's</em> what I call successful marketing.</p>\n<p>Yesterday I covered what I see when I look at \"logic\" as an AI technique.&nbsp; I see something with a particular shape, a particular power, and a well-defined domain of useful application where cognition is concerned.&nbsp; Logic is good for leaping from crisp real-world events to compact general laws, and then verifying that a given manipulation of the laws preserves truth.&nbsp; It isn't even <em>remotely close</em> to the whole, or the center, of a mathematical outlook on cognition.</p>\n<p>But for a long time, years and years, there was a tremendous focus in Artificial Intelligence on what I call \"<a href=\"http://www.overcomingbias.com/2008/07/detached-lever.html\">suggestively named</a> <a href=\"http://www.overcomingbias.com/2007/11/artificial-addi.html\">LISP tokens</a>\" - a misuse of logic to try to handle cases like \"Socrates is human, all humans are mortal, therefore Socrates is mortal\".&nbsp; For many researchers, this one small element of math was indeed their universe.</p>\n<p>And then along came the <em>amazing revolution,</em> the <em>new</em> AI, namely connectionism.</p>\n<p><a id=\"more\"></a></p>\n<p>In the beginning (1957) was Rosenblatt's Perceptron.&nbsp; It was, I believe, billed as being inspired by the brain's biological neurons.&nbsp; The Perceptron had exactly two layers, a set of input units, and a single binary output unit.&nbsp; You multiplied the inputs by the weightings on those units, added up the results, and took the sign: that was the classification.&nbsp; To learn from the training data, you checked the current classification on an input, and if it was wrong, you dropped a delta on all the weights to nudge the classification in the right direction.</p>\n<p>The Perceptron could only learn to deal with training data that was linearly separable - points in a hyperspace that could be cleanly separated by a hyperplane.</p>\n<p>And that was <em>all</em> that this amazing algorithm, \"inspired by the brain\", could do.</p>\n<p>In 1969, Marvin Minsky and Seymour Papert pointed out that Perceptrons couldn't learn the XOR function because it wasn't linearly separable.&nbsp; This killed off research in neural networks for the next ten years.</p>\n<p>Now, you might think to yourself:&nbsp; \"Hey, what if you had <em>more than two layers</em> in a neural network?&nbsp; Maybe <em>then</em> it could learn the XOR function?\"</p>\n<p>Well, but if you know a bit of linear algebra, you'll realize that if the units in your neural network have outputs that are linear functions of input, then any number of hidden layers is going to behave the same way as a single layer - you'll only be able to learn functions that are linearly separable.</p>\n<p>Okay, so what if you had hidden layers and the outputs <em>weren't</em> linear functions of the input?</p>\n<p>But you see - no one had any idea how to <em>train</em> a neural network like that.&nbsp; Cuz, like, then <em>this</em> weight would affect <em>that</em> output and that <em>other</em> output too, nonlinearly, so how were you supposed to figure out how to nudge the weights in the right direction?</p>\n<p>Just make random changes to the network and see if it did any better?&nbsp; You may be underestimating how much computing power it takes to do things the truly stupid way.&nbsp; It wasn't a popular line of research.</p>\n<p>Then along came this brilliant idea, called \"backpropagation\":</p>\n<p>You handed the network a training input.&nbsp; The network classified it incorrectly.&nbsp; So you took the partial derivative of the output error (in layer N) with respect to each of the individual nodes in the preceding layer (N - 1).&nbsp; Then you could calculate the partial derivative of the output error with respect to any single weight or bias in the layer N - 1.&nbsp; And you could also go ahead and calculate the partial derivative of the output error with respect to each node in the layer N - 2.&nbsp; So you did layer N - 2, and then N - 3, and so on back to the input layer.&nbsp; (Though backprop nets usually had a grand total of 3 layers.)&nbsp; Then you just nudged the whole network a delta - that is, nudged each weight or bias by delta times its partial derivative with respect to the output error.</p>\n<p>It says a lot about the nonobvious difficulty of doing math that it took <em>years</em> to come up with this algorithm.</p>\n<p>I find it difficult to put into words just <em>how obvious</em> this is in retrospect.&nbsp; You're just taking a system whose behavior is a differentiable function of continuous paramaters, and sliding the whole thing down the slope of the error function.&nbsp; There are much more clever ways to train neural nets, taking into account more than the first derivative, e.g. <a href=\"http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\">conjugate gradient</a> optimization, and these take some effort to understand even if you know calculus.&nbsp; But backpropagation is ridiculously simple.&nbsp; Take the network, take the partial derivative of the error function with respect to each weight in the network, slide it down the slope.</p>\n<p>If I didn't know the history of connectionism, and I didn't know scientific history in general - if I had needed to guess <em><a href=\"http://www.overcomingbias.com/2007/08/hindsight-deval.html\">without benefit of hindsight</a></em> how long it <em>ought</em> to take to go from Perceptrons to backpropagation - then I would probably say something like:&nbsp; \"Maybe a couple of hours?&nbsp; Lower bound, <a href=\"http://www.overcomingbias.com/2008/05/eld-science.html\">five minutes</a> - upper bound, three days.\"</p>\n<p>\"Seventeen years\" would have <em>floored</em> me.</p>\n<p>And I <em>know</em> that backpropagation may be slightly less obvious if you don't have the idea of \"gradient descent\" as a standard optimization technique bopping around in your head.&nbsp; I <em>know</em> that these were smart people, and I'm doing the equivalent of complaining that Newton only invented first-year undergraduate stuff, etc.</p>\n<p>So I'm just mentioning this little historical note about the timescale of mathematical progress, to emphasize that all the people who say \"AI is 30 years away so we don't need to worry about Friendliness theory yet\" have moldy jello in their skulls.</p>\n<p>(Which I suspect is part of a general syndrome where people's picture of Science comes from reading press releases that announce important discoveries, so that they're like, \"Really?&nbsp; You do science?&nbsp; What kind of important discoveries do you announce?\"&nbsp; Apparently, in their world, when AI finally <em>is</em> \"visibly imminent\", someone just needs to issue a press release to announce the completion of Friendly AI theory.)</p>\n<p>Backpropagation is not just clever; much more importantly, it turns out to work well in real life on a wide class of tractable problems.&nbsp; Not all \"neural network\" algorithms use backprop, but if you said, \"networks of connected units with continuous parameters and differentiable behavior which learn by traveling up a performance gradient\", you would cover a pretty large swathe.</p>\n<p>But the real cleverness is in how neural networks were marketed.</p>\n<p>They left out the math.</p>\n<p>To me, at least, it seems that a backprop neural network involves <em>substantially deeper</em> mathematical ideas than \"Socrates is human, all humans are mortal, Socrates is mortal\".&nbsp; Newton versus Aristotle.&nbsp; I would even say that a neural network is more analyzable - since it does more real cognitive labor on board a computer chip where I can actually <em>look</em> at it, rather than relying on inscrutable human operators who type \"|- Human(Socrates)\" into the keyboard under God knows what circumstances.</p>\n<p>But neural networks were not marketed as cleverer math.&nbsp; Instead they were marketed as a revolt against Spock.</p>\n<p>No, more than that - the neural network was the new champion of the Other Side of the Force - the antihero of a Manichaean conflict between Law and Chaos.&nbsp; And all good researchers and true were called to fight on the side of Chaos, to overthrow the corrupt Authority and its Order.&nbsp; To champion Freedom and Individuality against Control and Uniformity.&nbsp; To Decentralize instead of Centralize, substitute Empirical Testing for mere Proof, and replace Rigidity with Flexibility.</p>\n<p>I suppose a grand conflict between Law and Chaos, beats trying to explain calculus in a press release.</p>\n<p>But the thing is, a neural network isn't an avatar of Chaos any more than an expert system is an avatar of Law.</p>\n<p>It's just... you know... a system with continuous parameters and differentiable behavior traveling up a performance gradient.</p>\n<p>And logic is a great way of <a href=\"http://www.overcomingbias.com/2008/11/first-order-log.html\">verifying truth preservation by syntactic manipulation of compact generalizations that are true in crisp models</a>.&nbsp; That's it.&nbsp; That's all.&nbsp; This kind of logical AI is not the avatar of Math, Reason, or Law.</p>\n<p>Both algorithms do what they do, and are what they are; nothing more.</p>\n<p>But the successful marketing campaign said,</p>\n<p>\"The failure of logical systems to produce real AI has shown that intelligence isn't logical.&nbsp; Top-down design doesn't work; we need bottom-up techniques, like neural networks.\"</p>\n<p>And this is what I call the Lemon Glazing Fallacy, which generates an argument for a fully arbitrary New Idea in AI using the following template:</p>\n<ul>\n<li>Major premise:&nbsp; All previous AI efforts failed to yield true intelligence.</li>\n<li>Minor premise:&nbsp; All previous AIs were built without delicious lemon glazing.</li>\n<li>Conclusion:&nbsp; If we build AIs with delicious lemon glazing, they will work.</li>\n</ul>\n<p>This only has the appearance of plausibility if you present a <a href=\"http://www.overcomingbias.com/2008/01/the-two-party-s.html\">Grand Dichotomy</a>.&nbsp; It doesn't do to say \"AI Technique #283 has failed for years to produce general intelligence - that's why you need to adopt my new AI Technique #420.\"&nbsp; Someone might ask, \"Well, that's very nice, but what about AI technique #59,832?\"</p>\n<p>No, you've got to make 420 and &not;420 into the <em>whole universe</em> - allow <em>only</em> these two possibilities - put them on opposite sides of the Force - so that ten thousand failed attempts to build AI are actually arguing <em>for</em> your own success.&nbsp; All those failures are weighing down the <em>other</em> side of the scales, pushing up your own side... right?&nbsp; (In Star Wars, the Force has at least one Side that does seem pretty Dark.&nbsp; But who says the Jedi are the Light Side just because they're not Sith?)</p>\n<p>Ten thousand failures don't tell you what <em>will</em> work.&nbsp; They don't even say what should <em>not</em> be part of a successful AI system.&nbsp; <a href=\"http://www.overcomingbias.com/2007/12/reversed-stupid.html\">Reversed stupidity is not intelligence.</a></p>\n<p>If you remove the power cord from your computer, it will stop working.&nbsp; You can't thereby conclude that <em>everything</em> about the current system is wrong, and an optimal computer should not have an Intel processor or Nvidia video card or case fans or run on electricity.&nbsp; Even though your current system has these properties, and it doesn't work.</p>\n<p>As it so happens, I do believe that the type of systems usually termed GOFAI will <em>not</em> yield general intelligence, even if you run them on a computer the size of the moon.&nbsp; But this opinion follows from my own view of intelligence.&nbsp; It does <em>not</em> follow, even as suggestive evidence, from the historical fact that a thousand systems built using Prolog did not yield general intelligence.&nbsp; So far as the logical sequitur goes, one might as well say that Silicon-Based AI has shown itself deficient, and we must try to build transistors out of carbon atoms instead.</p>\n<p>Not to mention that neural networks have also been \"failing\" (i.e., not yet succeeding) to produce <em>real AI</em> for 30 years now.&nbsp; I don't think this particular raw fact licenses any conclusions in particular.&nbsp; But at least don't tell me it's still the <em>new revolutionary</em> idea in AI.</p>\n<p>This is the original example I used when I talked about <a href=\"http://www.overcomingbias.com/2007/10/outside-the-box.html\">the \"Outside the Box\" box</a> - people think of \"amazing new AI idea\" and return their first <a href=\"http://www.overcomingbias.com/2007/10/cached-thoughts.html\">cache hit</a>, which is \"neural networks\" due to a successful marketing campaign <em>thirty goddamned years ago.</em>&nbsp; I mean, not every old idea is <em>bad</em> - but to still be marketing it as the new defiant revolution?&nbsp; Give me a break.</p>\n<p>And pity the poor souls who try to think outside the \"outside the box\" box - outside the <em>ordinary</em> bounds of logical AI vs. connectionist AI - and, after mighty strains, propose <em>a hybrid system that includes both logical and neural-net components.</em></p>\n<p>It goes to show that compromise is not always the path to optimality - though it may sound Deeply Wise to say that the universe must balance between Law and Chaos.</p>\n<p>Where do Bayesian networks fit into this dichotomy?&nbsp; They're parallel, asynchronous, decentralized, distributed, probabilistic.&nbsp; And they can be proven correct from the axioms of probability theory.&nbsp; You can preprogram them, or learn them from a corpus of unsupervised data - using, in some cases, formally correct Bayesian updating.&nbsp; They can reason based on incomplete evidence.&nbsp; Loopy Bayes nets, rather than computing the correct probability estimate, might compute an approximation using Monte Carlo - but the approximation provably converges - but we don't run long enough to converge...</p>\n<p>Where does <em>that</em> fit on the axis that runs from logical AI to neural networks?&nbsp; And the answer is that it doesn't.&nbsp; It doesn't fit.</p>\n<p>It's not that Bayesian networks \"combine the advantages of logic and neural nets\".&nbsp; They're simply a different point in the space of algorithms, with different properties.</p>\n<p>At the inaugural seminar of Redwood Neuroscience, I once saw a presentation describing a robot that started out walking on legs, and learned to run... in real time, over the course of around a minute.&nbsp; The robot was stabilized in the Z axis, but it was still <em>pretty darned impressive.</em>&nbsp; (When first exhibited, someone apparently stood up and said \"<a href=\"http://www.overcomingbias.com/2007/08/i-defy-the-data.html\">You sped up that video, didn't you?</a>\" because they couldn't believe it.)</p>\n<p>This robot ran on a \"neural network\" built by <em>detailed</em> study of biology.&nbsp; The network had twenty neurons or so.&nbsp; Each neuron had a separate name and its own equation.&nbsp; And believe me, the robot's builders <em>knew</em> how that network worked.</p>\n<p>Where does <em>that</em> fit into the grand dichotomy?&nbsp; Is it top-down?&nbsp; Is it bottom-up?&nbsp; Calling it \"parallel\" or \"distributed\" seems like kind of a silly waste when you've only got 20 neurons - who's going to bother multithreading that?</p>\n<p>This is what a <em>real</em> biologically inspired system looks like.&nbsp; And let me say again, that video of the running robot would have been damned impressive even if it <em>hadn't</em> been done <em>using only twenty neurons</em>.&nbsp; But that biological network didn't much resemble - at all, really - the artificial neural nets that are built using abstract understanding of gradient optimization, like backprop.</p>\n<p>That network of 20 neurons, each with its own equation, built and understood from careful study of biology - where does it fit into the Manichaean conflict?&nbsp; It doesn't.&nbsp; It's just a different point in AIspace.</p>\n<p>At a conference ysterday, I spoke to someone who thought that Google's translation algorithm was a triumph of Chaotic-aligned AI, because none of the people on the translation team spoke Arabic and yet they built an Arabic translator using a massive corpus of data.&nbsp; And I said that, while I wasn't familiar in detail with Google's translator, the little I knew about it led me to believe that they were using well-understood algorithms - Bayesian ones, in fact - and that if no one on the translation team knew any Arabic, this was no more significant than Deep Blue's programmers playing poor chess.</p>\n<p>Since Peter Norvig also happened to be at the conference, I asked him about it, and Norvig said that they started out doing an actual Bayesian calculation, but then took a couple of steps away.&nbsp; I remarked, \"Well, you probably weren't doing the <em>real</em> Bayesian calculation anyway - assuming conditional independence where it doesn't exist, and stuff\", and Norvig said, \"Yes, so we've already established what kind of algorithm it is, and now we're just haggling over the price.\"</p>\n<p>Where does <em>that</em> fit into the axis of logical AI and neural nets?&nbsp; It doesn't even <em>talk</em> to that axis.&nbsp; It's just a different point in the design space.</p>\n<p>The grand dichotomy is a lie - which is to say, a highly successful marketing campaign which managed to position two particular fragments of optimization as the Dark Side and Light Side of the Force.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "fpEBgFE7fgpxTm9BF": 1, "bY5MaF2EATwDkomvu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "juomoqiNzeAuq4JMm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 34, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "1147", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-17T19:38:20.000Z", "modifiedAt": null, "url": null, "title": "Whither OB?", "slug": "whither-ob", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:52.554Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/59ithK3WGEMFk7iHJ/whither-ob", "pageUrlRelative": "/posts/59ithK3WGEMFk7iHJ/whither-ob", "linkUrl": "https://www.lesswrong.com/posts/59ithK3WGEMFk7iHJ/whither-ob", "postedAtFormatted": "Monday, November 17th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Whither%20OB%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhither%20OB%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F59ithK3WGEMFk7iHJ%2Fwhither-ob%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Whither%20OB%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F59ithK3WGEMFk7iHJ%2Fwhither-ob", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F59ithK3WGEMFk7iHJ%2Fwhither-ob", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 557, "htmlBody": "<p>Robin plans to cut back posting shortly, after he and I have our long-awaited Disagreement about AI self-improvement.&nbsp; As for myself - I'm not finished, but I'm way over schedule and need to move on soon.&nbsp; I'm not going to stop posting entirely (I doubt I could if I tried) but I'm not going to be posting daily.</p>\n\n<p>There are three directions that Overcoming Bias could go from here:</p>\n\n<p>First, we could find enough <em>good</em> authors to keep going at a post per day.&nbsp; Say, seven people who can and will write one post per week.&nbsp; We can't compromise on quality, though.</p>\n\n<p>Second, we could try to shift to a more community-based format.&nbsp; Our most popular post <em>ever</em>, still getting hits to this day, was not written by Robin or myself or any of the recurring editors.&nbsp; It's &quot;<a href=\"http://www.overcomingbias.com/2008/02/my-favorite-lia.html\">My Favorite Liar</a>&quot; by Kai Chang, about the professor who inserted one false statement into each lecture.&nbsp; If one-tenth of our readers contributed a single story as good as this... but neither Robin nor myself have time to vet them all.&nbsp; So one approach would be to have a community forum where anyone could post, readers voted the posts up and down, and a front page to which the editors promoted posts deemed worthy.&nbsp; I understand that Scoop has software like this, but I would like to know <strong>if our readers can recommend better community software</strong> (see below).</p>\n\n<p>Third, we could close OB to new submissions and keep the archives online eternally, saying, &quot;It had a good run.&quot;&nbsp; As Nick put it, we shouldn't keep going if it means a slow degeneration.</p><a id=\"more\"></a><p>My own perspective:&nbsp; <em>Overcoming Bias</em> presently gets over a quarter-million monthly pageviews.&nbsp; We've built something that seems like it should be important.&nbsp; It feels premature, but I would like to try to launch an online rationalist community.</p>\n\n<p>At this point, I'm advocating a hybrid approach:&nbsp; Keep OB open with fewer posts of the same <em>gravitas,</em> hang out a sign for new authors, and also try starting up a community-based site with user submissions and more frequent shorter posts.&nbsp; I've got plenty of light stuff to post, links and the like.</p>\n\n<p>But:&nbsp; <strong>What software should we use to support a rationalist community?</strong></p>\n\n<p>The Oxford Future of Humanity Institute and the Singularity Institute have volunteered to provide funding if necessary, so we aren't limited to free software.</p>\n\n<p>And obviously we're not looking for software that lets our users throw sheep at one another.&nbsp; The Internet already offers enough ways to waste time, thank you.&nbsp; More like - how people can find each other geographically and meet up; something Reddit-like for upvoting and downvoting posts, links, and comments; better comment threading; ways to see only new comments on posts you've flagged - that sort of thing.&nbsp; You know, actually useful stuff.&nbsp; A lot of Web 2.0 seems to be designed for people with lots of time to waste, but I don't think we can assume that fact about our readership.</p>\n\n<p>Even if you don't know the name of the software, if there's a community site you visit that does an exceptional job - letting users upvote and downvote to keep the quality high, threading discussions while still giving busy people a fast way to see what new comments have been posted, making it easy for both newcomers and oldtimers - go ahead and say what we should be looking at.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Xw6pxiicjuv6NJWjf": 8, "MfpEPj6kJneT9gWT6": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "59ithK3WGEMFk7iHJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "1148", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-18T02:54:06.000Z", "modifiedAt": null, "url": null, "title": "Failure By Analogy", "slug": "failure-by-analogy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:58.960Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/C4EjbrvG3PvZzizZb/failure-by-analogy", "pageUrlRelative": "/posts/C4EjbrvG3PvZzizZb/failure-by-analogy", "linkUrl": "https://www.lesswrong.com/posts/C4EjbrvG3PvZzizZb/failure-by-analogy", "postedAtFormatted": "Tuesday, November 18th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Failure%20By%20Analogy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFailure%20By%20Analogy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC4EjbrvG3PvZzizZb%2Ffailure-by-analogy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Failure%20By%20Analogy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC4EjbrvG3PvZzizZb%2Ffailure-by-analogy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC4EjbrvG3PvZzizZb%2Ffailure-by-analogy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1898, "htmlBody": "<p><strong>Previously in series</strong>:&nbsp; <a href=\"/lw/vv/logical_or_connectionist_ai/\">Logical or Connectionist AI?</a><br /><strong>Followup to</strong>:&nbsp; <a href=\"/lw/rj/surface_analogies_and_deep_causes/\">Surface Analogies and Deep Causes</a></p>\n\n<blockquote><p>&quot;One of [the Middle Ages'] characteristics was that\n'reasoning by analogy' was rampant; another characteristic was almost\ntotal intellectual stagnation, and we now see why the two go together. \nA reason for mentioning this is to point out that, by developing a keen\near for unwarranted analogies, one can detect a lot of medieval\nthinking today.&quot;<br />&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- <a href=\"http://www.cs.utexas.edu/users/EWD/transcriptions/EWD10xx/EWD1036.html\">Edsger W. Dijkstra</a></p>\n\n<p>&lt;geoff&gt; neural nets are over-rated<br />&lt;starglider&gt; Their potential is overrated.<br />&lt;geoff&gt; their potential is us\n\n\n<br />&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- #sl4</p></blockquote><p>Wasn't it in some sense reasonable to have high hopes of neural networks?&nbsp; After all, they're <a href=\"/lw/tf/dreams_of_ai_design/\">just like the human brain</a>, which is also massively parallel, distributed, asynchronous, and -</p>\n\n<p>Hold on.&nbsp; Why not analogize to an earthworm's brain, instead of a human's?</p><a id=\"more\"></a><p>A backprop network with sigmoid units... actually doesn't much resemble biology <em>at all</em>.&nbsp; Around as much as a voodoo doll resembles its victim.&nbsp; The surface\nshape may look vaguely similar in extremely superficial aspects at a\nfirst glance.&nbsp; But the interiors and behaviors, and basically the whole\nthing apart from the surface, are nothing <em>at all</em> alike.&nbsp; All that biological neurons have in common with gradient-optimization ANNs is... the spiderwebby look.</p>\n\n<p>\n\n\n\nAnd who says that the spiderwebby look is the <em>important</em> fact about biology?&nbsp; Maybe the performance of biological brains has nothing to do with being made <em>out of neurons</em>, and everything to do with the <a href=\"/lw/l6/no_evolutions_for_corporations_or_nanodevices/\">cumulative selection pressure</a>\nput into the design.&nbsp; Just\nlike how the performance of biological brains has little to do with\nproteins being held together by van der Waals forces, instead of\nthe much stronger covalent bonds that hold together silicon.&nbsp; Sometimes\nevolution gets stuck with poor\nbuilding material, and it can't refactor because it can't execute\nsimultaneous changes to migrate the design.&nbsp; If biology does some neat\ntricks with chemistry, it's because of the greater design pressures\nexerted by natural selection, not because the building materials are so\nwonderful.</p>\n\n<p>Maybe neurons are just what brains <em>happen</em> to be made out of, because the <a href=\"/lw/kr/an_alien_god/\">blind idiot god</a> is <a href=\"/lw/kt/evolutions_are_stupid_but_work_anyway/\">too stupid</a> to sit down and invent transistors.&nbsp; All the modules get made out of neurons because that's all there <em>is,</em> even if the cognitive work would be much better-suited to a 2GHz CPU.</p>\n\n<blockquote><p>&quot;Early attempts to make flying machines often did things\nlike attaching beak onto the front, or trying to make a wing which\nwould flap like a bird's wing.&nbsp; (This extraordinary persistent idea is\nfound in Leonardo's notebooks and in a textbook on airplane design\npublished in 1911.)&nbsp; It is easy for us to smile at such naivete, but\none should realize that it made good sense at the time.&nbsp; What birds did\nwas incredible, and nobody really knew how they did it.&nbsp; It always\nseemed to involve feathers and flapping.&nbsp; Maybe the beak was critical\nfor stability...&quot;<br />&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- Hayes and Ford, &quot;Turing Test Considered Harmful&quot;</p></blockquote>\n\n<p>So... why <em>didn't</em> the flapping-wing designs work?&nbsp; Birds flap wings and they fly.&nbsp; The flying machine flaps its wings.&nbsp; <em>Why, oh why, doesn't it fly?</em></p>\n\n<p>Because... well... it just doesn't.&nbsp; This kind of analogic reasoning is not binding on Reality.</p>\n\n<p>One of the basic tests to apply to reasoning that sounds kinda-good\nis &quot;How shocked can I justifiably be if Reality comes back and says 'So\nwhat'?&quot;</p>\n\n<p>For example:&nbsp; Suppose that, after keeping track of the motions of\nthe planets for centuries, and after confirming the underlying theory\n(General Relativity) to 14 decimal places, we predict where Mercury\nwill be on July 1st, 2009.&nbsp; So we have a prediction, but that's not the\nsame thing as a fact, right?&nbsp; Anyway, we look up in the sky on July\n1st, 2009, and Reality says &quot;So what!&quot; - the planet Mercury has shifted\noutward to the same orbit as Mars.</p>\n\n<p>In a case like this, I would be highly indignant and would probably sue Reality for breach of contract.</p>\n\n<p>But suppose alternatively that, in the last twenty years, real\nestate prices have never gone down.&nbsp; You say, &quot;Real estate prices have\nnever gone down - therefore, they won't go down next year!&quot;&nbsp; And next\nyear, Reality says &quot;So what?&quot;&nbsp; It seems to me that you have no right to\nbe shocked.&nbsp; You have used an argument to which Reality can easily say\n&quot;So what?&quot;\n</p><blockquote><p>&quot;Nature is the ultimate bigot, because it is obstinately\nand intolerantly devoted to its own prejudices and absolutely refuses\nto yield to the most persuasive rationalizations of humans.&quot;<br />&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; -- J. R. Molloy</p></blockquote>\n\n<p>It's actually pretty hard to find arguments <em>so</em> persuasive that <em>even Reality</em>\nfinds them binding.&nbsp; This is why Science is more difficult - why it's\nharder to successfully predict reality - than medieval scholars once\nthought.</p>\n\n<p>One class of persuasive arguments that Reality quite often ignores\nis the Law of Similarity - that is, the argument that things which look\nsimilar ought to behave similarly.</p>\n\n<p>A medieval alchemist puts lemon glazing onto a lump of lead.&nbsp; The\nlemon glazing is yellow, and gold is yellow.&nbsp; It seems like it <em>ought</em>\nto work... but the lead obstinately refuses to turn into gold.&nbsp; Reality\njust comes back and says, &quot;So what?&nbsp; Things can be similar in some\naspects without being similar in other aspects.&quot;</p>\n\n<p>You should be <em>especially</em> suspicious when someone says, &quot;I am building X, which will do P, because it is similar to Y, which also does P.&quot;</p>\n\n<p>An abacus performs addition; and the beads of solder on a circuit\nboard bear a certain surface resemblance to the beads on an abacus. \nNonetheless, <a href=\"/lw/rj/surface_analogies_and_deep_causes/\">the circuit board does not perform addition <em>because</em> we can find a surface similarity to the abacus</a>. \nThe Law of Similarity and Contagion is not relevant.&nbsp; &nbsp;The circuit\nboard would work in just the same fashion if every abacus upon Earth\nvanished in a puff of smoke, or if the beads of an abacus looked\nnothing like solder.&nbsp; A computer chip is not powered by its <em>similarity</em> to anything else, it just <em>is.</em>&nbsp; It exists in its own right, for its own reasons.</p>\n\n<p>The Wright Brothers calculated that their plane would fly - before\nit ever flew - using reasoning that took no account whatsoever of their\naircraft's similarity to a bird.&nbsp; They did look at birds (and I have\nlooked at neuroscience) but the final calculations did not mention\nbirds (I am fairly confident in asserting).&nbsp; A working airplane does\nnot fly <em>because</em> it has wings &quot;just like a bird&quot;.&nbsp; An airplane\nflies because it is an airplane, a thing that exists in its own right;\nand it would fly just as high, no more and no less, if no bird had ever\nexisted.</p>\n\n<p>The general form of failing-by-analogy runs something like this:</p>\n\n<ul><li>You want property P.</li>\n\n<li>X has property P.</li>\n\n<li>You build Y, which has one or two surface similarities S to X.</li>\n\n<li>You argue that Y resembles X and should also P.</li>\n\n<li>Yet there is no reasoning which you can do on Y as a\nthing-in-itself to show that it will have property P, regardless of\nwhether or not X had ever existed.</li></ul>\n\n<p>Analogical reasoning of this type is a very <em>weak</em> form of understanding.&nbsp; Reality often says &quot;So what?&quot; and ignores the argument.</p>\n\n<p>The one comes to us and says:&nbsp; &quot;Calculate how many synaptic\ntransmissions per second take place in the human brain.&nbsp; This is the\ncomputing power required for human-equivalent intelligence.&nbsp; Raise\nenough venture capital to buy a supercomputer which performs the same\nnumber of floating-point operations per second.&nbsp; Intelligence is bound\nto emerge from a machine that powerful.&quot;</p>\n\n<p>So you reply:&nbsp; &quot;I'm sorry, I've never seen a human brain and I don't know anything about them.&nbsp; So, <em>without</em>\ntalking about a human brain, can you explain how you calculated that\n10^17 floating-point operations per second is the exact amount\nnecessary and sufficient to yield human-equivalent intelligence?&quot;\n</p>\n\n<p>And the one says:&nbsp; &quot;...&quot;\n\n</p>\n\n<p>You ask:&nbsp; &quot;Say, what is this property of 'human-equivalent\nintelligence' which you expect to get?&nbsp; Can you explain it to me\nwithout pointing to a human?&quot;\n</p>\n\n<p>And the one says:&nbsp; &quot;...&quot;\n\n</p>\n\n<p>You ask:&nbsp; &quot;What makes you think that large amounts of computing\npower have something to do with 'intelligence', anyway?&nbsp; Can you answer\nwithout pointing to the example of the human brain?&nbsp; Pretend that I've\nnever seen an 'intelligence' and that I have no reason as yet to\nbelieve any such thing can exist.&quot;\n</p>\n\n<p>But you get the idea.</p>\n\n<p>Now imagine that you go to the Wright Brothers and say:&nbsp; &quot;I've never\nseen a bird.&nbsp; Why does your aircraft have 'wings'?&nbsp; And what is it you\nmean by 'flying'?&quot;</p>\n\n<p>And the Wright Brothers respond:&nbsp; &quot;Well, by flying, we mean that\nthis big heavy object is going to rise off the ground and move through\nthe air without being supported.&nbsp; Once the plane is moving forward, <a href=\"http://jef.raskincenter.org/published/coanda_effect.html\">the\nwings accelerate air downward, which generates lift that keeps the\nplane aloft</a>.&quot;</p>\n\n<p>If two processes have forms that are nearly <em>identical</em>,\nincluding internal structure that is similar to as many decimal places\nas you care to reason about, then you may be able to almost-prove\nresults from one to the other.&nbsp; But if there is even one difference in\nthe internal structure, then any number of other similarities may be\nrendered void.&nbsp; Two deterministic computations with identical data and\nidentical rules <em>will</em> yield identical outputs.&nbsp; But if a single input\nbit is flipped from zero to one, the outputs are no longer required to\nhave <em>anything</em> in common.&nbsp; The strength of analogical reasoning can be destroyed by a single perturbation.\n\n</p>\n\n<p>Yes, <em>sometimes</em> analogy works.&nbsp; But the more complex and dissimilar the\nobjects are, the less likely it is to work.&nbsp; The narrower the\nconditions required for success, the less likely it is to work.&nbsp; The\nmore complex the machinery doing the job, the less likely it is to\nwork.&nbsp; The more shallow your understanding of the object of the\nanalogy, the more you are looking at its surface characteristics rather\nthan its deep mechanisms, the less likely analogy is to work.\n</p>\n\n<p>Analogy might work for something on the order of:&nbsp; &quot;I crossed this\nriver using a fallen log last time, so if I push another log across it,\nI might be able to cross it again.&quot;&nbsp; It doesn't work for creating\nobjects of the order of complexity of, say, a toaster oven.&nbsp; (And hunter-gatherer bands face many rivers to cross, but not many\ntoaster ovens to rewire.)\n</p>\n<p>Admittedly, analogy often works in mathematics - much <em>better</em> than it\ndoes in science, in fact.&nbsp; In mathematics you can go back and <em>prove</em>\nthe idea which analogy originally suggested.&nbsp; In mathematics, you get\nquick feedback about which analogies worked and which analogies didn't,\nand soon you pick up the pattern.&nbsp; And in mathematics you can always\nsee the entire insides of things; you are not stuck examining the\nsurface of an opaque mystery.&nbsp; Mathematical proposition A may be\nanalogous to mathematical proposition B, which suggests the method; but\nafterward you can go back and prove A in its own right, regardless of\nwhether or not B is true.&nbsp; In some cases you may need proposition B as\na lemma, but certainly not all cases.\n</p>\n\n\n\n<p>Which is to say: despite the <em>misleading surface similarity</em>, the &quot;analogies&quot;\nwhich mathematicians use are not analogous to the &quot;analogies&quot; of alchemists, and you cannot reason from the success of one to the\nsuccess of the other. </p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "M9oWHR2XGLmg2DaZp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "C4EjbrvG3PvZzizZb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 20, "extendedScore": null, "score": 3e-05, "legacy": true, "legacyId": "1149", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["juomoqiNzeAuq4JMm", "6ByPxcGDhmx74gPSm", "p7ftQ6acRkgo6hqHb", "XC7Kry5q6CD9TyG4K", "pLRogvJLPPg6Mrvg4", "jAToJHtg39AMTAuJo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-18T07:14:55.000Z", "modifiedAt": null, "url": null, "title": "Failure By Affective Analogy", "slug": "failure-by-affective-analogy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:02.092Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o8Ju8pTGvAfkkg6xZ/failure-by-affective-analogy", "pageUrlRelative": "/posts/o8Ju8pTGvAfkkg6xZ/failure-by-affective-analogy", "linkUrl": "https://www.lesswrong.com/posts/o8Ju8pTGvAfkkg6xZ/failure-by-affective-analogy", "postedAtFormatted": "Tuesday, November 18th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Failure%20By%20Affective%20Analogy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFailure%20By%20Affective%20Analogy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8Ju8pTGvAfkkg6xZ%2Ffailure-by-affective-analogy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Failure%20By%20Affective%20Analogy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8Ju8pTGvAfkkg6xZ%2Ffailure-by-affective-analogy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8Ju8pTGvAfkkg6xZ%2Ffailure-by-affective-analogy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1088, "htmlBody": "<p><strong>Previously in series</strong>:&nbsp; <a href=\"/lw/vx/failure_by_analogy/\">Failure By Analogy</a></p>\n<p>Alchemy is a way of thinking that humans do not <em>instinctively</em> spot as stupid.&nbsp; Otherwise alchemy would never have been popular, even in medieval days.&nbsp; Turning lead into gold by mixing it with things that seemed similar to gold, sounded <em>every bit as reasonable</em>, back in the day, as trying to build a flying machine with flapping wings.&nbsp; (And yes, it was worth trying <em>once,</em> but you should <em>notice</em> if Reality keeps saying \"So what?\")</p>\n<p>And the final and most dangerous form of <a href=\"/lw/vx/failure_by_analogy/\">failure by analogy</a> is to say <em>a lot of nice things</em> about X, which is similar to Y, so we should expect nice things of Y. You may also say horrible things about Z, which is the polar opposite of Y, so if Z is bad, Y should be good.</p>\n<p>Call this \"failure by affective analogy\".</p>\n<p><a id=\"more\"></a></p>\n<p>Failure by affective analogy is when you don't just say, \"This lemon glazing is yellow, gold is yellow, QED.\"&nbsp; But rather say:</p>\n<p>\"And now we shall add delicious lemon glazing to the formula for the Philosopher's Stone the root of all wisdom, since lemon glazing is beautifully yellow, like gold is beautifully yellow, and also lemon glazing is delightful on the tongue, indicating that it is possessed of a superior potency that delights the senses, just as the beauty of gold delights the senses...\"</p>\n<p>That's why you find people saying things like, \"Neural networks are decentralized, <em>just like democracies</em>\" or \"Neural networks are emergent, <em>just like capitalism</em>\".</p>\n<p>A summary of the Standard Prepackaged Revolutionary New AI Paradigm might look like the following - and when reading, ask yourself how many of these ideas are <em>affectively laden:</em></p>\n<ul>\n<li>The Dark Side is Top-Down.&nbsp; The Light Side is Bottom-Up. </li>\n<li>The Dark Side is Centralized.&nbsp; The Light Side is Distributed. </li>\n<li>The Dark Side is Logical.&nbsp; The Light Side is Fuzzy. </li>\n<li>The Dark Side is Serial.&nbsp; The Light Side is Parallel. </li>\n<li>The Dark Side is Rational.&nbsp; The Light Side is Intuitive. </li>\n<li>The Dark Side is Deterministic.&nbsp; The Light Side is Stochastic. </li>\n<li>The Dark Side tries to Prove things.&nbsp; The Light Side tries to Test them. </li>\n<li>The Dark Side is Hierarchical.&nbsp; The Light Side is Heterarchical. </li>\n<li>The Dark Side is Clean.&nbsp; The Light Side is Noisy. </li>\n<li>The Dark Side operates in Closed Worlds.&nbsp; The Light Side operates in Open Worlds. </li>\n<li>The Dark Side is Rigid.&nbsp; The Light Side is Flexible. </li>\n<li>The Dark Side is Sequential.&nbsp; The Light Side is Realtime. </li>\n<li>The Dark Side demands Control and Uniformity.&nbsp; The Light Side champions Freedom and Individuality. </li>\n<li>The Dark Side is Lawful.&nbsp; The Light Side, on the other hand, is <a href=\"/lw/vm/lawful_creativity/\"><em>Creative</em></a>.</li>\n</ul>\n<p>By means of this tremendous package deal fallacy, lots of good feelings are generated about the New Idea (even if it's thirty years old).&nbsp; Enough nice words may even manage to start an <a href=\"/lw/lm/affective_death_spirals/\">affective death spiral</a>.&nbsp; Until finally, via the standard channels of <a href=\"/lw/lg/the_affect_heuristic/\">affect heuristic</a> and <a href=\"/lw/lj/the_halo_effect/\">halo effect</a>, it seems that the New Idea will <em>surely</em> be able to accomplish some extremely difficult end -</p>\n<p>- like, say, <span style=\"font-style: italic;\">true</span><em> general intelligence</em> -</p>\n<p>- even if you can't quite give a walkthrough of the internal mechanisms which are going to produce that output.</p>\n<p>(Why yes, I <em>have</em> seen <a href=\"/lw/uc/aboveaverage_ai_scientists/\">AGIfolk</a> trying to pull this on Friendly AI - as they explain how all we need to do is stamp the AI with the properties of Democracy and Love and Joy and Apple Pie and paint an American Flag on the case, and <em>surely</em> it will be Friendly as well - though they can't quite walk through internal cognitive mechanisms.)</p>\n<p>From such reasoning as this (and <a href=\"/lw/tf/dreams_of_ai_design/\">this</a>), came the string of false promises that were published in the newspapers (and led futurists who grew up during that era to <a href=\"/lw/qu/a_premature_word_on_ai/\">be very disappointed in AI, leading them to feel negative affect</a> that now causes them to put AI a hundred years in the future).</p>\n<p>Let's say it again:&nbsp; <a href=\"/lw/lw/reversed_stupidity_is_not_intelligence/\">Reversed stupidity is not intelligence</a> - if people are making bad predictions you should just discard them, not reason from their failure.</p>\n<p>But there is a certain lesson to be learned.&nbsp; A bounded rationalist cannot do all things, but the true Way should not <em>overpromise</em> - it should not (systematically/regularly/on average) hold out the prospect of success, and then deliver failure.&nbsp; Even a bounded rationalist can aspire to be <em>well calibrated,</em> to not assign 90% probability unless they really <em>do</em> have good enough information to be right nine times out of ten.&nbsp; If you only have good enough information to be right 6 times out of 10, just say 60% instead.&nbsp; A bounded rationalist cannot do all things, but the true Way does not overpromise.</p>\n<p>If you want to avoid failed promises of AI... then history suggests, I think, that you should not expect good things out of your AI system unless you have a good idea of how <em>specifically</em> it is going to happen.&nbsp; I don't mean <a href=\"/lw/vg/building_something_smarter/\">writing out the exact internal program state in advance</a>.&nbsp; But I also don't mean saying that the refrigeration unit will <a href=\"/lw/tf/dreams_of_ai_design/\">cool down the AI and make it more contemplative</a>.&nbsp; For myself, I seek to know the laws governing the AI's <a href=\"/lw/vo/lawful_uncertainty/\">lawful uncertainty</a> and <a href=\"/lw/vm/lawful_creativity/\">lawful creativity</a> - though I don't expect to know the full content of its future knowledge, or the exact design of its future inventions.</p>\n<p>Don't want to be disappointed?&nbsp; Don't hope!</p>\n<p>Don't ask yourself if you're <em>allowed to believe</em> that your AI design will work.</p>\n<p>Don't guess.&nbsp; Know.</p>\n<p>For this much I do know - if I don't <em>know</em> that my AI design will work, it won't.</p>\n<p>There are various obvious caveats that need to be attached here, and various obvious stupid interpretations of this principle not to make.&nbsp; You can't be sure a search will return successfully before you have run it -</p>\n<p>- but you should understand on a gut level:&nbsp; If you are <em>hoping</em> that your AI design will work, it will fail.&nbsp; If you <em>know</em> that your AI design will work, then it <em>might</em> work.</p>\n<p>And on the Friendliness part of that you should hold yourself to an even higher standard - ask yourself if you are <em>forced</em> to believe the AI will be Friendly - because in that aspect, above all, you must constrain Reality so tightly that even Reality is not allowed to say, \"So what?\"&nbsp; This is a very tough test, but if you do not apply it, you will just find yourself trying to paint a flag on the case, and hoping.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "Kj9q8FXoauL7mQDWt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o8Ju8pTGvAfkkg6xZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 15, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "1150", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["C4EjbrvG3PvZzizZb", "KKLQp934n77cfZpPn", "XrzQW69HpidzvBxGr", "Kow8xRzpfkoY7pa69", "ACGeaAk6KButv2xwQ", "9HGR5qatMGoz4GhKj", "p7ftQ6acRkgo6hqHb", "iD5baT42zYAkWJPMB", "qNZM3EGoE5ZeMdCRt", "GpvvQzf3pPyPsBepr", "msJA6B9ZjiiZxT6EZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-18T18:37:33.000Z", "modifiedAt": null, "url": null, "title": "The Weak Inside View", "slug": "the-weak-inside-view", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:33.370Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w9KWNWFTXivjJ7rjF/the-weak-inside-view", "pageUrlRelative": "/posts/w9KWNWFTXivjJ7rjF/the-weak-inside-view", "linkUrl": "https://www.lesswrong.com/posts/w9KWNWFTXivjJ7rjF/the-weak-inside-view", "postedAtFormatted": "Tuesday, November 18th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Weak%20Inside%20View&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Weak%20Inside%20View%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw9KWNWFTXivjJ7rjF%2Fthe-weak-inside-view%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Weak%20Inside%20View%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw9KWNWFTXivjJ7rjF%2Fthe-weak-inside-view", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw9KWNWFTXivjJ7rjF%2Fthe-weak-inside-view", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1501, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; &nbsp;<a href=\"/lw/ri/the_outside_views_domain/\">The Outside View's Domain</a></p>\n\n<p>When I met Robin in Oxford for a recent conference, we had a preliminary discussion on the Singularity - this is where Robin suggested using <a href=\"/lw/vd/intelligence_in_economics/\">production functions</a>.&nbsp; And at one point Robin said something like, &quot;Well, let's see whether your theory's predictions fit previously observed growth rate curves,&quot; which surprised me, because I'd never thought of that at all.</p>\n\n<p>It had never occurred to me that my view of optimization ought to produce quantitative predictions.&nbsp; It seemed like something only an economist would try to do, as 'twere.&nbsp; (In case it's not clear, sentence 1 is self-deprecating and sentence 2 is a compliment to Robin.&nbsp; --EY)</p>\n\n<p>Looking back, it's not that I made a choice to deal only in qualitative predictions, but that it didn't really occur to me to do it any other way.</p>\n\n<p>Perhaps I'm prejudiced against the Kurzweilian crowd, and their Laws of Accelerating Change and the like.&nbsp; Way back in the distant beginning that feels like a different person, I went around talking about Moore's Law and the extrapolated arrival time of &quot;human-equivalent hardware&quot; a la Moravec.&nbsp; But at some point I figured out that if you weren't exactly reproducing the brain's algorithms, porting cognition to fast serial hardware and to human design instead of evolved adaptation would toss the numbers out the window - and that how much hardware you needed depended on how smart you were - and that sort of thing.</p>\n\n<p>Betrayed, I decided that the whole Moore's Law thing was silly and a corruption of futurism, and I restrained myself to qualitative predictions (and retrodictions) thenceforth.</p><a id=\"more\"></a><p>Though this is to some extent <a href=\"/lw/js/the_bottom_line/\">an argument produced after the conclusion</a>, I would explain my reluctance to venture into <em>quantitative</em> futurism, via the following trichotomy:</p>\n\n<ul><li>On problems whose pieces are individually <em>precisely</em> predictable, you can use the Strong Inside View to calculate a final outcome that has never been seen before - plot the trajectory of the first moon rocket before it is ever launched, or verify a computer chip before it is ever manufactured.</li>\n\n<li>On problems that are drawn from a barrel of causally similar problems, where human optimism runs rampant and unforeseen troubles are common, the <a href=\"/lw/jg/planning_fallacy/\">Outside View beats the Inside View</a>.&nbsp; Trying to visualize the course of history piece by piece, will turn out to not (for humans) work so well, and you'll be better off assuming a probable distribution of results similar to previous historical occasions - without trying to adjust for all the reasons why <em>this</em> time will be different and better.</li>\n\n<li>But on problems that are new things under the Sun, where there's a huge change of context and a structural change in underlying causal forces, the <a href=\"/lw/ri/the_outside_views_domain/\">Outside View also fails</a> - try to use it, and you'll just get into arguments about what is the proper domain of &quot;similar historical cases&quot; or what conclusions can be drawn therefrom.&nbsp; In this case, the best we can do is use the Weak Inside View - visualizing the causal process - to produce <em>loose qualitative conclusions about only those issues where there seems to be lopsided support.</em></li></ul>\n\n<p>So to me it seems &quot;obvious&quot; that my view of optimization is only strong enough to produce loose qualitative conclusions, and that it can only be matched to its retrodiction of history, or wielded to produce future predictions, on the level of <a href=\"/lw/ti/qualitative_strategies_of_friendliness/\">qualitative physics</a>.</p>\n\n<p>&quot;Things should speed up here&quot;, I could maybe say.&nbsp; But not &quot;The doubling time of this exponential should be cut in half.&quot;</p>\n\n<p>I aspire to a deeper understanding of <em>intelligence</em> than this, mind you.&nbsp; But I'm not sure that even perfect Bayesian enlightenment, would let me predict <em>quantitatively</em> how long it will take an AI to solve various problems in advance of it solving them.&nbsp; That might just rest on features of an unexplored solution space which I can't guess in advance, even though I understand the process that searches.</p>\n\n<p>Robin keeps asking me what I'm getting at by talking about some reasoning as &quot;deep&quot; while other reasoning is supposed to be &quot;surface&quot;.&nbsp; One thing which makes me worry that something is &quot;surface&quot;, is when it involves generalizing a level N feature across a shift in level N-1 causes.</p>\n\n<p>For example, suppose you say &quot;Moore's Law has held for the last sixty years, so it will hold for the next sixty years, even after the advent of superintelligence&quot; (as Kurzweil seems to believe, since he draws his graphs well past the point where you're buying &quot;a billion times human brainpower for $1000&quot;).</p>\n\n<p>Now, if the Law of Accelerating Change were an exogenous, ontologically fundamental, precise physical law, then you wouldn't expect it to change with the advent of superintelligence.</p>\n\n<p>But to the extent that you believe Moore's Law depends on human engineers, and that the timescale of Moore's Law has something to do with the timescale on which human engineers think, then extrapolating Moore's Law across the advent of superintelligence is extrapolating it across a shift in the previous causal generator of Moore's Law.</p>\n\n<p>So I'm worried when I see generalizations extrapolated <em>across</em> a change in causal generators not themselves described - i.e. the generalization itself is on a level of the outputs of those generators and doesn't describe the generators directly.</p>\n\n<p>If, on the other hand, you extrapolate Moore's Law out to 2015 because it's been reasonably steady up until 2008 - well, Reality is still allowed to say &quot;So what?&quot;, to a greater extent than we can expect to wake up one morning and find Mercury in Mars's orbit.&nbsp; But I wouldn't bet against you, if you just went ahead and drew the graph.</p>\n\n<p>So what's &quot;surface&quot; or &quot;deep&quot; depends on what kind of context shifts you try to extrapolate past.</p>\n\n<p>Robin Hanson <a href=\"http://www.overcomingbias.com/2008/06/singularity-out.html\">said</a>:</p><blockquote><p>Taking a long historical long view, <a href=\"http://www.overcomingbias.com/2008/06/economics-of-si.html\">we see</a> steady total growth rates punctuated by rare transitions when new faster growth modes appeared with little warning.&nbsp; We know of perhaps four such &quot;singularities&quot;: animal brains (~600MYA), humans (~2MYA), farming (~1OKYA), and industry (~0.2KYA).&nbsp; The statistics of previous transitions suggest we are perhaps overdue for another one, and would be substantially overdue in a century.&nbsp; The next transition would change the growth rate rather than capabilities directly, would take a few years at most, and the new doubling time would be a week to a month.</p></blockquote><p>Why do these transitions occur?&nbsp; Why have they been similar to each other?&nbsp; Are the same causes still operating?&nbsp; Can we expect the next transition to be similar for the same reasons?</p>\n\n<p>One may of course say, &quot;I don't know, I just look at the data, extrapolate the line, and venture this guess - the data is more sure than any hypotheses about causes.&quot;&nbsp; And that will be an interesting projection to make, at least.</p>\n\n<p>But you shouldn't be surprised at all if Reality says &quot;So what?&quot;&nbsp; I mean - real estate prices went up for a long time, and then they went down.&nbsp; And that didn't even require a tremendous shift in the underlying nature and causal mechanisms of real estate.</p>\n\n<p>To stick my neck out further:&nbsp; I am <em>liable to trust the Weak Inside View over a &quot;surface&quot; extrapolation,</em> if the Weak Inside View drills down to a deeper causal level and the balance of support is sufficiently lopsided.</p>\n\n<p>I will go ahead and say, &quot;I don't care if you say that Moore's Law has held for the last <em>hundred</em> years.&nbsp; Human thought was a primary causal force in producing Moore's Law, and your statistics are all over a domain of human neurons running at the same speed.&nbsp; If you substitute better-designed minds running at a million times human clock speed, the rate of progress ought to speed up - <em>qualitatively</em> speaking.&quot;</p>\n\n<p>That is, the prediction is without giving precise numbers, or supposing that it's still an exponential curve; computation might spike to the limits of physics and then stop forever, etc.&nbsp; But I'll go ahead and say that the rate of technological progress ought to <em>speed up,</em> given the said counterfactual intervention on underlying causes to increase the thought speed of engineers by a factor of a million.&nbsp; I'll be downright indignant if Reality says &quot;So what?&quot; and has the superintelligence make <em>slower</em> progress than human engineers instead.&nbsp; It really does seem like an argument so strong that even Reality ought to be persuaded.</p>\n\n<p>It would be interesting to ponder what kind of historical track records have prevailed in such a clash of predictions - trying to extrapolate &quot;surface&quot; features across shifts in underlying causes without speculating about those underlying causes, versus trying to use the Weak Inside View on those causes and arguing that there is &quot;lopsided&quot; support for a qualitative conclusion; in a case where the two came into conflict...</p>\n\n<p>...kinda hard to think of what that historical case would be, but perhaps I only lack history.</p>\n\n<p>Robin, how surprised would you be if your sequence of long-term exponentials just... didn't continue?&nbsp; If the next exponential was too fast, or too slow, or something other than an exponential?&nbsp; To what degree would you be indignant, if Reality said &quot;So what?&quot;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"rWzGNdjuep56W5u2d": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w9KWNWFTXivjJ7rjF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 28, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "1151", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pqoxE3AGMbse68dvb", "4uDfpNTrdhEYEb2jm", "34XxbRFe54FycoCDw", "CPm5LTwHrvBJCa9h5", "AWaJvBMb9HGBwtNqd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-19T15:00:00.000Z", "modifiedAt": null, "url": null, "title": "The First World Takeover", "slug": "the-first-world-takeover", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:34.842Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/spKYZgoh3RmhxMqyu/the-first-world-takeover", "pageUrlRelative": "/posts/spKYZgoh3RmhxMqyu/the-first-world-takeover", "linkUrl": "https://www.lesswrong.com/posts/spKYZgoh3RmhxMqyu/the-first-world-takeover", "postedAtFormatted": "Wednesday, November 19th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20First%20World%20Takeover&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20First%20World%20Takeover%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FspKYZgoh3RmhxMqyu%2Fthe-first-world-takeover%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20First%20World%20Takeover%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FspKYZgoh3RmhxMqyu%2Fthe-first-world-takeover", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FspKYZgoh3RmhxMqyu%2Fthe-first-world-takeover", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1950, "htmlBody": "<p>Before Robin and I move on to talking about the Future, it seems to me wise to check if we have disagreements in our view of the Past. Which might be much easier to discuss - and maybe even resolve...&nbsp; So...</p>\n\n<p>In the beginning was the Bang.&nbsp; For nine billion years afterward, nothing much happened.</p>\n\n<p>Stars formed, and burned for long periods or short periods depending on their structure; but &quot;successful&quot; stars that burned longer or brighter did not pass on their characteristics to other stars.&nbsp; The first replicators were yet to come.</p>\n\n<p>It was the Day of the Stable Things, when your probability of seeing something was given by its probability of accidental formation times its duration.&nbsp; Stars last a long time; there are many helium atoms.</p>\n\n<p>It was the Era of Accidents, before the dawn of optimization.&nbsp; You'd only expect to see something with <a href=\"/lw/va/measuring_optimization_power/\">40 bits of optimization</a> if you looked through a trillion samples.&nbsp; Something with 1000 bits' worth of functional complexity?&nbsp; You wouldn't expect to find that in the whole universe. </p>\n\n<p>I would guess that, if you were going to be stuck on a desert island and you wanted to stay entertained as long as possible, then you should sooner choose to examine the complexity of the cells and biochemistry of a single Earthly butterfly, over all the stars and astrophysics in the visible universe beyond Earth.</p>\n\n<p>It was the Age of Boredom.</p><a id=\"more\"></a><p>The hallmark of the Age of Boredom was not lack of natural resources - it wasn't that the universe was low on hydrogen - but, rather, the lack of any <em>cumulative </em>search.&nbsp; If one star burned longer or brighter, that didn't affect the probability distribution of the next star to form.&nbsp; There was no search but blind search.&nbsp; Everything from scratch, not even looking at the <a href=\"/lw/vp/worse_than_random/\">neighbors of previously successful points</a>. Not hill-climbing, not mutation and selection, not even discarding patterns already failed.&nbsp; Just a random sample from the same distribution, over and over again.</p>\n\n<p>The Age of Boredom ended with the first replicator.</p>\n\n<p>(Or the first replicator to catch on, if there were failed alternatives lost to history - but this seems unlikely, given the Fermi Paradox; a replicator should be more improbable than that, or the stars would teem with life already.)</p>\n\n<p>Though it might be most dramatic to think of a single RNA strand a few dozen bases long, forming by pure accident after who-knows-how-many chances on who-knows-how-many planets, another class of hypotheses deals with catalytic hypercycles - chemicals whose presence makes it more likely for other chemicals to form, with the arrows happening to finally go around in a circle.&nbsp; If so, RNA would just be a crystallization of that hypercycle into a single chemical that can both take on enzymatic shapes, and store information in its sequence for easy replication.</p>\n\n<p>The catalytic hypercycle is worth pondering, since it reminds us that the universe wasn't quite drawing its random patterns from the <em>same</em> distribution every time - the formation of a longlived star made it more likely for a planet to form (if not another star to form), the formation of a planet made it more likely for amino acids and RNA bases to form in a pool of muck somewhere (if not more likely for planets to form).</p>\n\n<p>In this flow of probability, patterns in one attractor leading to other attractors becoming stronger, there was finally born a <em>cycle </em>- perhaps a single strand of RNA, perhaps a crystal in clay, perhaps a catalytic hypercycle - and that was the dawn.</p>\n\n<p>What makes this cycle significant?&nbsp; Is it the large amount of <em>material</em> that the catalytic hypercycle or replicating RNA strand could absorb into its pattern?</p>\n\n<p>Well, but any given mountain on Primordial Earth would probably weigh vastly more than the total mass devoted to copies of the first replicator.&nbsp; What effect does mere mass have on optimization?</p>\n\n<p>Suppose the first replicator had a probability of formation of 10<sup>-30</sup>. If that first replicator managed to make 10,000,000,000 copies of itself (I don't know if this would be an overestimate or underestimate for a tidal pool) then this would increase your probability of encountering the replicator-pattern by a factor of 10<sup>10</sup>, the total probability going up to 10<sup>-20</sup>. (If you were observing &quot;things&quot; at random, that is, and not just on Earth but on all the planets with tidal pools.)&nbsp; So that was a kind of optimization-directed probability flow.</p>\n\n<p>But vastly more important, in the scheme of things, was this - that the first replicator made copies of itself, and some of those copies were errors.</p>\n\n<p>That is, <em>it explored the neighboring regions of the search space</em> - some of which contained better replicators - and then those replicators ended up with more probability flowing into them, which explored <em>their</em> neighborhoods.</p>\n\n<p>Even in the Age of Boredom there were always regions of attractor-space that were the gateways to other regions of attractor-space.&nbsp; Stars begot planets, planets begot tidal pools.&nbsp; But that's not the same as a replicator begetting a replicator - it doesn't search a <em>neighborhood</em>, find something that better matches a criterion (in this case, the criterion of effective replication) and then search <em>that</em> neighborhood, over and over.</p>\n\n<p>This did require a certain amount of raw material to act as replicator feedstock.&nbsp; But the significant thing was not how much material was recruited into the world of replication; the significant thing was the search, and the material just carried out that search. If, somehow, there'd been some way of doing the same search without all that raw material - if there'd just been a little beeping device that determined how well a pattern <em>would</em> replicate, and incremented a binary number representing &quot;how much attention&quot; to pay to that pattern, and then searched neighboring points in proportion to that number - well, that would have searched just the same.&nbsp; It's not something that evolution <em>can</em> do, but if it happened, it would generate the same information.</p>\n\n<p>Human brains routinely outthink the evolution of whole species, species whose net weights of biological material outweigh a human brain a million times over - the gun against a lion's paws.&nbsp; It's not the amount of raw material, it's the search.</p>\n\n<p>In the evolution of replicators, the raw material happens to <em>carry out</em> the search - but don't think that the key thing is how much gets produced, how much gets consumed.&nbsp; The raw material is just a way of keeping score.&nbsp; True, even in principle, you do need <em>some</em> negentropy and <em>some</em> matter to <em>perform the computation.</em> But the same search could theoretically be performed with much less material - examining fewer copies of a pattern, to draw the same conclusions, using more efficient updating on the evidence. Replicators <em>happen</em> to use the number of copies produced of themselves, as a way of keeping score.</p>\n\n<p>But what really matters isn't the production, it's the search.</p>\n\n<p>If, after the first primitive replicators had managed to produce a few tons of themselves, you deleted all those tons of biological material, and substituted a few dozen cells here and there from the future - a single algae, a single bacterium - to say nothing of a whole multicellular <em>C. elegans</em> earthworm with a 302-neuron <em>brain</em> - then Time would leap forward by billions of years, even if the total mass of Life had just apparently shrunk.&nbsp; The <em>search</em> would have leapt ahead, and <em>production</em> would recover from the apparent &quot;setback&quot; in a handful of easy doublings.</p>\n\n<p>The first replicator was the first great break in History - the first Black Swan that would have been unimaginable by any surface analogy.&nbsp; No extrapolation of previous trends could have spotted it - you'd have had to dive down into causal modeling, in enough detail to visualize the unprecedented search.</p>\n\n<p>Not that I'm saying I <em>would</em> have guessed, without benefit of hindsight - if somehow I'd been there as a disembodied and unreflective spirit, knowing only the previous universe as my guide - having no highfalutin' concepts of &quot;intelligence&quot; or &quot;natural selection&quot; because those things didn't exist in my environment, and I had no mental mirror in which to see <em>myself</em> - and indeed, who <em>should</em> have guessed it with short of godlike intelligence?&nbsp; When all the previous history of the universe contained no break in History that sharp?&nbsp; The replicator was the <em>first</em> Black Swan.</p>\n\n<p>Maybe I, seeing the first replicator as a disembodied unreflective spirit, would have said, &quot;Wow, what an amazing notion - some of the things I see won't form with high probability, or last for long times - they'll be things that are good at copying themselves, instead.&nbsp; It's the new, third reason for seeing a lot of something!&quot;&nbsp; But would I have been imaginative enough to see the way to amoebas, to birds, to humans?&nbsp; Or would I have just expected it to hit the walls of the tidal pool and stop?</p>\n\n<p>Try telling a disembodied spirit who had watched the whole history of the universe <em>up to that point</em> about the birds and the bees, and they would think you were <em>absolutely and entirely out to lunch.</em>&nbsp; For nothing <em>remotely like that</em> would have been found anywhere else in the universe - and it would obviously take an exponential and <em>ridiculous</em> amount of time to accidentally form a pattern like that, no matter how good it was at replicating itself once formed - and as for it happening many times over in a connected ecology, when the first replicator in the tidal pool took such a long time to happen - why, that would just be <em>madness.</em>&nbsp; The <a href=\"/lw/j6/why_is_the_future_so_absurd/\">Absurdity Heuristic</a> would come into play.&nbsp; Okay, it's neat that a little molecule can replicate itself - but this notion of a &quot;squirrel&quot; is <em>insanity.</em>&nbsp; So far beyond a Black Swan that you can't even call it a swan anymore.</p>\n\n<p>That first replicator took over the world - in what sense?&nbsp; Earth's crust, Earth's magma, far outweighs its mass of Life.&nbsp; But Robin and I both suspect, I think, that the fate of the universe, and all those distant stars that outweigh us, will end up shaped by Life.&nbsp; So that the universe ends up hanging quite heavily on the existence of that first replicator, and <em>not</em> on the counterfactual states of any particular other molecules nearby...&nbsp; In that sense, a small handful of atoms once seized the reins of Destiny.</p>\n\n<p>How?&nbsp; How did the first replicating pattern take over the world? Why didn't all those other molecules get an equal vote in the process?</p>\n\n<p>Well, that initial replicating pattern was doing <em>some</em> kind of search - <em>some</em> kind of optimization - and nothing else in the Universe was even <em>trying.</em> Really it was evolution that took over the world, not the first replicating pattern per se - you don't see many copies of it around any more.&nbsp; But still, once upon a time the thread of Destiny was seized and concentrated and spun out from a small handful of atoms.</p>\n\n<p>The first replicator did not set in motion a <em>clever</em> optimization process.&nbsp; Life didn't even have sex yet, or DNA to store information at very high fidelity.&nbsp; But the rest of the Universe had zip.&nbsp; In the kingdom of blind chance, the myopic optimization process is king.</p>\n\n<p>Issues of &quot;sharing improvements&quot; or &quot;trading improvements&quot; wouldn't even arise - there were no partners from outside.&nbsp; All the agents, all the actors of our modern world, are descended from that first replicator, and none from the mountains and hills.</p>\n\n<p>And that was the story of the First World Takeover, when a shift in the <em>structure</em> of optimization - namely, moving from no optimization whatsoever, to natural selection - produced a stark discontinuity with previous trends; and squeezed the flow of the whole universe's destiny through the needle's eye of a single place and time and pattern.</p>\n\n<p>That's Life.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nvKzwpiranwy29HFJ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "spKYZgoh3RmhxMqyu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 30, "extendedScore": null, "score": 4.5e-05, "legacy": true, "legacyId": "1152", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Q4hLMDrFd8fbteeZ8", "GYuKqAL95eaWTDje5", "Ga2HSwf9iQe64JwAa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-20T01:07:46.000Z", "modifiedAt": null, "url": null, "title": "Whence Your Abstractions?", "slug": "whence-your-abstractions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:38.785Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YyYwRxajPkAyajKXx/whence-your-abstractions", "pageUrlRelative": "/posts/YyYwRxajPkAyajKXx/whence-your-abstractions", "linkUrl": "https://www.lesswrong.com/posts/YyYwRxajPkAyajKXx/whence-your-abstractions", "postedAtFormatted": "Thursday, November 20th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Whence%20Your%20Abstractions%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhence%20Your%20Abstractions%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYyYwRxajPkAyajKXx%2Fwhence-your-abstractions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Whence%20Your%20Abstractions%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYyYwRxajPkAyajKXx%2Fwhence-your-abstractions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYyYwRxajPkAyajKXx%2Fwhence-your-abstractions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 906, "htmlBody": "<p><strong>Reply to</strong>:&nbsp; <a href=\"http://www.overcomingbias.com/2008/11/abstraction-vs.html\">Abstraction, Not Analogy</a></p>\n\n<p>Robin <a href=\"http://www.overcomingbias.com/2008/11/abstraction-vs.html#comment-139753978\">asks</a>:</p><blockquote><p><span id=\"comment-139753978-content\">Eliezer, have I completely failed\nto communicate here?&nbsp; You have previously said nothing is similar enough\nto this new event for analogy to be useful, so all we have is &quot;causal\nmodeling&quot; (though you haven't explained what you mean by this in this\ncontext).&nbsp; This post is a reply saying, no, there are more ways using\nabstractions; analogy and causal modeling are two particular ways to\nreason via abstractions, but there are many other ways.</span></p></blockquote><p>Well... it shouldn't be surprising if <a href=\"/lw/ke/illusion_of_transparency_why_no_one_understands/\">you've communicated less than you thought</a>.&nbsp; Two people, both of whom know that disagreement is not allowed, have a persistent disagreement.&nbsp; It doesn't excuse anything, but - wouldn't it be <em>more</em> surprising if their disagreement rested on intuitions that were <em>easy</em> to convey in words, and points readily dragged into the light?</p>\n\n<p>I didn't think from the beginning that I was succeeding in communicating.&nbsp; Analogizing Doug Engelbart's mouse to a self-improving AI is for me such a flabbergasting notion - indicating such completely different ways of thinking about the problem - that I am trying to step back and find the differing sources of our differing intuitions.</p>\n\n<p>(Is that such an odd thing to do, if we're really following down the path of not agreeing to disagree?)</p>\n\n<p>&quot;Abstraction&quot;, for me, is a word that means a partitioning of possibility - a <a href=\"/lw/o0/where_to_draw_the_boundary/\">boundary</a> around possible things, events, patterns.&nbsp; They are <a href=\"/lw/np/disputing_definitions/\">in no sense neutral</a>; they act as signposts saying &quot;lump these things together for predictive purposes&quot;.&nbsp; To use the word &quot;singularity&quot; as ranging over human brains, farming, industry, and self-improving AI, is very nearly to finish your thesis right there.</p><a id=\"more\"></a><p>I wouldn't be surprised to find that, in a real AI, 80% of the actual computing crunch goes into drawing the right boundaries to make the actual reasoning possible.&nbsp; The question &quot;Where do abstractions come from?&quot; cannot be taken for granted.</p>\n\n<p>Boundaries are drawn by appealing to other boundaries.&nbsp; To draw the boundary &quot;human&quot; around things that wear clothes and speak language and have a certain shape, you must have previously noticed the boundaries around clothing and language.&nbsp; And your visual cortex already has a (damned sophisticated) system for categorizing visual scenes into shapes, and the shapes into categories.</p>\n\n<p>It's very much worth distinguishing between boundaries drawn by noticing a set of similarities, and boundaries drawn by reasoning about causal interactions.</p>\n\n<p>There's a big difference between saying &quot;I predict that Socrates, <em>like other humans I've observed</em>, will fall into the class of 'things that die when drinking hemlock'&quot; and saying &quot;I predict that Socrates, whose biochemistry I've observed to have this-and-such characteristics, will have his neuromuscular junction disrupted by the coniine in the hemlock - even though I've never seen that happen, I've seen lots of organic molecules and I know how they behave.&quot;</p>\n\n<p>But above all - ask where the abstraction comes from!</p>\n\n<p>To see a hammer is not good to hold high in a lightning storm, we draw on pre-existing objects that you're not supposed to hold electrically conductive things to high altitudes - this is a predrawn boundary, found by us in books; probably originally learned from experience and then further explained by theory.&nbsp; We just test the hammer to see if it fits in a pre-existing boundary, that is, a boundary we drew before we ever thought about the hammer.</p>\n\n<p>To evaluate the cost to carry a hammer in a tool kit, you probably visualized the process of putting the hammer in the kit, and the process of carrying it.&nbsp; Its mass determines the strain on your arm muscles.&nbsp; Its volume and <em>shape</em> - not just &quot;volume&quot;, as you can see as soon as that is pointed out - determine the difficulty of fitting it into the kit.&nbsp; You said &quot;volume and mass&quot; but that was an approximation, and as soon as I say &quot;volume and mass and shape&quot; you say, &quot;Oh, of course that's what I meant&quot; - based on a causal visualization of trying to fit some weirdly shaped object into a toolkit, or e.g. a thin ten-foot thin pin of low volume and high annoyance.&nbsp; So you're redrawing the boundary based on a causal visualization which shows that other characteristics can be relevant <em>to the consequence you care about.</em></p>\n\n<p>None of your examples talk about drawing <em>new</em> conclusions about the hammer by <em>analogizing it to other things</em> rather than directly assessing its characteristics in their own right, so it's not all that good an example when it comes to making predictions about self-improving AI by putting it into a group of similar things that includes farming or industry.</p>\n\n<p>But drawing that particular boundary would already rest on <em>causal</em> reasoning that tells you which abstraction to use.&nbsp; &nbsp;Very much an Inside View, and a Weak Inside View, even if you try to go with an Outside View after that.</p>\n\n<p>Using an &quot;abstraction&quot; that covers such massively different things, will often be met by a differing intuition that makes a different abstraction, <em>based on a different causal visualization</em> behind the scenes.&nbsp; &nbsp;That's what you want to drag into the light - not just say, &quot;Well, I expect this Singularity to resemble past Singularities.&quot;</p>\n\n<p>Robin said:</p><blockquote><p><span id=\"comment-139753978-content\">I am of course open to different\nway to conceive of &quot;the previous major singularities&quot;.&nbsp; I have\npreviously tried to conceive of them in terms of sudden growth\nspeedups.</span></p></blockquote><p><span id=\"comment-139753978-content\">Is that the root source for your abstraction - &quot;things that do sudden growth speedups&quot;?&nbsp; I mean... is that really what you want to go with here?<br /> </span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5GYzBE6q89w74dqfk": 8}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YyYwRxajPkAyajKXx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "1153", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sSqoEw9eRP2kPKLCz", "d5NyJ2Lf6N22AD9PB", "7X2j8HAkWdmMoS8PE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-21T05:39:25.000Z", "modifiedAt": null, "url": null, "title": "Observing Optimization", "slug": "observing-optimization", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:37.296Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GbGzP4LZTBN8dyd8c/observing-optimization", "pageUrlRelative": "/posts/GbGzP4LZTBN8dyd8c/observing-optimization", "linkUrl": "https://www.lesswrong.com/posts/GbGzP4LZTBN8dyd8c/observing-optimization", "postedAtFormatted": "Friday, November 21st 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Observing%20Optimization&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AObserving%20Optimization%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGbGzP4LZTBN8dyd8c%2Fobserving-optimization%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Observing%20Optimization%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGbGzP4LZTBN8dyd8c%2Fobserving-optimization", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGbGzP4LZTBN8dyd8c%2Fobserving-optimization", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1669, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/rk/optimization_and_the_singularity/\">Optimization and the Singularity</a></p>\n\n<p>In&nbsp; &quot;<a href=\"/lw/rk/optimization_and_the_singularity/\">Optimization and the Singularity</a>&quot; I pointed out that history since the first replicator, including human history to date, has <em>mostly</em> been a case of <em>nonrecursive</em> optimization - where you've got one thingy doing the optimizing, and another thingy getting optimized.&nbsp; When evolution builds a better amoeba, that doesn't change <em>the structure of evolution</em>\n- the mutate-reproduce-select cycle.</p>\n\n<p>But there are exceptions to this\nrule, such as the invention of sex, which affected the structure of\nnatural selection itself - transforming it to\nmutate-recombine-mate-reproduce-select.</p>\n\n<p>I was surprised when Robin, in &quot;<a href=\"http://www.overcomingbias.com/2008/06/eliezers-meta-l.html\">Eliezer's Meta-Level Determinism</a>&quot; took that idea and ran with it and <a href=\"http://www.overcomingbias.com/2008/06/eliezers-meta-l.html\">said</a>:</p>\n<blockquote><p>...his view does seem to make testable predictions\nabout history.&nbsp; It suggests the introduction of natural selection and\nof human culture coincided with the very largest capability growth rate\nincreases.&nbsp; It suggests that the next largest increases were much\nsmaller and coincided in biology with the introduction of cells and\nsex, and in humans with the introduction of writing and science.&nbsp; And\nit suggests other rate increases were substantially smaller.</p></blockquote><p>It\nhadn't occurred to me to try to derive that kind of testable\nprediction.&nbsp; Why?&nbsp; Well, partially because I'm not an economist.&nbsp; (Don't get me wrong, it was a virtuous step to try.)&nbsp; But\nalso because the whole issue looked to me like it was a lot more\ncomplicated than that, so it hadn't occurred to me to try to directly\nextract predictions.</p>\n\n<p>What is this &quot;capability growth rate&quot; of which you speak, Robin? \nThere are old, old controversies in evolutionary biology involved here.</p><a id=\"more\"></a><p>Just to start by pointing out the obvious - if there are fixed\nresources available, only so much grass to be eaten or so many rabbits\nto consume, then any evolutionary &quot;progress&quot; that we would recognize as\nproducing a better-designed organism, may just result in the\ndisplacement of the old allele by the new allele - <em>not</em> any increase in\nthe population as a whole.&nbsp; It's quite possible to have a new wolf that\nexpends 10% more energy per day to be 20% better at hunting, and in\nthis case <a href=\"/lw/l5/evolving_to_extinction/\">the sustainable wolf population will decrease</a> as new wolves\nreplace old.</p>\n\n<p>If I was going to talk about the effect that a meta-level change might have on the &quot;optimization velocity&quot;\nof natural selection, I would talk about the time for a new adaptation to replace\nan old adaptation after a shift in selection pressures - not the total\npopulation or total biomass or total morphological complexity (see below).</p>\n\n<p>Likewise in human history - farming was an important innovation for purposes of optimization, not because it changed the human brain all that much, but because it meant that there were a hundred times as many brains around; and even more importantly, that there were surpluses that could support specialized professions.&nbsp; But many innovations in human history may have consisted of new, improved, more harmful weapons - which would, if anything, have decreased the sustainable population size (though &quot;no effect&quot; is more likely - fewer people means more food means more people).</p>\n\n<p>Or similarly: there's a talk somewhere where either Warren Buffett or Charles Munger mentions how they hate to hear about technological improvements in certain industries - because even if investing a few million can cut the cost of production by 30% or whatever, the barriers to competition are so low that the consumer captures all the gain.&nbsp; So they <em>have</em> to invest to keep up with competitors, and the investor doesn't get much return.</p>\n\n<p>I'm trying to measure the optimization velocity of information, not production or growth rates.&nbsp; At the tail end of a very long process, knowledge finally does translate into power - guns or nanotechnology or whatever.&nbsp; But along that long way, if you're measuring the number of material copies of the same stuff (how many wolves, how many people, how much grain), you may not be getting much of a glimpse at optimization velocity.&nbsp; Too many complications along the causal chain.</p>\n\n<p>And this is not just my problem.</p>\n\n<p>Back in the bad old days of pre-1960s evolutionary biology, it was widely taken for granted that there\nwas such a thing as progress, that it proceeded forward over time, and\nthat modern human beings were at the apex.</p>\n\n\n\n<p>George Williams's <em>Adaptation and Natural Selection,</em>\nmarking the so-called &quot;Williams Revolution&quot; in ev-bio that flushed out\na lot of the romanticism and anthropomorphism, spent most of one chapter questioning the seemingly common-sensical metrics\nof &quot;progress&quot;.&nbsp; </p>\n\n<p>Biologists sometimes spoke of &quot;morphological complexity&quot;\nincreasing over time.&nbsp; But how do you measure that, exactly?&nbsp; And at what point in life do you\nmeasure it if the organism goes through multiple stages?&nbsp; Is an amphibian more advanced than a mammal, since its\ngenome has to store the information for multiple stages of life?</p>\n\n<p>&quot;There are life cycles enormously more complex than that of a frog,&quot;\nWilliams wrote. &quot;The lowly and 'simple' liver fluke...&quot; goes through\nstages that include a waterborne stage that swims using cilia; finds\nand burrows into a snail and then transforms into a sporocyst; that\nreproduces by budding to produce redia; that migrate in the snail and\nreproduce asexually; then transform into cercaria, that, by wiggling a\ntail, burrows out of the snail and swims to a blade of grass; where they\ntransform into dormant metacercaria; that are eaten by sheep and then\nhatch into a young fluke inside the sheep; then transform into adult flukes; which spawn fluke zygotes...&nbsp; So how &quot;advanced&quot; is\nthat?</p>\n<p>Williams also pointed out that there would be a limit to how much\ninformation evolution could maintain in the genome against degenerative\npressures - which seems like a good principle in practice, though I\nmade <a href=\"/lw/ku/natural_selections_speed_limit_and_complexity/\">some mistakes on OB in trying to describe the theory</a>.&nbsp; \nTaxonomists often take a current form and call the historical trend\ntoward it &quot;progress&quot;, but is that <em>upward</em> motion, or just substitution of some adaptations for other adaptations in response to changing selection pressures?</p>\n\n<p>&quot;Today the\nfishery biologists greatly fear such archaic fishes as the bowfin,\ngarpikes , and lamprey, because they are such outstandingly effective\ncompetitors,&quot; Williams noted.</p>\n\n<p>So if I were talking about the effect of e.g. sex as a meta-level innovation, then I would expect e.g. an increase in the total biochemical and morphological\ncomplexity that could be maintained - the lifting of a previous upper\nbound, followed by an accretion of information.&nbsp; And I might expect a\nchange in the velocity of new adaptations replacing old adaptations.</p>\n\n<p>But to get from there, to something that shows up in the fossil record - that's not a trivial step.</p>\n\n<p>I recall reading, somewhere or other, about an ev-bio controversy\nthat ensued when one party spoke of the &quot;sudden burst of creativity&quot;\nrepresented by the Cambrian explosion, and wondered why evolution was\nproceeding so much more slowly nowadays.&nbsp; And another party responded\nthat the Cambrian differentiation was mainly visible <em>post hoc</em> - that\nthe groups of animals we have <em>now,</em> first differentiated from one another <em>then,</em>\nbut that <em>at the time</em> the differences were not as large as they loom\nnowadays.&nbsp; That is, the actual velocity of adaptational change wasn't\nremarkable by comparison to modern times, and only hindsight causes us\nto see those changes as &quot;staking out&quot; the ancestry of the major animal\ngroups.</p>\n\n<p>I'd be surprised to learn that sex had no effect on the velocity of\nevolution.&nbsp; It looks like it should increase the speed and\nnumber of substituted adaptations, and also increase the complexity\nbound on the total genetic information that can be maintained against\nmutation.&nbsp; But to go from there, to just looking at the fossil record\nand seeing <em>faster progress</em> - it's not just me who thinks that this jump to phenomenology is tentative, difficult, and controversial.</p>\n\n\n\n<p>Should you expect more speciation after the invention of sex, or\nless?&nbsp; The first impulse is to say &quot;more&quot;, because sex seems like it\nshould increase the optimization velocity and speed up time.&nbsp; But sex\nalso creates mutually reproducing <em>populations</em>, that share genes\namong themselves, as opposed to asexual lineages - so might that act as\na centripetal force?</p>\n\n<p>I don't even propose to answer this question, just\npoint out that it is actually quite <em>standard</em> for the\nphenomenology of evolutionary theories - the question of which\nobservables are predicted - to be a major difficulty.&nbsp; Unless you're dealing with really <em>easy</em> qualitative questions like &quot;Should I find rabbit fossils in the pre-Cambrian?&quot;&nbsp; (I try to only make predictions about AI, using my theory of optimization, when it looks like an <em>easy</em> question.)</p>\n\n<p>Yes, it's more convenient for scientists when theories make easily testable, readily observable predictions.&nbsp; But when I look back at the history of life, and the history of humanity, my first priority is to ask &quot;What's going on here?&quot;, and only afterward see if I can manage to make non-obvious retrodictions.&nbsp; I can't just start with the goal of having a convenient phenomenology.&nbsp; Or similarly: the theories I use to organize my understanding of the history of optimization to date, have lots of parameters, e.g. the optimization-efficiency curve that describes optimization output as a function of resource input, or the question of how many low-hanging fruit exist in the neighborhood of a given search point.&nbsp; Does a larger population of wolves increase the velocity of natural selection, by covering more of the search neighborhood for possible mutations?&nbsp; If so, is that a logarithmic increase with population size, or what?&nbsp; - But I can't just wish my theories into being simpler.</p>\n\n<p>If Robin has a <em>simpler</em>\ncausal model, with fewer parameters, that stands directly behind observables and easily coughs up testable\npredictions, which fits the data well, and obviates the need for my own abstractions like &quot;optimization efficiency&quot; -</p>\n\n<p>- then I may have to discard my own attempts at theorizing.&nbsp; But observing a series of material growth modes doesn't\ncontradict a causal model of optimization behind the scenes, because it's a pure phenomenology, not itself a causal model - it\ndoesn't say whether a given innovation had any effect on the optimization velocity of the process that produced future object-level innovations that actually changed growth modes, etcetera.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nZCb9BSnmXZXSNA2u": 1, "nvKzwpiranwy29HFJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GbGzP4LZTBN8dyd8c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "1154", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HFTn3bAT6uXSNwv4m", "gDNrpuwahdRrDJ9iY", "QcnkFgojszmo9k4xk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-21T23:05:47.000Z", "modifiedAt": null, "url": null, "title": "Life's Story Continues", "slug": "life-s-story-continues", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:19.343Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RXLoo3pXgY2hjdXrg/life-s-story-continues", "pageUrlRelative": "/posts/RXLoo3pXgY2hjdXrg/life-s-story-continues", "linkUrl": "https://www.lesswrong.com/posts/RXLoo3pXgY2hjdXrg/life-s-story-continues", "postedAtFormatted": "Friday, November 21st 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Life's%20Story%20Continues&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALife's%20Story%20Continues%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRXLoo3pXgY2hjdXrg%2Flife-s-story-continues%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Life's%20Story%20Continues%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRXLoo3pXgY2hjdXrg%2Flife-s-story-continues", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRXLoo3pXgY2hjdXrg%2Flife-s-story-continues", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1533, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/w0/the_first_world_takeover/\">The First World Takeover</a></p>\n\n<p>As <a href=\"/lw/w0/the_first_world_takeover/\">last we looked at the planet</a>, Life's long search in organism-space had only just gotten started.</p>\n\n<p>When I try to structure my understanding of the unfolding process of\nLife, it seems to me that,\nto understand the <em>optimization velocity</em> at any given point, I\nwant to break down that velocity using the following <a href=\"/lw/w1/whence_your_abstractions/\">abstractions</a>:</p>\n\n<ul><li>The searchability of the neighborhood of the current location,\nand the availability of good/better alternatives in that rough region. \nMaybe call this the <em>optimization slope</em>.&nbsp; Are the fruit low-hanging or high-hanging, and how large are the\nfruit?</li>\n\n<li>The <em>optimization resources</em>, like the amount of computing power\navailable to a fixed program, or the number of individuals in a population pool.</li>\n\n<li>The <em>optimization efficiency</em>, a curve that gives the amount of\nsearchpower generated by a given investiture of resources, which is\npresumably a function of the optimizer's structure at that point in\ntime.</li></ul><a id=\"more\"></a><p>Example:&nbsp; If an <em>object-level</em> adaptation enables more\nefficient extraction of resources, and thereby increases the total\npopulation that can be supported by fixed available resources, then this\nincreases the <em>optimization resources</em> and perhaps the optimization\nvelocity.</p>\n\n<p>How much does optimization velocity increase - how hard does this object-level innovation hit back to the meta-level?</p>\n<p>If a population is small enough that not\nall mutations are occurring in each generation, then a larger\npopulation decreases the time for a given mutation to show up.&nbsp; If the fitness improvements offered by beneficial mutations follow an exponential distribution, then - I'm not actually doing the math here, just sort of eyeballing - I would expect the optimization velocity to go as log population size, up to a maximum where the search neighborhood is explored thoroughly.&nbsp; (You could test this in the lab, though not just by eyeballing the fossil record.)</p>\n<p>This doesn't mean <em>all</em> optimization processes would have a momentary velocity that goes as the log of momentary resource investment up to a maximum.&nbsp; Just one mode of evolution would have this character.&nbsp; And even under these assumptions, evolution's <em>cumulative</em> optimization wouldn't go as log of <em>cumulative</em> resources - the log-pop curve is just the instantaneous velocity. \nIf we assume that the variance of the neighborhood remains the same\nover the course of exploration (good points have better neighbors with\nsame variance ad infinitum), and that the population size remains the\nsame, then we should see linearly cumulative optimization over time. \nAt least until we start to hit the information bound on maintainable\ngenetic information...</p>\n<p>These are the sorts of abstractions that I think are required to describe the history of life on Earth in terms of optimization. \nAnd I also think that if you don't talk optimization, then you won't be\nable to understand the causality - there'll just be these mysterious\nunexplained progress modes that change now and then.&nbsp; In the same way\nyou have to talk natural selection to understand observed evolution,\nyou have to talk optimization velocity to understand observed\nevolutionary speeds.</p>\n\n<p>The first thing to realize is that meta-level changes are rare, so most of what we see in the historical record will be structured by the <em>search neighborhoods</em> - the way that one innovation opens up the way for additional innovations.&nbsp; That's going to be most of the story, not because meta-level innovations are unimportant, but because they are rare.</p>\n\n<p>In &quot;<a href=\"http://www.overcomingbias.com/2008/06/eliezers-meta-l.html\">Eliezer's Meta-Level Determinism</a>&quot;, Robin lists the following dramatic events traditionally noticed in the fossil record:</p><blockquote><p>Any Cells, Filamentous Prokaryotes, Unicellular Eukaryotes, Sexual Eukaryotes, Metazoans</p></blockquote><p>And he describes &quot;the last three strong transitions&quot; as:</p><blockquote><p>Humans, farming, and industry</p></blockquote><p>So let me describe what I see when I look at these events, plus some others, through the lens of my abstractions:</p>\n\n<p><em>Cells:</em>&nbsp; Force a set of genes, RNA strands, or catalytic chemicals to share a common reproductive fate.&nbsp; (This is the real point of the cell boundary, not &quot;protection from the environment&quot; - it keeps the fruits of chemical labor inside a spatial boundary.)&nbsp; But, as we've defined our abstractions, this is mostly a matter of optimization slope - the quality of the search neighborhood.&nbsp; The advent of cells opens up a tremendously rich new neighborhood defined by <em>specialization</em> and division of labor.&nbsp; It also increases the slope by ensuring that chemicals get to keep the fruits of their own labor in a spatial boundary, so that fitness advantages increase.&nbsp; But does it hit back to the meta-level?&nbsp; How you define that seems to me like a matter of taste.&nbsp; Cells don't quite change the mutate-reproduce-select cycle.&nbsp; But if we're going to define sexual recombination as a meta-level innovation, then we should also define cellular isolation as a meta-level innovation.</p>\n\n<p>It's worth noting that modern genetic algorithms have not, to my knowledge, reached anything like the level of intertwined complexity that characterizes modern unicellular organisms.&nbsp; Modern genetic algorithms seem more like they're producing individual chemicals, rather than being able to handle individually complex modules.&nbsp; So the cellular transition may be a hard one.</p>\n\n<p><em>DNA:</em>&nbsp; I haven't yet looked up the standard theory on this, but I would sorta expect it to come <em>after</em> cells, since a ribosome seems like the sort of thing you'd have to keep around in a defined spatial location.&nbsp; DNA again opens up a huge new search neighborhood by separating the functionality of chemical shape from the demands of reproducing the pattern.&nbsp; Maybe we should rule that anything which restructures the search neighborhood this\ndrastically should count as a hit back to the meta-level.&nbsp; (Whee, our\nabstractions are already breaking down.)&nbsp; Also, DNA directly hits back to the meta-level by carrying information at higher fidelity, which increases the total storable information.</p>\n\n<p><em>Filamentous prokaryotes, unicellular eukaryotes:</em>&nbsp; Meh, so what.</p>\n\n<p><em>Sex:</em>&nbsp; The archetypal example of a rare meta-level innovation.&nbsp; Evolutionary biologists still puzzle over how exactly this one managed to happen.</p>\n\n<p><em>Metazoans:</em>&nbsp; The key here is not cells aggregating into colonies with similar genetic heritages; the key here is the controlled specialization of cells with an identical genetic heritage.&nbsp; This opens up a huge new region of the search space, but does not particularly change the nature of evolutionary optimization.</p>\n\n<p>Note that opening a sufficiently huge gate in the search neighborhood, may <em>result</em> in a meta-level innovation being uncovered shortly thereafter.&nbsp; E.g. if cells make ribosomes possible.&nbsp; One of the main lessons in this whole history is that <em>one thing leads to another</em>.</p>\n\n<p>Neurons, for example, may have been the key enabling factor in enabling large-motile-animal body plans, because they enabled one side of the organism to talk with the other.</p>\n\n<p>This brings us to the age of brains, which will be the topic of the next post.</p>\n\n<p>But in the meanwhile, I just want to note that my view is nothing as simple as &quot;meta-level determinism&quot; or &quot;the impact of something is proportional to how meta it is; non-meta things must have small impacts&quot;.&nbsp; Nothing much <em>meta</em> happened between the age of sexual metazoans and the age of humans - brains were getting more sophisticated over that period, but that didn't change the nature of evolution.</p>\n\n<p>Some object-level innovations are small, some are medium-sized, some are huge.&nbsp; It's no wonder if you look at the historical record and see a Big Innovation that doesn't look the least bit meta, but had a huge impact by itself <em>and</em> led to lots of other innovations by opening up a new neighborhood picture of search space.&nbsp; This is allowed.&nbsp; Why wouldn't it be?</p>\n\n<p>You can even get exponential acceleration without anything meta - if, for example, the more knowledge you have, or the more genes you have, the more opportunities you have to make good improvements to them.&nbsp; Without any increase in optimization pressure, the neighborhood gets higher-sloped as you climb it.</p>\n\n<p>My thesis is more along the lines of, &quot;If this is the picture <em>without</em> recursion, just imagine what's going to happen when we <em>add</em> recursion.&quot;</p>\n\n<p>To anticipate one possible objection:&nbsp; I don't expect Robin to disagree that modern\ncivilizations underinvest in meta-level improvements because they take\ntime to yield cumulative effects, are new things that don't have certain payoffs, and worst of all, tend to be public goods.&nbsp; That's\nwhy we don't have billions of dollars flowing into prediction markets,\nfor example.&nbsp; I, Robin, or Michael Vassar could probably think for five minutes and name five major probable-big-win meta-level improvements that society isn't investing in.<br />\n</p>\n<p>So if meta-level improvements are rare in the fossil record, it's not necessarily because it would be<em> hard</em> to improve on evolution, or because meta-level improving doesn't accomplish much.&nbsp; Rather, evolution doesn't do anything <em>because</em> it will have a long-term payoff a thousand generations later. \nAny meta-level improvement also has to grant an object-level fitness\nadvantage in, say, the next two generations, or it will go extinct. \nThis is why we can't solve the puzzle of how sex evolved by pointing\ndirectly to how it speeds up evolution.&nbsp; &quot;This speeds up evolution&quot; is just not a valid reason for something to evolve.</p>\n\n<p>Any creative evolutionary biologist could probably think for five minutes and come up with five <em>great</em> ways that evolution could have improved on evolution - but which happen to be more complicated than the wheel, which evolution evolved on only three known occasions - or don't happen to grant an <em>immediate</em> fitness benefit to a handful of implementers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nvKzwpiranwy29HFJ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RXLoo3pXgY2hjdXrg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 20, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "1155", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["spKYZgoh3RmhxMqyu", "YyYwRxajPkAyajKXx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-23T07:26:41.000Z", "modifiedAt": null, "url": null, "title": "Surprised by Brains", "slug": "surprised-by-brains", "viewCount": null, "lastCommentedAt": "2018-12-04T01:21:58.436Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XQirei3crsLxsCQoi/surprised-by-brains", "pageUrlRelative": "/posts/XQirei3crsLxsCQoi/surprised-by-brains", "linkUrl": "https://www.lesswrong.com/posts/XQirei3crsLxsCQoi/surprised-by-brains", "postedAtFormatted": "Sunday, November 23rd 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Surprised%20by%20Brains&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASurprised%20by%20Brains%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXQirei3crsLxsCQoi%2Fsurprised-by-brains%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Surprised%20by%20Brains%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXQirei3crsLxsCQoi%2Fsurprised-by-brains", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXQirei3crsLxsCQoi%2Fsurprised-by-brains", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2244, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/w3/lifes_story_continues/\">Life's Story Continues</a></p>\n\n<p>Imagine two agents who've <em>never seen an intelligence</em> -\nincluding, somehow, themselves - but who've seen the rest of the\nuniverse up until now, arguing about what these newfangled &quot;humans&quot;\nwith their &quot;language&quot; might be able to do...</p><blockquote><p>Believer:&nbsp; Previously, evolution has taken hundreds of thousands of years to <a href=\"/lw/kt/evolutions_are_stupid_but_work_anyway/\">create new complex adaptations with many working parts</a>.&nbsp; I believe that, thanks to brains and language, we may see a <em>new</em> era, an era of <em>intelligent design.</em> \nIn this era, complex causal systems - with many interdependent parts\nthat collectively serve a definite function - will be created by the cumulative work of many brains\nbuilding upon each others' efforts.</p>\n\n<p>Skeptic:&nbsp; I see - you think that brains might have\nsomething like a 50% speed advantage over natural selection?&nbsp; So\nit might take a while for brains to catch up, but after another eight billion\nyears, brains will be in the lead.&nbsp; But this planet's Sun will swell up\nby then, so -</p>\n\n<p>Believer:&nbsp; <em>Thirty percent?</em>&nbsp; I was thinking more like <em>three orders of magnitude.</em> \nWith thousands of brains working together and building on each others'\nefforts, whole complex machines will be designed on the timescale of\nmere millennia - no, <em>centuries!</em></p>\n\n<p>Skeptic:&nbsp; <em>What?</em></p>\n\n<p>Believer:&nbsp; You heard me.</p></blockquote>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<a id=\"more\"></a><blockquote><p>Skeptic:&nbsp; Oh, come on!&nbsp; There's absolutely no empirical evidence for\nan assertion like that!&nbsp; Animal brains have been around for hundreds of\nmillions of years without doing anything like what you're saying.&nbsp; I\nsee no reason to think that life-as-we-know-it will end, just because\nthese hominid brains have learned to send low-bandwidth signals over their vocal cords.&nbsp; Nothing like what\nyou're saying has happened before in <em>my</em> experience -</p>\n\n<p>Believer:&nbsp; That's kind of the <em>point,</em> isn't it?&nbsp; That nothing\nlike this has happened before?&nbsp; And besides, there <em>is</em> precedent for\nthat kind of Black Swan - namely, the first replicator.</p>\n\n<p>Skeptic:&nbsp; Yes, there is precedent in the replicators.&nbsp; Thanks\nto our observations of evolution, we have extensive knowledge and many\nexamples of how optimization works.&nbsp; We know, in particular, that\noptimization isn't easy - it takes millions of years to climb up through the search space.&nbsp; Why should\n&quot;brains&quot;, even if they optimize, produce such different results?</p>\n\n<p>Believer:&nbsp; Well, natural selection is just <a href=\"/lw/kt/evolutions_are_stupid_but_work_anyway/\">the very first optimization process that got started accidentally</a>.&nbsp; &nbsp;These newfangled brains were <em>designed by</em> evolution, rather than, like evolution itself, being a natural process that got started by accident.&nbsp; So &quot;brains&quot; are far more sophisticated - why, just <em>look</em> at them.&nbsp; Once they get started on cumulative optimization - FOOM!</p>\n\n<p>Skeptic:&nbsp; So far, brains are a lot <em>less</em> impressive than natural selection.&nbsp; These &quot;hominids&quot; you're so interested in - can these creatures' handaxes really be compared to the majesty of a dividing cell?</p>\n\n<p>Believer:&nbsp; That's because they only just got started on language and <em>cumulative</em> optimization.</p>\n\n<p>Skeptic:&nbsp; Really?&nbsp; Maybe it's because the principles of natural\nselection are simple and elegant for creating complex designs, and all\nthe convolutions of brains are only good for chipping handaxes in a\nhurry.&nbsp; Maybe brains simply don't scale to detail work.&nbsp; Even if we\ngrant the highly dubious assertion that brains are more efficient than natural\nselection - which you seem to believe on the basis of just <em>looking</em> at brains and seeing the convoluted folds - well, there still has to be a law of diminishing returns.</p>\n\n<p>Believer:&nbsp; Then why have brains been getting steadily larger over\ntime?&nbsp; That doesn't look to me like evolution is running into\ndiminishing returns.&nbsp; If anything, the recent example of hominids\nsuggests that once brains get large and complicated <em>enough,</em> the fitness advantage for <em>further</em> improvements is even <em>greater -</em></p>\n\n<p>Skeptic:&nbsp; Oh, that's probably just sexual selection!&nbsp; I mean, if you\nthink that a bunch of brains will produce new complex machinery in just\na hundred years, then why not suppose that a brain the size of a <em>whole planet</em> could produce a <em>de novo</em> complex causal system with many interdependent elements in a <em>single day?</em></p>\n\n<p>Believer:&nbsp; You're attacking a strawman here - I never said anything like <em>that.</em></p>\n\n<p>Skeptic:&nbsp; Yeah?&nbsp; Let's hear you assign a <em>probability</em> that a brain the size of a planet could produce a new complex design in a single day.</p>\n\n<p>Believer:&nbsp; The size of a <em>planet?</em>&nbsp; <em>(Thinks.)</em>&nbsp; Um... ten percent.</p>\n\n<p>Skeptic:&nbsp; <em>(Muffled choking sounds.)</em></p>\n\n<p>Believer:&nbsp; Look, brains are <em>fast.</em>&nbsp; I can't rule it out in <em>principle</em> -</p>\n\n<p>Skeptic:&nbsp; Do you understand how long a <em>day</em> is?&nbsp; It's the amount of time for the Earth to spin on its <em>own</em> axis, <em>once</em>.&nbsp; One sunlit period, one dark period.&nbsp; There are 365,242 of them in a <em>single millennium.</em></p>\n\n<p>Believer:&nbsp; Do you understand how long a <em>second</em> is?&nbsp; That's\nhow long it takes a brain to see a fly coming in, target it in the air,\nand eat it.&nbsp; There's 86,400 of them in a day.</p>\n\n<p>Skeptic:&nbsp; Pffft, and chemical interactions in cells happen in\nnanoseconds.&nbsp; Speaking of which, how are these brains going to build <em>any</em> sort of complex machinery without access to ribosomes?&nbsp; They're just going to run around on the grassy plains in <em>really optimized</em> patterns until they get tired and fall over.&nbsp; There's nothing they can use to build proteins or even control tissue structure.</p>\n\n<p>Believer:&nbsp; Well, life didn't <em>always</em> have ribosomes, right?&nbsp; The first replicator didn't.</p>\n\n<p>Skeptic:&nbsp; So brains will evolve their own ribosomes?</p>\n\n<p>Believer:&nbsp; Not necessarily ribosomes.&nbsp; Just <em>some</em> way of making things.</p>\n\n<p>Skeptic:&nbsp; Great, so call me in another hundred million years when <em>that</em> evolves, and I'll start worrying about brains.</p>\n\n<p>Believer:&nbsp; No, the brains will <em>think</em> of a way to get their own ribosome-analogues.</p>\n\n<p>Skeptic:&nbsp; No matter what they <em>think,</em> how are they going to <em>make anything</em> without ribosomes?</p>\n\n<p>Believer:&nbsp; They'll think of a way.</p>\n\n<p>Skeptic:&nbsp; Now you're just treating brains as magic fairy dust.</p>\n\n<p>Believer:&nbsp; The first replicator would have been magic fairy dust by comparison with anything that came before it -</p>\n\n<p>Skeptic:&nbsp; That doesn't license throwing common sense out the window.</p>\n\n<p>Believer:&nbsp; What you call &quot;common sense&quot; is exactly what would have caused you to assign negligible probability to the actual outcome of the first replicator.&nbsp; Ergo, not so sensible as it seems, if you want to get your predictions actually <em>right,</em> instead of <em>sounding reasonable.</em></p>\n\n<p>Skeptic:&nbsp; And your belief that in the Future it will only take a hundred years to optimize a complex causal system with dozens of interdependent parts - you think this is how you get it <em>right?</em></p>\n\n<p>Believer:&nbsp; Yes!&nbsp; Sometimes, in the pursuit of truth, you have to be courageous - to stop worrying about how you sound in front of your friends - to think outside the box - to imagine <a href=\"/lw/j6/why_is_the_future_so_absurd/\">futures fully as absurd as the Present would seem without benefit of hindsight</a> - and even, yes, say things that sound completely ridiculous and outrageous by comparison with the Past.&nbsp; That is why I boldly dare to say - pushing out my guesses to the limits of where Truth drives me, without fear of sounding silly - that in the <em>far</em> future, a billion years from now when brains are more highly evolved, they will find it possible to design a complete machine with a <em>thousand</em> parts in as little as <em>one decade!</em></p>\n\n<p>Skeptic:&nbsp; You're just digging yourself deeper.&nbsp; I don't even understand <em>how</em> brains are supposed to optimize so much faster.&nbsp; To find out the fitness of a mutation, you've got to run millions of real-world tests, right?&nbsp; And even then, an environmental shift can make all your optimization worse than nothing, and there's no way to predict&nbsp; <em>that</em> no matter <em>how</em> much you test -</p>\n\n<p>Believer:&nbsp; Well, a brain is <em>complicated,</em> right?&nbsp; I've been looking at them for a while and even I'm not totally sure I understand what goes on in there.</p>\n\n<p>Skeptic:&nbsp; Pffft!&nbsp; What a ridiculous excuse.&nbsp; </p>\n\n<p>Believer:&nbsp; I'm sorry, but it's the truth - brains <em>are</em> harder to understand.</p>\n\n<p>Skeptic:&nbsp; Oh, and I suppose evolution is trivial?</p>\n\n<p>Believer:&nbsp; By comparison... yeah, actually.</p>\n\n<p>Skeptic:&nbsp; Name me <em>one</em> factor that explains why you think brains will run so fast.</p>\n\n<p>Believer:&nbsp; Abstraction.</p>\n\n<p>Skeptic:&nbsp; Eh?&nbsp; &nbsp;Abstrah-shun?</p>\n\n<p>Believer:&nbsp; It... um... lets you know about parts of the search space you haven't actually searched yet, so you can... sort of... skip right to where you need to be -</p>\n\n<p>Skeptic:&nbsp; I see.&nbsp; And does this power work by clairvoyance, or by precognition?&nbsp; Also, do you get it from a potion or an amulet?\n</p>\n\n<p>Believer:&nbsp; The brain looks at the fitness of just a few points in\nthe search space - does some complicated processing - and voila, it\nleaps to a much higher point!</p>\n<p>Skeptic:&nbsp; Of course.&nbsp; I knew teleportation had to fit in here somewhere.</p>\n<p>Believer:&nbsp; See, the fitness of <em>one</em> point tells you something about <em>other</em> points -</p>\n\n<p>Skeptic:&nbsp; Eh?&nbsp; I don't see how that's possible without running another million tests.</p>\n\n<p>Believer:&nbsp; You just <em>look</em> at it, dammit!</p>\n\n<p>Skeptic:&nbsp; With what kind of sensor?&nbsp; It's a search space, not a bug to eat!</p>\n\n<p> Believer:&nbsp; The search space is compressible -</p>\n\n<p>Skeptic:&nbsp; Whaa?&nbsp; This is a design space of possible genes we're talking about, not a folding bed -</p>\n\n<p>Believer:&nbsp; Would you stop talking about genes already!&nbsp; Genes are on the way out!&nbsp; The future belongs to ideas!</p>\n\n<p>Skeptic:&nbsp; Give. Me. A. Break.</p>\n\n<p>Believer:&nbsp; Hominids alone shall carry the burden of destiny!</p>\n\n<p>Skeptic:&nbsp; They'd die off in a week without plants to eat.&nbsp; You probably don't know this, because you haven't studied ecology, but ecologies are <em>complicated</em> - no single species ever &quot;carries the burden of destiny&quot; by itself.&nbsp; But that's another thing - why are you postulating that it's just the hominids who go FOOM?&nbsp; What about the other primates?&nbsp; These chimpanzees are practically their cousins - why wouldn't they go FOOM too?</p>\n\n<p>Believer:&nbsp; Because it's all going to shift to the level of <em>ideas,</em> and the hominids will build on each other's ideas without the chimpanzees participating -</p>\n\n<p>Skeptic:&nbsp; You're begging the question.&nbsp; Why won't chimpanzees be part of the economy of ideas?&nbsp; Are you familiar with Ricardo's Law of Comparative Advantage?&nbsp; Even if chimpanzees are worse at everything than hominids, the hominids will still trade with them and all the other brainy animals.</p>\n\n<p>Believer:&nbsp; The cost of explaining an idea to a chimpanzee will exceed any benefit the chimpanzee can provide.</p>\n\n<p>Skeptic:&nbsp; But <em>why should that be true?</em>&nbsp; Chimpanzees only forked off from hominids a few million years ago.&nbsp; They have 95% of their genome in common with the hominids.&nbsp; The vast majority of optimization that went into producing hominid brains also went into producing chimpanzee brains.&nbsp; If hominids are good at trading ideas, chimpanzees will be 95% as good at trading ideas.&nbsp; Not to mention that all of your ideas belong to the far future, so that both hominids, and chimpanzees, and many other species will have evolved much more complex brains before <em>anyone</em> starts building their own cells -</p>\n\n<p>Believer:&nbsp; I think we could see as little as a million years pass between when these creatures first invent a means of storing information with persistent digital accuracy - their equivalent of DNA - and when they build machines as complicated as cells.</p>\n\n<p>Skeptic:&nbsp; Too many assumptions... I don't even know where to start...&nbsp; Look, right now brains are <em>nowhere near</em> building cells.&nbsp; It's going to take a <em>lot</em> more evolution to get to that point, and many other species will be much further along the way by the time hominids get there.&nbsp; Chimpanzees, for example, will have learned to talk -</p>\n\n<p>Believer:&nbsp; It's the <em>ideas</em> that will accumulate optimization, not the brains.</p>\n\n<p>Skeptic:&nbsp; Then I say again that if hominids can do it, chimpanzees will do it 95% as well.</p>\n\n<p>Believer:&nbsp; You might get discontinuous returns on brain complexity.&nbsp; Like... even though the hominid lineage split off from chimpanzees very recently, and only a few million years of evolution have occurred since then, the chimpanzees won't be able to keep up.</p>\n\n<p>Skeptic:&nbsp; <em>Why?</em></p>\n\n<p>Believer:&nbsp; Good question.</p>\n\n<p>Skeptic:&nbsp; Does it have a good <em>answer?</em></p>\n\n<p>Believer:&nbsp; Well, there might be compound interest on learning during the maturational period... or something about the way a mind flies through the search space, so that slightly more powerful abstracting-machinery can create abstractions that correspond to much faster travel... or some kind of feedback loop involving a brain powerful enough to control <em>itself</em>... or some kind of critical threshold built into the nature of cognition as a problem, so that a single missing gear spells the difference between walking and flying... or the hominids get started down some kind of sharp slope in the genetic fitness landscape, involving many changes in sequence, and the chimpanzees haven't gotten started down it yet... or <em>all</em> these statements are true and interact multiplicatively... I know that a few million years doesn't seem like much time, but really, quite a lot can happen.&nbsp; It's hard to untangle.</p>\n\n<p>Skeptic:&nbsp; I'd say it's hard to <em>believe.</em></p>\n\n<p>Believer:&nbsp; Sometimes it seems that way to me too!&nbsp; But I think that in a mere ten or twenty million years, we won't have a choice.</p></blockquote>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yXNtYNHJB54T3bGm3": 1, "ac84EpK6mZbPLzmqj": 1, "oiRp4T6u5poc8r9Tj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XQirei3crsLxsCQoi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 51, "extendedScore": null, "score": 7.8e-05, "legacy": true, "legacyId": "1156", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 51, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RXLoo3pXgY2hjdXrg", "jAToJHtg39AMTAuJo", "Ga2HSwf9iQe64JwAa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-24T09:33:40.000Z", "modifiedAt": null, "url": null, "title": "Cascades, Cycles, Insight...", "slug": "cascades-cycles-insight", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:30.390Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dq3KsCsqNotWc8nAK/cascades-cycles-insight", "pageUrlRelative": "/posts/dq3KsCsqNotWc8nAK/cascades-cycles-insight", "linkUrl": "https://www.lesswrong.com/posts/dq3KsCsqNotWc8nAK/cascades-cycles-insight", "postedAtFormatted": "Monday, November 24th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cascades%2C%20Cycles%2C%20Insight...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACascades%2C%20Cycles%2C%20Insight...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdq3KsCsqNotWc8nAK%2Fcascades-cycles-insight%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cascades%2C%20Cycles%2C%20Insight...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdq3KsCsqNotWc8nAK%2Fcascades-cycles-insight", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdq3KsCsqNotWc8nAK%2Fcascades-cycles-insight", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2301, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/w4/surprised_by_brains/\">Surprised by Brains</a></p>\n\n<p><em>Five sources of discontinuity:&nbsp; 1, 2, and 3...</em></p>\n\n<p><strong>Cascades</strong> are when one thing leads to another.&nbsp; Human brains are effectively discontinuous with chimpanzee brains due to a whole bag of design improvements, even though they and we share 95% genetic material and only a few million years have elapsed since the branch.&nbsp; Why this whole series of improvements in us, relative to chimpanzees?&nbsp; Why haven't some of the same improvements occurred in other primates?</p>\n\n<p>Well, this is not a question on which one may speak with authority (<a href=\"/lw/kj/no_one_knows_what_science_doesnt_know/\">so far as I know</a>).&nbsp; But I would venture an unoriginal guess that, in the hominid line, one thing led to another.</p>\n\n<p>The chimp-level task of modeling others, in the hominid line, led to improved self-modeling which supported recursion which enabled language which birthed politics that increased the selection pressure for outwitting which led to sexual selection on wittiness...</p>\n\n<p>...or something.&nbsp; It's hard to tell by looking at the fossil record what happened in what order and why.&nbsp; The point being that it wasn't <em>one optimization</em> that pushed humans ahead of chimps, but rather a <em>cascade</em> of optimizations that, in <em>Pan</em>, never got started.</p>\n\n<p>We fell up the stairs, you might say.&nbsp; It's not that the first stair ends the world, but if you fall up one stair, you're more likely to fall up the second, the third, the fourth...</p><a id=\"more\"></a><p>I will concede that farming was a watershed invention in the history of\nthe human species, though it intrigues me for a different reason than\nRobin.&nbsp; Robin, presumably, is interested because the economy grew by\ntwo orders of magnitude, or something like that.&nbsp; But did having a\nhundred times as many humans, lead to a hundred times as much thought-optimization <em>accumulating</em>\nper unit time?&nbsp; It doesn't seem likely, especially in the age before\nwriting and telephones.&nbsp; But farming, because of its sedentary and\nrepeatable nature, led to repeatable trade, which led to debt records.&nbsp; Aha!\n- now we have <em>writing.</em>&nbsp; <em>There's</em> a significant invention, from the perspective of cumulative optimization by brains.&nbsp; Farming isn't writing but it <em>cascaded to</em> writing.</p>\n\n<p>\nFarming also cascaded (by way of surpluses and cities) to support <em>professional specialization</em>.&nbsp; I suspect that having someone spend their whole life\nthinking about topic X instead of a hundred farmers occasionally\npondering it, is a more significant jump in cumulative optimization\nthan the gap between a hundred farmers and one hunter-gatherer pondering something.</p>\n\n<p>\nFarming is not the same trick as professional specialization or writing, but it <em>cascaded</em> to professional specialization and writing, and so the pace of human history picked up enormously after agriculture.&nbsp; Thus I would interpret the story.</p>\n\n<p>From a zoomed-out perspective, cascades can lead to what look like discontinuities in the historical record, <em>even given</em> a steady optimization pressure in the background.&nbsp; It's not that natural selection <em>sped up</em> during hominid evolution.&nbsp; But the search neighborhood contained a low-hanging fruit of high slope... that led to another fruit... which led to another fruit... and so, walking at a constant rate, we fell up the stairs.&nbsp; If you see what I'm saying.</p>\n\n<p><em>Predicting</em> what sort of things are likely to cascade, seems like a very difficult sort of problem.</p>\n\n<p>But I will venture the observation that - with a sample size of one, and an optimization process very different from human thought - there was a cascade in the region of the transition from primate to human intelligence.</p>\n\n<p><strong>Cycles</strong> happen when you connect the output pipe to the input pipe in a <em>repeatable</em> transformation.&nbsp; You might think of them as a special case of cascades with very high regularity.&nbsp; (From which you'll note that in the cases above, I talked about cascades through <em>differing</em> events: farming -&gt; writing.)</p>\n\n<p>The notion of cycles as a source of <em>discontinuity</em> might seem counterintuitive, since it's so regular.&nbsp; But consider this important lesson of history:</p>\n\n<p>Once upon a time, in a squash court beneath Stagg Field at the University of Chicago, physicists were building a shape like a giant doorknob out of alternate layers of graphite and uranium...</p>\n\n<p>The key number for the &quot;pile&quot; is the effective neutron multiplication factor.&nbsp; When a uranium atom splits, it releases neutrons - some right away, some after delay while byproducts decay further.&nbsp; Some neutrons escape the pile, some neutrons strike another uranium atom and cause an additional fission.&nbsp; The effective neutron multiplication factor, denoted <em>k</em>, is the average number of neutrons from a single fissioning uranium atom that cause another fission.&nbsp; At <em>k</em> less than 1, the pile is &quot;subcritical&quot;.&nbsp; At <em>k</em> &gt;= 1, the pile is &quot;critical&quot;.&nbsp; Fermi calculates that the pile will reach <em>k</em>=1 between layers 56 and 57.</p>\n\n<p>On December 2nd in 1942, with layer 57 completed, Fermi orders the final experiment to begin.&nbsp; All but one of the control rods (strips of wood covered with neutron-absorbing cadmium foil) are withdrawn.&nbsp; At 10:37am, Fermi orders the final control rod withdrawn about half-way out.&nbsp; The geiger counters click faster, and a graph pen moves upward.&nbsp; &quot;This is not it,&quot; says Fermi, &quot;the trace will go to this point and level off,&quot; indicating a spot on the graph.&nbsp; In a few minutes the graph pen comes to the indicated point, and does not go above it.&nbsp; Seven minutes later, Fermi orders the rod pulled out another foot.&nbsp; Again the radiation rises, then levels off.&nbsp; The rod is pulled out another six inches, then another, then another.</p>\n\n<p>At 11:30, the slow rise of the graph pen is punctuated by an enormous CRASH - an emergency control rod, triggered by an ionization chamber, activates and shuts down the pile, which is still short of criticality.</p>\n\n<p>Fermi orders the team to break for lunch.</p>\n\n<p>At 2pm the team reconvenes, withdraws and locks the emergency control rod, and moves the control rod to its last setting.&nbsp; Fermi makes some measurements and calculations, then again begins the process of withdrawing the rod in slow increments.&nbsp; At 3:25pm, Fermi orders the rod withdrawn another twelve inches.&nbsp; &quot;This is going to do it,&quot; Fermi says.&nbsp; &quot;Now it will become self-sustaining.&nbsp; The trace will climb and continue to climb.&nbsp; It will not level off.&quot;</p>\n\n<p> Herbert Anderson recounted (as told in Rhodes's <em>The Making of the Atomic Bomb</em>):</p><blockquote><p>&quot;At first you could hear the sound of the neutron counter, clickety-clack, clickety-clack. Then the clicks came more and more rapidly, and after a while they began to merge into a roar; the counter couldn't follow anymore. That was the moment to switch to the chart recorder. But when the switch was made, everyone watched in the sudden silence the mounting deflection of the recorder's pen. It was an awesome silence. Everyone realized the significance of that switch; we were in the high intensity regime and the counters were unable to cope with the situation anymore. Again and again, the scale of the recorder had to be changed to accomodate the neutron intensity which was increasing more and more rapidly. Suddenly Fermi raised his hand. 'The pile has gone critical,' he announced. No one present had any doubt about it.&quot;</p></blockquote><p>Fermi kept the pile running for twenty-eight minutes, with the neutron intensity doubling every two minutes.</p>\n\n<p>That first critical reaction had <em>k</em> of 1.0006.</p>\n\n<p>It might seem that a cycle, with the same thing happening over and over again, ought to exhibit continuous behavior.&nbsp; In one sense it does.&nbsp; But if you pile on one more uranium brick, or pull out the control rod another twelve inches, there's one hell of a big difference between <em>k</em> of 0.9994 and <em>k</em> of 1.0006.</p>\n\n<p>If, rather than being able to calculate, rather than foreseeing and taking cautions, Fermi had just reasoned that 57 layers ought not to behave all that differently from 56 layers - well, it wouldn't have been a good year to be a student at the University of Chicago.</p>\n\n<p>The inexact analogy to the domain of self-improving AI is left as an exercise for the reader, at least for now.</p>\n\n<p>Economists like to measure cycles because they happen repeatedly.&nbsp; You take a potato and an hour of labor and make a potato clock which you sell for two potatoes; and you do this over and over and over again, so an economist can come by and watch how you do it.</p>\n\n<p>As I <a href=\"/lw/vd/intelligence_in_economics/\">noted here at some length</a>, economists are much less likely to go around measuring how many scientific discoveries it takes to produce a <em>new</em> scientific discovery.&nbsp; All the discoveries are individually dissimilar and it's hard to come up with a common currency for them.&nbsp; The analogous problem will prevent a self-improving AI from being <em>directly</em> analogous to a uranium heap, with almost perfectly smooth exponential increase at a calculable rate.&nbsp; You can't apply the same software improvement to the same line of code over and over again, you've got to invent a new improvement each time.&nbsp; But if self-improvements are triggering more self-improvements with great <em>regularity,</em> you might stand a long way back from the AI, blur your eyes a bit, and ask:&nbsp; <em>What is the AI's average</em><em> neutron multiplication factor?</em></p>\n\n\n\n<p>Economics seems to me to be <a href=\"/lw/vd/intelligence_in_economics/\">largely the study of production <em>cycles</em></a> - highly regular repeatable value-adding actions.&nbsp; This doesn't seem to me like a very deep abstraction so far as the study of optimization goes, because it leaves out the creation of <em>novel knowledge </em>and<em> novel designs</em> - further <em>informational </em>optimizations.&nbsp; Or rather, treats productivity improvements as a mostly exogenous factor produced by black-box engineers and scientists.&nbsp; (If I underestimate your power and merely parody your field, by all means inform me what kind of economic study has been done of such things.)&nbsp; (<strong>Answered:</strong>&nbsp; This literature goes by the name &quot;endogenous growth&quot;.&nbsp; See comments <a href=\"http://www.overcomingbias.com/2008/11/cascades-cycles.html#comment-140280102\">starting here</a>.)&nbsp; So far as I can tell, economists do not venture into asking where discoveries <em>come from</em>, leaving the mysteries of the brain to cognitive scientists.</p>\n\n<p>(Nor do I object to this division of labor - it just means that you may have to drag in some extra concepts from outside economics if you want an account <em>of self-improving Artificial Intelligence.</em>&nbsp; Would most economists even object to that statement?&nbsp; But if you think you can do the whole analysis using standard econ concepts, then I'm willing to see it...)</p>\n\n<p><strong>Insight</strong> is that mysterious thing humans do by grokking the search space, wherein one piece of highly abstract knowledge (e.g. Newton's calculus) provides the master key to a huge set of problems.&nbsp; Since humans deal in the compressibility of compressible search spaces (at least the part <em>we</em> can compress) we can bite off huge chunks in one go.&nbsp; This is not mere cascading, where one solution leads to another:</p>\n\n<p>Rather, an &quot;insight&quot; is a chunk of knowledge <em>which, if you possess it, decreases the cost of solving a whole range of governed problems.</em></p>\n\n<p>There's a parable I once wrote - I forget what for, I think ev-bio - which dealt with creatures who'd <em>evolved</em> addition in response to some kind of environmental problem, and not with overly sophisticated brains - so they started with the ability to add 5 to things (which was a significant fitness advantage because it let them solve some of their problems), then accreted another adaptation to add 6 to odd numbers.&nbsp; Until, some time later, there wasn't a <em>reproductive advantage</em> to &quot;general addition&quot;, because the set of special cases covered almost everything found in the environment.</p>\n\n<p>There may be even be a real-world example of this.&nbsp; If you glance at a set, you should be able to instantly distinguish the numbers one, two, three, four, and five, but seven objects in an arbitrary (non-canonical pattern) will take at least one noticeable instant to count.&nbsp; IIRC, it's been suggested that we have hardwired numerosity-detectors but only up to five.</p>\n\n<p>I say all this, to note the difference between evolution nibbling bits off the immediate search neighborhood, versus the human ability to do things in one fell swoop.</p>\n\n<p>Our compression of the search space is also responsible for <em>ideas cascading much more easily than adaptations</em>.&nbsp; We actively examine good ideas, looking for neighbors.</p>\n\n<p>But an insight is higher-level than this; it consists of understanding what's &quot;good&quot; about an idea in a way that divorces it from any single point in the search space.&nbsp; In this way you can crack whole volumes of the solution space in one swell foop.&nbsp; The insight of calculus apart from gravity is again a good example, or the insight of mathematical physics apart from calculus, or the insight of math apart from mathematical physics.</p>\n\n<p>Evolution is not completely barred from making &quot;discoveries&quot; that decrease the cost of a very wide range of further discoveries.&nbsp; Consider e.g. the ribosome, which was capable of manufacturing a far wider range of proteins than whatever it was actually making at the time of its adaptation: this is a general cost-decreaser for a wide range of adaptations.&nbsp; It likewise seems likely that various types of neuron have reasonably-general learning paradigms built into them (gradient descent, Hebbian learning, more sophisticated optimizers) that have been reused for many more problems than they were originally invented for.</p>\n\n<p>A ribosome is something like insight: an item of &quot;knowledge&quot; that tremendously decreases the cost of inventing a wide range of solutions.&nbsp; But even evolution's best &quot;insights&quot; are not quite like the human kind.&nbsp; A sufficiently powerful human insight often approaches a closed form - it doesn't feel like you're <em>exploring</em> even a compressed search space.&nbsp; You just apply the insight-knowledge to whatever your problem, and out pops the now-obvious solution.<br />\n\n\n</p>\n\n<p>Insights have often cascaded, in human history - even\nmajor insights.&nbsp; But they don't quite cycle - you can't repeat the\nidentical pattern Newton used originally to get a new kind of calculus\nthat's twice and then three times as powerful.</p>\n\n<p>Human AI programmers who have insights into intelligence may acquire discontinuous advantages over others who lack those insights.&nbsp; <em>AIs themselves</em> will experience discontinuities in their growth trajectory associated with <em>becoming able to do AI theory itself</em><em> -</em> a watershed moment in the FOOM.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"oiRp4T6u5poc8r9Tj": 2, "sYm3HiWcfZvrGu3ui": 2, "5f5c37ee1b5cdee568cfb2b5": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dq3KsCsqNotWc8nAK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 24, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "1157", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XQirei3crsLxsCQoi", "vNBxmcHpnozjrJnJP", "4uDfpNTrdhEYEb2jm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-25T09:10:38.000Z", "modifiedAt": null, "url": null, "title": "...Recursion, Magic", "slug": "recursion-magic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:08.952Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rJLviHqJMTy8WQkow/recursion-magic", "pageUrlRelative": "/posts/rJLviHqJMTy8WQkow/recursion-magic", "linkUrl": "https://www.lesswrong.com/posts/rJLviHqJMTy8WQkow/recursion-magic", "postedAtFormatted": "Tuesday, November 25th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20...Recursion%2C%20Magic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A...Recursion%2C%20Magic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrJLviHqJMTy8WQkow%2Frecursion-magic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=...Recursion%2C%20Magic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrJLviHqJMTy8WQkow%2Frecursion-magic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrJLviHqJMTy8WQkow%2Frecursion-magic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1642, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/w5/cascades_cycles_insight/\">Cascades, Cycles, Insight...</a></p>\n<p><em>...4, 5 sources of discontinuity.</em></p>\n<p><strong>Recursion</strong> is probably the most difficult part of this topic.&nbsp; We have historical records aplenty of <em>cascades,</em> even if untangling the causality is difficult.&nbsp; <em>Cycles</em> of reinvestment are the heartbeat of the modern economy.&nbsp; An <em>insight</em> that makes a hard problem easy, is something that I hope you've experienced at least once in your life...</p>\n<p>But we don't have a whole lot of experience redesigning our own neural circuitry.</p>\n<p>We have these wonderful things called \"optimizing compilers\".&nbsp; A compiler translates programs in a high-level language, into machine code (though these days it's often a virtual machine).&nbsp; An \"optimizing compiler\", obviously, is one that improves the program as it goes.</p>\n<p>So why not write an optimizing compiler <em>in its own language</em>, and then <em>run it on itself?</em>&nbsp; And then use the resulting <em>optimized optimizing compiler,</em> to recompile itself yet <em>again,</em> thus producing an <em>even more optimized optimizing compiler -</em></p>\n<p>Halt!&nbsp; Stop!&nbsp; Hold on just a minute!&nbsp; An optimizing compiler is not supposed to change the logic of a program - the input/output relations.&nbsp; An optimizing compiler is only supposed to produce code that does <em>the same thing, only faster.</em>&nbsp; A compiler isn't remotely near understanding what the program is <em>doing</em> and why, so it can't presume to construct <em>a better input/output function</em>.&nbsp; We just presume that the programmer wants a fixed input/output function computed as fast as possible, using as little memory as possible.</p>\n<p>So if you run an optimizing compiler on its own source code, and then use the product to do the same again, it should produce the <em>same output</em> on both occasions - at most, the first-order product will run <em>faster</em> than the original compiler.</p>\n<p>If we want a computer program that experiences <em>cascades</em> of self-improvement, the path of the optimizing compiler does not lead there - the \"improvements\" that the optimizing compiler makes upon itself, do not <em>improve its ability to improve itself</em>.</p>\n<p><a id=\"more\"></a></p>\n<p>Now if you are one of those annoying nitpicky types, like me, you will notice a flaw in this logic: suppose you built an optimizing compiler that searched over a sufficiently wide range of possible optimizations, that it did not ordinarily have <em>time</em> to do a full search of its own space - so that, when the optimizing compiler ran out of time, it would just implement whatever speedups it had already discovered.&nbsp; Then the optimized optimizing compiler, although it would only implement the same logic faster, would do more optimizations in the same time - and so the second output would not equal the first output.</p>\n<p>Well... that probably doesn't buy you much.&nbsp; Let's say the optimized program is 20% faster, that is, it gets 20% more done in the same time.&nbsp; Then, unrealistically assuming \"optimization\" is linear, the 2-optimized program will be 24% faster, the 3-optimized program will be 24.8% faster, and so on until we top out at a 25% improvement.&nbsp; <a href=\"/lw/w5/cascades_cycles_insight/\"><em>k</em> &lt; 1</a>.</p>\n<p>So let us turn aside from optimizing compilers, and consider a more interesting artifact, EURISKO.</p>\n<p>To the best of my inexhaustive knowledge, EURISKO may <em>still</em> be the most sophisticated self-improving AI ever built - in the 1980s, by Douglas Lenat before he started wasting his life on Cyc.&nbsp; EURISKO was applied in domains ranging from the <a href=\"http://web.archive.org/web/20100123135422/http://www.aliciapatterson.org/APF0704/Johnson/Johnson.html\">Traveller war game</a> (EURISKO became champion without having ever before fought a human) to VLSI circuit design.</p>\n<p>EURISKO used \"heuristics\" to, for example, design potential space fleets.&nbsp; It also had <em>heuristics for suggesting new heuristics</em>, and metaheuristics could apply to any heuristic, including metaheuristics.&nbsp; E.g. EURISKO started with the heuristic \"investigate extreme cases\" but moved on to \"investigate cases close to extremes\".&nbsp; The heuristics were written in RLL, which stands for Representation Language Language.&nbsp; According to Lenat, it was figuring out how to represent the heuristics in such fashion that they could usefully modify themselves without always just breaking, that consumed most of the conceptual effort in creating EURISKO.</p>\n<p>But EURISKO did not go foom.</p>\n<p>EURISKO could modify even the metaheuristics that modified heuristics.&nbsp; EURISKO was, in an important sense, more recursive than either humans or natural selection - a new thing under the Sun, a cycle more closed than anything that had ever existed in this universe.</p>\n<p>Still, EURISKO ran out of steam.&nbsp; Its self-improvements did not spark a sufficient number of new self-improvements.&nbsp; This should not really be too surprising - it's not as if EURISKO started out with human-level intelligence <em>plus</em> the ability to modify itself - its self-modifications were either <a href=\"/lw/kt/evolutions_are_stupid_but_work_anyway/\">evolutionarily blind</a>, or produced by the simple procedural rules of some heuristic or other.&nbsp; That's not going to navigate the search space very fast on an atomic level.&nbsp; Lenat did not stand dutifully apart from his creation, but stepped in and helped EURISKO prune its own heuristics.&nbsp; But in the end EURISKO ran out of steam, and Lenat couldn't push it any further.</p>\n<p>EURISKO lacked what I called \"insight\" - that is, the type of abstract knowledge that lets humans fly through the search space.&nbsp; And so its recursive <em>access</em> to its own heuristics proved to be for nought.</p>\n<p>Unless, y'know, you're counting becoming world champion at Traveller without ever previously playing a human, as some sort of accomplishment.</p>\n<p>But it is, thankfully, a little harder<em>&nbsp;</em>than that to destroy the world - as Lenat's experimental test informed us.</p>\n<p>Robin previously asked why <a href=\"http://www.overcomingbias.com/2008/11/engelbarts-uber.html\">Douglas Engelbart did not take over the world</a>, despite his vision of a team building tools to improve tools, and his anticipation of tools like computer mice and hypertext.</p>\n<p>One reply would be, \"Sure, a computer gives you a 10% advantage in doing various sorts of problems, some of which include computers - but there's still a lot of work that the computer <em>doesn't</em> help you with - and the mouse doesn't run off and write better mice entirely on its own - so <em>k</em> &lt; 1, and it still takes large amounts of human labor to advance computer technology as a whole - plus a lot of the interesting knowledge is nonexcludable so it's hard to capture the value you create - and that's why Buffett could manifest a better take-over-the-world-with-sustained-higher-interest-rates than Engelbart.\"</p>\n<p>But imagine that Engelbart had built a computer mouse, and discovered that each click of the mouse raised his IQ by one point.&nbsp; Then, perhaps, we would have had a <em>situation</em> on our hands.</p>\n<p>Maybe you could diagram it something like this:</p>\n<ol>\n<li>Metacognitive level:&nbsp; <a href=\"/lw/kr/an_alien_god/\">Evolution</a> is the metacognitive algorithm which produced the wiring patterns and low-level developmental rules for human brains.</li>\n<li>Cognitive level:&nbsp; The brain processes its knowledge (including procedural knowledge) using algorithms that quite mysterious to the user within them.&nbsp; Trying to program AIs with the sort of instructions humans give each other usually proves not to do anything: <a href=\"/lw/sp/detached_lever_fallacy/\">the machinery activated by the levers is missing</a>.</li>\n<li>Metaknowledge level:&nbsp; Knowledge and skills associated with e.g. \"science\" as an activity to carry out using your brain - instructing you <em>when</em> to try to think of new hypotheses using your mysterious creative abilities.</li>\n<li>Knowledge level:&nbsp; Knowing how gravity works, or how much weight steel can support.</li>\n<li>Object level:&nbsp; Specific actual problems, like building a bridge or something.</li>\n</ol>\n<p>This is a <em>causal</em> tree, and changes at levels <em>closer to root</em> have greater impacts as the effects cascade downward.</p>\n<p>So one way of looking at it is:&nbsp; \"A computer mouse isn't recursive enough.\"</p>\n<p>This is an issue that I need to address at further length, but for today I'm out of time.</p>\n<p><strong>Magic</strong> is the final factor I'd like to point out, at least for now, in considering sources of discontinuity for self-improving minds.&nbsp; By \"magic\" I naturally do not refer to <a href=\"/lw/tv/excluding_the_supernatural/\">this</a>.&nbsp; Rather, \"magic\" in the sense that if you asked 19th-century Victorians what they thought the future would bring, they would have talked about flying machines or gigantic engines, and a very few true visionaries would have suggested space travel or Babbage computers.&nbsp; Nanotechnology, not so much.</p>\n<p>The future has a reputation for accomplishing feats which the past thought impossible.&nbsp; Future civilizations have even broken what past civilizations thought (incorrectly, of course) to be the laws of physics.&nbsp; If prophets of 1900 AD - never mind 1000 AD - had tried to bound the powers of human civilization a billion years later, some of those impossibilities would have been accomplished before the century was out; transmuting lead into gold, for example.&nbsp; Because we remember future civilizations surprising past civilizations, it has become cliche that we can't put limits on our great-grandchildren.</p>\n<p>And yet everyone in the 20th century, in the 19th century, and in the 11th century, was human.&nbsp; There is also the sort of magic that a human gun is to a wolf, or the sort of magic that human genetic engineering is to natural selection.</p>\n<p>To \"improve your own capabilities\" is an instrumental goal, and if a smarter intelligence than my own is focused on that goal, <a href=\"/lw/v7/expected_creative_surprises/\">I should expect to be surprised</a>.&nbsp; The mind may find ways to produce <em>larger jumps</em> in capability than I can visualize myself.&nbsp; Where higher creativity than mine is at work and looking for shorter shortcuts, the discontinuities that <em>I</em> imagine may be dwarfed by the discontinuities that <em>it</em> can imagine.</p>\n<p>And remember how <em>little</em> progress it takes - just a hundred years of human time, with everyone still human - to turn things that would once have been \"unimaginable\" into heated debates about feasibility.&nbsp; So if you build a mind smarter than you, and it thinks about how to go FOOM quickly, and it goes FOOM <em>faster than you imagined possible,</em> you really have no right to complain - based on the history of mere human history, you should have expected a significant probability of being surprised.&nbsp; Not, surprised that the nanotech is 50% faster than you thought it would be.&nbsp; Surprised the way the Victorians would have been surprised by nanotech.</p>\n<p>Thus the last item on my (current, somewhat ad-hoc) list of reasons to expect discontinuity:&nbsp; Cascades, cycles, insight, recursion, magic.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 2, "oiRp4T6u5poc8r9Tj": 2, "ac84EpK6mZbPLzmqj": 2, "5f5c37ee1b5cdee568cfb2b5": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rJLviHqJMTy8WQkow", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 23, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "1158", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dq3KsCsqNotWc8nAK", "jAToJHtg39AMTAuJo", "pLRogvJLPPg6Mrvg4", "zY4pic7cwQpa9dnyk", "u6JzcFtPGiznFgDxP", "rEDpaTTEzhPLz4fHh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-25T21:47:18.000Z", "modifiedAt": null, "url": null, "title": "The Complete Idiot's Guide to Ad Hominem", "slug": "the-complete-idiot-s-guide-to-ad-hominem", "viewCount": null, "lastCommentedAt": "2021-10-11T03:49:40.088Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pXpt2HqCqxqjeZc4u/the-complete-idiot-s-guide-to-ad-hominem", "pageUrlRelative": "/posts/pXpt2HqCqxqjeZc4u/the-complete-idiot-s-guide-to-ad-hominem", "linkUrl": "https://www.lesswrong.com/posts/pXpt2HqCqxqjeZc4u/the-complete-idiot-s-guide-to-ad-hominem", "postedAtFormatted": "Tuesday, November 25th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Complete%20Idiot's%20Guide%20to%20Ad%20Hominem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Complete%20Idiot's%20Guide%20to%20Ad%20Hominem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpXpt2HqCqxqjeZc4u%2Fthe-complete-idiot-s-guide-to-ad-hominem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Complete%20Idiot's%20Guide%20to%20Ad%20Hominem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpXpt2HqCqxqjeZc4u%2Fthe-complete-idiot-s-guide-to-ad-hominem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpXpt2HqCqxqjeZc4u%2Fthe-complete-idiot-s-guide-to-ad-hominem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<p>Stephen Bond writes the definitive word on <em>ad hominem</em> in &quot;<a href=\"http://plover.net/~bonds/adhominem.html\">the ad hominem fallacy fallacy</a>&quot;:</p><blockquote><p>In reality, ad hominem is unrelated to sarcasm or personal abuse.&nbsp; <em>Argumentum \nad hominem</em> is the logical fallacy of attempting to undermine a speaker's \nargument by attacking the speaker instead of addressing the argument.&nbsp; The mere \npresence of a personal attack does not indicate ad hominem: the attack must be \nused for the purpose of undermining the argument, or otherwise the logical fallacy \nisn't there.</p>\n\n<p>[...]</p><blockquote><p><em>\nA: &quot;All rodents are mammals, but a weasel isn't a rodent, so it can't be\na mammal.&quot;\n<br />B: &quot;You evidently know nothing about logic. This does not logically follow.&quot;\n</em></p></blockquote>\n\n<p>B's argument is still not ad hominem.&nbsp; B does not imply that A's \nsentence does not logically follow <em>because</em> A knows nothing about logic.&nbsp; B \nis still addressing the substance of A's argument...</p></blockquote><p>This is too beautiful, thorough, and precise to not post.&nbsp; HT to <a href=\"http://news.ycombinator.com/item?id=376670\">sfk on HN</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dJ6eJxJrCEget7Wb6": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pXpt2HqCqxqjeZc4u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 4.595938953324923e-07, "legacy": true, "legacyId": "1159", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-26T08:31:09.000Z", "modifiedAt": null, "url": null, "title": "Engelbart: Insufficiently Recursive", "slug": "engelbart-insufficiently-recursive", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:02.533Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NCb28Xdv7xDajtqtS/engelbart-insufficiently-recursive", "pageUrlRelative": "/posts/NCb28Xdv7xDajtqtS/engelbart-insufficiently-recursive", "linkUrl": "https://www.lesswrong.com/posts/NCb28Xdv7xDajtqtS/engelbart-insufficiently-recursive", "postedAtFormatted": "Wednesday, November 26th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Engelbart%3A%20Insufficiently%20Recursive&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEngelbart%3A%20Insufficiently%20Recursive%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNCb28Xdv7xDajtqtS%2Fengelbart-insufficiently-recursive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Engelbart%3A%20Insufficiently%20Recursive%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNCb28Xdv7xDajtqtS%2Fengelbart-insufficiently-recursive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNCb28Xdv7xDajtqtS%2Fengelbart-insufficiently-recursive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2131, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/w5/cascades_cycles_insight/\">Cascades, Cycles, Insight</a>, <a href=\"/lw/w6/recursion_magic/\">Recursion, Magic</a><br /><strong>Reply to</strong>:&nbsp; <a href=\"http://www.overcomingbias.com/2008/11/engelbarts-uber.html\">Engelbart As Ubertool?</a></p>\n<p>When Robin originally <a href=\"http://www.overcomingbias.com/2008/11/engelbarts-uber.html\">suggested</a> that Douglas Engelbart, best known as the inventor of the computer mouse, would have been a good candidate for taking over the world via <a href=\"http://www.overcomingbias.com/2008/11/fund-ubertool.html\">compound interest on tools that make tools</a>, my initial reaction was \"What on Earth?&nbsp; With a <em>mouse?</em>\"</p>\n<p>On reading the initial portions of Engelbart's \"<a href=\"http://www.bootstrap.org/augdocs/friedewald030402/augmentinghumanintellect/AHI62.pdf\">Augmenting Human Intellect: A Conceptual Framework</a>\", it became a lot clearer where Robin was coming from.</p>\n<p>Sometimes it's hard to see through the eyes of the past.&nbsp; Engelbart was a computer pioneer, and in the days when all these things were just getting started, he had a vision of using computers to systematically augment human intelligence.&nbsp; That was what he thought computers were <em>for</em>.&nbsp; That was the ideology lurking behind the mouse.&nbsp; Something that makes its users smarter - now that sounds a bit more plausible as an UberTool.</p>\n<p>Looking back at Engelbart's plans with benefit of hindsight, I see two major factors that stand out:</p>\n<ol>\n<li>Engelbart committed the Classic Mistake of AI: underestimating how much cognitive work gets done by hidden algorithms running beneath the surface of introspection, and overestimating what you can do by fiddling with the <a href=\"/lw/sp/detached_lever_fallacy/\">visible control levers</a>.</li>\n<li>Engelbart <a href=\"/lw/j7/anchoring_and_adjustment/\">anchored</a> on the way that someone <em>as intelligent as Engelbart</em> would use computers, but there was only one of him - and due to point 1 above, he couldn't use computers to make other people as smart as him.</li>\n</ol>\n<p><a id=\"more\"></a></p>\n<p>To start with point 2:&nbsp; They had more reverence for computers back in the old days.&nbsp; Engelbart visualized a system carefully designed to flow with every step of a human's work and thought, assisting every iota it could manage along the way.&nbsp; And the human would be trained to work with the computer, the two together dancing a seamless dance.</p>\n<p>And the problem with this, was not <em>just</em> that computers got cheaper and that programmers wrote their software more hurriedly.</p>\n<p>There's a now-legendary story about <a href=\"http://moishelettvin.blogspot.com/2006/11/windows-shutdown-crapfest.html\">the Windows Vista shutdown menu</a>, a simple little feature into which 43 different Microsoft people had input.&nbsp; The debate carried on for over a year.&nbsp; The final product ended up as the lowest common denominator - a couple of hundred lines of code and a very visually unimpressive menu.</p>\n<p>So even when lots of people spent a tremendous amount of time thinking about a single feature of the system - it still didn't end up very impressive.&nbsp; Jef Raskin could have done better than that, I bet.&nbsp; But Raskins and Engelbarts are rare.</p>\n<p>You see the same effect in <a href=\"http://www.e-drexler.com/d/06/00/EOC/EOC_Chapter_14.html\">Eric Drexler's chapter on hypertext in </a><em><a href=\"http://www.e-drexler.com/d/06/00/EOC/EOC_Chapter_14.html\">Engines of Creation</a>:</em>&nbsp; Drexler imagines the power of the Web to... use two-way links and user annotations to promote informed criticism.&nbsp; (<a href=\"/lw/j1/stranger_than_history/\">As opposed to the way we actually use it.</a>)&nbsp; And if the average Web user were Eric Drexler, the Web probably <em>would</em> work that way by now.</p>\n<p>But no piece of software that has yet been developed, by mouse or by Web, can turn an average human user into Engelbart or Raskin or Drexler.&nbsp; You would very probably have to reach into the brain and rewire neural circuitry directly; I don't think <em>any</em> sense input or motor interaction would accomplish such a thing.</p>\n<p>Which brings us to point 1.</p>\n<p>It does look like Engelbart was under the spell of the \"<a href=\"/lw/vt/the_nature_of_logic/\">logical</a>\" paradigm that prevailed in AI at the time he made his plans.&nbsp; (Should he even lose points for that?&nbsp; He went with the mainstream of that science.)&nbsp; He did not see it as an <a href=\"/lw/un/on_doing_the_impossible/\">impossible</a> problem to have computers help humans <em>think</em> - he seems to have underestimated the difficulty in much the same way that the field of AI once severely underestimated the work it would take to make computers themselves solve cerebral-seeming problems.&nbsp; (Though I am saying this, reading heavily between the lines of one single paper that he wrote.)&nbsp; He talked about how the core of thought is symbols, and speculated on how computers could help people manipulate those symbols.</p>\n<p>I have already said much on why people tend to underestimate the amount of serious heavy lifting that gets done by cognitive algorithms hidden inside black boxes that run out of your introspective vision, and overestimating what you can do by duplicating the easily visible introspective control levers.&nbsp; The word \"apple\", for example, is a visible lever; you can say it or not say it, <a href=\"/lw/sp/detached_lever_fallacy/\">its presence or absence is salient</a>.&nbsp; The algorithms of a visual cortex that let you visualize what an apple would look like upside-down - we all have these in common, and they are not introspectively accessible.&nbsp; Human beings knew about apples a long, long time before they knew there was even such a thing as the visual cortex, let alone beginning to unravel the algorithms by which it operated.</p>\n<p>Robin Hanson <a href=\"http://www.overcomingbias.com/2008/11/recursion-magic.html#comment-140356756\">asked</a> me:</p>\n<blockquote><span id=\"comment-140356756-content\">\n<p>\"You really think an office worker with modern computer tools is only 10% more productive than one with 1950-era non-computer tools?&nbsp; Even at the task of creating better computer tools?\"</p>\n</span></blockquote>\n<p>But remember the parable of the optimizing compiler run on its own source code - maybe it makes itself 50% faster, but only once; the changes don't increase its ability to make future changes.&nbsp; So indeed, we should not be too impressed by a 50% increase in office worker productivity - not for purposes of asking about FOOMs.&nbsp; We should ask whether that increase in productivity translates into tools that create further increases in productivity.</p>\n<p>And this is where the problem of underestimating hidden labor, starts to bite.&nbsp; Engelbart rhapsodizes (accurately!) on the wonders of being able to cut and paste text while writing, and how superior this should be compared to the typewriter.&nbsp; But suppose that Engelbart overestimates, by a factor of 10, how much of the intellectual labor of writing goes into fighting the typewriter.&nbsp; Then because Engelbart can only help you cut and paste more easily, and <em>cannot</em> rewrite those hidden portions of your brain that labor to come up with good sentences and good arguments, the actual improvement he delivers is a tenth of what he thought it would be.&nbsp; An anticipated 20% improvement becomes an actual 2% improvement.&nbsp; <em>k</em> way less than 1.</p>\n<p>This will hit particularly hard if you think that computers, with some hard work on the user interface, and some careful training of the humans, ought to be able to help humans with the type of \"creative insight\" or \"scientific labor\" that goes into <em>inventing new things to do with the computer.</em>&nbsp; If you thought that the surface symbols were where most of the intelligence resided, you would anticipate that computer improvements would hit back hard to this meta-level, and create people who were more scientifically creative and who could design even better computer systems.</p>\n<p>But if really you can only help people <em>type up</em> their ideas, while all the hard creative labor happens in the shower thanks to very-poorly-understood cortical algorithms - then you are much less like neutrons cascading through uranium, and much more like an optimizing compiler that gets a single speed boost and no more.&nbsp; It looks like the person is 20% more productive, but in the aspect of intelligence that potentially <em>cascades to further improvements</em> they're only 2% more productive, if that.</p>\n<p>(Incidentally... I once met a science-fiction author of a previous generation, and mentioned to him that the part of my writing I most struggled with, was my tendency to revise and revise and revise things I had already written, instead of writing new things.&nbsp; And he said, \"Yes, that's why I went back to the typewriter.&nbsp; The word processor made it too easy to revise things; I would do too much polishing, and writing stopped being fun for me.\"&nbsp; It made me wonder if there'd be demand for an <em>author's word processor </em>that wouldn't let you revise anything until you finished your first draft.</p>\n<p>But this could be chalked up to the humans not being trained as carefully, nor the software designed as carefully, as in the process Engelbart envisioned.)</p>\n<p>Engelbart wasn't trying to take over the world <em>in person,</em> or with a small group.&nbsp; Yet had he <em>tried</em> to go the <a href=\"http://www.overcomingbias.com/2008/11/fund-ubertool.html\">UberTool</a> route, we can reasonably expect he would have failed - that is, failed at advancing far beyond the outside world in internal computer technology, while selling only UberTool's services to outsiders.</p>\n<p>Why?&nbsp; Because it takes too much <em>human</em> labor to develop computer software and computer hardware, and this labor cannot be automated away as a one-time cost.&nbsp; If the world outside your window has a thousand times as many brains, a 50% productivity boost that only cascades to a 10% and then a 1% additional productivity boost, will not let you win against the world.&nbsp; If your UberTool was <em>itself a mind,</em> if cascades of self-improvement could <em>fully</em> automate away more and more of the <em>intellectual</em> labor performed by the outside world - then it would be a different story.&nbsp; But while the development path wends inexorably through thousands and millions of engineers, and you <em>can't</em> divert that path through an internal computer, you're not likely to pull far ahead of the world.&nbsp; You can just choose between giving your own people a 10% boost, or selling your product on the market to give lots of people a 10% boost.</p>\n<p>You can have trade secrets, and sell only your services or products - many companies follow that business plan; any company that doesn't sell its source code does so.&nbsp; But this is just keeping one small advantage to yourself, and adding that as a cherry on top of the technological progress handed you by the outside world.&nbsp; It's not having more technological progress inside than outside.</p>\n<p>If you're getting most of your technological progress <em>handed to you</em> - your resources not being sufficient to do it in-house - then you won't be able to apply your private productivity improvements to most of your actual velocity, since most of your actual velocity will come from outside your walls.&nbsp; If you only create 1% of the progress that you use, then a 50% improvement becomes a 0.5% improvement.&nbsp; The domain of potential recursion and potential cascades is much smaller, diminishing <em>k</em>.&nbsp; As if only 1% of the uranium <em>generating</em> your neutrons, were available for <em>chain reactions</em> to be fissioned further.</p>\n<p>We don't live in a world that cares intensely about milking every increment of velocity out of scientific progress.&nbsp; A 0.5% improvement is easily lost in the noise.&nbsp; Corporations and universities routinely put obstacles in front of their internal scientists that cost them more than 10% of their potential.&nbsp; This is one of those problems where not everyone is Engelbart (and you can't just rewrite their source code either).</p>\n<p>For completeness, I should mention that there are generic obstacles to pulling an UberTool.&nbsp; Warren Buffett has gotten a sustained higher interest rate than the economy at large, and is widely <em>believed</em> to be capable of doing so indefinitely.&nbsp; In principle, the economy could have invested hundreds of billions of dollars as soon as Berkshire Hathaway had a sufficiently long track record to rule out chance.&nbsp; Instead, Berkshire has grown mostly by compound interest.&nbsp; We <em>could</em> live in a world where asset allocations were ordinarily given as a mix of stocks, bonds, real estate, and Berkshire Hathaway.&nbsp; We don't live in that world for a number of reasons: financial advisors not wanting to make themselves appear irrelevant, strange personal preferences on the part of Buffett...</p>\n<p>The economy doesn't always do the obvious thing, like flow money into Buffett until his returns approach the average return of the economy.&nbsp; Interest rate differences much higher than 0.5%, on matters that people care about far more intensely than Science, are ignored if they're not presented in exactly the right format to be seized.</p>\n<p>And it's not easy for individual scientists or groups to capture the value created by scientific progress.&nbsp; Did Einstein die with 0.1% of the value that he created?&nbsp; Engelbart in particular doesn't seem to have <em>tried</em> to be Bill Gates, at least not as far as I know.</p>\n<p>With that in mind - in one sense Engelbart succeeded at a good portion of what he <em>actually set out</em> to do: computer mice <em>did </em>take over the world.</p>\n<p>But it was a broad slow cascade that mixed into the usual exponent of economic growth.&nbsp; Not a concentrated fast FOOM.&nbsp; To produce a concentrated FOOM, you've got to be able to swallow as much as possible of the processes <em>driving</em> the FOOM <em>into</em> the FOOM.&nbsp; Otherwise you can't improve those processes and you can't cascade through them and your <em>k</em> goes down.&nbsp; Then your interest rates won't even be as much higher than normal as, say, Warren Buffett's.&nbsp; And there's no grail to be <em>won,</em> only profits to be made:&nbsp; If you have no realistic hope of beating the world, you may as well join it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sPpZRaxpNNJjw55eu": 1, "5f5c37ee1b5cdee568cfb2b5": 1, "oiRp4T6u5poc8r9Tj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NCb28Xdv7xDajtqtS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 19, "extendedScore": null, "score": 2.9e-05, "legacy": true, "legacyId": "1160", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dq3KsCsqNotWc8nAK", "rJLviHqJMTy8WQkow", "zY4pic7cwQpa9dnyk", "bMkCEZoBNhgRBtzoj", "h3vdnR34ZvohDEFT5", "c93eRh3mPaN62qrD2", "fpecAJLG9czABgCe9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-27T09:54:00.000Z", "modifiedAt": null, "url": null, "title": "Total Nano Domination", "slug": "total-nano-domination", "viewCount": null, "lastCommentedAt": "2020-12-09T20:18:02.635Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5hX44Kuz5No6E6RS9/total-nano-domination", "pageUrlRelative": "/posts/5hX44Kuz5No6E6RS9/total-nano-domination", "linkUrl": "https://www.lesswrong.com/posts/5hX44Kuz5No6E6RS9/total-nano-domination", "postedAtFormatted": "Thursday, November 27th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Total%20Nano%20Domination&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATotal%20Nano%20Domination%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5hX44Kuz5No6E6RS9%2Ftotal-nano-domination%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Total%20Nano%20Domination%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5hX44Kuz5No6E6RS9%2Ftotal-nano-domination", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5hX44Kuz5No6E6RS9%2Ftotal-nano-domination", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2930, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/w8/engelbart_insufficiently_recursive/\">Engelbart: Insufficiently Recursive</a></p>\n\n<p>The computer revolution had <a href=\"/lw/w5/cascades_cycles_insight/\">cascades and insights</a> aplenty.&nbsp; Computer tools are routinely used to create tools, from using a C compiler to write a Python interpreter, to using theorem-proving software to help design computer chips.&nbsp; I would not <em>yet</em> rate computers as being very deeply <a href=\"/lw/w6/recursion_magic/\"><em>recursive</em></a> - I don't think they've improved our own thinking processes even so much as the Scientific Revolution - <em>yet.</em>&nbsp; But some of the ways that computers are used to improve computers, verge on being repeatable (<a href=\"/lw/w5/cascades_cycles_insight/\">cyclic</a>).</p>\n\n<p>Yet no individual, no localized group, nor even country, managed to get a sustained advantage in computing power, compound the interest on cascades, and take over the world.&nbsp; There was never a Manhattan moment when a computing advantage <em>temporarily</em> gave one country a supreme military advantage, like the US and its atomic bombs for that brief instant at the end of WW2.&nbsp; In computing there was no equivalent of &quot;We've just crossed <a href=\"/lw/w5/cascades_cycles_insight/\">the\nsharp threshold of criticality</a>, and now our pile doubles its neutron\noutput every <em>two minutes,</em> so we can produce lots of plutonium and you can't.&quot;</p>\n\n<p>Will the development of nanotechnology go the same way as computers - a smooth, steady developmental curve spread across many countries, no one project taking into itself a substantial fraction of the world's whole progress?&nbsp; Will it be more like the Manhattan Project, one country gaining a (temporary?) huge advantage at huge cost?&nbsp; Or could a small group with an initial advantage cascade and outrun the world?</p><a id=\"more\"></a><p>Just to make it clear why we might worry about this for nanotech, rather than say car manufacturing - if you can build things from atoms,\nthen the environment contains an unlimited supply of perfectly machined\nspare parts.&nbsp; If your molecular factory can build solar cells, it can acquire energy as well.</p>\n\n<p>So full-fledged Drexlerian <a href=\"http://en.wikipedia.org/wiki/Molecular_nanotechnology\">molecular nanotechnology</a> can plausibly automate away much of the <em>manufacturing</em> in its <em>material</em>\nsupply chain.&nbsp; If you already have nanotech, you may not need to\nconsult the outside economy for inputs of energy or raw material.</p>\n\n<p>This makes it more plausible that a nanotech group could localize off, and do its own compound interest, away from the global economy.&nbsp; If you're Douglas Engelbart building better software, you still need to consult Intel for the hardware that runs your software, and the electric company for the electricity that powers your hardware.&nbsp; It would be a <em>considerable expense</em> to build your own fab lab for\nyour chips (that makes chips as good as Intel) and your own power\nstation for electricity (that supplies electricity as cheaply as the\nutility company).</p>\n\n<p>It's not just that this tends to entangle you with the fortunes of your trade partners, but also that - as an UberTool Corp keeping your trade secrets in-house - you can't improve the hardware you get, or drive down the cost of electricity, as long as these things are done outside.&nbsp; Your cascades can only go through what you do locally, so the more you do locally, the more likely you are to get a compound interest advantage.&nbsp; (Mind you, I don't think Engelbart could have gone\nFOOM even if he'd made his chips locally and supplied himself with\nelectrical power - I just don't think the compound advantage on using\ncomputers to make computers is powerful enough to sustain <a href=\"/lw/w5/cascades_cycles_insight/\"><em>k</em> &gt; 1</a>.)</p>\n\n<p>In general, the more capabilities are localized into one place, the less people will depend on their trade partners, the more they can cascade locally (apply their improvements to yield further improvements), and the more a &quot;critical cascade&quot; / FOOM sounds plausible.</p>\n\n<p>Yet self-replicating nanotech is a very <em>advanced</em> capability.&nbsp; You don't get it right off the bat.&nbsp; Sure, lots of biological stuff has this capability, but this is a misleading coincidence - it's not that self-replication is <em>easy,</em> but that evolution, <em>for its own <a href=\"/lw/kr/an_alien_god/\">alien</a> reasons</em>, tends to build it into everything.&nbsp; (Even individual cells, which is ridiculous.)</p>\n\n<p>In the <em>run-up</em> to nanotechnology, it seems not implausible to suppose a continuation of the modern world.&nbsp; Today, many different labs work on small pieces of nanotechnology - fortunes entangled with their trade partners, and much of their research velocity coming from advances in other laboratories.&nbsp; Current nanotech labs are dependent on the outside world for computers, equipment, science, electricity, and food; any single lab works on a small fraction of the puzzle, and contributes small fractions of the progress.</p>\n\n<p>In short, so far nanotech is going just the same way as computing.</p>\n\n<p>But it is a tad <a href=\"/lw/km/motivated_stopping_and_motivated_continuation/\">premature</a> - I would even say that it crosses the line into the &quot;silly&quot; species of futurism - to exhale a sigh of relief and say, &quot;Ah, that settles it - no need to consider any further.&quot;</p>\n\n<p>We all know how exponential multiplication works:&nbsp; 1 microscopic nanofactory, 2 microscopic nanofactories, 4 microscopic nanofactories... let's say there's 100 different groups working on self-replicating nanotechnology and one of those groups succeeds <em>one week earlier</em> than the others.&nbsp; <a href=\"http://www.foresight.org/nano/Ecophagy.html\">Rob Freitas</a> has calculated that some species of replibots could spread through the Earth in 2 days (even given what seem to me like highly conservative assumptions in a context where conservatism is not appropriate).</p>\n\n<p>So, even if the race seems very tight, whichever group gets replibots <em>first</em> can take over the world given a mere week's lead time -</p>\n\n<p>Yet wait!&nbsp; Just having replibots doesn't let you take over the world.&nbsp; You need fusion weapons, or surveillance bacteria, or some other way to actually <em>govern.</em>&nbsp; That's a lot of matterware - a lot of design and engineering work.&nbsp; A replibot advantage doesn't equate to a weapons advantage, unless, somehow, the planetary economy has already published the open-source details of fully debugged weapons that you can build with your newfound private replibots.&nbsp; Otherwise, a lead time of one week might not be anywhere near enough.</p>\n\n<p>Even more importantly - &quot;self-replication&quot; is not a binary, 0-or-1 attribute.&nbsp; Things can be partially self-replicating.&nbsp; You can have something that manufactures 25% of itself, 50% of itself, 90% of itself, or 99% of itself - but still needs one last expensive computer chip to complete the set.&nbsp; So if you have twenty-five countries racing, sharing some of their results and withholding others, there isn't <em>one morning</em> where you wake up and find that one country has self-replication.</p>\n\n<p>Bots become successively easier to manufacture; the factories get successively cheaper.&nbsp; By the time one country has bots that manufacture themselves from environmental materials, many other countries have bots that manufacture themselves from feedstock.&nbsp; By the time one country has bots that manufacture themselves entirely from feedstock, other countries have produced some bots using assembly lines.&nbsp; The nations also have all their old conventional arsenal, such as intercontinental missiles tipped with thermonuclear weapons, and these have deterrent effects against crude nanotechnology.&nbsp; No one ever gets a <em>discontinuous</em> military advantage, and the world is safe.&nbsp; (?)</p>\n\n<p>At this point, I do feel obliged to recall the notion of &quot;<a href=\"/lw/jk/burdensome_details/\">burdensome details</a>&quot;, that we're spinning a story out of many conjunctive details, any one of which could go wrong.&nbsp; This is not an argument in favor of anything in particular, just a reminder not to be seduced by stories that are too specific.&nbsp; When I contemplate the sheer raw power of nanotechnology, I don't feel confident that the fabric of society can even survive the <em>sufficiently plausible prospect</em> of its near-term arrival.&nbsp; If your intelligence estimate says that Russia (the new belligerent Russia under Putin) is going to get self-replicating nanotechnology in a year, what does that do to Mutual Assured Destruction?&nbsp; What if Russia makes a similar intelligence assessment of the US?&nbsp; What happens to the capital markets?&nbsp; I can't even foresee how our world will react to the <em>prospect</em> of various nanotechnological capabilities as they promise to be developed in the future's near future.&nbsp; Let alone envision how society would <em>actually change</em> as full-fledged molecular nanotechnology was developed, even if it were developed gradually...</p>\n\n<p>...but I suppose the Victorians might say the same thing about nuclear weapons or computers, and yet we still have a global economy - one that's actually lot more interdependent than theirs, thanks to nuclear weapons making small wars less attractive, and computers helping to coordinate trade.</p>\n\n<p>I'm willing to believe in the possibility of a smooth, gradual ascent to nanotechnology, so that no one state - let alone any corporation or small group - ever gets a discontinuous advantage.</p>\n\n<p>The main reason I'm willing to believe this is because of the difficulties of <em>design</em> and <em>engineering</em>, even after all manufacturing is solved.&nbsp; When I read Drexler's <em>Nanosystems</em>, I thought:&nbsp; &quot;Drexler uses properly conservative assumptions everywhere I can see, except in one place - debugging.&nbsp; He assumes that any failed component fails visibly, immediately, and without side effects; <em>this</em> is not conservative.&quot;</p>\n\n<p>In <em>principle,</em> we have complete control of our computers - every bit and byte is under human command - and yet it still takes an immense amount of engineering work on top of that to make the bits do what we want.&nbsp; This, and not any difficulties of manufacturing things once they <em>are</em> designed, is what takes an international supply chain of millions of programmers.</p>\n\n<p>But we're <em>still</em> not out of the woods.</p>\n\n<p>Suppose that, by a providentially incremental and distributed process, we arrive at a world of full-scale molecular nanotechnology - a world where <em>designs,</em> if not finished material goods, are traded among parties.&nbsp; In a global economy large enough that no one actor, or even any one state, is doing more than a fraction of the total engineering.</p>\n\n<p>It would be a <em>very</em> different world, I expect; and it's possible that my essay may have already degenerated into nonsense.&nbsp; But even if we still have a global economy after getting this far - then we're <em>still</em> not out of the woods.</p>\n\n<p>Remember those <a href=\"http://www.overcomingbias.com/2008/11/brain-emulation.html\">ems</a>?&nbsp; The emulated humans-on-a-chip?&nbsp; The uploads?</p>\n\n<p>Suppose that, with molecular nanotechnology already in place, there's an international race for reliable uploading - with some results shared, and some results private - with many state and some nonstate actors.</p>\n\n<p>And suppose the race is so tight, that the first state to develop working researchers-on-a-chip, only has a <em>one-day</em> lead time over the other actors.</p>\n\n<p>That is - one day before anyone else, they develop uploads sufficiently undamaged, or capable of sufficient recovery, that the ems can carry out research and development.&nbsp; In the domain of, say, uploading.</p>\n\n<p>There are other teams working on the problem, but their uploads are still a little off, suffering seizures and having memory faults and generally having their cognition degraded to the point of not being able to contribute.&nbsp; (NOTE:&nbsp; I think this whole future is a wrong turn and we should stay away from it; I am not endorsing this.)</p>\n\n<p>But this one team, though - their uploads still have a few problems, but they're at least sane enough and smart enough to start... fixing their problems themselves?</p>\n\n<p>If there's already full-scale nanotechnology around when this happens, then even with some inefficiency built in, the first uploads may be running at ten thousand times human speed.&nbsp; Nanocomputers are powerful stuff.</p>\n\n<p>And in an hour, or around a year of internal time, the ems may be able to upgrade themselves to a hundred thousand times human speed, and fix some of the remaining problems.</p>\n\n<p>And in another hour, or ten years of internal time, the ems may be able to get the factor up to a million times human speed, and start working on intelligence enhancement...</p>\n\n<p>One could, of course, voluntarily publish the improved-upload protocols to the world, and give everyone else a chance to join in.&nbsp; But you'd have to trust that not a single one of your partners were holding back a trick that lets them run uploads at ten times your own maximum speed (once the bugs were out of the process).&nbsp; That kind of advantage could snowball quite a lot, in the first sidereal day.</p>\n\n<p>Now, if uploads are <em>gradually</em> developed <em>at a time when computers are too slow to run them quickly</em> - meaning, <em>before</em> molecular nanotech and nanofactories come along - then this whole scenario is averted; the first high-fidelity uploads, running at a hundredth of human speed, will grant no special advantage.&nbsp; (Assuming that no one is pulling any spectacular snowballing tricks with intelligence enhancement - but they would have to snowball fast and hard, to confer advantage on a small group running at low speeds.&nbsp; The same could be said of brain-computer interfaces, developed before or after nanotechnology, if running in a small group at merely human speeds.&nbsp; I would credit their world takeover, but I suspect Robin Hanson wouldn't at this point.)</p>\n\n<p>Now, I don't <em>really</em> believe in any of this - this whole scenario, this whole world I'm depicting.&nbsp; In real life, I'd expect someone to brute-force an unFriendly AI on one of those super-ultimate-nanocomputers, followed in short order by the end of the world.&nbsp; But that's a separate issue.&nbsp; And this whole world seems too much like our own, after too much technological change, to be realistic to me.&nbsp; World government with an insuperable advantage?&nbsp; Ubiquitous surveillance?&nbsp; I don't like the ideas, but both of them would change the game dramatically...</p>\n\n<p>But the real point of this essay is to illustrate a point more important than nanotechnology: <strong>as optimizers become more self-swallowing, races between them are more unstable.</strong></p>\n\n\n\n<p>If you sent a modern computer back in time to 1950 - containing many modern software tools in compiled form, but no future history or declaratively stored future science - I would guess that the recipient could <em>not</em> use it to take over the world.&nbsp; Even if the USSR got it.&nbsp; Our computing <em>industry</em> is a very powerful thing, but it relies on a supply chain of chip factories.</p>\n\n<p>If someone got a future <em>nanofactory</em> with a library of future nanotech applications - including designs for things like fusion power generators and surveillance bacteria - they might really be able to <em>take over the world</em>.&nbsp; The nanofactory swallows its own supply chain; it incorporates replication within itself.&nbsp; If the owner fails, it won't be for lack of factories.&nbsp; It will be for lack of ability to develop new matterware fast enough, and apply existing matterware fast enough, to take over the world.</p>\n\n<p>I'm not saying that nanotech <em>will</em> appear from nowhere with a library of designs - just making a point about concentrated power and the instability it implies.</p>\n\n<p>Think of all the tech news that you hear about once - say, an article on Slashdot about yada yada 50% improved battery technology - and then you never hear about again, because it was too expensive or too difficult to manufacture.</p>\n\n<p>Now imagine a world where the news of a 50% improved battery technology comes down the wire, and the head of some country's defense agency is sitting down across from engineers and intelligence officers and saying, &quot;We have five minutes before all of our rival's weapons are adapted to incorporate this new technology; how does that affect our balance of power?&quot;&nbsp; Imagine that happening as often as &quot;amazing breakthrough&quot; articles appear on Slashdot.</p>\n\n<p>I don't mean to doomsay - the Victorians would probably be pretty surprised we haven't blown up the world with our ten-minute ICBMs, but we don't live in their world - well, maybe doomsay just a little - but the point is:&nbsp; <em>It's less stable.</em>&nbsp; Improvements cascade faster once you've swallowed your manufacturing supply chain.</p>\n\n<p>And if you sent back in time a single nanofactory, <em>and</em> a single upload living inside it - then the world might end in five minutes or so, as we bios measure time.</p>\n\n\n\n<p>The point being, not that an upload <em>will</em> suddenly appear, but that now you've swallowed your supply chain <em>and</em> your R&amp;D chain.</p>\n\n<p>And so this world is correspondingly more unstable, even if all the actors start out in roughly the same place.&nbsp; Suppose a state manages to get one of those Slashdot-like technology improvements - only this one lets uploads think 50% faster - and they get it fifty minutes before anyone else, at a point where uploads are running ten thousand times as fast as human (50 mins = ~1 year) - and in that extra half-year, the uploads manage to find another couple of 50% improvements...</p>\n\n<p>Now, you <em>can</em> suppose that all the actors are all trading all of their advantages and holding nothing back, so everyone stays nicely synchronized.</p>\n\n<p>Or you can suppose that enough trading is going on, that most of the research any group benefits from comes from <em>outside</em> that group, and so a 50% advantage for a local group doesn't cascade much.</p>\n\n<p>But again, that's not the point.&nbsp; The point is that in modern times, with the modern computing industry, where commercializing an advance requires building a new computer factory, a bright idea that has gotten as far as showing a 50% improvement in the laboratory, is merely one more article on Slashdot.</p>\n\n<p>If everything could instantly be rebuilt via nanotech, that laboratory demonstration could precipitate an instant international military crisis.</p>\n\n<p>And if there are uploads around, so that a cute little 50% advancement in a certain kind of hardware, recurses back to imply <em>50% greater speed at all future research</em> - then this Slashdot article could become the key to world domination.</p>\n\n<p>As systems get more self-swallowing, they cascade harder; and even if all actors start out equivalent, races between them get much more unstable.&nbsp; I'm not claiming it's impossible for that world to be stable.&nbsp; The Victorians might have thought that about ICBMs.&nbsp; But that subjunctive world contains <em>additional</em> instability compared to our own, and would need <em>additional</em> centripetal forces to end up as stable as our own.</p>\n\n<p>I expect Robin to disagree with some part of this essay, but I'm not sure which part or how.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3uE2pXvbcnS9nnZRE": 1, "pGqRLe9bFDX2G2kXY": 1, "XJjvxWB68GYpts93N": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5hX44Kuz5No6E6RS9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 20, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "1161", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NCb28Xdv7xDajtqtS", "dq3KsCsqNotWc8nAK", "rJLviHqJMTy8WQkow", "pLRogvJLPPg6Mrvg4", "L32LHWzy9FzSDazEg", "Yq6aA4M3JKWaQepPJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-28T04:17:00.000Z", "modifiedAt": null, "url": null, "title": "Thanksgiving Prayer", "slug": "thanksgiving-prayer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:05.832Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QB6BkkpwiecfF6Ekq/thanksgiving-prayer", "pageUrlRelative": "/posts/QB6BkkpwiecfF6Ekq/thanksgiving-prayer", "linkUrl": "https://www.lesswrong.com/posts/QB6BkkpwiecfF6Ekq/thanksgiving-prayer", "postedAtFormatted": "Friday, November 28th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thanksgiving%20Prayer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThanksgiving%20Prayer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQB6BkkpwiecfF6Ekq%2Fthanksgiving-prayer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thanksgiving%20Prayer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQB6BkkpwiecfF6Ekq%2Fthanksgiving-prayer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQB6BkkpwiecfF6Ekq%2Fthanksgiving-prayer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 137, "htmlBody": "<p>At tonight's Thanksgiving, Erin remarked on how this was her first real Thanksgiving dinner away from her family, and that it was an odd feeling to just sit down and eat without any prayer beforehand.&nbsp; (Yes, she's a solid atheist in no danger whatsoever, thank you for asking.)</p>\n\n<p>And as she said this, it reminded me of how wrong it is to give gratitude to God for blessings that actually come from our fellow human beings putting in a great deal of work.</p>\n\n<p>So I at once put my hands together and said,</p>\n\n<p>&quot;Dear Global Economy, we thank thee for thy economies of scale, thy professional specialization, and thy international networks of trade under Ricardo's Law of Comparative Advantage, without which we would all starve to death while trying to assemble the ingredients for such a dinner as this.&nbsp; Amen.&quot;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hXTqT62YDTTiqJfxG": 1, "hNFdS3rRiYgqqD8aM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QB6BkkpwiecfF6Ekq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 32, "extendedScore": null, "score": 4.9e-05, "legacy": true, "legacyId": "1162", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-29T10:57:24.000Z", "modifiedAt": null, "url": null, "title": "Chaotic Inversion", "slug": "chaotic-inversion", "viewCount": null, "lastCommentedAt": "2020-12-14T19:22:33.183Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NyFtHycJvkyNjXNsP/chaotic-inversion", "pageUrlRelative": "/posts/NyFtHycJvkyNjXNsP/chaotic-inversion", "linkUrl": "https://www.lesswrong.com/posts/NyFtHycJvkyNjXNsP/chaotic-inversion", "postedAtFormatted": "Saturday, November 29th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Chaotic%20Inversion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChaotic%20Inversion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNyFtHycJvkyNjXNsP%2Fchaotic-inversion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Chaotic%20Inversion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNyFtHycJvkyNjXNsP%2Fchaotic-inversion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNyFtHycJvkyNjXNsP%2Fchaotic-inversion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 446, "htmlBody": "<p>I was recently having a conversation with some friends on the topic of hour-by-hour productivity and willpower maintenance&mdash;something I've struggled with my whole life.</p>\n<p>I can <a href=\"/lw/un/on_doing_the_impossible/\">avoid running away from a hard problem the first time I see it</a> (perseverance on a timescale of seconds), and I can stick to the same problem for years; but to keep working on a timescale of <em>hours</em> is a constant battle for me.&nbsp; It goes without saying that I've already read reams and reams of advice; and the most help I got from it was realizing that a sizable fraction other creative professionals had the same problem, and couldn't beat it either, no matter how reasonable<em>&nbsp;</em>all the advice sounds.</p>\n<p>\"What do you do when you can't work?\" my friends asked me.&nbsp; (Conversation probably not accurate, this is a very loose gist.)</p>\n<p>And I replied that I usually browse random websites, or watch a short video.</p>\n<p>\"Well,\" they said, \"if you know you can't work for a while, you should watch a movie or something.\"</p>\n<p>\"Unfortunately,\" I replied, \"I have to do something whose time comes in short units, like browsing the Web or watching short videos, because I might become able to work again at any time, and I can't predict when&mdash;\"</p>\n<p>And then I stopped, because I'd just had a revelation.</p>\n<p><a id=\"more\"></a></p>\n<p>I'd always thought of my workcycle as something <em>chaotic,</em> something <em>unpredictable.</em>&nbsp; I never used those words, but that was the way I <em>treated</em> it.</p>\n<p>But here my friends seemed to be implying&mdash;what a strange thought&mdash;that <em>other</em> people could predict when they would become able to work again, and structure their time accordingly.</p>\n<p>And it occurred to me for the first time that I might have been committing that damned old chestnut the <a href=\"/lw/oi/mind_projection_fallacy/\">Mind Projection Fallacy</a>, right out there in my ordinary everyday life instead of high abstraction.</p>\n<p>Maybe it wasn't that my productivity was<em> unusually </em><em>chaotic;</em> maybe I was just<em> unusually </em><em>stupid</em> with respect to predicting it.</p>\n<p>That's what inverted stupidity looks like&mdash;chaos.&nbsp; Something hard to handle, hard to grasp, hard to guess, something you can't do anything with.&nbsp; It's not just an idiom for high abstract things like Artificial Intelligence.&nbsp; It can apply in ordinary life too.</p>\n<p>And the reason we don't think of the alternative explanation \"I'm stupid\", is <em>not</em>&mdash;I suspect&mdash;that we think so highly of ourselves.&nbsp; It's just that we don't think of ourselves at all.&nbsp; We just see <a href=\"/lw/oi/mind_projection_fallacy/\">a chaotic feature of the environment</a>.</p>\n<p>So now it's occurred to me that my productivity problem may not be chaos, but my own stupidity.</p>\n<p>And that may or may not help anything.&nbsp; It certainly doesn't fix the problem right away.&nbsp; Saying \"I'm ignorant\" doesn't make you knowledgeable.</p>\n<p>But it is, at least, a different path than saying \"it's too chaotic\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"udPbn9RthmgTtHMiG": 1, "PJKgSRkXkCqXmCk3M": 7, "r7qAjcbfhj2256EHH": 1, "YrLoz567b553YouZ2": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NyFtHycJvkyNjXNsP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}], "voteCount": 74, "baseScore": 85, "extendedScore": null, "score": 0.000126, "legacy": true, "legacyId": "1163", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "p3TndjYbdYaiWwm9x", "canonicalCollectionSlug": "rationality", "canonicalBookId": "ah2GqzZSeBpW9QHgb", "canonicalNextPostSlug": "reductionism", "canonicalPrevPostSlug": "think-like-reality", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 87, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fpecAJLG9czABgCe9", "ZTRiSNmeGQK8AkdN2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-11-30T16:45:58.000Z", "modifiedAt": null, "url": null, "title": "Singletons Rule OK", "slug": "singletons-rule-ok", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:57.953Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rSTpxugJxFPoRMkGW/singletons-rule-ok", "pageUrlRelative": "/posts/rSTpxugJxFPoRMkGW/singletons-rule-ok", "linkUrl": "https://www.lesswrong.com/posts/rSTpxugJxFPoRMkGW/singletons-rule-ok", "postedAtFormatted": "Sunday, November 30th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Singletons%20Rule%20OK&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingletons%20Rule%20OK%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrSTpxugJxFPoRMkGW%2Fsingletons-rule-ok%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Singletons%20Rule%20OK%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrSTpxugJxFPoRMkGW%2Fsingletons-rule-ok", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrSTpxugJxFPoRMkGW%2Fsingletons-rule-ok", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1398, "htmlBody": "<p><strong>Reply to</strong>:&nbsp; <a href=\"http://www.overcomingbias.com/2008/11/total-tech-wars.html\">Total Tech Wars</a></p>\n\n<p>How <em>does</em> one end up with a persistent disagreement between two rationalist-wannabes who are both aware of Aumann's Agreement Theorem and its implications?</p>\n\n<p>Such a case is likely to turn around two axes: object-level incredulity (&quot;no matter <em>what</em> AAT says, proposition X can't<em> really</em> be true&quot;) and meta-level distrust (&quot;they're trying to be rational despite their emotional commitment, but are they really capable of that?&quot;).</p>\n\n<p>So far, Robin and I have focused on the object level in trying to hash out our disagreement.&nbsp; Technically, I can't speak for Robin; but at least in my <em>own</em> case, I've acted thus because I anticipate that a meta-level argument about trustworthiness wouldn't lead anywhere interesting.&nbsp; Behind the scenes, I'm doing what I can to make sure my brain is actually capable of updating, and presumably Robin is doing the same.</p>\n\n<p>(The linchpin of my own current effort in this area is to tell\nmyself that I ought to be learning something while having this\nconversation, and that I shouldn't miss any scrap of original thought\nin it - the <a href=\"http://www.overcomingbias.com/2007/08/update-yourself.html\">Incremental Update </a>technique. \nBecause I can genuinely believe that a conversation like this should\nproduce new thoughts, I can turn that feeling into genuine\nattentiveness.)</p>\n\n<p>Yesterday, Robin inveighed hard against what he called &quot;total tech wars&quot;, and what I call &quot;winner-take-all&quot; scenarios:</p><blockquote><p>Robin:&nbsp; &quot;If you believe the other side is totally committed to total victory,\nthat surrender is unacceptable, and that all interactions are zero-sum,\nyou may conclude your side must never cooperate with them, nor tolerate\nmuch internal dissent or luxury.&quot;</p></blockquote><p>Robin and I both have emotional commitments and we both acknowledge the danger of that.&nbsp; There's <a href=\"http://www.overcomingbias.com/2007/04/feeling_rationa.html\">nothing irrational about feeling</a>, <em>per se;</em> only <em>failure to update</em> is blameworthy.&nbsp; But Robin seems to be <em>very</em> strongly against winner-take-all technological scenarios, and I don't understand why.</p>\n\n<p>Among other things, I would like to ask if Robin has a <a href=\"http://www.overcomingbias.com/2008/02/leave-retreat.html\">Line of Retreat</a> set up here - if, regardless of how he estimates the <em>probabilities,</em> he can <em>visualize what he would do if</em> a winner-take-all scenario were true.</p><a id=\"more\"></a><p>Yesterday Robin wrote:</p><blockquote><p><span id=\"comment-140838978-content\">&quot;Eliezer, if everything is at stake\nthen 'winner take all' is 'total war'; it doesn't really matter if they\nshoot you or just starve you to death.&quot;</span></p></blockquote><p>We both have our emotional commitments, but I don't quite understand this reaction.</p>\n\n<p>First, to me it's obvious that a &quot;winner-take-all&quot; <em>technology</em> should be defined as one in which, <em>ceteris paribus,</em> a local entity tends to end up with the <em>option</em> of becoming one kind of <a href=\"http://www.nickbostrom.com/fut/singleton.html\">Bostromian singleton</a> - the decisionmaker of a global order in which there is a single decision-making entity at the highest level.&nbsp; (A superintelligence with unshared nanotech would count as a singleton; a federated world government with its own military would be a different kind of singleton; or you can imagine something like a galactic operating system with a root account controllable by 80% majority vote of the populace, etcetera.)</p>\n\n<p>The winner-take-all <em>option</em> is created by properties of the technology landscape, which is not a moral stance.&nbsp; Nothing is said about an agent with that <em>option,</em> <em>actually</em> becoming a singleton.&nbsp; Nor about <em>using</em> that power to shoot people, or reuse their atoms for something else, or grab all resources and let them starve (though &quot;all resources&quot; should include their atoms anyway).</p>\n\n<p>Nothing is yet said about various patches that could try to avert a <em>technological</em> scenario that contains upward cliffs of progress - e.g. binding agreements enforced by source code examination or continuous monitoring, in advance of the event.&nbsp; (Or if you think that rational agents <a href=\"http://www.overcomingbias.com/2008/09/iterated-tpd.html\">cooperate on the Prisoner's Dilemma</a>, so much work might not be required to coordinate.)</p>\n\n<p>Superintelligent agents <em>not</em> in a humanish <a href=\"http://www.overcomingbias.com/2008/08/rightness-redux.html\">moral reference frame</a> - AIs that are just maximizing paperclips or <a href=\"http://www.overcomingbias.com/2008/08/pebblesorting-p.html\">sorting pebbles</a> - who happen on the option of becoming a Bostromian Singleton, and who have <em>not</em> previously executed any somehow-binding treaty; will <em>ceteris paribus</em> choose to grab all resources in service of their utility function, including the atoms now composing humanity.&nbsp; I don't see how you could reasonably deny this!&nbsp; It's a straightforward decision-theoretic choice between payoff 10 and payoff 1000!</p>\n\n<p>But conversely, there are <a href=\"http://www.overcomingbias.com/2008/06/minds-in-genera.html\">possible agents in mind design space</a> who, given the <em>option</em> of becoming a singleton, will <em>not</em> kill you, starve you, reprogram you, tell you how to live your life, or even meddle in your destiny unseen.&nbsp; See <a href=\"http://www.nickbostrom.com/fut/singleton.html\">Bostrom's (short) paper</a> on the possibility of good and bad singletons of various types.</p>\n\n<p>If Robin thinks it's <em>impossible</em> to have a Friendly AI or maybe even any sort of benevolent superintelligence at all, even the descendants of human uploads - if Robin is assuming that superintelligent agents <em>will</em> act according to roughly selfish motives, and that <em>only</em> economies of trade are necessary and sufficient to prevent holocaust - then Robin may have no <a href=\"http://www.overcomingbias.com/2008/02/leave-retreat.html\">Line of Retreat</a> open, as I try to argue that AI has an upward cliff built in.</p>\n\n<p>And in this case, it might be time well spent, to first address the question of whether Friendly AI is a reasonable thing to try to accomplish, so as to create that line of retreat.&nbsp; Robin and I are both trying hard to be rational despite emotional commitments; but there's no particular reason to <em>needlessly</em> place oneself in the position of trying to persuade, or trying to accept, that everything of value in the universe is certainly doomed.</p>\n\n<p>For me, it's particularly hard to understand Robin's position in this, because for me the <em>non-</em>singleton future is the one that is obviously abhorrent.</p>\n\n<p>If you have lots of entities with root permissions on matter, any of whom has the physical capability to attack any other, then you have entities spending huge amounts of precious negentropy on defense and deterrence.&nbsp; If there's no centralized system of property rights in place for selling off the universe to the highest bidder, then you have a race to <a href=\"http://hanson.gmu.edu/filluniv.pdf\">burn the cosmic commons</a>, and the degeneration of the vast majority of all agents into <a href=\"http://hanson.gmu.edu/hardscra.pdf\">rapacious hardscrapple frontier</a> replicators.</p>\n\n<p>To me this is a vision of <em>futility</em> - one in which a future light cone that <em>could</em> have been full of happy, safe agents having complex fun, is mostly wasted by agents trying to seize resources and defend them so they can send out seeds to seize more resources.</p>\n\n<p>And it should also be mentioned that any future in which slavery or child abuse is <em>successfully</em> prohibited, is a world that has <em>some</em> way of preventing agents from doing certain things with their computing power.&nbsp; There are vastly worse possibilities than slavery or child abuse opened up by future technologies, which I flinch from referring to even as much as I did in the previous sentence.&nbsp; There are things I don't want to happen to <em>anyone</em> - including a population of a septillion captive minds running on a star-powered Matrioshka Brain that is owned, and <em>defended</em> against all rescuers, by the mind-descendant of Lawrence Bittaker (serial killer, aka &quot;Pliers&quot;).&nbsp; I want to <em>win</em> against the horrors that exist in this world and the horrors that could exist in tomorrow's world - to have them never happen ever again, or, for the <em>really</em> awful stuff, never happen in the first place.&nbsp; And that victory requires the Future to have certain <em>global</em> properties.</p>\n\n<p>But there are other ways to get singletons besides falling up a technological cliff.&nbsp; So that would be my Line of Retreat:&nbsp; If minds can't self-improve quickly enough to take over, then try for the path of uploads setting up a centralized Constitutional operating system with a root account controlled by majority vote, or something like that, to prevent their descendants from <em>having</em> to burn the cosmic commons.</p>\n\n<p>So for me, <em>any satisfactory outcome</em> seems to necessarily involve, if not a singleton, the existence of certain stable <em>global</em> properties upon the future - sufficient to <em>prevent</em> burning the cosmic commons, <em>prevent</em> life's degeneration into rapacious hardscrapple frontier replication, and <em>prevent</em> supersadists torturing septillions of helpless dolls in private, obscure star systems.</p>\n\n<p>Robin has written about burning the cosmic commons and rapacious hardscrapple frontier existences.&nbsp; This doesn't imply that Robin approves of these outcomes.&nbsp; But Robin's strong rejection even of winner-take-all <em>language</em> and <em>concepts,</em> seems to suggest that our emotional commitments are something like 180 degrees opposed.&nbsp; Robin seems to feel the same way about singletons as I feel about \u00acsingletons.</p>\n\n<p>But <em>why?</em>&nbsp; I don't think our real values are that strongly opposed - though we may have verbally-described and attention-prioritized those values in different ways.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qHDus5MuMNqQxJbjD": 1, "sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rSTpxugJxFPoRMkGW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 18, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "1164", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-01T04:45:00.000Z", "modifiedAt": null, "url": null, "title": "Disappointment in the Future", "slug": "disappointment-in-the-future", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:29.208Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yyiyz34p6QxWBmZ9k/disappointment-in-the-future", "pageUrlRelative": "/posts/yyiyz34p6QxWBmZ9k/disappointment-in-the-future", "linkUrl": "https://www.lesswrong.com/posts/yyiyz34p6QxWBmZ9k/disappointment-in-the-future", "postedAtFormatted": "Monday, December 1st 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Disappointment%20in%20the%20Future&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADisappointment%20in%20the%20Future%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyyiyz34p6QxWBmZ9k%2Fdisappointment-in-the-future%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Disappointment%20in%20the%20Future%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyyiyz34p6QxWBmZ9k%2Fdisappointment-in-the-future", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyyiyz34p6QxWBmZ9k%2Fdisappointment-in-the-future", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 967, "htmlBody": "<p>This seems worth posting around now...&nbsp; As I've previously observed, futuristic visions are <a href=\"/lw/hi/futuristic_predictions_as_consumable_goods/\">produced as entertainment, sold today and consumed today</a>.&nbsp; A TV station interviewing an economic or diplomatic pundit doesn't bother to show what that pundit predicted three years ago and how the predictions turned out.&nbsp; Why would they?&nbsp; Futurism Isn't About Prediction.</p>\n\n<p>But <a href=\"http://www.imminst.org/forum/My-Disappointment-at-the-Future-t17025.html\">someone on the ImmInst forum actually went and compiled a list</a> of Ray Kurzweil's predictions in 1999 for the years 2000-2009.&nbsp; We're not out of 2009 yet, but right now it's not looking good...</p><blockquote><p>\u00b7 Individuals primarily use portable computers<br />\u00b7 Portable computers have dramatically become lighter and thinner<br />\u00b7 Personal computers are available in a wide range of sizes and shapes, and are commonly embedded in clothing and jewelry, like wrist watches, rings, earrings and other body ornaments<br />\u00b7 Computers with a high-resolution visual interface range from rings and pins and credit cards up to the size of a thin book. People typically have at least a dozen computers on and around their bodies, which are networked, using body LANS (local area networks)<br />\u00b7 These computers monitor body functions, provide automated identity to conduct financial transactions and allow entry into secure areas. They also provide directions for navigation, and a variety of other services.<br />\u00b7 Most portable computers do not have keyboards</p></blockquote><a id=\"more\"></a><blockquote><p>\u00b7 Rotating memories such as Hard Drives, CD roms, and DVDs are on their way out.<br />\u00b7 Most users have servers on their homes and offices where they keep large stores of digital objects, including, among other things, virtual reality environments, although these are still on an early stage<br />\u00b7 Cables are disappearing, <br />\u00b7 The majority of texts is created using continuous speech recognition, or CSR (dictation software). CSRs are very accurate, far more than the human transcriptionists, who were used up until a few years ago.<br />\u00b7 Books, magazines, and newspapers are now routinely read on displays that are the size of small books<br />\u00b7 Computer displays built into eyeglasses are also used. These specialized glasses allow the users to see the normal environment while creating a virtual image that appears to hover in front of the viewer. <br />\u00b7 Computers routinely include moving picture image cameras and are able to reliably identify their owners from their faces<br />\u00b7 Three dimensional chips are commonly used<br />\u00b7 Students from all ages have a portable computer, very thin and soft, weighting less than 1 pound. They interact with their computers primarily by voice and by pointing with a device that looks like a pencil. Keybords still exist but most textual language is created by speaking.<br />\u00b7 Intelligent courseware has emerged as a common means of learning, recent controversial studies have shown that students can learn basic skills such as reading and math just as readily with interactive learning software as with human teachers.<br />\u00b7 Schools are increasingly relying on software approaches. Many children learn to read on their own using personal computers before entering grade school.<br />\u00b7 Persons with disabilities are rapidly overcoming their handicaps through intelligent technology<br />\u00b7 Students with reading disabilities routinely use print to speech reading systems <br />\u00b7 Print to speech reading machines for the blind are now very small, inexpensive, palm-size devices that can read books.<br />\u00b7 Useful navigation systems have finally been developed to assist blind people in moving and avoiding obstacles. Those systems use GPS technology. The blind person communicates with his navigation system by voice.<br />\u00b7 Deaf persons commonly use portable speech-to-text listening machines which display a real time transcription of what people are saying. The deaf user has the choice of either reading the transcribed speech as displayed text or watching an animated person gesturing in sign language.<br />\u00b7 Listening machines cal also translate what is being said into another language in real-time, so they are commonly used by hearing people as well.<br />\u00b7 There is a growing perception that the primary disabilities of blindness, deafness, and physical impairment do not necessarily. Disabled persons routinely describe their disabilities as mere inconveniences.<br />\u00b7 In communications, translate telephone technology is commonly used. This allow you to speak in English, while your Japanese friend hears you in Japanese, and vice-versa.<br />\u00b7 Telephones are primarily wireless and include high resolution moving images.<br />\u00b7 Heptic technologies are emerging. They allow people to touch and feel objects and other persons at a distance. These force-feedback devices are wildly used in games and in training simulation systems. Interactive games routinely include all encompassing all visual and auditory environments.<br />\u00b7 The 1999 chat rooms have been replaced with virtual environments.<br />\u00b7 At least half of all transactions are conducted online<br />\u00b7 Intelligent routes are in use, primarily for long distance travel. Once your car\u2019s computer\u2019s guiding system locks on to the control sensors on one of these highways, you can sit back, and relax.<br />\u00b7 There is a growing neo-luditte movement.</p></blockquote><p>Now, just to be clear, I don't want you to look at all that and think, &quot;Gee, the future goes more slowly than expected - technological progress must be naturally slow.&quot;</p>\n\n<p>More like, &quot;Where are you pulling all these <a href=\"/lw/jk/burdensome_details/\">burdensome details</a> from, anyway?&quot;</p>\n\n<p>If you looked at all that and said, &quot;Ha ha, how wrong; now I have my <em>own</em> amazing prediction for what the future will be like, <em>it won't be like that</em>,&quot; then you're really missing the whole &quot;you have to work a whole lot harder to produce veridical beliefs about the future, and often the info you want is simply not obtainable&quot; business.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8daMDi9NEShyLqxth": 1, "33BrBRSrRQS4jEHdk": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yyiyz34p6QxWBmZ9k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 15, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "1165", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mZJs7FxxmhMvFxuse", "Yq6aA4M3JKWaQepPJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-01T20:49:11.000Z", "modifiedAt": null, "url": null, "title": "Recursive Self-Improvement", "slug": "recursive-self-improvement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:58.166Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JBadX7rwdcRFzGuju/recursive-self-improvement", "pageUrlRelative": "/posts/JBadX7rwdcRFzGuju/recursive-self-improvement", "linkUrl": "https://www.lesswrong.com/posts/JBadX7rwdcRFzGuju/recursive-self-improvement", "postedAtFormatted": "Monday, December 1st 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Recursive%20Self-Improvement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARecursive%20Self-Improvement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJBadX7rwdcRFzGuju%2Frecursive-self-improvement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Recursive%20Self-Improvement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJBadX7rwdcRFzGuju%2Frecursive-self-improvement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJBadX7rwdcRFzGuju%2Frecursive-self-improvement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3814, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/w3/lifes_story_continues/\">Life's Story Continues</a>, <a href=\"/lw/w4/surprised_by_brains/\">Surprised by Brains</a>, <a href=\"/lw/w5/cascades_cycles_insight/\">Cascades, Cycles, Insight</a>, <a href=\"/lw/w6/recursion_magic/\">Recursion, Magic</a>, <a href=\"/lw/w8/engelbart_insufficiently_recursive/\">Engelbart: Insufficiently Recursive</a>, <a href=\"/lw/w9/total_nano_domination/\">Total Nano Domination</a></p>\n\n<p>I think that at some point in the development of Artificial Intelligence, we are likely to see a <em>fast, local</em>\nincrease in capability - &quot;AI go FOOM&quot;.&nbsp; Just to be clear on the claim,\n&quot;fast&quot; means on a timescale of weeks or hours rather than years or\ndecades; and &quot;FOOM&quot; means way the hell smarter than anything else\naround, capable of delivering in short time periods technological\nadvancements that would take humans decades, probably including\nfull-scale molecular nanotechnology (that it gets by e.g. ordering\ncustom proteins over the Internet with 72-hour turnaround time).&nbsp; Not,\n&quot;ooh, it's a little <a href=\"/lw/qk/that_alien_message/\">Einstein</a> but it doesn't have any robot hands, how cute&quot;.</p>\n\n<p>Most people who object to this scenario, object to the &quot;fast&quot; part. \nRobin Hanson objected to the &quot;local&quot; part.&nbsp; I'll try to handle both, though not all in one shot today.</p>\n\n<p>We are setting forth to analyze the developmental velocity of an Artificial Intelligence.&nbsp; We'll break down this velocity into <a href=\"/lw/w3/lifes_story_continues/\">optimization slope, optimization resources, and optimization efficiency</a>.&nbsp; We'll need to understand <a href=\"/lw/w5/cascades_cycles_insight/\">cascades, cycles, insight</a> and <span style=\"text-decoration: underline;\">r</span><a href=\"/lw/w6/recursion_magic/\">ecursion</a>; and we'll stratify our recursive levels into the <a href=\"/lw/w6/recursion_magic/\">metacognitive, cognitive, metaknowledge, knowledge, and object level</a>.</p>\n\n<p>Quick review:</p><a id=\"more\"></a><ul><li>&quot;Optimization slope&quot; is the goodness and number of\nopportunities in the volume of solution space you're currently\nexploring, on whatever your problem is;</li>\n\n<li>&quot;Optimization resources&quot; is how much computing power, sensory\nbandwidth, trials, etc. you have available to explore opportunities;</li>\n\n<li>&quot;Optimization efficiency&quot; is how well you use your resources.&nbsp; This\nwill be determined by the goodness of your current mind design - the\npoint in mind design space that is your current self - along with its\nknowledge and metaknowledge (see below).</li></ul>\n\n<p>Optimizing <em>yourself</em> is a special case, but it's one we're about to spend a lot of time talking about.</p>\n\n<p>By the time any mind solves some kind of <em>actual problem,</em>\nthere's actually been a huge causal lattice of optimizations applied -\nfor example, humans brain evolved, and then humans developed the idea\nof science, and then applied the idea of science to generate knowledge\nabout gravity, and then you use this knowledge of gravity to finally\ndesign a damn bridge or something.</p>\n\n<p>So I shall stratify this causality into levels - the <a href=\"/lw/o0/where_to_draw_the_boundary/\">boundaries</a> being semi-arbitrary, but you've got to draw them somewhere:</p>\n\n<ul><li>&quot;Metacognitive&quot; is the optimization that builds the brain - in\nthe case of a human, natural selection; in the case of an AI, either\nhuman programmers or, after some point, the AI itself.</li>\n\n<li>&quot;Cognitive&quot;, in humans, is the labor performed by your neural\ncircuitry, algorithms that consume large amounts of computing power but\nare mostly opaque to you.&nbsp; You know what you're seeing, but you don't\nknow how the visual cortex works.&nbsp; The Root of All Failure in AI is to\nunderestimate those algorithms because you can't see them...&nbsp; In an AI,\nthe lines between procedural and declarative knowledge are\ntheoretically blurred, but in practice it's often possible to\ndistinguish cognitive algorithms and cognitive content.</li>\n\n<li>&quot;Metaknowledge&quot;:&nbsp; Discoveries about how to discover, &quot;Science&quot;\nbeing an archetypal example, &quot;Math&quot; being another.&nbsp; You can think of\nthese as reflective cognitive content (knowledge about how to think).</li>\n\n<li>&quot;Knowledge&quot;:&nbsp; Knowing how gravity works.</li>\n\n<li>&quot;Object level&quot;:&nbsp; Specific actual problems like building a bridge or something.</li></ul>\n\n<p>I am arguing that an AI's developmental velocity will not be smooth;\nthe following are some classes of phenomena that might lead to\nnon-smoothness.&nbsp; First, a couple of points that weren't raised earlier:</p>\n\n<ul><li><em>Roughness:</em>&nbsp; A search space can be naturally rough - have unevenly distributed <em>slope.</em> \nWith constant optimization pressure, you could go through a long phase\nwhere improvements are easy, then hit a new volume of the search space\nwhere improvements are tough.&nbsp; Or vice versa.&nbsp; Call this factor <em>roughness.</em></li>\n\n<li><em>Resource overhangs:</em>&nbsp; Rather than resources growing\nincrementally by reinvestment, there's a big bucket o' resources behind\na locked door, and once you unlock the door you can walk in and take\nthem all.</li></ul>\n\n<p>And these other factors previously covered:</p>\n\n<ul><li><em>Cascades</em> are when one development leads the way to\nanother - for example, once you discover gravity, you might find it\neasier to understand a coiled spring.</li>\n\n<li><em>Cycles</em> are feedback loops where a process's output becomes\nits input on the next round.&nbsp; As the classic example of a fission chain\nreaction illustrates, a cycle whose underlying processes are\ncontinuous, may show qualitative changes of surface behavior - a\nthreshold of criticality - the difference between each neutron leading\nto the emission of 0.9994 additional neutrons versus each neutron\nleading to the emission of 1.0006 additional neutrons.&nbsp; <em>k</em> is the effective neutron multiplication factor and I will use it metaphorically.</li>\n\n<li><em>Insights</em> are items of knowledge that tremendously decrease\nthe cost of solving a wide range of problems - for example, once you\nhave the calculus insight, a whole range of physics problems become a\nwhole lot easier to solve.&nbsp; Insights let you fly through, or teleport\nthrough, the solution space, rather than searching it by hand - that\nis, &quot;insight&quot; represents knowledge about the structure of the search\nspace itself.</li></ul>\n\n<p>and finally,</p>\n\n<ul><li><em>Recursion</em> is the sort of thing that happens when you hand the AI the object-level problem of &quot;redesign your own cognitive algorithms&quot;.</li></ul>\n\n<p>Suppose I go to an AI programmer and say, &quot;Please write me a program\nthat plays chess.&quot;&nbsp; The programmer will tackle this using their\nexisting knowledge and insight in the domain of chess and search trees;\nthey will apply any metaknowledge they have about how to solve\nprogramming problems or AI problems; they will process this knowledge\nusing the deep algorithms of their neural circuitry; and this neutral\ncircuitry will have been designed (or rather its wiring algorithm\ndesigned) by natural selection.</p>\n\n<p>If you go to a sufficiently sophisticated AI - more sophisticated\nthan any that currently exists - and say, &quot;write me a chess-playing\nprogram&quot;, the same thing might happen:&nbsp; The AI would use its knowledge,\nmetaknowledge, and existing cognitive algorithms.&nbsp; Only the AI's <em>metacognitive</em> level would be, not natural selection, but the <em>object level</em> of the programmer who wrote the AI, using <em>their</em> knowledge and insight etc.</p>\n\n<p>Now suppose that instead you hand the AI the problem, &quot;Write a\nbetter algorithm than X for storing, associating to, and retrieving\nmemories&quot;.&nbsp; At first glance this may appear to be just another\nobject-level problem that the AI solves using its current knowledge,\nmetaknowledge, and cognitive algorithms.&nbsp; And indeed, in one sense it\nshould be just another object-level problem.&nbsp; But it so happens that\nthe AI itself uses algorithm X to store associative memories, so if the\nAI can improve on this algorithm, it can rewrite its code to use the\nnew algorithm X+1.</p>\n\n<p>This means that the AI's <em>metacognitive</em> level - the\noptimization process responsible for structuring the AI's cognitive\nalgorithms in the first place - has now collapsed to identity with the\nAI's <em>object</em> level.</p>\n\n<p>For some odd reason, I run into a lot of people who vigorously deny\nthat this phenomenon is at all novel; they say, &quot;Oh, humanity is\nalready self-improving, humanity is already going through a FOOM,\nhumanity is already in a Singularity&quot; etc. etc.</p>\n\n<p>Now to me, it seems clear that - at this point in the game, in advance of the observation - it is <em>pragmatically</em>\nworth drawing a distinction between inventing agriculture and using\nthat to support more professionalized inventors, versus directly\nrewriting your own source code in RAM.&nbsp; Before you can even <em>argue</em>\nabout whether the two phenomena are likely to be similar in practice,\nyou need to accept that they are, in fact, two different things to be\nargued <em>about.</em></p>\n\n<p>And I do expect them to be very distinct in practice.&nbsp; Inventing\nscience is not rewriting your neural circuitry.&nbsp; There is a tendency to\n<em>completely overlook</em> the power of brain algorithms, because they\nare invisible to introspection.&nbsp; It took a long time historically for\npeople to realize that there <em>was</em> such a thing as a cognitive\nalgorithm that could underlie thinking.&nbsp; And then, once you point out\nthat cognitive algorithms exist, there is a tendency to tremendously\nunderestimate them, because you don't know the specific details of how\nyour hippocampus is storing memories well or poorly - you don't know\nhow it could be improved, or what difference a slight degradation could\nmake.&nbsp; You can't draw detailed causal links between the wiring of your\nneural circuitry, and your performance on real-world problems.&nbsp; All you\ncan <em>see</em> is the knowledge and the metaknowledge, and that's where all your causal links go; that's all that's <em>visibly</em> important.</p>\n\n<p>To see the brain circuitry vary, you've got to look at a chimpanzee,\nbasically.&nbsp; Which is not something that most humans spend a lot of time\ndoing, because chimpanzees can't play our games.</p>\n\n<p>You can also see the tremendous overlooked power of the brain\ncircuitry by observing what happens when people set out to program what\nlooks like &quot;knowledge&quot; into Good-Old-Fashioned AIs, semantic nets and\nsuch.&nbsp; Roughly, nothing happens.&nbsp; Well, research papers happen.&nbsp; But no\nactual intelligence happens.&nbsp; Without those opaque, overlooked,\ninvisible brain algorithms, there is no real knowledge - only a tape\nrecorder playing back human words.&nbsp; If you have a small amount of fake\nknowledge, it doesn't do anything, and if you have a huge amount of\nfake knowledge programmed in at huge expense, it still doesn't do\nanything.</p>\n\n<p>So the cognitive level - in humans, the level of neural circuitry\nand neural algorithms - is a level of tremendous but invisible power. \nThe difficulty of penetrating this invisibility and creating a real\ncognitive level is what stops modern-day humans from creating AI.&nbsp; (Not\nthat an AI's cognitive level would be made of neurons or anything\nequivalent to neurons; it would just do cognitive labor on the same <a href=\"http://intelligence.org/upload/LOGI/\">level of organization</a>.&nbsp; Planes don't flap their wings, but they have to produce lift somehow.)</p>\n\n<p>Recursion that can rewrite the cognitive level is <em>worth distinguishing</em>.</p>\n\n<p>But to some, having a term so <a href=\"/lw/ic/the_virtue_of_narrowness/\">narrow</a> as to refer to an AI rewriting its own source code, and not to humans inventing farming, seems <a href=\"/lw/ic/the_virtue_of_narrowness/\">hardly open, hardly embracing, hardly communal</a>; for we all know that <a href=\"/lw/ic/the_virtue_of_narrowness/\">to say two things are similar shows greater enlightenment than saying that they are different</a>.&nbsp; Or maybe it's as simple as identifying &quot;recursive self-improvement&quot; as a term with positive <a href=\"/lw/lg/the_affect_heuristic/\">affective valence</a>, so you figure out a way to apply that term to humanity, and then you get a nice dose of warm fuzzies.&nbsp; Anyway.</p>\n\n<p>So what happens when you start rewriting cognitive algorithms?</p>\n\n<p>Well, we do have <em>one</em> well-known historical case of an\noptimization process writing cognitive algorithms to do further\noptimization; this is the case of <a href=\"/lw/kr/an_alien_god/\">natural selection, our alien god.</a></p>\n\n<p>Natural selection seems to have produced a pretty smooth trajectory\nof more sophisticated brains over the course of hundreds of millions of\nyears.&nbsp; That gives us our first data point, with these characteristics:</p>\n\n<ul><li>Natural selection on sexual multicellular eukaryotic life can probably be treated as, to first order, an optimizer of <em>roughly constant efficiency and constant resources.</em></li>\n\n<li>Natural selection does not have anything akin to insights.&nbsp; It does\nsometimes stumble over adaptations that prove to be surprisingly\nreusable outside the context for which they were adapted, but it\ndoesn't fly through the search space like a human.&nbsp; Natural selection\nis just <em>searching the immediate neighborhood of its present point in the solution space, over and over and over.</em></li>\n\n<li>Natural selection <em>does</em> have cascades; adaptations open up the way for further adaptations.</li></ul>\n\n<p>So - <em>if</em> you're navigating the search space via the <a href=\"/lw/kt/evolutions_are_stupid_but_work_anyway/\">ridiculously stupid and inefficient</a> method of looking at the neighbors of the current point, without insight - with constant optimization pressure - then...</p>\n\n<p>Well, I've heard it claimed that the evolution of biological brains\nhas accelerated over time, and I've also heard that claim challenged. \nIf there's actually been an acceleration, I would tend to attribute\nthat to the &quot;adaptations open up the way for further adaptations&quot;\nphenomenon - the more brain genes you have, the more chances for a\nmutation to produce a new brain gene.&nbsp; (Or, more complexly: the more\norganismal error-correcting mechanisms the brain has, the more likely a\nmutation is to produce something useful rather than fatal.)&nbsp; In the\ncase of hominids in particular over the last few million years, we may\nalso have been experiencing accelerated <em>selection</em> on brain proteins, <em>per se</em> - which I would attribute to sexual selection, or brain variance accounting for a greater proportion of total fitness variance.</p>\n\n<p>Anyway, what we definitely do <em>not</em> see under these conditions is <em>logarithmic</em> or <em>decelerating </em>progress.&nbsp; It did <em>not</em> take ten times as long to go from <em>H. erectus</em> to <em>H. sapiens</em> as from <em>H. habilis</em> to <em>H. erectus</em>. Hominid evolution did <em>not</em> take eight hundred million years of additional time, after evolution immediately produced <em>Australopithecus</em>-level brains in just a few million years after the invention of neurons themselves.</p>\n\n<p>And another, similar observation: human intelligence does <em>not</em>\nrequire a hundred times as much computing power as chimpanzee\nintelligence.&nbsp; Human brains are merely three times too large, and our\nprefrontal cortices six times too large, for a primate with our body\nsize.</p>\n\n<p>Or again:&nbsp; It does not seem to require 1000 times as many genes to\nbuild a human brain as to build a chimpanzee brain, even though human\nbrains can build toys that are a thousand times as neat.</p>\n\n<p>Why is this important?&nbsp; Because it shows that with <em>constant optimization pressure</em> from natural selection and <em>no intelligent insight,</em> there were <em>no diminishing returns</em> to a search for better brain designs up to at least the human level.&nbsp; There were probably <em>accelerating</em> returns (with a low acceleration factor).&nbsp; There are no <em>visible speedbumps,</em> <a href=\"/lw/kj/no_one_knows_what_science_doesnt_know/\">so far as I know</a>.</p>\n\n<p>But all this is to say only of natural selection, which is not recursive.</p>\n\n<p>If you have an investment whose output is not coupled to its input - say, you have a bond, and the bond pays you a certain amount of interest every year, and you spend the interest every year - then this will tend to return you a linear amount of money over time.&nbsp; After one year, you've received $10; after 2 years, $20; after 3 years, $30.</p>\n\n<p>Now suppose you <em>change</em> the qualitative physics of the investment, by coupling the output pipe to the input pipe.&nbsp; Whenever you get an interest payment, you invest it in more bonds.&nbsp; Now your returns over time will follow the curve of compound interest, which is exponential.&nbsp; (Please note:&nbsp; <em>Not all accelerating processes are smoothly exponential.</em>&nbsp; But this one happens to be.)</p>\n\n<p>The first process grows at a rate that is linear over <em>time</em>; the second process grows at a rate that is linear in its <em>cumulative return so far.</em></p>\n\n<p>The too-obvious mathematical idiom to describe the impact of recursion is replacing an equation</p><blockquote><p>y = f(t)</p></blockquote><p>with</p><blockquote><p>dy/dt = f(y)</p></blockquote><p>For example, in the case above, reinvesting our returns transformed the <em>linearly</em> growing</p><blockquote><p>y = m*t</p></blockquote><p>into</p><blockquote><p>y' = m*y</p></blockquote><p>whose solution is the exponentially growing</p><blockquote><p>y = e^(m*t)</p></blockquote><p>Now... I do not think you can <em>really</em> solve equations like this to get anything like a description of a self-improving AI.</p>\n\n<p>But it's the obvious reason why I <em>don't</em> expect the future to be a continuation of past trends.&nbsp; The future contains a feedback loop that the past does not.</p>\n\n<p>As a different Eliezer Yudkowsky wrote, very long ago:</p>\n\n<p>&quot;If computing power doubles every eighteen months, what happens when computers are doing the research?&quot;</p>\n\n<p>And this sounds horrifyingly naive to my present ears, because that's not really how it works at all - but still, it illustrates the idea of &quot;the future contains a feedback loop that the past does not&quot;.</p>\n\n<p>History up until this point was a long story about natural selection producing humans, and then, after humans hit a certain threshold, humans starting to rapidly produce knowledge and metaknowledge that could - among other things - feed more humans and support more of them in lives of professional specialization.</p>\n\n<p>To a first approximation, natural selection held still during human cultural development.&nbsp; Even if <a href=\"http://www.google.com/search?hl=en&amp;q=farewell+to+alms&amp;btnG=Search\">Gregory Clark's crazy ideas</a> are crazy enough to be true - i.e., some human populations evolved lower discount rates and more industrious work habits over the course of just a few hundred years from 1200 to 1800 - that's just tweaking a few relatively small parameters; it is not the same as developing new complex adaptations with lots of interdependent parts.&nbsp; It's not a <a href=\"/lw/ql/my_childhood_role_model/\">chimp-human type gap</a>.</p>\n\n<p>So then, <em>with human cognition remaining more or less constant,</em> we found that knowledge feeds off knowledge with <em>k</em> &gt; 1 - given a background of roughly constant cognitive algorithms at the human level.&nbsp; We discovered major chunks of metaknowledge, like Science and the notion of Professional Specialization, that changed the exponents of our progress; having lots more humans around, due to e.g. the object-level innovation of farming, may have have also played a role.&nbsp; Progress in any one area tended to be choppy, with large insights leaping forward, followed by a lot of slow incremental development.</p>\n\n<p>With history <em>to date,</em> we've got a series of integrals looking something like this:</p><blockquote><p>Metacognitive = natural selection, optimization efficiency/resources roughly constant</p>\n\n<p>Cognitive = Human intelligence = integral of evolutionary optimization velocity over a few hundred million years, then roughly <em>constant</em> over the last ten thousand years</p>\n\n<p>Metaknowledge = Professional Specialization, Science, etc. = integral over cognition we did about procedures to follow in thinking, where metaknowledge can also feed on itself, there were major insights and cascades, etc.</p>\n\n<p>Knowledge = all that actual science, engineering, and general knowledge accumulation we did = integral of cognition+metaknowledge(current knowledge) over time, where knowledge feeds upon itself in what seems to be a roughly exponential process</p>\n\n<p>Object level = stuff we actually went out and did = integral of cognition+metaknowledge+knowledge(current solutions); over a short timescale this tends to be smoothly exponential to the degree that the people involved understand the idea of investments competing on the basis of interest rate, but over medium-range timescales the exponent varies, and on a long range the exponent seems to increase</p></blockquote><p>If you were to summarize that in one breath, it would be, &quot;with constant natural selection pushing on brains, progress was linear or mildly accelerating; with constant brains pushing on metaknowledge and knowledge and object-level progress feeding back to metaknowledge and optimization resources, progress was exponential or mildly superexponential&quot;.</p>\n\n<p>Now fold back the object level so that it becomes the metacognitive level.</p>\n\n<p>And note that we're doing this through a chain of differential equations, not just one; it's the <em>final</em> output at the object level, after all those integrals, that becomes the velocity of metacognition.</p>\n\n<p>You should get...</p>\n\n<p>...very fast progress?&nbsp; Well, no, not necessarily.&nbsp; You can also get nearly <em>zero</em> progress.</p>\n\n<p>If you're a recursified <a href=\"/lw/w6/recursion_magic/\">optimizing compiler</a>,\nyou rewrite yourself just once, get a single boost in speed (like 50%\nor something), and then never improve yourself any further, ever again.</p>\n\n<p>If you're <a href=\"/lw/w6/recursion_magic/\">EURISKO</a>,\nyou manage to modify some of your metaheuristics, and the\nmetaheuristics work noticeably better, and they even manage to make a\nfew further modifications to themselves, but then the whole process\nruns out of steam and flatlines.</p>\n\n<p>It was human intelligence that produced these artifacts to begin with.&nbsp; Their <em>own</em> optimization power is far short of human - so incredibly weak that, after they push themselves along a little, they can't push any further.&nbsp; Worse, their optimization at any given level is characterized by a limited number of opportunities, which once used up are gone - extremely sharp diminishing returns.</p>\n\n<p>When you fold a complicated, choppy, cascade-y chain of differential equations in on itself via recursion, <em>it should either flatline or blow up.</em>&nbsp; You would need <em>exactly the right law of diminishing returns</em> to fly through the extremely narrow <em>soft takeoff keyhole.</em></p>\n\n<p>The <em>observed history of optimization to date</em> makes this <em>even more unlikely.</em>&nbsp; I don't see any reasonable way that you can have constant evolution produce human intelligence on the observed historical trajectory (linear or accelerating), and constant human intelligence produce science and technology on the observed historical trajectory (exponential or superexponential), and <em>fold that in on itself,</em> and get out something whose rate of progress is in any sense <em>anthropomorphic.</em>&nbsp; From our perspective it should either flatline or FOOM.<br />\n\n\n\n</p>\n\n<p>When you first build an AI, it's a baby - if it had to improve <em>itself,</em> it would almost immediately flatline.&nbsp; So you push it along using your own cognition, metaknowledge, and knowledge - <em>not</em>\ngetting any benefit of recursion in doing so, just the usual human\nidiom of knowledge feeding upon itself and insights cascading into\ninsights.&nbsp; Eventually the AI becomes sophisticated enough to start\nimproving <em>itself</em>, not just small improvements, but improvements\nlarge enough to cascade into other improvements.&nbsp; (Though right now,\ndue to lack of human insight, what happens when modern researchers push\non their AGI design is mainly nothing.)&nbsp; And then you get what I. J. Good called an &quot;intelligence explosion&quot;.</p>\n\n<p>I even want to say that the functions and curves being such as to allow hitting the soft takeoff keyhole, is <em>ruled out</em> by observed history to date.&nbsp; But there are small conceivable loopholes, like &quot;maybe all the curves change drastically and completely as soon as we get past the part we know about in order to give us exactly the right anthropomorphic final outcome&quot;, or &quot;maybe the trajectory for insightful optimization of intelligence has a\nlaw of diminishing returns where blind evolution gets accelerating returns&quot;.</p>\n\n<p>There's other factors contributing to hard takeoff, like the existence of hardware overhang in the form of the poorly defended Internet and fast serial computers.&nbsp; There's more than one possible species of AI we could see, given this whole analysis.&nbsp; I haven't yet touched on the issue of localization (though the basic issue is obvious: the initial recursive cascade of an intelligence explosion can't race through human brains because human brains are not modifiable until the AI is already superintelligent).</p>\n\n<p>But today's post is already too long, so I'd best continue tomorrow.</p>\n\n<p><strong>Post scriptum:</strong>&nbsp; It occurred to me just after writing this that I'd been victim of a cached Kurzweil thought in speaking of the knowledge level as &quot;exponential&quot;.&nbsp; Object-level resources are exponential in human history because of physical cycles of reinvestment.&nbsp; If you try defining knowledge as productivity per worker, I expect that's exponential too (or productivity growth would be unnoticeable by now as a component in economic progress).&nbsp; I wouldn't be surprised to find that published journal articles are growing exponentially.&nbsp; But I'm not quite sure that it makes sense to say humanity has learned as much since 1938 as in all earlier human history... though I'm quite willing to believe we produced more goods... then again we surely learned more since 1500 than in all the time before.&nbsp; Anyway, human knowledge being &quot;exponential&quot; is a more complicated issue than I made it out to be.&nbsp; But human object level is more clearly exponential or superexponential.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2b5": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JBadX7rwdcRFzGuju", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 27, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": "1166", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RXLoo3pXgY2hjdXrg", "XQirei3crsLxsCQoi", "dq3KsCsqNotWc8nAK", "rJLviHqJMTy8WQkow", "NCb28Xdv7xDajtqtS", "5hX44Kuz5No6E6RS9", "5wMcKNAwB6X4mp9og", "d5NyJ2Lf6N22AD9PB", "yDfxTj9TKYsYiWH5o", "Kow8xRzpfkoY7pa69", "pLRogvJLPPg6Mrvg4", "jAToJHtg39AMTAuJo", "vNBxmcHpnozjrJnJP", "3Jpchgy53D2gB5qdk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-02T20:44:26.000Z", "modifiedAt": null, "url": null, "title": "Hard Takeoff", "slug": "hard-takeoff", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:38.514Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tjH8XPxAnr6JRbh7k/hard-takeoff", "pageUrlRelative": "/posts/tjH8XPxAnr6JRbh7k/hard-takeoff", "linkUrl": "https://www.lesswrong.com/posts/tjH8XPxAnr6JRbh7k/hard-takeoff", "postedAtFormatted": "Tuesday, December 2nd 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hard%20Takeoff&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHard%20Takeoff%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtjH8XPxAnr6JRbh7k%2Fhard-takeoff%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hard%20Takeoff%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtjH8XPxAnr6JRbh7k%2Fhard-takeoff", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtjH8XPxAnr6JRbh7k%2Fhard-takeoff", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3161, "htmlBody": "<p><strong>Continuation of</strong>:&nbsp; <a href=\"/lw/we/recursive_selfimprovement/\">Recursive Self-Improvement</a></p>\n<p>Constant natural selection pressure, operating on the genes of the hominid line, produced improvement in brains over time that seems to have been, roughly, <em>linear or accelerating;</em> the operation of constant human brains on a pool of knowledge seems to have produced returns that are, very roughly, <em>exponential or superexponential.</em>&nbsp; (<a href=\"http://hanson.gmu.edu/longgrow.pdf\">Robin proposes</a> that human progress is well-characterized as a series of exponential modes with diminishing doubling times.)</p>\n<p>Recursive self-improvement - an AI rewriting its own cognitive algorithms - identifies the object level of the AI with a force acting on the metacognitive level; it \"closes the loop\" or \"folds the graph in on itself\".&nbsp; E.g. the difference between returns on a constant investment in a bond, and reinvesting the returns into purchasing further bonds, is the difference between the equations y = f(t) = m*t, and dy/dt = f(y) = m*y whose solution is the compound interest exponential, y = e^(m*t).</p>\n<p>When you fold a whole chain of differential equations in on itself like this, it should either peter out rapidly as improvements fail to yield further improvements, or else go FOOM.&nbsp; An <em>exactly right law of diminishing returns</em> that lets the system fly through the <em>soft takeoff keyhole</em> is unlikely - <em>far</em> more unlikely than seeing such behavior in a system with a roughly-constant underlying optimizer, like evolution improving brains, or human brains improving technology.&nbsp; Our present life is no good indicator of things to come.</p>\n<p>Or to try and compress it down to a slogan that fits on a T-Shirt - not that I'm saying this is a good idea - \"Moore's Law is exponential <em>now;</em> it would be really odd if it <em>stayed</em> exponential with the improving computers <em>doing the research.</em>\"&nbsp; I'm not saying you literally get dy/dt = e^y that goes to infinity after finite time - and hardware improvement is in some ways the least interesting factor here - but should we really see the same curve we do now?</p>\n<p>RSI is the biggest, most interesting, hardest-to-analyze, sharpest break-with-the-past contributing to the notion of a \"hard takeoff\" aka \"AI go FOOM\", but it's nowhere near being the <em>only</em> such factor.&nbsp; <a href=\"/lw/w4/surprised_by_brains/\">The advent of human intelligence was a discontinuity with the past</a> even <em>without</em> RSI...</p>\n<p><a id=\"more\"></a></p>\n<p>...which is to say that observed evolutionary history - the discontinuity between humans, and chimps who share 95% of our DNA - <em>lightly</em> suggests a critical threshold built into the capabilities that we think of as \"general intelligence\", a machine that becomes far more powerful once the last gear is added.</p>\n<p>This is only a <em>light</em> suggestion because the branching time between humans and chimps <em>is</em> enough time for a good deal of complex adaptation to occur.&nbsp; We could be looking at the sum of a <a href=\"/lw/w5/cascades_cycles_insight/\">cascade</a>, not the addition of a final missing gear.&nbsp; On the other hand, we can look at the gross brain anatomies and see that human brain anatomy and chimp anatomy have not diverged all that much.&nbsp; On the gripping hand, there's the sudden cultural revolution - the sudden increase in the sophistication of artifacts - that accompanied the appearance of anatomically Cro-Magnons just a few tens of thousands of years ago.</p>\n<p>Now of course this might all just be completely inapplicable to the development trajectory of AIs built by human programmers rather than by evolution.&nbsp; But it at least <em>lightly suggests,</em> and provides a hypothetical <em>illustration</em> of, a discontinuous leap upward in capability that results from a natural feature of the solution space - a point where you go from sorta-okay solutions to totally-amazing solutions as the result of a few final tweaks to the mind design.</p>\n<p>I could potentially go on about this notion for a bit - because, in an evolutionary trajectory, it can't <em>literally</em> be a \"missing gear\", the sort of discontinuity that follows from removing a gear that an otherwise functioning machine was built around.&nbsp; So if you suppose that a final set of changes was enough to produce a sudden huge leap in effective intelligence, it does demand the question of what those changes were.&nbsp; Something to do with reflection - the brain modeling or controlling itself - would be one obvious candidate.&nbsp; Or perhaps a change in motivations (more curious individuals, using the brainpower they have in different directions) in which case you <em>wouldn't</em> expect that discontinuity to appear in the AI's development, but you would expect it to be more effective at earlier stages than humanity's evolutionary history would suggest...&nbsp; But you could have whole journal issues about that one question, so I'm just going to leave it at that.</p>\n<p>Or consider the notion of sudden resource bonanzas.&nbsp; Suppose there's a semi-sophisticated Artificial General Intelligence running on a cluster of a thousand CPUs.&nbsp; The AI has not hit a wall - it's still improving itself - but its self-improvement is going so <em>slowly</em> that, the AI calculates, it will take another fifty years for it to engineer / implement / refine just the changes it currently has in mind.&nbsp; Even if this AI would go FOOM eventually, its current progress is so slow as to constitute being flatlined...</p>\n<p>So the AI turns its attention to examining certain blobs of binary code - code composing operating systems, or routers, or DNS services - and then takes over all the poorly defended computers on the Internet.&nbsp; This may not require what humans would regard as genius, just the ability to examine lots of machine code and do relatively low-grade reasoning on millions of bytes of it.&nbsp; (I have a saying/hypothesis that a <em>human</em> trying to write <em>code</em> is like someone without a visual cortex trying to paint a picture - we can do it eventually, but we have to go pixel by pixel because we lack a sensory modality for that medium; it's not our native environment.)&nbsp; The Future may also have more legal ways to obtain large amounts of computing power quickly.</p>\n<p>This sort of resource bonanza is intriguing in a number of ways.&nbsp; By assumption, optimization <em>efficiency</em> is the same, at least for the moment - we're just plugging a few orders of magnitude more resource into the current input/output curve.&nbsp; With a stupid algorithm, a few orders of magnitude more computing power will buy you only a linear increase in performance - I would not fear Cyc even if ran on a computer the size of the Moon, because there is no there there.</p>\n<p>On the other hand, humans have a brain three times as large, and a prefrontal cortex six times as large, as that of a standard primate our size - so with software improvements of the sort that natural selection made over the last five million years, it does not require exponential increases in computing power to support linearly greater intelligence.&nbsp; Mind you, this sort of biological analogy is always fraught - maybe a human has not much more cognitive horsepower than a chimpanzee, the same underlying tasks being performed, but in a few more domains and with greater reflectivity - the engine outputs the same horsepower, but a few gears were reconfigured to turn each other less wastefully - and so you wouldn't be able to go from human to super-human with just another sixfold increase in processing power... or something like that.</p>\n<p>But if the lesson of biology suggests anything, it is that you do not run into logarithmic returns on <em>processing power</em> in the course of reaching human intelligence, even when that processing power increase is strictly parallel rather than serial, provided that you are at least as good as writing software to take advantage of that increased computing power, as natural selection is at producing adaptations - five million years for a sixfold increase in computing power.</p>\n<p>Michael Vassar observed in yesterday's comments that humans, by spending linearly more time studying chess, seem to get linear increases in their chess rank (across a wide range of rankings), while putting exponentially more time into a search algorithm is usually required to yield the same range of increase.&nbsp; Vassar called this \"bizarre\", but I find it quite natural.&nbsp; Deep Blue searched the raw game tree of chess; Kasparavo searched the compressed regularities of chess.&nbsp; It's not surprising that the simple algorithm is logarithmic and the sophisticated algorithm is linear.&nbsp; One might say similarly of the course of human progress seeming to be closer to exponential, while evolutionary progress is closer to being linear.&nbsp; Being able to understand the regularity of the search space counts for quite a lot.</p>\n<p>If the AI is somewhere in between - not as brute-force as Deep Blue, nor as compressed as a human - then maybe a 10,000-fold increase in computing power will only buy it a 10-fold increase in optimization velocity... but that's still quite a speedup.</p>\n<p>Furthermore, all <em>future</em> improvements the AI makes to itself will now be amortized over 10,000 times as much computing power to apply the algorithms.&nbsp; So a single improvement to <em>code</em> now has more impact than before; it's liable to produce more further improvements.&nbsp; Think of a uranium pile.&nbsp; It's always running the same \"algorithm\" with respect to neutrons causing fissions that produce further neutrons, but just piling on more uranium can cause it to go from subcritical to supercritical, as any given neutron has more uranium to travel through and a higher chance of causing future fissions.</p>\n<p>So just the resource bonanza represented by \"eating the Internet\" or \"discovering an application for which there is effectively unlimited demand, which lets you rent huge amounts of computing power while using only half of it to pay the bills\" - even though this event isn't particularly <em>recursive</em> of itself, just an object-level fruit-taking - could potentially drive the AI from subcritical to supercritical.</p>\n<p>Not, mind you, that this will happen with an AI that's just stupid.&nbsp; But an AI already improving itself <em>slowly </em>- that's a different case.</p>\n<p>Even if this doesn't happen - if the AI uses this newfound computing power at all effectively, its optimization efficiency will increase more quickly than before; just because the AI has <em>more</em> optimization power to apply to the task of increasing its own efficiency, thanks to the sudden bonanza of optimization resources.</p>\n<p>So the <em>whole trajectory</em> can conceivably change, just from so simple and straightforward and unclever and uninteresting-seeming an act, as eating the Internet.&nbsp; (Or renting a bigger cloud.)</p>\n<p>Agriculture changed the course of human history by supporting a larger population - and that was just a question of having more humans around, not individual humans having a brain a hundred times as large.&nbsp; This gets us into the whole issue of the returns on scaling individual brains not being anything like the returns on scaling the number of brains.&nbsp; A big-brained human has around four times the cranial volume of a chimpanzee, but 4 chimps != 1 human.&nbsp; (And for that matter, 60 squirrels != 1 chimp.)&nbsp; Software improvements here almost certainly completely dominate hardware, of course.&nbsp; But having a thousand scientists who collectively read all the papers in a field, and who talk to each other, is not like having one superscientist who has read all those papers and can correlate their contents directly using native cognitive processes of association, recognition, and abstraction.&nbsp; Having more humans talking to each other using low-bandwidth words, cannot be expected to achieve returns similar to those from scaling component cognitive processes within a coherent cognitive system.</p>\n<p>This, too, is an idiom outside human experience - we <em>have</em> to solve big problems using lots of humans, because there is no way to solve them using ONE BIG human.&nbsp; But it never occurs to anyone to substitute four chimps for one human; and only a certain very foolish kind of boss thinks you can substitute ten programmers with one year of experience for one programmer with ten years of experience.</p>\n<p>(Part of the general Culture of Chaos that praises emergence and thinks evolution is smarter than human designers, also has a mythology of groups being inherently superior to individuals.&nbsp; But this is generally a matter of poor individual rationality, and various arcane group structures that are supposed to compensate; rather than an inherent fact about cognitive processes somehow <em>scaling better when chopped up into distinct brains.</em>&nbsp; If that were <em>literally</em> more efficient, evolution would have designed humans to have four chimpanzee heads that argued with each other.&nbsp; In the realm of AI, it seems much more straightforward to have a single cognitive process that lacks the emotional stubbornness to cling to its accustomed theories, and doesn't <em>need</em> to be argued out of it at gunpoint or replaced by a new generation of grad students.&nbsp; I'm not going to delve into this in detail for now, just warn you to be suspicious of this particular creed of the Culture of Chaos; it's not like they actually <em>observed</em> the relative performance of a hundred humans versus one BIG mind with a brain fifty times human size.)</p>\n<p>So yes, there was a lot of software improvement involved - what we are seeing with the modern human brain size, is probably not so much the brain volume <em>required</em> to support the software improvement, but rather the <em>new evolutionary equilibrium</em> for brain size <em>given</em> the improved software.</p>\n<p>Even so - hominid brain size increased by a factor of five over the course of around five million years.&nbsp; You might want to think <em>very seriously</em> about the contrast between that idiom, and a successful AI being able to expand onto five thousand times as much hardware over the course of five minutes - when you are pondering possible hard takeoffs, and whether the AI trajectory ought to look similar to human experience.</p>\n<p>A subtler sort of hardware overhang, I suspect, is represented by modern CPUs have a 2GHz <em>serial</em> speed, in contrast to neurons that spike 100 times per second on a good day.&nbsp; The \"hundred-step rule\" in computational neuroscience is a rule of thumb that any postulated neural algorithm which runs in realtime has to perform its job in less than 100 <em>serial</em> steps one after the other.&nbsp; We do not understand how to efficiently use the computer hardware we have now, to do intelligent thinking.&nbsp; But the much-vaunted \"massive parallelism\" of the human brain, is, I suspect, <a href=\"/lw/k5/cached_thoughts/\">mostly cache lookups</a> to make up for the sheer awkwardness of the brain's <em>serial</em> slowness - if your computer ran at 200Hz, you'd have to resort to all sorts of absurdly massive parallelism to get anything done in realtime.&nbsp; I suspect that, if <em>correctly designed,</em> a midsize computer cluster would be able to get high-grade thinking done at a serial speed much faster than human, even if the total parallel computing power was less.</p>\n<p>So that's another kind of overhang: because our computing hardware has run so far ahead of AI <em>theory,</em> we have incredibly fast computers we don't know how to use <em>for thinking;</em> getting AI <em>right</em> could produce a huge, discontinuous jolt, as the speed of high-grade thought on this planet suddenly dropped into computer time.</p>\n<p>A still subtler kind of overhang would be represented by human <a href=\"/lw/qk/that_alien_message/\">failure to use our gathered experimental data efficiently</a>.</p>\n<p>On to the topic of insight, another potential source of discontinuity.&nbsp; The course of hominid evolution was driven by evolution's neighborhood search; if the evolution of the brain accelerated to some degree, this was probably due to existing adaptations creating a greater number of possibilities for further adaptations.&nbsp; (But it couldn't accelerate past a certain point, because evolution is limited in how much selection pressure it can apply - if someone succeeds in breeding due to adaptation A, that's less variance left over for whether or not they succeed in breeding due to adaptation B.)</p>\n<p>But all this is searching the raw space of genes.&nbsp; Human design intelligence, or sufficiently sophisticated AI design intelligence, isn't like that.&nbsp; One might even be tempted to make up a completely different curve out of thin air - like, intelligence will take all the easy wins first, and then be left with only higher-hanging fruit, while increasing complexity will defeat the ability of the designer to make changes.&nbsp; So where blind evolution accelerated, intelligent design will run into diminishing returns and grind to a halt.&nbsp; And as long as you're making up fairy tales, you might as well further add that the law of diminishing returns will be exactly right, and have bumps and rough patches in exactly the right places, to produce a smooth gentle takeoff even after recursion and various hardware transitions are factored in...&nbsp; One also wonders why the story about \"intelligence taking easy wins first in designing brains\" <em>tops out</em> at or before human-level brains, rather than going <em>a long way beyond human</em> before topping out.&nbsp; But one suspects that if you tell <em>that</em> story, there's no point in inventing a law of diminishing returns to begin with.</p>\n<p>(Ultimately, if the character of physical law is anything like our current laws of physics, there will be limits to what you can do on finite hardware, and limits to how much hardware you can assemble in finite time, but if they are very <em>high</em> limits relative to human brains, it doesn't affect the basic prediction of hard takeoff, \"AI go FOOM\".)</p>\n<p>The main thing I'll venture into actually expecting from adding \"insight\" to the mix, is that there'll be a discontinuity at the point where the AI <em>understands how to do AI theory,</em> the same way that human researchers try to do AI theory.&nbsp; An AI, to swallow its own optimization chain, must not just be able to rewrite its own source code; it must be able to, say, rewrite <em>Artificial Intelligence: A Modern Approach</em> (2nd Edition).&nbsp; An ability like this seems (untrustworthily, but I don't know what else to trust) like it ought to appear at around the same time that the architecture is at the level of, or approaching the level of, being able to handle what humans handle - being no shallower than an actual human, whatever its inexperience in various domains.&nbsp; It would produce further discontinuity at around that time.</p>\n<p>In other words, when the AI becomes smart enough to <em>do AI theory,</em> that's when I expect it to fully swallow its own optimization chain and for the <em>real</em> FOOM to occur - though the AI might <em>reach</em> this point as part of a cascade that started at a more primitive level.</p>\n<p>All these complications is why I don't believe we can <em>really</em> do any sort of math that will predict <em>quantitatively</em> the trajectory of a hard takeoff.&nbsp; You can make up models, but real life is going to include all sorts of discrete jumps, bottlenecks, bonanzas, insights - and the \"fold the curve in on itself\" paradigm of recursion is going to amplify even small roughnesses in the trajectory.</p>\n<p>So I stick to qualitative predictions.&nbsp; \"AI go FOOM\".</p>\n<p>Tomorrow I hope to tackle locality, and a bestiary of some possible qualitative trajectories the AI might take given this analysis.&nbsp; Robin Hanson's summary of \"primitive AI fooms to sophisticated AI\" doesn't fully represent my views - that's just one entry in the bestiary, albeit a major one.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"CztjQPSTuaQcfbyh8": 2, "oiRp4T6u5poc8r9Tj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tjH8XPxAnr6JRbh7k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 30, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "1167", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JBadX7rwdcRFzGuju", "XQirei3crsLxsCQoi", "dq3KsCsqNotWc8nAK", "2MD3NMLBPCqPfnfre", "5wMcKNAwB6X4mp9og"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-03T21:20:35.000Z", "modifiedAt": null, "url": null, "title": "Permitted Possibilities, & Locality", "slug": "permitted-possibilities-and-locality", "viewCount": null, "lastCommentedAt": "2021-07-04T04:18:15.924Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oFKLSvbDkX8h7amRs/permitted-possibilities-and-locality", "pageUrlRelative": "/posts/oFKLSvbDkX8h7amRs/permitted-possibilities-and-locality", "linkUrl": "https://www.lesswrong.com/posts/oFKLSvbDkX8h7amRs/permitted-possibilities-and-locality", "postedAtFormatted": "Wednesday, December 3rd 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Permitted%20Possibilities%2C%20%26%20Locality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APermitted%20Possibilities%2C%20%26%20Locality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoFKLSvbDkX8h7amRs%2Fpermitted-possibilities-and-locality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Permitted%20Possibilities%2C%20%26%20Locality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoFKLSvbDkX8h7amRs%2Fpermitted-possibilities-and-locality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoFKLSvbDkX8h7amRs%2Fpermitted-possibilities-and-locality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3454, "htmlBody": "<p><strong>Continuation of</strong>:&nbsp; <a href=\"http://www.overcomingbias.com/2008/12/hard-takeoff.html\">Hard Takeoff</a></p>\n\n<p>The analysis given in the last two days permits more than one possible AI trajectory:</p>\n\n<ol><li>Programmers, smarter than evolution at finding tricks that work, but operating without fundamental insight or with only partial insight, create a mind that is dumber than the researchers but performs lower-quality operations much faster.&nbsp; This mind reaches <em>k</em> &gt; 1, cascades up to the level of a very smart human, <em>itself</em> achieves insight into intelligence, and undergoes the really fast part of the FOOM, to superintelligence.&nbsp; This would be the major nightmare scenario for the origin of an unFriendly AI.</li>\n\n<li>Programmers operating with partial insight, create a mind that performs a number of tasks very well, but can't really handle self-modification let alone AI theory.&nbsp; A mind like this might progress with something like smoothness, pushed along by the researchers rather than itself, even all the way up to average-human capability - not having the insight into its own workings to push itself any further.&nbsp; We also suppose that the mind is either already using huge amounts of available hardware, or scales <em>very</em> poorly, so it cannot go FOOM just as a result of adding a hundred times as much hardware.&nbsp; This scenario seems less likely to my eyes, but it is not <em>ruled out</em> by any effect I can see.</li>\n\n<li>Programmers operating with strong insight into intelligence, directly create along an efficient and planned pathway, a mind capable of modifying itself with deterministic precision - provably correct or provably noncatastrophic self-modifications.&nbsp; This is the only way I can see to achieve narrow enough targeting to create a Friendly AI.&nbsp; The &quot;natural&quot; trajectory of such an agent would be slowed by the requirements of precision, and sped up by the presence of insight; but because this is a Friendly AI, notions like &quot;You can't yet improve yourself this far, your goal system isn't verified enough&quot; would play a role.</li></ol>\n\n<p>So these are some things that I think are permitted to happen, albeit that case 2 would count as a hit against me to some degree because it does seem unlikely.</p>\n\n<p>Here are some things that <em>shouldn't</em> happen, on my analysis:</p>\n\n\n<a id=\"more\"></a><ul><li>An ad-hoc self-modifying AI as in (1) undergoes a cycle of self-improvement, starting from stupidity, that\ncarries it up to the level of a very smart human - and then stops,\nunable to progress any further.&nbsp; (The upward slope in this region is supposed to be very steep!)<br />\n</li>\n\n<li>A mostly non-self-modifying AI as in (2) is pushed by its\nprogrammers up to a roughly human level... then to the level of a very\nsmart human... then to the level of a mild transhuman... but the mind still\ndoes not achieve insight into its own workings and still does not\nundergo an intelligence explosion - just continues to increase smoothly\nin intelligence from there.</li></ul>\n\n<p>And I also don't think this is allowed:</p>\n\n<ul><li>The &quot;scenario that Robin Hanson seems to think is the line-of-maximum-probability for AI as heard and summarized by Eliezer Yudkowsky&quot;:<br /><ul><li>No one AI that does everything humans do, but rather a large, diverse population of AIs.&nbsp; These AIs have various <em>domain-specific</em> competencies that are &quot;human+ level&quot; - not just in the sense of Deep Blue beating Kasparov, but in the sense that in these domains, the AIs seem to have good &quot;common sense&quot; and can e.g. recognize, comprehend and handle situations that weren't in their original programming.&nbsp; But only in the special domains for which that AI was crafted/trained.&nbsp; Collectively, these AIs may be strictly more competent than any one human, but no individual AI is more competent than any one human.</li>\n\n<li>Knowledge and even skills are widely traded in this economy of AI systems.</li>\n\n<li>In concert, these AIs, and their human owners, and the economy that surrounds them, undergo a <em>collective</em> FOOM of self-improvement.&nbsp; No local agent is capable of doing all this work, only the collective system.</li>\n\n<li>The FOOM's benefits are distributed through a whole global economy\nof trade partners and suppliers, including existing humans and\ncorporations, though existing humans and corporations may form an\nincreasingly small fraction of the New Economy.</li>\n\n<li>This FOOM looks like an exponential curve of compound interest, like the modern world but with a substantially shorter doubling time.</li></ul></li></ul>\n\n<p>Mostly, Robin seems to think that uploads will come first, but that's a whole 'nother story.&nbsp; So far as AI goes, this looks like Robin's maximum line of probability - and if I got this mostly wrong or all wrong, that's no surprise.&nbsp; Robin Hanson did the same to me when summarizing what he thought were my own positions.&nbsp; I have never thought, in prosecuting this Disagreement, that we were starting out with a mostly good understanding of what the Other was thinking; and this seems like an important thing to have always in mind.</p>\n\n<p>So - bearing in mind that I may well be criticizing a straw misrepresentation, and that I know this full well, but I am just trying to guess my best - here's what I see as wrong with the elements of this scenario:</p>\n\n<p>\n\n\u2022 The abilities we call &quot;human&quot; are the final products of an <a href=\"http://www.overcomingbias.com/2008/10/intelligence-in.html\">economy of mind</a> - not in the sense that there are selfish agents in it, but in the sense that there are production lines; and I would even expect evolution to enforce something approaching fitness as a common unit of currency.&nbsp; (Enough selection pressure to create an adaptation from scratch should be enough to fine-tune the resource curves involved.)&nbsp; It's the production lines, though, that are the main point - that your brain has specialized parts and the specialized parts pass information around.&nbsp; All of this goes on behind the scenes, but it's what finally <em>adds up</em> to any <em>single</em> human ability.\n\n</p>\n\n<p>In other words, trying to get humanlike performance in <em>just one</em> domain, is divorcing a final product of that economy from all the work that stands behind it.&nbsp; It's like having a global economy that can <em>only</em> manufacture toasters, but not dishwashers or light bulbs.&nbsp; You can have something like Deep Blue that beats humans at chess in an inhuman, specialized way; but I don't think it would be easy to get humanish performance at, say, biology R&amp;D, without a whole mind and architecture standing behind it, that would also be able to accomplish other things.&nbsp; Tasks that draw on our cross-domain-ness, or our long-range real-world strategizing, or our ability to formulate new hypotheses, or our ability to use very high-level abstractions - I don't think that you would be able to replace a human in just that one job, without also having something that would be able to learn many different jobs.</p>\n\n<p>I think it is a fair analogy to the idea that you shouldn't see a global economy that can manufacture toasters but not manufacture anything else.</p>\n\n<p>This is why I don't think we'll see a system of AIs that are diverse, individually highly specialized, and <em>only collectively</em> able to do anything a human can do.</p>\n\n<p>\u2022 Trading cognitive content around between diverse AIs is more difficult and less likely than it might sound.&nbsp; Consider the field of AI as it works today.&nbsp; Is there <em>any</em> standard database of cognitive content that you buy off the shelf and plug into your amazing new system, whether it be a chessplayer or a new data-mining algorithm?&nbsp; If it's a chess-playing program, there are databases of stored games - but that's not the same as having databases of preprocessed cognitive content.</p>\n\n<p>So far as I can tell, the diversity of cognitive architectures acts as a <em>tremendous</em> barrier to trading around cognitive content.&nbsp; If you have many AIs around that are all built on the same architecture by the same programmers, they might, <em>with a fair amount of work,</em> be able to pass around learned cognitive content.&nbsp; Even this is less trivial than it sounds.&nbsp; If two AIs both see an apple for the first time, and they both independently form concepts about that apple, and they both independently build some new cognitive content around those concepts, then their <em>thoughts</em> are effectively written in a different language.&nbsp; By seeing a single apple at the same time, they could identify a concept they both have in mind, and in this way build up a common language...</p>\n\n<p>...the point being that even when two separated minds are running literally the same source code, it is still difficult for them to trade new knowledge <em>as raw cognitive content</em> without having a special language designed just for sharing knowledge.</p>\n\n<p>Now suppose the two AIs are built around different architectures.</p>\n\n<p>The barrier this opposes to a true, cross-agent, literal &quot;economy of mind&quot;, is so strong, that in the vast majority of AI applications you set out to write today, you will not bother to import any standardized preprocessed cognitive content.&nbsp; It will be easier for your AI application to start with some standard examples - databases of <em>that</em> sort of thing do exist, in some fields anyway - and <em>redo all the cognitive work of learning</em> on its own.</p>\n\n<p>That's how things stand today.</p>\n\n<p>And I have to say that looking over the diversity of architectures proposed at any AGI conference I've attended, it is very hard to imagine directly trading cognitive content between any two of them.&nbsp; It would be an immense amount of work just to set up a language in which they could communicate what they take to be facts about the world - never mind preprocessed cognitive content.</p>\n\n<p>This is a force for <em>localization:</em> unless the condition I have just described changes drastically, it means that agents will be able to do their own cognitive labor, rather than needing to get their brain content manufactured elsewhere, or even being <em>able</em> to get their brain content manufactured elsewhere.&nbsp; I can imagine there being an exception to this for <em>non</em>-diverse agents that are deliberately designed to carry out this kind of trading within their code-clade.&nbsp; (And in the long run, difficulties of translation seems less likely to stop superintelligences.)</p>\n\n<p>But in <em>today's</em> world, it seems to be the rule that when you write a new AI program, you can sometimes get preprocessed raw data, but you will not buy any preprocessed cognitive content - the internal content of your program will come from within your program.</p>\n\n<p>And it actually does seem to me that AI would have to get <em>very</em> sophisticated before it got over the &quot;hump&quot; of increased sophistication making sharing harder instead of easier.&nbsp; I'm not sure this is pre-takeoff sophistication we're talking about, here.&nbsp; And the cheaper computing power is, the easier it is to just share the <em>data</em> and do the <em>learning</em> on your own.</p>\n\n<p>Again - in today's world, sharing of cognitive content between diverse AIs doesn't happen, even though there are lots of machine learning algorithms out there doing various jobs.&nbsp; You could say things would happen differently in the future, but it'd be up to you to make that case.</p>\n\n<p>\u2022 Understanding the difficulty of interfacing diverse AIs, is the next step toward understanding why it's likely to be a <em>single coherent</em> cognitive system that goes FOOM via recursive self-improvement.&nbsp; The same sort of barriers that apply to trading direct cognitive content, would also apply to trading changes in cognitive source code.</p>\n\n<p>It's a whole lot easier to modify the source code in the interior of your own mind, than to take that modification, and sell it to a friend who happens to be written on different source code.</p>\n\n<p>Certain kinds of abstract insights would be more tradeable, among sufficiently sophisticated minds; and the major insights might be well worth selling - like, if you invented a new <em>general</em> algorithm at some subtask that many minds perform.&nbsp; But if you again look at the modern state of the field, then you find that it is only a few algorithms that get any sort of general uptake.</p>\n\n<p>And if you hypothesize minds that understand these algorithms, and the improvements to them, and what these algorithms are for, and how to implement and engineer them - then these are already very sophisticated minds, at this point, they are AIs that can do their own AI theory.&nbsp; So the hard takeoff has to have not already started, yet, at this point where there are many AIs around that can do AI theory.&nbsp; If they can't do AI theory, diverse AIs are likely to experience great difficulties trading code improvements among themselves.</p>\n\n<p>This is another localizing force.&nbsp; It means that the improvements you make to yourself, and the compound interest earned on those improvements, is likely to stay local.</p>\n\n<p>If the scenario with an AI takeoff is anything at all like the modern world in which all the attempted AGI projects have completely incommensurable architectures, then any self-improvements will definitely stay put, not spread.</p>\n\n<p>\u2022 But suppose that the situation <em>did</em> change drastically from today, and that you had a community of diverse AIs which were sophisticated enough to share cognitive content, code changes, and even insights.&nbsp; And suppose even that this is true at the <em>start</em> of the FOOM - that is, the community of diverse AIs got all the way up to that level, without yet using a FOOM or starting a FOOM at a time when it would still be localized.</p>\n\n<p>We can even suppose that most of the code improvements, algorithmic insights, and cognitive content driving any particular AI, is coming from outside that AI - sold or shared - so that the improvements the AI makes to <em>itself,</em> do not dominate its total velocity.</p>\n\n<p>Fine.&nbsp; The <em>humans</em> are not out of the woods.</p>\n\n<p>Even if we're talking about uploads, it will be immensely more difficult to apply any of the algorithmic insights that are tradeable between AIs, to the undocumented human brain, that is a huge mass of spaghetti code, that was never designed to be upgraded, that is not end-user-modifiable, that is not hot-swappable, that is written for a completely different architecture than what runs efficiently on modern processors...</p>\n\n<p>And biological humans?&nbsp; Their neurons just go on doing whatever neurons do, at 100 cycles per second (tops).</p>\n\n<p>So this FOOM that follows from recursive self-improvement, the cascade effect of using your increased intelligence to rewrite your code and make yourself even smarter -</p>\n\n<p>The barriers to sharing cognitive improvements among diversely designed AIs, are large; the barriers to sharing with uploaded humans, are incredibly huge; the barrier to sharing with biological humans, is essentially absolute.&nbsp; (Barring a (benevolent) superintelligence with nanotechnology, but if one of those is around, you have already won.)</p>\n\n<p>In this hypothetical global economy of mind, the humans are like a country that no one can invest in, that cannot adopt any of the new technologies coming down the line.</p>\n\n<p>I once observed that Ricardo's Law of Comparative Advantage is the theorem that unemployment should not exist.&nbsp; The gotcha being that if someone is sufficiently unreliable, there is a cost to you to train them, a cost to stand over their shoulders and monitor them, a cost to check their results for accuracy - the existence of unemployment in our world is a combination of transaction costs like taxes, regulatory barriers like minimum wage, and above all, <em>lack of trust</em>.&nbsp; There are a dozen things I would pay someone else to do for me - if I wasn't paying taxes on the transaction, and if I could trust a stranger as much as I trust myself (both in terms of their honesty and of acceptable quality of output).&nbsp; Heck, I'd as soon have some formerly unemployed person walk in and spoon food into my mouth while I kept on typing at the computer - if there were no transaction costs, and I trusted them.</p>\n\n<p>If high-quality thought drops into a speed closer to computer time by a few orders of magnitude, no one is going to take a subjective year to explain to a biological human an idea that they will be barely able to grasp, in exchange for an even slower guess at an answer that is probably going to be wrong anyway.</p>\n\n<p>Even <em>uploads</em> could easily end up doomed by this effect, not just because of the immense overhead cost and slowdown of running their minds, but because of the continuing error-proneness of the human architecture.&nbsp; Who's going to trust a giant messy undocumented neural network, any more than you'd run right out and hire some unemployed guy off the street to come into your house and do your cooking?</p>\n\n<p>This FOOM leaves humans behind -</p>\n\n<p>- unless you go the route of Friendly AI, and make a superintelligence that simply <em>wants</em> to help humans, not for any economic value that humans provide to it, but because that is its nature.</p>\n\n<p>And just to be clear on something - which really should be clear by now, from all my other writing, but maybe you're just wandering in - it's not that having squishy things running around on two legs is the ultimate height of existence.&nbsp; But if you roll up a random AI with a random utility function, it just ends up turning the universe into patterns we would not find very eudaimonic - turning the galaxies into paperclips.&nbsp; If you try a haphazard attempt at making a &quot;nice&quot; AI, the sort of not-even-half-baked theories I see people coming up with on the spot and occasionally writing whole books about, like using reinforcement learning on pictures of smiling humans to train the AI to value happiness, yes this was a book, then the AI just transforms the galaxy into tiny molecular smileyfaces...</p>\n\n<p>It's not some small, mean desire to survive for myself at the price of greater possible futures, that motivates me.&nbsp; The thing is - those greater possible futures, they don't happen automatically.&nbsp; There are stakes on the table that are so much an invisible background of your existence that it would never occur to you they could be lost; and these things will be shattered by default, if not specifically preserved.</p>\n\n<p>\u2022 And as for the idea that the whole thing would happen slowly enough for humans to have plenty of time to react to things - a smooth exponential shifted into a shorter doubling time - of that, I spoke yesterday.&nbsp; Progress seems to be exponential now, more or less, or at least accelerating, and that's with constant human brains.&nbsp; If you take a nonrecursive accelerating function and fold it in on itself, you are going to get superexponential progress.&nbsp; &quot;If computing power doubles every eighteen months, what happens when computers are doing the research&quot; should not just be a faster doubling time.&nbsp; (Though, that said, on any sufficiently short timescale, progress might well <em>locally</em> approximate an exponential because investments will shift in such fashion that the marginal returns on investment balance, even in the interior of a single mind; interest rates consistent over a timespan imply smooth exponential growth over that timespan.)</p>\n\n<p>You can't count on warning, or time to react.&nbsp; If an accident sends a sphere of plutonium, not critical, but <em>prompt critical,</em> neutron output can double in a tenth of a second even with <em>k</em> = 1.0006.&nbsp; It can deliver a killing dose of radiation or blow the top off a nuclear reactor before you have time to draw a breath.&nbsp; Computers, like neutrons, already run on a timescale much faster than human thinking.&nbsp; We are already past the world where we can definitely count on having time to react.</p>\n\n<p>When you move into the transhuman realm, you also move into the realm of adult problems.&nbsp; To wield great power carries a price in great precision.&nbsp; You can build a nuclear reactor but you can't ad-lib it.&nbsp; On the problems of this scale, if you want the universe to end up a worthwhile place, you can't just throw things into the air and trust to luck and later correction.&nbsp; That might work in childhood, but not on adult problems where the price of one mistake can be instant death.</p>\n\n<p>Making it into the future is an adult problem.&nbsp; That's not a death sentence.&nbsp; I think.&nbsp; It's not the <em>inevitable</em> end of the world.&nbsp; I hope.&nbsp; But if you want human<em>kind</em> to survive, and the future to be a worthwhile place, then this will take careful crafting of the first superintelligence - not just letting economics <em>or whatever</em> take its easy, natural course.&nbsp; The easy, natural course is fatal - not just to ourselves but to all our hopes.</p>\n\n<p>That, itself, is natural.&nbsp; It is only to be expected.&nbsp; To hit a narrow target you must aim; to reach a good destination you must steer; to win, you must make an extra-ordinary effort.</p>\n\n", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PDJ6KqJBRzvKPfuS3": 1, "pGqRLe9bFDX2G2kXY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oFKLSvbDkX8h7amRs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 20, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "1168", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-04T13:58:51.000Z", "modifiedAt": null, "url": null, "title": "Underconstrained Abstractions", "slug": "underconstrained-abstractions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:41.113Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BpSEpGxtF664uRNkf/underconstrained-abstractions", "pageUrlRelative": "/posts/BpSEpGxtF664uRNkf/underconstrained-abstractions", "linkUrl": "https://www.lesswrong.com/posts/BpSEpGxtF664uRNkf/underconstrained-abstractions", "postedAtFormatted": "Thursday, December 4th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Underconstrained%20Abstractions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUnderconstrained%20Abstractions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBpSEpGxtF664uRNkf%2Funderconstrained-abstractions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Underconstrained%20Abstractions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBpSEpGxtF664uRNkf%2Funderconstrained-abstractions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBpSEpGxtF664uRNkf%2Funderconstrained-abstractions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1416, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/vz/the_weak_inside_view/\">The Weak Inside View</a></p>\n\n<p><a href=\"http://www.overcomingbias.com/2008/12/test-near-apply.html\">Saith Robin</a>:</p><blockquote><p>&quot;It is easy, way too easy, to generate new mechanisms, accounts, theories, and abstractions.&nbsp; To see if such things are <em>useful</em>,\nwe need to vet them, and that is easiest &quot;nearby&quot;, where we know a\nlot.&nbsp; When we want to deal with or understand things &quot;far&quot;, where we\nknow little, we have little choice other than to rely on mechanisms,\ntheories, and concepts that have worked well near.&nbsp; Far is just the\nwrong place to try new things.&quot;</p></blockquote><p>Well... I understand why one would have that reaction.&nbsp; But I'm not sure we can <em>really</em> get away with that.</p>\n\n<p>When possible, I try to talk in concepts that can be verified with\nrespect to existing history.&nbsp; When I talk about natural selection not\nrunning into a law of diminishing returns on genetic complexity or\nbrain size, I'm talking about something that we can try to verify by\nlooking at the capabilities of other organisms with brains big and\nsmall.&nbsp; When I talk about the boundaries to sharing cognitive content\nbetween AI programs, you can look at the field of AI the way it works\ntoday and see that, lo and behold, there isn't a lot of cognitive\ncontent shared.</p>\n\n<p>But in my book this is just <em>one</em> trick in a <em>library</em> of\nmethodologies for dealing with the Future, which is, in general, a hard\nthing to predict.</p>\n\n<p>Let's say that instead of using my complicated-sounding disjunction (many <em>different</em> reasons why the growth trajectory might contain an upward cliff, which don't <em>all</em> have to be true), I instead staked my <em>whole</em> story on the critical\nthreshold of human intelligence.&nbsp; Saying, &quot;Look how sharp the slope is\nhere!&quot; - well, it would <em>sound</em> like a simpler story.&nbsp; It would be closer\nto fitting on a T-Shirt.&nbsp; And by talking about <em>just</em> that one\nabstraction and no others, I could make it sound like I was dealing in\nverified historical facts - humanity's evolutionary history is\nsomething that has already happened.</p>\n\n<p>But speaking of an abstraction being &quot;verified&quot; by previous history is a tricky thing.&nbsp; There is this little problem of <em>underconstraint</em> - of there being more than one possible abstraction that the data &quot;verifies&quot;.</p><a id=\"more\"></a><p>In &quot;<a href=\"/lw/w5/cascades_cycles_insight/\">Cascades, Cycles, Insight</a>&quot;\nI said that economics does not seem to me to deal much in the origins\nof novel knowledge and novel designs, and said, &quot;If I underestimate\nyour power and merely parody your field, by all\nmeans inform me what kind of economic study has been done of such\nthings.&quot;&nbsp; This challenge was answered by <a href=\"http://www.overcomingbias.com/2008/11/cascades-cycles.html#comment-140280102\">comments</a>\ndirecting me to some papers on &quot;endogenous growth&quot;, which happens to be\nthe name of theories that don't take productivity improvements as\nexogenous forces.</p>\n\n<p>I've looked at some literature on endogenous growth.&nbsp; And don't get\nme wrong, it's probably not too bad as economics.&nbsp; However, the seminal\nliterature talks about ideas being generated by combining other ideas,\nso that if you've got N ideas already and you're combining them three\nat a time, that's a potential N!/((3!)(N - 3!)) new ideas to explore. \nAnd then goes on to note that, in this case, there will be vastly more\nideas than anyone can explore, so that the rate at which ideas are\nexploited will depend more on a paucity of explorers than a paucity of\nideas.</p>\n\n<p>Well... first of all, the notion that &quot;ideas are generated by\ncombining other ideas N at a time&quot; is not exactly an amazing AI theory;\nit is an economist looking at, essentially, the whole problem of AI,\nand trying to solve it in 5 seconds or less.&nbsp; It's not as if any\nexperiment was performed to actually watch ideas recombining.&nbsp; Try to\nbuild an AI around this theory and you will find out in very short\norder how useless it is as an account of where ideas come from...</p>\n\n<p>But more importantly, if the only proposition you actually <em>use</em> in your theory is that there are more ideas than people to exploit them, then this is the only proposition that can even be <em>partially</em> verified by testing your theory.</p>\n\n<p>Even if a recombinant growth theory can be fit to the data, then the historical data still underconstrains the <em>many</em>\npossible abstractions that might describe the number of possible ideas\navailable - any hypothesis that has around &quot;more ideas than people to\nexploit them&quot; will fit the same data equally well.&nbsp; You should simply\nsay, &quot;I assume there are more ideas than people to\nexploit them&quot;, not go so far into mathematical detail as to talk about\nN choose 3 ideas.&nbsp; It's not that the dangling math here is\nunderconstrained by the <em>previous</em> data, but that you're not even using it <em>going forward.</em></p>\n\n<p>(And does it even fit the data?&nbsp; I have friends in venture capital\nwho would laugh like hell at the notion that there's an unlimited\nnumber of really good ideas out there.&nbsp; Some kind of Gaussian or\npower-law or something distribution for the goodness of available ideas\nseems more in order...&nbsp; I don't object to &quot;endogenous growth&quot;\nsimplifying things for the sake of having one simplified abstraction\nand seeing if it fits the data well; we all have to do that.&nbsp; Claiming\nthat the underlying math doesn't <em>just</em> let you build a useful model, but <em>also</em>\nhas a fairly direct correspondence to reality, ought to be a whole\n'nother story, in economics - or so it seems to me.)</p>\n\n<p>(If I merely misinterpret the endogenous growth literature or underestimate its sophistication, by all means correct me.)</p>\n\n\n\n<p>The further away you get from highly regular things like atoms,\nand the closer you get to surface phenomena that are the final products\nof many moving parts, the more history underconstrains the abstractions\nthat\nyou use.&nbsp; This is part of what makes futurism difficult.&nbsp; If there were\nobviously only one story that fit the data, who would bother to use\nanything else?</p>\n\n<p>Is Moore's Law a story about the increase in computing power <em>over time</em>\n- the number of transistors on a chip, as a function of how far the\nplanets have spun in their orbits, or how many times a light wave\nemitted from a cesium atom has changed phase?</p>\n\n<p>Or does the same data equally verify a hypothesis about exponential\nincreases in investment in manufacturing facilities and R&amp;D, with\nan even higher exponent, showing a law of diminishing returns?</p>\n\n<p>Or is Moore's Law showing the increase in computing power, as a\nfunction of some kind of optimization pressure applied by human\nresearchers, themselves thinking at a certain rate?</p>\n\n<p>That last one might seem hard to verify, since we've never watched\nwhat happens when a chimpanzee tries to work in a chip R&amp;D lab.&nbsp; But on some raw, elemental level - would\nthe history of the world <em>really</em> be just the same, proceeding on <em>just exactly</em>\nthe same timeline as the planets move in their orbits, if, for these\nlast fifty years, the researchers themselves had been running on the\nlatest generation of computer chip at any given point?&nbsp; That sounds to\nme even sillier than having a financial model in which there's no way\nto ask what happens if real estate prices go down.</p>\n\n<p>And then, when you apply the abstraction going forward, there's the\nquestion of whether there's more than one way to apply it - which is one reason why a\nlot of futurists tend to dwell in great gory detail on the past events\nthat seem to support their abstractions, but just <em>assume</em> a single application forward.</p>\n\n<p>E.g. Moravec in '88, spending a lot of time talking about how much\n&quot;computing power&quot; the human brain seems to use - but much less time\ntalking about whether an AI would use the same amount of computing\npower, or whether using Moore's Law to extrapolate the first\nsupercomputer of this size is the right way to time the arrival of AI. \n(Moravec thought we were supposed to have AI around <em>now,</em> based on his calculations - and he <em>under</em>estimated the size of the supercomputers we'd actually have in 2008.)</p>\n\n<p>That's another part of what makes futurism difficult - after you've\ntold your story about the past, even if it seems like an abstraction\nthat can be &quot;verified&quot; with respect to the past (but what if you\noverlooked an alternative story for the same evidence?) that often\nleaves a lot of slack with regards to exactly what will happen with\nrespect to that abstraction, going forward.</p>\n\n<p>So if it's not as simple as <em>just</em> using the one trick of finding abstractions you can easily verify on available data...</p>\n\n<p>...what are some other tricks to use?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5GYzBE6q89w74dqfk": 7}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BpSEpGxtF664uRNkf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "1169", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["w9KWNWFTXivjJ7rjF", "dq3KsCsqNotWc8nAK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-05T21:03:20.000Z", "modifiedAt": null, "url": null, "title": "Sustained Strong Recursion", "slug": "sustained-strong-recursion", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:47.182Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9wZnasT3uXzmFCcaB/sustained-strong-recursion", "pageUrlRelative": "/posts/9wZnasT3uXzmFCcaB/sustained-strong-recursion", "linkUrl": "https://www.lesswrong.com/posts/9wZnasT3uXzmFCcaB/sustained-strong-recursion", "postedAtFormatted": "Friday, December 5th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sustained%20Strong%20Recursion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASustained%20Strong%20Recursion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9wZnasT3uXzmFCcaB%2Fsustained-strong-recursion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sustained%20Strong%20Recursion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9wZnasT3uXzmFCcaB%2Fsustained-strong-recursion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9wZnasT3uXzmFCcaB%2Fsustained-strong-recursion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2801, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/w5/cascades_cycles_insight/\">Cascades, Cycles, Insight</a>, <a href=\"/lw/w6/recursion_magic/\">Recursion, Magic</a></p>\n\n<p>We seem to have a sticking point at the concept of &quot;recursion&quot;, so I'll zoom in.</p>\n\n<p>You have a friend who, even though he makes plenty of money, just spends all that money every month.&nbsp; You try to persuade your friend to <em>invest</em> a little - making valiant attempts to explain the wonders of compound interest by pointing to analogous processes in nature, like fission chain reactions.</p>\n\n<p>&quot;All right,&quot; says your friend, and buys a ten-year bond for $10,000, with an annual coupon of $500.&nbsp; Then he sits back, satisfied.&nbsp; &quot;There!&quot; he says.&nbsp; &quot;Now I'll have an extra $500 to spend every year, without my needing to do any work!&nbsp; And when the bond comes due, I'll just roll it over, so this can go on <em>indefinitely.</em>&nbsp; Surely, <em>now</em> I'm taking advantage of <em>the power of recursion!</em>&quot;</p>\n\n<p>&quot;Um, no,&quot; you say.&nbsp; &quot;That's not exactly what I had in mind when I talked about 'recursion'.&quot;</p>\n\n<p>&quot;But I used some of my cumulative money earned, to increase my very earning <em>rate,</em>&quot; your friend points out, quite logically.&nbsp; &quot;If that's not 'recursion', what <em>is?</em>&nbsp; My earning power has been 'folded in on itself', just like you talked about!&quot;</p>\n\n<p>&quot;Well,&quot; you say, &quot;not exactly.&nbsp; Before, you were earning $100,000 per year, so your cumulative earnings went as 100000 * t.&nbsp; Now, your cumulative earnings are going as 100500 * t.&nbsp; That's not really much of a change.&nbsp; What we want is for your cumulative earnings to go as B * e^At for some constants A and B - to grow <em>exponentially.</em>&quot;</p>\n\n<p>&quot;<em>Exponentially!</em>&quot; says your friend, shocked.</p>\n\n<p>&quot;Yes,&quot; you say, &quot;recursification has an amazing power to transform growth curves.&nbsp; In this case, it can turn a linear process into an exponential one.&nbsp; But to get that effect, you have to <em>reinvest the coupon payments</em> you get on your bonds - or at least reinvest some of them, instead of just spending them all.&nbsp; And you must be able to do this <em>over and over again.</em>&nbsp; Only <em>then</em> will you get the 'folding in' transformation, so that instead of your cumulative earnings going as y = F(t) = A*t, your earnings will go as the differential equation dy/dt = F(y) = A*y whose solution is y = e^(A*t).&quot;</p><a id=\"more\"></a><p>(I'm going to go ahead and leave out various constants of integration; feel free to add them back in.)\n\n</p>\n\n<p>&quot;Hold on,&quot; says your friend.&nbsp; &quot;I don't understand the justification for what you just did there.&quot;</p>\n\n<p>&quot;Right now,&quot; you explain, &quot;you're earning a steady income at your job, and you also have $500/year from the bond you bought.&nbsp; These are just things that go on generating money at a constant rate per unit time, in the background.&nbsp; So your cumulative earnings are the integral of that constant rate.&nbsp; If your earnings are y, then dy/dt = A, which resolves to y = At.&nbsp; But now, suppose that instead of having these constant earning forces operating in the background, we introduce a strong <em>feedback loop</em> from your cumulative earnings to your earning power.&quot;</p>\n\n<p>&quot;But I bought this one bond here -&quot; says your friend.</p>\n\n<p>&quot;That's not enough for a <em>strong</em> feedback loop,&quot; you say.&nbsp; &quot;Future increases in your cumulative earnings aren't going to increase the value of this one bond, or your salary, any <em>further.</em>&nbsp; One unit of force transmitted back is not a feedback loop - it has to be <em>repeatable.</em>&nbsp; You need a <em>sustained</em> recursion, not a one-off event.&quot;</p>\n\n<p>&quot;Okay,&quot; says your friend, &quot;how about if I buy a $100 bond every year, then?&nbsp; Will <em>that</em> satisfy the strange requirements of this ritual?&quot;</p>\n\n<p>&quot;Still not a strong feedback loop,&quot; you say.&nbsp; &quot;Suppose that next year your salary went up $10,000/year - no, an even simpler example; suppose $10,000 fell in your lap out of the sky.&nbsp; If you only buy $100/year of bonds, that extra $10,000 isn't going to make any long-term difference to the earning curve.&nbsp; But if you're in the habit of investing 50% of found money, then there's a <em>strong</em> feedback loop from your cumulative earnings back to your earning power - we can pump up the cumulative earnings and watch the earning power rise as a direct result.&quot;</p>\n\n<p>&quot;How about if I just invest 0.1% of all my earnings, including the coupons on my bonds?&quot; asks your friend.</p>\n\n<p>&quot;Well...&quot; you say slowly.&nbsp; &quot;That would be a <em>sustained</em> feedback loop but an extremely <em>weak</em> one, where marginal changes to your earnings have relatively small marginal effects on future earning power.&nbsp; I guess it would genuinely be a recursified process, but it would take a long time for the effects to become apparent, and any stronger recursions would easily outrun it.&quot;</p>\n\n<p>&quot;Okay,&quot; says your friend, &quot;I'll start by investing a dollar, and I'll fully reinvest all the earnings from it, and the earnings on those earnings as well -&quot;</p>\n\n<p>&quot;I'm not really sure there are any good investments that will let you invest just a dollar without it being eaten up in transaction costs,&quot; you say, &quot;and it might not make a difference to anything on the timescales we have in mind - though there's an old story about a king, and grains of wheat placed on a chessboard...&nbsp; But realistically, a dollar isn't enough to get started.&quot;</p>\n\n<p>&quot;All right,&quot; says your friend, &quot;suppose I start with $100,000 in bonds, and reinvest 80% of the coupons on those bonds plus rolling over all the principle, at a 5% interest rate, and we ignore inflation for now.&quot;</p>\n\n<p>&quot;Then,&quot; you reply, &quot;we have the differential equation dy/dt = 0.8 * 0.05 * y, with the initial condition y = $100,000 at t=0, which works out to y = $100,000 * e^(.04*t).&nbsp; Or if you're reinvesting discretely rather than continuously, y = $100,000 * (1.04)^t.&quot;</p>\n\n<p>We can similarly view the self-optimizing compiler in this light - it speeds itself up once, but never makes any further improvements, like buying a single bond; it's not a sustained recursion.</p>\n\n<p>And now let us turn our attention to Moore's Law.</p>\n\n<p>I am not a fan of Moore's Law.&nbsp; I think it's a red herring.&nbsp; I don't think you can forecast AI arrival times by using it, I don't think that AI (especially the good kind of AI) depends on Moore's Law continuing.&nbsp; I am agnostic about how long Moore's Law can continue - I simply leave the question to those better qualified, because it doesn't interest me very much...</p>\n\n<p>But for our next simpler illustration of a strong recursification, we shall consider Moore's Law.</p>\n\n<p>Tim Tyler serves us the duty of representing our strawman, repeatedly <a href=\"http://www.overcomingbias.com/2008/12/recursive-self.html#comment-141138416\">telling us</a>, &quot;But chip engineers use computers <em>now,</em> so Moore's Law is <em>already recursive!</em>&quot;</p>\n\n<p>To test this, we perform the equivalent of the thought experiment where we drop $10,000 out of the sky - push on the cumulative &quot;wealth&quot;, and see what happens to the output rate.</p>\n\n<p>Suppose that Intel's engineers could only work using computers of the sort available in 1998.&nbsp; How much would the next generation of computers be slowed down?</p>\n\n<p>Suppose we gave Intel's engineers computers from 2018, in sealed black boxes (not transmitting any of 2018's knowledge).&nbsp; How much would Moore's Law speed up?</p>\n\n<p>I don't work at Intel, so I can't actually answer those questions.&nbsp; I think, though, that if you said in the first case, &quot;Moore's Law would drop way down, to something like 1998's level of improvement measured linearly in additional transistors per unit time,&quot; you would be way off base.&nbsp; And if you said in the second case, &quot;I think Moore's Law would speed up by an order of magnitude, doubling every 1.8 months, until they caught up to the '2018' level,&quot; you would be equally way off base.</p>\n\n<p>In both cases, I would expect the actual answer to be &quot;not all that much happens&quot;.&nbsp; Seventeen instead of eighteen months, nineteen instead of eighteen months, something like that.</p>\n\n<p>Yes, Intel's engineers have computers on their desks.&nbsp; But the serial speed or per-unit price of computing power is not, so far as I know, the limiting resource that bounds their research velocity.&nbsp; You'd probably have to ask someone at Intel to find out how much of their corporate income they spend on computing clusters / supercomputers, but I would guess it's not much compared to how much they spend on salaries or fab plants.</p>\n\n<p>If anyone from Intel reads this, and wishes to explain to me how it would be unbelievably difficult to do their jobs using computers from ten years earlier, so that Moore's Law would slow to a crawl - then I stand ready to be corrected.&nbsp; But relative to my present state of partial knowledge, I would say that this does not look like a strong feedback loop.</p>\n\n<p>However...</p>\n\n<p>Suppose that the <em>researchers themselves</em> are running as uploads, software on the computer chips produced by their own factories.</p>\n\n<p>Mind you, this is not the tiniest bit realistic.&nbsp; By my standards it's not even a very <em>interesting</em> way of looking at the Singularity, because it does not deal with <em>smarter</em> minds but merely <em>faster</em> ones - it dodges the really difficult and interesting part of the problem.</p>\n\n<p>Just as nine women cannot gestate a baby in one month; just as ten thousand researchers cannot do in one year what a hundred researchers can do in a hundred years; so too, a chimpanzee cannot do four years what a human can do in one year, even though the chimp has around one-fourth the human's cranial capacity.&nbsp; And likewise a chimp cannot do in 100 years what a human does in 95 years, even though they share 95% of our genetic material.</p>\n\n<p><em>Better-designed</em> minds don't scale the same way as <em>larger</em> minds, and <em>larger</em> minds don't scale the same way as <em>faster</em> minds, any more than <em>faster</em> minds scale the same way as <em>more numerous</em> minds.&nbsp; So the notion of merely <em>faster</em> researchers, in my book, fails to address the interesting part of the &quot;intelligence explosion&quot;.</p>\n\n<p>Nonetheless, for the sake of illustrating this matter in a relatively simple case...</p>\n\n<p>Suppose the researchers and engineers themselves - and the rest of the humans on the planet, providing a market for the chips and investment for the factories - are all running on the same computer chips that are the product of these selfsame factories.&nbsp; Suppose also that robotics technology stays on the same curve and provides these researchers with fast manipulators and fast sensors.&nbsp; We also suppose that the technology feeding Moore's Law has not yet hit physical limits.&nbsp; And that, as human brains are already highly parallel, we can speed them up even if Moore's Law is manifesting in increased parallelism instead of faster serial speeds - we suppose the uploads aren't <em>yet</em> being run on a fully parallelized machine, and so their actual serial speed goes up with Moore's Law.&nbsp; Etcetera.</p>\n\n<p>In a fully naive fashion, we just take the economy the way it is today, and run it on the computer chips that the economy itself produces.</p>\n\n<p>In our world where human brains run at constant speed (and eyes and hands work at constant speed), Moore's Law for computing power <em>s</em> is:</p><blockquote><p>s = R(t) = e^t</p></blockquote><p>The function R is the Research curve that relates the amount of Time <em>t</em> passed, to the current Speed of computers <em>s</em>.</p>\n\n<p>To understand what happens when the researchers themselves are running on computers, we simply suppose that R does not relate computing technology to <em>sidereal</em> time - the orbits of the planets, the motion of the stars - but, rather, relates computing technology to the amount of subjective time spent researching it.</p>\n\n<p>Since in <em>our</em> world, subjective time is a linear function of sidereal time, this hypothesis fits <em>exactly the same curve R</em> to observed human history so far.</p>\n\n<p>Our direct measurements of observables do not constrain between the two hypotheses</p><blockquote><p>Moore's Law is exponential in the number of orbits of Mars around the Sun</p></blockquote><p>and</p><blockquote><p>Moore's Law is exponential in the amount of subjective time that researchers spend thinking, and experimenting and building using a proportional amount of sensorimotor bandwidth.</p></blockquote><p>But our prior knowledge of causality may lead us to prefer the second hypothesis.</p>\n\n<p>So to understand what happens when the Intel engineers themselves run on computers (and use robotics) subject to Moore's Law, we recursify and get:</p><blockquote><p>dy/dt = s = R(y) = e^y</p></blockquote><p>Here <em>y</em> is the total amount of elapsed <em>subjective</em> time, which at any given point is increasing according to the computer speed <em>s</em> given by Moore's Law, which is determined by the same function R that describes how Research converts elapsed subjective time into faster computers.&nbsp; Observed human history to date roughly matches the hypothesis that R is exponential with a doubling time of eighteen subjective months (or whatever).</p>\n\n<p>Solving</p><blockquote><p>dy/dt = e^y</p></blockquote><p>yields</p><blockquote><p>y = -ln(C - t)</p></blockquote><p>One observes that this function goes to +infinity at a finite time C.</p>\n\n<p>This is only to be expected, given our assumptions.&nbsp; After eighteen sidereal months, computing speeds double; after another eighteen subjective months, or nine sidereal months, computing speeds double again; etc.</p>\n\n<p>Now, unless the physical universe works in a way that is not only <em>different</em> from the current standard model, but has a different <em>character of physical law</em> than the current standard model, you can't <em>actually</em> do infinite computation in finite time.</p>\n\n<p>Let us suppose that if our biological world had no Singularity, and Intel just kept on running as a company, populated by humans, forever, that Moore's Law would start to run into trouble around 2020.&nbsp; Say, after 2020 there would be a ten-year gap where chips simply stagnated, until the next doubling occurred after a hard-won breakthrough in 2030.</p>\n\n<p>This just says that R(y) is not an indefinite exponential curve.&nbsp; By hypothesis, from subjective years 2020 to 2030, R(y) is flat, corresponding to a constant computer speed <em>s</em>.&nbsp; So dy/dt is constant over this same time period:&nbsp; Total elapsed subjective time <em>y</em> grows at a linear rate, and as <em>y</em> grows, R(y) and computing speeds remain flat, until ten subjective years have passed.&nbsp; So the <em>sidereal</em> bottleneck lasts ten subjective years times the current sidereal/subjective conversion rate at 2020's computing speeds.<br />\n\n</p>\n\n<p>In short, the whole scenario behaves exactly like what you would expect - the simple transform really does describe the naive scenario of &quot;drop the economy into the timescale of its own computers&quot;.</p>\n<p>After subjective year 2030, things pick up again, maybe - there are ultimate physical limits on computation, but they're pretty damned high, and we've got a ways to go until there.&nbsp; But maybe Moore's Law is slowing down - going subexponential, and then as the physical limits are approached, logarithmic, and then simply giving out.</p>\n\n<p>But whatever your beliefs about where Moore's Law ultimately goes, you can just map out the way you would expect the research function R to work as a function of sidereal time in our own world, and then apply the transformation dy/dt = R(y) to get the progress of the uploaded civilization over sidereal time t.&nbsp; (Its progress over <em>subjective</em> time is simply given by R.)</p>\n\n<p>If sensorimotor bandwidth is the critical limiting resource, then we instead care about R&amp;D on fast sensors and fast manipulators.&nbsp; We want R_sm(y) instead R(y), where R_sm is the progress rate of sensors and manipulators, as a function of elapsed sensorimotor time.&nbsp; And then we write dy/dt = R_sm(y) and crank on the equation again to find out what the world looks like from a sidereal perspective.</p>\n\n<p>We can verify that the Moore's Researchers scenario is a strong positive feedback loop by performing the &quot;drop $10,000&quot; thought experiment.&nbsp; Say, we drop in chips from another six doublings down the road - letting the researchers run on those faster chips, while holding constant their state of technological knowledge.</p>\n\n<p>Lo and behold, this drop has a rather <em>large</em> impact, much larger than the impact of giving faster computers to our own biological world's Intel.&nbsp; <em>Subjectively</em> the impact may be unnoticeable - as a citizen, you just see the planets slow down again in the sky.&nbsp; But sidereal growth rates increase by a factor of 64.</p>\n\n<p>So this is indeed deserving of the names, &quot;strong positive feedback loop&quot; and &quot;sustained recursion&quot;.</p>\n\n<p>As disclaimed before, all this isn't <em>really</em> going to happen.&nbsp; There would be effects like those Robin Hanson prefers to analyze, from being able to spawn new researchers as the cost of computing power decreased.&nbsp; You might be able to pay more to get researchers twice as fast.&nbsp; Above all, someone's bound to try hacking the uploads for increased intelligence... and then those uploads will hack themselves even further...&nbsp; Not to mention that it's not clear how this civilization cleanly dropped into computer time in the first place.</p>\n\n<p>So no, this is not supposed to be a realistic vision of the future.</p>\n\n<p>But, alongside our earlier parable of compound interest, it <em>is</em> supposed to be an illustration of how strong, sustained recursion has much more drastic effects on the shape of a growth curve, than a one-off case of one thing leading to another thing.&nbsp; Intel's engineers <em>running on</em> computers is not like Intel's engineers <em>using</em> computers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PDJ6KqJBRzvKPfuS3": 1, "6nS8oYmSMuFMaiowF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9wZnasT3uXzmFCcaB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 15, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "1170", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dq3KsCsqNotWc8nAK", "rJLviHqJMTy8WQkow"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-06T14:26:15.000Z", "modifiedAt": null, "url": null, "title": "Is That Your True Rejection?", "slug": "is-that-your-true-rejection", "viewCount": null, "lastCommentedAt": "2020-03-16T06:46:33.809Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TGux5Fhcd7GmTfNGC/is-that-your-true-rejection", "pageUrlRelative": "/posts/TGux5Fhcd7GmTfNGC/is-that-your-true-rejection", "linkUrl": "https://www.lesswrong.com/posts/TGux5Fhcd7GmTfNGC/is-that-your-true-rejection", "postedAtFormatted": "Saturday, December 6th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20That%20Your%20True%20Rejection%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20That%20Your%20True%20Rejection%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTGux5Fhcd7GmTfNGC%2Fis-that-your-true-rejection%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20That%20Your%20True%20Rejection%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTGux5Fhcd7GmTfNGC%2Fis-that-your-true-rejection", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTGux5Fhcd7GmTfNGC%2Fis-that-your-true-rejection", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 968, "htmlBody": "\n\n\n\n  \n\n  \n\n  <p>It happens every now and then that someone encounters some of my transhumanist-side beliefs&#x2014;as opposed to my ideas having to do with human rationality&#x2014;strange, exotic-sounding ideas like superintelligence and Friendly AI. And the one rejects them. </p>\n\n  <p>If the one is called upon to explain the rejection, not uncommonly the one says, &#x201C;Why should I believe anything Yudkowsky says? He doesn&#x2019;t have a PhD!&#x201D;</p>\n\n  <p>And occasionally someone else, hearing, says, &#x201C;Oh, you should get a PhD, so that people will listen to you.&#x201D; Or this advice may even be offered by the same one who expressed disbelief, saying, &#x201C;Come back when you have a PhD.&#x201D;</p>\n\n  <p>Now, there are good and bad reasons to get a PhD. This is one of the bad ones.</p>\n\n  <p>There are many reasons why someone might actually have an initial adverse reaction to transhumanist theses. Most are matters of pattern recognition, rather than verbal thought: the thesis calls to mind an associated category like &#x201C;strange weird idea&#x201D; or &#x201C;science fiction&#x201D; or &#x201C;end-of-the-world cult&#x201D; or &#x201C;overenthusiastic youth.&#x201D;<span><sup><a href=\"#fn1x34\" id=\"fn1x34-bk\">1</a></sup></span><span id=\"x40-41001f1\"> Immediately, at the speed of perception, the idea is rejected.</span></p>\n\n  <p>If someone afterward says, &#x201C;Why not?&#x201D; this launches a search for justification, but the search won&#x2019;t necessarily hit on the true reason. By &#x201C;&#x2018;true reason,&#x201D; I don&#x2019;t mean the <em>best</em> reason that could be offered. Rather, I mean whichever causes were decisive as a matter of historical fact, at the <em>very first</em> moment the rejection occurred.</p>\n\n  <p>Instead, the search for justification hits on the justifying-sounding fact, &#x201C;This speaker does not have a PhD.&#x201D; But I also don&#x2019;t have a PhD when I talk about human rationality, so why is the same objection not raised there?</p>\n\n  <p>More to the point, if I <em>had</em> a PhD, people would not treat this as a decisive factor indicating that they ought to believe everything I say. Rather, the same initial rejection would occur, for the same reasons; and the search for justification, afterward, would terminate at a different stopping point.</p>\n\n  <p>They would say, &#x201C;Why should I believe <em>you</em>? You&#x2019;re just some guy with a PhD! There are lots of those. Come back when you&#x2019;re well-known in your field and tenured at a major university.&#x201D;</p>\n\n  <p>But do people <em>actually</em> believe arbitrary professors at Harvard who say weird things? Of course not.</p>\n\n  <p>If you&#x2019;re saying things that sound <em>wrong</em> to a novice, as opposed to just rattling off magical-sounding technobabble about leptical quark braids in N + 2 dimensions; and if the hearer is a stranger, unfamiliar with you personally and unfamiliar with the subject matter of your field; then I suspect that the point at which the average person will actually start to grant credence overriding their initial impression, purely because of academic credentials, is somewhere around the Nobel Laureate level. If that. Roughly, you need whatever level of academic credential qualifies as &#x201C;beyond the mundane.&#x201D;</p>\n\n  <p>This is more or less what happened to Eric Drexler, as far as I can tell. He presented his vision of nanotechnology, and people said, &#x201C;Where are the technical details?&#x201D; or &#x201C;Come back when you have a PhD!&#x201D; And Eric Drexler spent six years writing up technical details and got his PhD under Marvin Minsky for doing it. And <em>Nanosystems</em> is a great book. But did the same people who said, &#x201C;Come back when you have a PhD,&#x201D; actually change their minds at all about molecular nanotechnology? Not so far as I ever heard.</p>\n\n  <p>This might be an important thing for young businesses and new-minted consultants to keep in mind&#x2014;that what your failed prospects <em>tell</em> you is the reason for rejection may not make the <em>real</em> difference; and you should ponder that carefully before spending huge efforts. If the venture capitalist says, &#x201C;If only your sales were growing a little faster!&#x201D; or if the potential customer says, &#x201C;It seems good, but you don&#x2019;t have feature X,&#x201D; that may not be the <em>true</em> rejection. Fixing it may, or may not, change anything.</p>\n\n  <p>And it would also be something to keep in mind during disagreements. Robin Hanson and I share a belief that two rationalists should not agree to disagree: they should not have common knowledge of epistemic disagreement unless something is very wrong.<span><sup><a href=\"#fn2x34\" id=\"fn2x34-bk\">2</a></sup></span><span id=\"x40-41002f2\"></span></p>\n\n  <p>I suspect that, in general, if two rationalists set out to resolve a disagreement that persisted past the first exchange, they should expect to find that the true sources of the disagreement are either hard to communicate, or hard to expose. E.g.:</p>\n\n  <ul>\n    <li>Uncommon, but well-supported, scientific knowledge or math;</li>\n\n    <li>Long inferential distances;</li>\n\n    <li>Hard-to-verbalize intuitions, perhaps stemming from specific visualizations;</li>\n\n    <li>Zeitgeists inherited from a profession (that may have good reason for it);</li>\n\n    <li>Patterns perceptually recognized from experience;</li>\n\n    <li>Sheer habits of thought;</li>\n\n    <li>Emotional commitments to believing in a particular outcome;</li>\n\n    <li>Fear that a past mistake could be disproved;</li>\n\n    <li>Deep self-deception for the sake of pride or other personal benefits.</li>\n  </ul>\n\n  <p>If the matter were one in which <em>all</em> the true rejections could be <em>easily</em> laid on the table, the disagreement would probably be so straightforward to resolve that it would never have lasted past the first meeting.</p>\n\n  <p>&#x201C;Is this my true rejection?&#x201D; is something that both disagreers should surely be asking <em>themselves</em>, to make things easier on the other person. However, attempts to directly, publicly psychoanalyze the other may cause the conversation to degenerate <em>very</em> fast, from what I&#x2019;ve seen.</p>\n\n  <p>Still&#x2014;&#x201C;Is that your true rejection?&#x201D; should be fair game for Disagreers to humbly ask, if there&#x2019;s any productive way to pursue that sub-issue. Maybe the rule could be that you can openly ask, &#x201C;Is that simple straightforward-sounding reason your <em>true</em> rejection, or does it come from intuition-X or professional-zeitgeist-Y ?&#x201D; While the more embarrassing possibilities lower on the table are left to the Other&#x2019;s conscience, as their own responsibility to handle.</p>\n\n  <div class=\"footnotes\">\n    \n\n    <p><span><sup><a href=\"#fn1x34-bk\" id=\"fn1x34\">1</a></sup></span>See &#x201C;<a href=\"https://lesswrong.com/rationality/science-as-attire\">Science as Attire</a>&#x201D; in <em>Map and Territory</em>.</p>\n\n    <p><span><sup><a href=\"#fn2x34-bk\" id=\"fn2x34\">2</a></sup></span>See <span id=\"cite.0.Finney.2006\">Hal Finney, &#x201C;Agreeing to Agree,&#x201D; <em>Overcoming Bias</em> (blog), 2006,</span> <a href=\"http://www.overcomingbias.com/2006/12/agreeing_to_agr.html\">http://www.overcomingbias.com/2006/12/agreeing_to_agr.html</a>.</p>\n  </div>\n\n", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZzxvopS4BwLuQy42n": 6, "Ng8Gice9KNkncxqcj": 2, "LDTSbmXtokYAsEq8e": 2, "iP2X4jQNHMWHRNPne": 7, "ZXFpyQWPB5ideFbEG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TGux5Fhcd7GmTfNGC", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 88, "baseScore": 105, "extendedScore": null, "score": 0.000156, "legacy": true, "legacyId": "1171", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "GSqFqc646rsRd2oyz", "canonicalCollectionSlug": "rationality", "canonicalBookId": "coGq3LC5Yn4vZiu6k", "canonicalNextPostSlug": "entangled-truths-contagious-lies", "canonicalPrevPostSlug": "fake-justification", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 105, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 100, "af": false, "version": "2.0.0", "pingbacks": {"Posts": ["4Bwr6s9dofvqPWakn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-07T20:05:33.000Z", "modifiedAt": null, "url": null, "title": "Artificial Mysterious Intelligence", "slug": "artificial-mysterious-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:16.242Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fKofLyepu446zRgPP/artificial-mysterious-intelligence", "pageUrlRelative": "/posts/fKofLyepu446zRgPP/artificial-mysterious-intelligence", "linkUrl": "https://www.lesswrong.com/posts/fKofLyepu446zRgPP/artificial-mysterious-intelligence", "postedAtFormatted": "Sunday, December 7th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Artificial%20Mysterious%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArtificial%20Mysterious%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfKofLyepu446zRgPP%2Fartificial-mysterious-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Artificial%20Mysterious%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfKofLyepu446zRgPP%2Fartificial-mysterious-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfKofLyepu446zRgPP%2Fartificial-mysterious-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1623, "htmlBody": "<p><strong>Previously in series</strong>:&nbsp; <a href=\"/lw/vy/failure_by_affective_analogy/\">Failure By Affective Analogy</a></p>\n<p>I once had a conversation that I still remember for its sheer, purified archetypicality.&nbsp; This was a nontechnical guy, but pieces of this dialog have also appeared in conversations I've had with professional <a href=\"/lw/uc/aboveaverage_ai_scientists/\">AIfolk</a>...</p>\n<blockquote>\n<p>Him:&nbsp; Oh, you're working on AI!&nbsp; Are you using neural networks?</p>\n<p>Me:&nbsp; I think emphatically <em>not</em>.</p>\n<p>Him:&nbsp; But neural networks are so wonderful!&nbsp; They solve problems and we don't have any idea how they do it!</p>\n<p>Me:&nbsp; If you are ignorant of a phenomenon, that is a fact about your state of mind, not a fact about the phenomenon itself.&nbsp; Therefore your ignorance of how neural networks are solving a specific problem, cannot be responsible for making them work better.</p>\n<p>Him:&nbsp; Huh?</p>\n<p>Me:&nbsp; If you don't know how your AI works, that is not good.&nbsp; It is bad.</p>\n<p>Him:&nbsp; Well, intelligence is much too difficult for us to understand, so we need to find <em>some</em> way to build AI without understanding how it works.</p>\n</blockquote>\n<p><a id=\"more\"></a></p>\n<blockquote>\n<p>Me:&nbsp; Look, even if you could do that, you wouldn't be able to predict any kind of positive outcome from it.&nbsp; For all you knew, the AI would go out and slaughter orphans.</p>\n<p>Him:&nbsp; Maybe we'll build Artificial Intelligence by scanning the brain and building a neuron-by-neuron duplicate.&nbsp; Humans are the only systems we know are intelligent.</p>\n<p>Me:&nbsp; It's hard to build a flying machine if the only thing you understand about flight is that somehow birds magically fly.&nbsp; What you need is a concept of aerodynamic lift, so that you can see how something can fly even if it isn't exactly like a bird.</p>\n<p>Him:&nbsp; That's too hard.&nbsp; We have to copy something that we know works.</p>\n<p>Me:&nbsp; <em>(reflectively)</em> What do people find so unbearably <em>awful</em> about the prospect of having to finally break down and solve the bloody problem?&nbsp; Is it really <em>that</em> horrible?</p>\n<p>Him:&nbsp; Wait... you're saying you want to actually <em>understand</em> intelligence?</p>\n<p>Me:&nbsp; Yeah.</p>\n<p>Him:&nbsp; <em>(aghast)</em>&nbsp; Seriously?</p>\n<p>Me:&nbsp; I don't know everything I need to know about intelligence, but I've learned a hell of a lot.&nbsp; Enough to know what happens if I try to build AI while there are still gaps in my understanding.</p>\n<p>Him:&nbsp; Understanding the problem is too hard.&nbsp; You'll never do it.</p>\n</blockquote>\n<p>That's not just a difference of opinion you're looking at, it's a <em>clash of cultures.</em></p>\n<p>For a long time, many different parties and factions in AI, adherent to more than one ideology, have been trying to build AI <em>without</em> understanding intelligence.&nbsp; And their habits of thought have become ingrained in the field, and even transmitted to parts of the general public.</p>\n<p>You may have heard proposals for building true AI which go something like this:</p>\n<ol>\n<li>Calculate how many operations the human brain performs every second.&nbsp; This is \"the only amount of computing power that we know is actually sufficient for human-equivalent intelligence\".&nbsp; Raise enough venture capital to buy a supercomputer that performs an equivalent number of floating-point operations in one second.&nbsp; Use it to run the most advanced available neural network algorithms.</li>\n<li>The brain is huge and complex.&nbsp; When the Internet becomes sufficiently huge and complex, intelligence is bound to emerge from the Internet.&nbsp; <em>(I get asked about this in 50% of my interviews.)</em></li>\n<li>Computers seem unintelligent because they lack common sense.&nbsp; Program a very large number of \"common-sense facts\" into a computer.&nbsp; Let it try to reason about the relation of these facts.&nbsp; Put a sufficiently huge quantity of knowledge into the machine, and intelligence will emerge from it.</li>\n<li>Neuroscience continues to advance at a steady rate.&nbsp; Eventually, super-MRI or brain sectioning and scanning will give us precise knowledge of the local characteristics of all human brain areas.&nbsp; So we'll be able to build a duplicate of the human brain by duplicating the parts.&nbsp; \"The human brain is the only example we have of intelligence.\"</li>\n<li>Natural selection produced the human brain.&nbsp; It is \"the only method that we know works for producing general intelligence\".&nbsp; So we'll have to scrape up a really huge amount of computing power, and <em>evolve</em> AI.</li>\n</ol>\n<p>What do all these proposals have in common?</p>\n<p>They are all ways to make yourself believe that you can build an Artificial Intelligence, even if you don't understand exactly how intelligence works.</p>\n<p>Now, such a belief is not necessarily <em>false!</em>&nbsp; Methods 4 and 5, if pursued long enough and with enough resources, <em>will</em> eventually work.&nbsp; (5 might require a computer the size of the Moon, but give it <em>enough</em> crunch and it will work, even if you have to simulate a quintillion planets and not just one...)</p>\n<p>But regardless of whether any given method would work in principle, the unfortunate habits of thought will already begin to arise, as soon as you start thinking of ways to create Artificial Intelligence without having to penetrate the<span style=\"font-style: italic;\">&nbsp;</span><em>mystery of intelligence</em>.</p>\n<p>I have already spoken of some of the hope-generating tricks that appear in the examples above.&nbsp; There is <a href=\"/lw/vx/failure_by_analogy/\">invoking similarity to humans</a>, or using <a href=\"/lw/vy/failure_by_affective_analogy/\">words that make you feel good</a>.&nbsp; But really, a lot of the trick here just consists of imagining yourself hitting the AI problem with a <em>really big rock.</em></p>\n<p>I know someone who goes around insisting that AI will cost a quadrillion dollars, and as soon as we're willing to spend a quadrillion dollars, we'll have AI, and we couldn't possibly get AI without spending a quadrillion dollars.&nbsp; \"Quadrillion dollars\" is his big rock, that he imagines hitting the problem with, even though he doesn't quite understand it.</p>\n<p>It often will not occur to people that the mystery of intelligence could be any more penetrable than it <em>seems:</em>&nbsp; By the power of the <a href=\"/lw/oi/mind_projection_fallacy/\">Mind Projection Fallacy</a>, being ignorant of how intelligence works will <a href=\"/lw/wb/chaotic_inversion/\">make it seem like intelligence is inherently impenetrable and chaotic</a>.&nbsp; They will think they possess a positive knowledge of intractability, rather than thinking, \"I am ignorant.\"</p>\n<p>And the thing to remember is that, for these last decades on end, <em>any</em> professional in the field of AI trying to build \"real AI\", had some reason for trying to do it without really understanding intelligence (<a href=\"/lw/tf/dreams_of_ai_design/\">various fake reductions aside</a>).</p>\n<p>The <a href=\"/lw/vv/logical_or_connectionist_ai/\">New Connectionists</a> accused the <a href=\"/lw/vt/the_nature_of_logic/\">Good-Old-Fashioned AI</a> researchers of not being parallel enough, not being fuzzy enough, not being emergent enough.&nbsp; But they did not say, \"There is too much you do not understand.\"</p>\n<p>The New Connectionists catalogued the flaws of GOFAI for years on end, with fiery castigation.&nbsp; But they couldn't ever actually say: \"How <em>exactly </em>are all these logical deductions going to produce 'intelligence', anyway?&nbsp; Can you walk me through the cognitive operations, step by step, which lead to that result?&nbsp; Can you explain 'intelligence' and how you plan to get it, without pointing to humans as an example?\"</p>\n<p>For they themselves would be subject to exactly the same criticism.</p>\n<p>In the house of glass, somehow, no one ever gets around to talking about throwing stones.</p>\n<p>To tell a lie, you have to lie about all the other facts entangled with that fact, and also lie about the methods used to arrive at beliefs:&nbsp; The culture of Artificial Mysterious Intelligence has developed its own <a href=\"/lw/uy/dark_side_epistemology/\">Dark Side Epistemology</a>, complete with reasons why it's actually <em>wrong</em> to try and understand intelligence.</p>\n<p>Yet when you step back from the bustle of this moment's history, and think about the long sweep of science - there was a time when stars were mysterious, when chemistry was mysterious, when life was mysterious.&nbsp; And in this era, much was attributed to black-box essences.&nbsp; And there were many hopes based on the <a href=\"/lw/vx/failure_by_analogy/\">similarity</a> of one thing to another.&nbsp; To many, I'm sure, alchemy just seemed very <em>difficult</em> rather than even seeming&nbsp; <em>mysterious;</em> most alchemists probably did not go around thinking, \"Look at how much I am disadvantaged by not knowing about the existence of chemistry!&nbsp; I must discover atoms and molecules as soon as possible!\"&nbsp; They just memorized libraries of random things you could do with acid, and bemoaned how difficult it was to create the Philosopher's Stone.</p>\n<p>In the end, though, what happened is that scientists achieved <a href=\"/lw/w5/cascades_cycles_insight/\">insight</a>, and <em>then</em> things got much easier to do.&nbsp; You also had a better idea of what you could or couldn't do.&nbsp; The problem stopped being <em>scary</em> and <em>confusing</em>.</p>\n<p>But you wouldn't hear a New Connectionist say, \"Hey, maybe all the failed promises of 'logical AI' were basically due to the fact that, in their epistemic condition, they had no right to expect their AIs to work in the first place, because they couldn't actually have sketched out the link in any more detail than a medieval alchemist trying to explain why a particular formula for the Philosopher's Stone will yield gold.\"&nbsp; It would be like the Pope attacking Islam on the basis that faith is not an adequate justification for asserting the existence of their deity.</p>\n<p>Yet in fact, the promises <em>did</em> fail, and so we can conclude that the promisers overreached what they had a right to expect.&nbsp; The Way is not omnipotent, and a bounded rationalist cannot do all things.&nbsp; But even a bounded rationalist can aspire not to overpromise - to only <em>say</em> you can do, that which you <em>can</em> do.&nbsp; So if we want to achieve that reliably, history shows that we should not accept certain kinds of hope.&nbsp; In the absence of insight, hopes tend to be unjustified because you lack the knowledge that would be needed to justify them.</p>\n<p>We humans have a difficult time working in the absence of insight.&nbsp; It doesn't reduce us all the way down to being <a href=\"/lw/kt/evolutions_are_stupid_but_work_anyway/\">as stupid as evolution</a>.&nbsp; But it makes everything difficult and tedious and annoying.</p>\n<p>If the prospect of having to finally break down and solve the bloody problem of intelligence seems scary, you underestimate the interminable hell of <em>not</em> solving it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "PJKgSRkXkCqXmCk3M": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fKofLyepu446zRgPP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 24, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "1172", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["o8Ju8pTGvAfkkg6xZ", "9HGR5qatMGoz4GhKj", "C4EjbrvG3PvZzizZb", "ZTRiSNmeGQK8AkdN2", "NyFtHycJvkyNjXNsP", "p7ftQ6acRkgo6hqHb", "juomoqiNzeAuq4JMm", "c93eRh3mPaN62qrD2", "XTWkjCJScy2GFAgDt", "dq3KsCsqNotWc8nAK", "jAToJHtg39AMTAuJo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-08T15:51:58.000Z", "modifiedAt": null, "url": null, "title": "True Sources of Disagreement", "slug": "true-sources-of-disagreement", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:52.547Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3pyLbH3BqevetQros/true-sources-of-disagreement", "pageUrlRelative": "/posts/3pyLbH3BqevetQros/true-sources-of-disagreement", "linkUrl": "https://www.lesswrong.com/posts/3pyLbH3BqevetQros/true-sources-of-disagreement", "postedAtFormatted": "Monday, December 8th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20True%20Sources%20of%20Disagreement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATrue%20Sources%20of%20Disagreement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3pyLbH3BqevetQros%2Ftrue-sources-of-disagreement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=True%20Sources%20of%20Disagreement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3pyLbH3BqevetQros%2Ftrue-sources-of-disagreement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3pyLbH3BqevetQros%2Ftrue-sources-of-disagreement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2284, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/wj/is_that_your_true_rejection/\">Is That Your True Rejection?</a></p>\n\n<p>I expected from the beginning, that <a href=\"/lw/wj/is_that_your_true_rejection/\">the difficult part of two rationalists reconciling a persistent disagreement, would be for them to expose the true sources of their beliefs</a>.</p>\n\n\n\n<p>One suspects that this will only work if each party takes responsibility for their own end; it's very hard to see inside someone else's head.&nbsp; Yesterday I exhausted myself mentally while out on my daily walk, asking myself the Question &quot;What do you think you know, and why do you think you know it?&quot; with respect to &quot;How much of the AI problem compresses to large insights, and how much of it is unavoidable nitty-gritty?&quot;&nbsp; Trying to either understand why my brain believed what it believed, or else force my brain to experience enough genuine doubt that I could reconsider the question and arrive at a real justification that way.&nbsp; It's hard to see how Robin Hanson could have done any of this work for me.</p>\n\n<p>Presumably a symmetrical fact holds about my lack of access to the real reasons why Robin believes what he believes.&nbsp; To understand the true source of a disagreement, you have to know why <em>both</em> sides believe what they believe - one reason why disagreements are hard to resolve.</p>\n\n<p>Nonetheless, here's my guess as to what this Disagreement is about:</p><a id=\"more\"></a><p>If I had to pinpoint a single thing that strikes me as &quot;disagree-able&quot; about the way Robin frames his analyses, it's that there are a lot of <em>opaque</em> agents running around, little black boxes assumed to be similar to humans, but there are more of them and they're less expensive to build/teach/run.&nbsp; They aren't even any <em>faster</em>, let alone smarter.&nbsp; (I don't think that standard economics says that doubling the population halves the doubling time, so it matters whether you're making more minds or faster ones.)</p>\n\n<p>This is Robin's model for uploads/ems, and his model for AIs doesn't seem to look any different.&nbsp; So that world looks like this one, except that the cost of &quot;human capital&quot; and labor is dropping according to (exogenous) Moore's Law , and it ends up that economic growth doubles every month instead of every sixteen years - but that's it.&nbsp; Being, myself, not an economist, this <em>does</em> look to me like a viewpoint with a distinctly economic zeitgeist.</p>\n\n<p>In my world, you look inside the black box.&nbsp; (And, to be symmetrical, I don't spend much time thinking about more than one box at a time - if I have more hardware, it means I have to figure out how to scale a bigger brain.)</p>\n\n<p>The human brain is a haphazard thing, thrown together by <a href=\"/lw/kt/evolutions_are_stupid_but_work_anyway/\">idiot evolution</a>, as an incremental layer of icing on a chimpanzee cake that never evolved to be generally intelligent, adapted in a distant world devoid of elaborate scientific arguments or computer programs or professional specializations.</p>\n\n<p>It's amazing we can get <em>anywhere</em> using the damn thing.&nbsp; But it's worth remembering that if there were any <em>smaller</em> modification of a chimpanzee that spontaneously gave rise to a technological civilization, we would be having this conversation at that lower level of intelligence instead.</p>\n\n<p>Human neurons run at less than a millionth the speed of transistors, transmit spikes at less than a millionth the speed of light, and dissipate around a million times the heat per synaptic operation as the thermodynamic minimum for a one-bit operation at room temperature.&nbsp; Physically speaking, it ought to be possible to run a brain at a million times the speed without shrinking it, cooling it, or invoking reversible computing or quantum computing.</p>\n\n<p>There's no reason to think that the brain's software is any closer to the limits of the possible than its hardware, and indeed, if you've been following along on Overcoming Bias this whole time, you should be well aware of the manifold known ways in which our high-level thought processes fumble even the simplest problems.</p>\n\n<p>Most of these are not deep, inherent flaws of intelligence, or limits of what you can do with a mere hundred trillion computing elements.&nbsp; They are the results of a <a href=\"/lw/kt/evolutions_are_stupid_but_work_anyway/\">really stupid process</a> that designed the retina backward, slapping together a brain we now use in contexts way outside its ancestral environment.</p>\n\n<p>Ten thousand researchers working for one year cannot do the same work as a hundred researchers working for a hundred years; a chimpanzee is one-fourth the volume of a human's but four chimps do not equal one human; a chimpanzee shares 95% of our DNA but a chimpanzee cannot understand 95% of what a human can.&nbsp; The scaling law for population is not the scaling law for time is not the scaling law for brain size is not the scaling law for mind design.</p>\n\n<p>There's a parable I sometimes use, about how <a href=\"/lw/w0/the_first_world_takeover/\">the first replicator</a> was not quite the end of <a href=\"/lw/w0/the_first_world_takeover/\">the era of stable accidents</a>, because the pattern of the first replicator was, of necessity, something that <em>could</em> happen by accident.&nbsp; It is only the <em>second</em> replicating pattern that you would never have seen without many copies of the first replicator around to give birth to it; only the <em>second</em> replicator that was part of the world of evolution, something you wouldn't see in a world of accidents.</p>\n\n<p>That first replicator must have looked like one of the most bizarre things in the whole history of time - this <em>replicator</em> created purely by <em>chance.</em>&nbsp; But the history of time could never have been set in motion, otherwise.</p>\n\n<p>And what a bizarre thing a human must be, a mind born entirely of evolution, a mind that was not created by another mind.</p>\n\n<p>We haven't yet <em>begun</em> to see the shape of the era of intelligence.</p>\n\n<p>Most of the universe is far more extreme than this gentle place, Earth's cradle.&nbsp; Cold vacuum or the interior of stars; either is far more common than the temperate weather of Earth's surface, where life first arose, in the balance between the extremes.&nbsp; And most possible intelligences are not balanced, like these first humans, in that strange small region of temperate weather between an amoeba and a Jupiter Brain.</p>\n\n<p>This is the challenge of my own profession - to break yourself loose of <a href=\"/lw/rm/the_design_space_of_mindsingeneral/\">the tiny human dot in mind design space</a>, in which we have lived our whole lives, our imaginations lulled to sleep by too-narrow experiences.</p>\n\n<p>For example, Robin says:</p><blockquote><p><em>Eliezer guesses that within a few weeks a single AI could grow</em> <em>via largely internal means from weak and unnoticed to so strong it takes over the world</em> [his italics]</p></blockquote><p>I suppose that to a human a &quot;week&quot; sounds like a temporal constant describing a &quot;short period of time&quot;, but it's actually 10^49 Planck intervals, or enough time for a population of 2GHz processor cores to perform 10^15 <em>serial</em> operations one after the other.</p>\n\n<p>Perhaps the thesis would sound less shocking if Robin had said, &quot;Eliezer guesses that 10^15 sequential operations might be enough to...&quot;</p>\n\n<p>One should also bear in mind that <a href=\"/lw/q9/the_failures_of_eld_science/\">the human brain, which is not designed for the primary purpose of scientific insights, does not spend its power efficiently on having many insights in minimum time</a>, but this issue is harder to understand than CPU clock speeds.</p>\n\n<p>Robin says he doesn't like &quot;<a href=\"http://www.overcomingbias.com/2008/12/test-near-apply.html\">unvetted abstractions</a>&quot;.&nbsp; Okay.&nbsp; That's a strong point.&nbsp; I get it.&nbsp; Unvetted abstractions go kerplooie, yes they do indeed.&nbsp; But something's wrong with using that as a justification for models where there are lots of little black boxes just like humans scurrying around, and we never pry open the black box and scale the brain bigger or redesign its software or even just <em>speed up</em> the damn thing.&nbsp; The interesting part of the problem is <em>harder to analyze,</em> yes - more distant from <a href=\"/lw/qj/einsteins_speed/\">the safety rails of overwhelming evidence</a> - but this is no excuse for <em>refusing to take it into account</em>.</p>\n\n<p>And in truth I do suspect that a strict policy against &quot;unvetted abstractions&quot; is not the <a href=\"/lw/wj/is_that_your_true_rejection/\">real issue</a> here.&nbsp; I <a href=\"/lw/wi/sustained_strong_recursion/\">constructed a simple model of an upload civilization running on the computers their economy creates</a>:&nbsp; If a non-upload civilization has an exponential Moore's Law, y = e^t, then, naively, an upload civilization ought to have dy/dt = e^y -&gt; y = -ln(C - t).&nbsp; <em>Not</em> necessarily up to infinity, but for as long as Moore's Law would otherwise stay exponential in a biological civilization.&nbsp; I walked though the implications of this model, showing that in many senses it behaves &quot;just like we would expect&quot; for describing a civilization running on its own computers.</p>\n\n<p>Compare this to Robin Hanson's &quot;<a href=\"http://hanson.gmu.edu/aigrow.pdf\">Economic Growth Given Machine Intelligence</a>&quot;, which Robin <a href=\"http://www.overcomingbias.com/2008/12/underconstraine.html#comment-141416616\">describes</a> as using &quot;one of the simplest endogenous growth models to explore how Moore's Law changes with computer-based workers.&nbsp; It is an early but crude attempt, but it is the sort of approach I think promising.&quot;&nbsp; Take a quick look at that paper.</p>\n\n<p>Now, consider the <em>abstractions</em> used in my Moore's Researchers scenario, versus the <em>abstractions</em> used in Hanson's paper above, and ask yourself <em>only</em> the question of which looks more &quot;vetted by experience&quot; - given that both are models of a sort that haven't been used before, in domains not actually observed, and that both give results quite different from the world we see and that would probably cause the vast majority of actual economists to say &quot;Naaaah.&quot;</p>\n\n<p><a href=\"/lw/wi/sustained_strong_recursion/\">Moore's Researchers</a> versus <a href=\"http://hanson.gmu.edu/aigrow.pdf\">Economic Growth Given Machine Intelligence</a> - if you didn't think about the <em>conclusions</em> in advance of the reasoning; and if you also neglected that one of these has been written up in a way that is more impressive to economics journals; and you just asked the question, &quot;To what extent is the math used here, constrained by our prior experience?&quot; then I would think that the race would at best be even.&nbsp; Or possibly favoring &quot;Moore's Researchers&quot; as being more simple and intuitive, and involving less novel math as measured in additional quantities and laws introduced.</p>\n\n<p>I ask in all humility if Robin's <a href=\"/lw/wj/is_that_your_true_rejection/\">true rejection</a> is a strictly evenhandedly applied rule that rejects unvetted abstractions.&nbsp; Or if, in fact, Robin finds my conclusions, and the sort of premises I use, to be <em>objectionable for other reasons</em> - which, so far as we know at this point, may well be <em>valid</em> objections - and so it appears to him that my abstractions bear <em>a larger burden of proof</em> than the sort of mathematical steps he takes in &quot;Economic Growth Given Machine Intelligence&quot;.&nbsp; But rather than offering the reasons why the burden of proof appears larger to him, he says instead that it is &quot;not vetted enough&quot;.</p>\n\n<p>One should understand that &quot;Your abstractions are unvetted!&quot; makes it difficult for me to engage properly.&nbsp; The core of my argument has to do with what happens when you pry open the black boxes that are your economic agents, and start fiddling with their brain designs, and leave the tiny human dot in mind design space.&nbsp; If all such possibilities are rejected <em>on the basis of their being &quot;unvetted&quot; by experience</em>, it doesn't leave me with much to talk about.</p>\n\n<p>Why not just accept the rejection?&nbsp; Because I expect that to give the wrong answer - I expect it to ignore the dominating factor in the Future, even if the dominating factor is harder to analyze.</p>\n\n<p>It shouldn't be surprising if a persistent disagreement ends up resting on that point where your attempt to take into account the other person's view, runs up against some question of simple fact where, it <em>seems</em> to you, <em>you know that can't possibly be right.</em></p>\n\n<p>For me, that point is reached when trying to visualize a model of interacting black boxes that behave like humans except they're cheaper to make.&nbsp; The world, which <a href=\"/lw/w0/the_first_world_takeover/\">shattered once with the with the first replicator</a>, and <a href=\"/lw/w4/surprised_by_brains/\">shattered for the second time with the emergence of human intelligence</a>, somehow does <em>not</em> shatter a third time.&nbsp; Even in the face of blowups of brain size far greater than the size transition from chimpanzee brain to human brain; and changes in design far larger than the design transition from chimpanzee brains to human brains; and simple serial thinking speeds that are, maybe even right from the beginning, thousands or millions of times faster.</p>\n\n<p>That's the point where I, having spent my career trying to look inside the black box, trying to wrap my tiny brain around the rest of mind design space that isn't like our small region of temperate weather, just can't make myself believe that the Robin-world is <em>really truly actually</em> the way the future will be.</p>\n\n<p>There are other things that seem like probable nodes of disagreement:</p>\n\n<p>Robin Hanson's description of Friendly AI development as &quot;<a href=\"http://www.overcomingbias.com/2008/11/total-tech-wars.html\">total war</a>&quot; that is harmful to even discuss, or his description of a realized Friendly AI as &quot;<span id=\"comment-141628468-content\">a God to rule us all&quot;.&nbsp; Robin must be visualizing an in-practice outcome very different from what I do, and this seems like a likely source of emotional fuel for the disagreement as well.<br /></span></p>\n\n<p><span id=\"comment-141628468-content\">Conversely, Robin Hanson <a href=\"http://www.overcomingbias.com/2008/12/wrapping-up.html#comment-141783330\">seems to approve of a scenario</a> where lots of AIs, of arbitrary motives, constitute the vast part of the economic productivity of the Solar System, because he thinks that humans will be protected under the legacy legal system that grew continuously out of the modern world, and that the AIs will be unable to coordinate to transgress the legacy legal system for fear of losing their own legal protections.&nbsp; I tend to visualize a somewhat different outcome, to put it mildly; and would symmetrically be suspected of emotional unwillingness to accept that outcome as inexorable.<br /></span></p>\n\n<p><span id=\"comment-141628468-content\">Robin <a href=\"http://www.overcomingbias.com/2008/12/i-heart-cyc.html\">doesn't dismiss Cyc out of hand</a> and even &quot;hearts&quot; it, which implies that we have an extremely different picture of how intelligence works.<br /></span></p>\n\n<p><span id=\"comment-141783330-content\">Like Robin, I'm also feeling burned on this conversation, and I doubt we'll finish it; but I should write at least two more posts to try to describe what I've learned, and some of the rules that I think I've been following.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KQP7fNjin8Zqg4N2x": 1, "oiRp4T6u5poc8r9Tj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3pyLbH3BqevetQros", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 4.6204356240360043e-07, "legacy": true, "legacyId": "1173", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TGux5Fhcd7GmTfNGC", "jAToJHtg39AMTAuJo", "spKYZgoh3RmhxMqyu", "tnWRXkcDi5Tw9rzXw", "ZxR8P8hBFQ9kC8wMy", "mpaqTWGiLT7GA3w76", "9wZnasT3uXzmFCcaB", "XQirei3crsLxsCQoi"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-09T15:13:04.000Z", "modifiedAt": null, "url": null, "title": "Disjunctions, Antipredictions, Etc.", "slug": "disjunctions-antipredictions-etc", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:09.700Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yzzoWR33S9C3m75e8/disjunctions-antipredictions-etc", "pageUrlRelative": "/posts/yzzoWR33S9C3m75e8/disjunctions-antipredictions-etc", "linkUrl": "https://www.lesswrong.com/posts/yzzoWR33S9C3m75e8/disjunctions-antipredictions-etc", "postedAtFormatted": "Tuesday, December 9th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Disjunctions%2C%20Antipredictions%2C%20Etc.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADisjunctions%2C%20Antipredictions%2C%20Etc.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyzzoWR33S9C3m75e8%2Fdisjunctions-antipredictions-etc%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Disjunctions%2C%20Antipredictions%2C%20Etc.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyzzoWR33S9C3m75e8%2Fdisjunctions-antipredictions-etc", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyzzoWR33S9C3m75e8%2Fdisjunctions-antipredictions-etc", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1562, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/wh/underconstrained_abstractions/\">Underconstrained Abstractions</a></p>\n\n<p><a href=\"/lw/wh/underconstrained_abstractions/\">Previously</a>:</p><blockquote><p>So if it's not as simple as <em>just</em> using the one trick of finding abstractions you can easily verify on available data, what are some other tricks to use?</p></blockquote><p>There are several, as you might expect...</p>\n\n<p>Previously I talked about &quot;<a href=\"/lw/wg/permitted_possibilities_locality/\">permitted possibilities</a>&quot;.&nbsp; There's a trick in debiasing that has mixed benefits, which is to try and visualize several specific possibilities instead of just one.</p>\n\n<p>The reason it has &quot;mixed benefits&quot; is that being specific, at all, can have <a href=\"/lw/jg/planning_fallacy/\">biasing effects relative to just imagining a typical case</a>.&nbsp; (And believe me, if I'd seen the outcome of a hundred planets in roughly our situation, I'd be talking about that instead of all this <a href=\"/lw/vz/the_weak_inside_view/\">Weak Inside View</a> stuff.)</p>\n\n<p>But if you're going to bother visualizing the future, it does seem to help to visualize more than one way it could go, instead of concentrating all your strength into <em>one</em> prediction.</p>\n\n<p>So I try not to ask myself &quot;What will happen?&quot; but rather &quot;Is this possibility allowed to happen, or is it prohibited?&quot;&nbsp; There are propositions that seem forced to me, but those should be relatively rare - the first thing to understand about the future is that it is hard to predict, and you shouldn't seem to be getting strong information about most aspects of it.</p><a id=\"more\"></a><p>Of course, if you allow more than one possibility, then you have to\ndiscuss more than one possibility, and the total length of your post\ngets longer.&nbsp; If you just eyeball the length of the post, it looks like\nan unsimple theory; and then talking about multiple possibilities makes\nyou sound weak and uncertain.\n\n</p>\n\n<p>As <a href=\"http://books.google.com/books?id=rcU1BsfrM2kC&amp;pg=PA188&amp;lpg=PA188&amp;dq=dawes+disjunctions+conjunctions&amp;source=web&amp;ots=xQIHlM579M&amp;sig=AbYiwhFIxLT4wBBVidNhpxp3qHI&amp;hl=en&amp;sa=X&amp;oi=book_result&amp;resnum=2&amp;ct=result\">Robyn Dawes notes</a>,</p><blockquote><p>&quot;In their summations lawyers avoid arguing from disjunctions in favor of conjunctions.&nbsp; (There are not many closing arguments that end, &quot;Either the defendant was in severe financial straits and murdered the decedent to prevent his embezzlement from being exposed or he was passionately in love with the same coworker and murdered the decedent in a fit of jealous rage or the decedent had blocked the defendant's promotion at work and the murder was an act of revenge.&nbsp; The State has given you solid evidence to support each of these alternatives, all of which would lead to the same conclusion: first-degree murder.&quot;)&nbsp; Rationally, of course, disjunctions are <em>much</em> more probable than are conjunctions.&quot;</p></blockquote><p>Another test I use is simplifiability - <em>after</em> I've analyzed out the idea, can I compress it <em>back</em> into an argument that fits on a T-Shirt, even if it loses something thereby?&nbsp; Here's an example of some compressions:</p>\n\n<ul><li>The whole notion of recursion and feeding object-level improvements back into meta-level improvements:&nbsp; &quot;If computing power per dollar doubles every eighteen months, what happens if computers are doing the research?&quot;</li>\n\n<li>No diminishing returns on complexity in the region of the transition to human intelligence:&nbsp; &quot;We're so similar to chimps in brain design, and yet so much more powerful; the upward slope must be really steep.&quot;</li>\n\n<li>Scalability of hardware:&nbsp; &quot;Humans have only four times the brain volume of chimps - now imagine an AI suddenly acquiring a thousand times as much power.&quot;</li></ul>\n\n<p>If the whole argument was that T-Shirt slogan, I wouldn't find it compelling - too simple and surface a metaphor.&nbsp; So you have to look more closely, and try visualizing some details, and make sure the argument can be consistently realized so far as you know.&nbsp; But if, <em>after</em> you do that, you can compress the argument back to fit on a T-Shirt again - even if it sounds naive and stupid in that form - then that helps show that the argument doesn't <em>depend</em> on all the details being true simultaneously; the details might be different while fleshing out the same core idea.</p>\n\n<p>Note also that the three statements above are to some extent disjunctive - you can imagine only one of them being true, but a hard takeoff still occurring for just that reason alone.</p>\n\n<p>Another trick I use is the idea of <em>antiprediction.</em>&nbsp; This is when the narrowness of our human experience distorts our metric on the answer space, and so you can make predictions that actually aren't far from maxentropy priors, but <em>sound</em> very startling.</p>\n\n<p>I shall explain:</p>\n\n<p>A news story about an Australian national lottery that was just starting up, interviewed a man on the street, asking him if he would play.&nbsp; He said yes.&nbsp; Then they asked him what he thought his odds were of winning.&nbsp; &quot;Fifty-fifty,&quot; he said, &quot;either I win or I don't.&quot;</p>\n\n<p>To predict your odds of winning the lottery, you should invoke the Principle of Indifference with respect to all possible combinations of lottery balls.&nbsp; But this man was invoking the Principle of Indifference with respect to the partition &quot;win&quot; and &quot;not win&quot;.&nbsp; To him, they sounded like equally simple descriptions; but the former partition contains only one combination, and the latter contains the other N million combinations.&nbsp; (If you don't agree with this analysis I'd like to sell you some lottery tickets.)</p>\n\n<p>So the <em>antiprediction</em> is just &quot;You won't win the lottery.&quot;&nbsp; And the one may say, &quot;What?&nbsp; How do you know that?&nbsp; You have no evidence for that!&nbsp; You can't prove that I won't win!&quot;&nbsp; So they are focusing far too much attention on a small volume of the answer space, artificially inflated by the way their attention dwells upon it.</p>\n\n<p>In the same sense, if you look at a television SF show, you see that <a href=\"/lw/so/humans_in_funny_suits/\">a remarkable number of aliens seem to have human body plans</a> - two arms, two legs, walking upright, right down to five fingers per hand and the location of eyes in the face.&nbsp; But this is a very narrow partition in the body-plan space; and if you just said, &quot;They won't look like humans,&quot; that would be an antiprediction that just steps outside this artificially inflated tiny volume in the answer space.</p>\n\n<p>Similarly with the true sin of television SF, which is too-human minds, even among aliens not meant to be sympathetic characters.&nbsp; &quot;If we meet aliens, they won't have a sense of humor,&quot; I antipredict; and to a human it sounds like I'm saying something highly specific, because <a href=\"/lw/tt/points_of_departure/\">all minds by default have a sense of humor</a>, and I'm predicting the presence of a no-humor attribute tagged on.&nbsp; But actually, I'm just predicting that a point in mind design volume is outside the narrow hyperplane that contains humor.</p>\n\n<p>An AI might go from infrahuman to transhuman in <em>less than a week?</em>&nbsp; But a week is 10^49 Planck intervals - if you just look at the exponential scale that stretches from the Planck time to the age of the universe, there's nothing special about the timescale that 200Hz humans happen to live on, any more than there's something special about the numbers on the lottery ticket you bought.</p>\n\n<p>If we're talking about a starting population of 2GHz processor cores, then any given AI that FOOMs at all, is likely to FOOM in less than 10^15 sequential operations or more than 10^19 sequential operations, because the region between 10^15 and 10^19 isn't all that wide a target.&nbsp; So less than a week or more than a century, and in the latter case that AI will be trumped by one of a shorter timescale.</p>\n\n<p>This is actually a pretty naive version of the timescale story.&nbsp; But as an example, it shows how a &quot;prediction&quot; that's close to just stating a maximum-entropy prior, can sound amazing, startling, counterintuitive, and futuristic.</p>\n\n<p>When I make an antiprediction supported by disjunctive arguments that are individually simplifiable, I feel <em>slightly</em> less nervous about departing the rails of vetted abstractions.&nbsp; (In particular, I regard this as sufficient reason <em>not</em> to trust the results of generalizations over only human experiences.)</p>\n\n<p>Finally, there are three tests I apply to figure out how strong my predictions are.</p>\n\n<p>The first test is to just ask myself the Question, &quot;What do you think you know, and why do you think you know it?&quot;&nbsp; The future is something I haven't yet observed; if my brain claims to know something about it with any degree of confidence, what are the reasons for that?&nbsp; The first test tries to align the strength of my predictions with things that I have reasons to believe - a basic step, but one which brains are surprisingly wont to skip.</p>\n\n<p>The second test is to ask myself &quot;How worried do I feel that I'll\nhave to write an excuse explaining why this happened anyway?&quot;&nbsp; If\nI don't feel worried about having to write an excuse - if I can stick\nmy neck out and not feel too concerned about ending up with egg on my\nface - then clearly my brain really <em>does</em> believe this thing quite strongly, not as a point to be <a href=\"/lw/i6/professing_and_cheering/\">professed</a> through enthusiastic argument, but as an ordinary sort of fact.&nbsp; Why?</p>\n\n<p>And the third test is the &quot;<a href=\"/lw/vx/failure_by_analogy/\">So what?</a>&quot; test - to what degree will I feel indignant if Nature comes back and says &quot;So what?&quot; to my clever analysis?&nbsp; Would I feel as indignant as if I woke up one morning to read in the newspaper that Mars had started orbiting the Sun in squares instead of ellipses?&nbsp; Or, to make it somewhat less strong, as if I woke up one morning to find that banks were charging negative interest on loans?&nbsp; If so, clearly I must possess some kind of <em>extremely</em> strong argument - one that even Nature Itself ought to find compelling, not just humans.&nbsp; What is it?\n\n\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "3uE2pXvbcnS9nnZRE": 1, "AHK82ypfxF45rqh9D": 2, "5f5c37ee1b5cdee568cfb1a0": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yzzoWR33S9C3m75e8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 27, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "1174", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BpSEpGxtF664uRNkf", "oFKLSvbDkX8h7amRs", "CPm5LTwHrvBJCa9h5", "w9KWNWFTXivjJ7rjF", "Zkzzjg3h7hW5Z36hK", "zrGzan92SxP27LWP9", "RmCjazjupRGcHSm5N", "C4EjbrvG3PvZzizZb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-10T00:13:46.000Z", "modifiedAt": null, "url": null, "title": "Bay Area Meetup Wed 12/10 @8pm", "slug": "bay-area-meetup-wed-12-10-8pm", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rxo4Gcxv63pa5B2Jq/bay-area-meetup-wed-12-10-8pm", "pageUrlRelative": "/posts/rxo4Gcxv63pa5B2Jq/bay-area-meetup-wed-12-10-8pm", "linkUrl": "https://www.lesswrong.com/posts/rxo4Gcxv63pa5B2Jq/bay-area-meetup-wed-12-10-8pm", "postedAtFormatted": "Wednesday, December 10th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bay%20Area%20Meetup%20Wed%2012%2F10%20%408pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABay%20Area%20Meetup%20Wed%2012%2F10%20%408pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frxo4Gcxv63pa5B2Jq%2Fbay-area-meetup-wed-12-10-8pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bay%20Area%20Meetup%20Wed%2012%2F10%20%408pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frxo4Gcxv63pa5B2Jq%2Fbay-area-meetup-wed-12-10-8pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frxo4Gcxv63pa5B2Jq%2Fbay-area-meetup-wed-12-10-8pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<p>Reminder: the second regular Overcoming Bias meetup is tomorrow, <a href=\"http://www.meetup.com/Bay-Area-Overcoming-Bias-Meetup/calendar/9263656/\">Wednesday at 8pm, at Techshop in Menlo park</a>.&nbsp; Please <a href=\"http://www.meetup.com/Bay-Area-Overcoming-Bias-Meetup/calendar/9263656/\">RSVP</a> so they know how many people are coming.</p>\n\n<p>If you're hearing about this for the first time, sign up for the Meetup group so you get future announcements!&nbsp; And please RSVP when you get them!&nbsp; We don't want to have to post this to the main blog every time.</p>\n\n<p>Robin Gane-McCalla will present some of his ideas on defining intelligence.&nbsp; As always, anyone with a paper to pass around, abstract to read out loud, or rationality-related cool video to show, is cordially invited to bring it along.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DQHWBcKeiLnyh9za9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rxo4Gcxv63pa5B2Jq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 4.6230392291213843e-07, "legacy": true, "legacyId": "1175", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-10T14:01:44.000Z", "modifiedAt": null, "url": null, "title": "The Mechanics of Disagreement", "slug": "the-mechanics-of-disagreement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:57.801Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RNLQ7846MvJWwxH52/the-mechanics-of-disagreement", "pageUrlRelative": "/posts/RNLQ7846MvJWwxH52/the-mechanics-of-disagreement", "linkUrl": "https://www.lesswrong.com/posts/RNLQ7846MvJWwxH52/the-mechanics-of-disagreement", "postedAtFormatted": "Wednesday, December 10th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Mechanics%20of%20Disagreement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Mechanics%20of%20Disagreement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRNLQ7846MvJWwxH52%2Fthe-mechanics-of-disagreement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Mechanics%20of%20Disagreement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRNLQ7846MvJWwxH52%2Fthe-mechanics-of-disagreement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRNLQ7846MvJWwxH52%2Fthe-mechanics-of-disagreement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1072, "htmlBody": "<p>Two ideal Bayesians cannot have common knowledge of disagreement; this is a theorem.&nbsp; If two rationalist-wannabes have common knowledge of a disagreement between them, what could be going wrong?</p>\n<p>The obvious interpretation of these theorems is that if you know that a cognitive machine is a rational processor of evidence, <a href=\"/lw/jl/what_is_evidence/\">its beliefs become evidence themselves</a>.</p>\n<p>If you design an AI and the AI says \"This fair coin came up heads with 80% probability\", then you know that the AI has accumulated evidence with an likelihood ratio of 4:1 favoring heads - because the AI only emits that statement under those circumstances.</p>\n<p>It's not a matter of charity; it's just that this is how you think the other cognitive machine works.</p>\n<p>And if you tell an ideal rationalist, \"I think this fair coin came up heads with 80% probability\", and they reply, \"I now think this fair coin came up heads with 25% probability\", and your sources of evidence are independent of each other, then you should accept this verdict, reasoning that (before you spoke) the other mind must have encountered evidence with a likelihood of 1:12 favoring tails.</p>\n<p>But this <em>assumes</em> that the other mind also thinks that <em>you're</em> processing evidence correctly, so that, by the time it says \"I now think this fair coin came up heads, p=.25\", it has already taken into account the full impact of all the evidence you know about, before adding more evidence of its own.</p>\n<p><a id=\"more\"></a></p>\n<p>If, on the other hand, the other mind doesn't trust your rationality, then it won't accept your evidence at face value, and the estimate that it gives won't integrate the full impact of the evidence you observed.</p>\n<p>So does this mean that when two rationalists trust each other's rationality less than completely, then they can agree to disagree?</p>\n<p>It's not that simple.&nbsp; Rationalists should not trust <em>themselves</em> entirely, either.</p>\n<p>So when the other mind accepts your evidence at less than face value, this doesn't say \"You are less than a perfect rationalist\", it says, \"I trust you less than you trust yourself; I think that you are discounting your own evidence too little.\"</p>\n<p>Maybe your raw arguments seemed to you to have a strength of 40:1, but you discounted for your own irrationality to a strength of 4:1, but the other mind thinks you still overestimate yourself and so it assumes that the actual force of the argument was 2:1.</p>\n<p>And if you <em>believe</em> that the other mind is discounting you in this way, and is unjustified in doing so, then when it says \"I now think this fair coin came up heads with 25% probability\", you might bet on the coin at odds of 57% in favor of heads - adding up your further-discounted evidence of 2:1 to the implied evidence of 1:6 that the other mind must have seen to give final odds of 2:6 - <em>if</em> you even fully trust the other mind's further evidence of 1:6.</p>\n<p>I think we have to be very careful to avoid interpreting this situation in terms of anything like a <em>reciprocal trade,</em> like two sides making <em>equal concessions</em> in order to reach agreement on a business deal.</p>\n<p>Shifting beliefs is not a concession that you make for the sake of others, expecting something in return; it is an advantage you take for your own benefit, to improve your own map of the world.&nbsp; I am, generally speaking, a <a href=\"http://www.ozyandmillie.org/d/20030324.html\">Millie-style altruist</a>; but when it comes to <em>belief shifts</em> I espouse a pure and principled selfishness: don't believe you're doing it for anyone's sake but your own.</p>\n<p>Still, I once read that there's a principle among con artists that the main thing is to get the mark to believe that <em>you trust them,</em> so that they'll feel obligated to trust you in turn.</p>\n<p>And - even if it's for completely different theoretical reasons - if you want to persuade a rationalist to shift belief to match yours, you either need to persuade them that you have all of the same evidence they do and have already taken it into account, or that you already fully trust their opinions as evidence, or that you know better than they do how much they themselves can be trusted.</p>\n<p>It's that last one that's the really sticky point, for obvious reasons of asymmetry of introspective access and asymmetry of motives for overconfidence - how do you resolve that conflict?&nbsp; (And if you started <em>arguing</em> about it, then the question wouldn't be which of these were more important as a factor, but rather, which of these factors the Other had under- or over-discounted in forming their estimate of a given person's rationality...)</p>\n<p>If I had to name a single reason why two wannabe rationalists wouldn't actually be able to agree in practice, it would be that, once you trace the argument to the meta-level where theoretically everything can be and must be resolved, the argument trails off into psychoanalysis and noise.</p>\n<p>And if you look at what goes on in <em>practice</em> between two arguing rationalists, it would probably mostly be trading object-level arguments; and the most meta it would get is trying to convince the other person that you've already taken their object-level arguments into account.</p>\n<p>Still, this does leave us with three clear reasons that someone might point to, to justify a persistent disagreement - even though the frame of mind of <em>justification</em> and having clear reasons to <em>point to</em> in front of others, is itself antithetical to the spirit of resolving disagreements - but even so:</p>\n<ul>\n<li><em>Clearly,</em> the Other's object-level arguments are flawed; no amount of trust that I can have for another person will make me believe that rocks fall upward.</li>\n<li><em>Clearly,</em> the Other is not taking my arguments into account; there's an obvious asymmetry in how well I understand them and have integrated their evidence, versus how much they understand me and have integrated mine.</li>\n<li><em>Clearly,</em> the Other is completely biased in how much they trust themselves over others, versus how I humbly and evenhandedly discount my own beliefs alongside theirs.</li>\n</ul>\n<p>Since we don't want to go around encouraging disagreement, one might do well to ponder how all three of these arguments are used by creationists to justify their persistent disagreements with scientists.</p>\n<p>That's one reason I say <em>clearly</em> - if it isn't obvious even to outside onlookers, maybe you shouldn't be confident of resolving the disagreement there.&nbsp; Failure at any of these levels implies failure at the meta-levels above it, but the higher-order failures might not be <em>clear</em>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"WH5ZmNSjZmK9SMj7k": 2, "LhX3F2SvGDarZCuh6": 2, "wzgcQCrwKfETcBpR9": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RNLQ7846MvJWwxH52", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 4.624150173191236e-07, "legacy": true, "legacyId": "1176", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6s3xABaXKPdFwA3FS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-11T17:41:43.000Z", "modifiedAt": null, "url": null, "title": "What I Think, If Not Why", "slug": "what-i-think-if-not-why", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:16.833Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z3kYdw54htktqt9Jb/what-i-think-if-not-why", "pageUrlRelative": "/posts/z3kYdw54htktqt9Jb/what-i-think-if-not-why", "linkUrl": "https://www.lesswrong.com/posts/z3kYdw54htktqt9Jb/what-i-think-if-not-why", "postedAtFormatted": "Thursday, December 11th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20I%20Think%2C%20If%20Not%20Why&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20I%20Think%2C%20If%20Not%20Why%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz3kYdw54htktqt9Jb%2Fwhat-i-think-if-not-why%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20I%20Think%2C%20If%20Not%20Why%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz3kYdw54htktqt9Jb%2Fwhat-i-think-if-not-why", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz3kYdw54htktqt9Jb%2Fwhat-i-think-if-not-why", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1171, "htmlBody": "<p><strong>Reply to</strong>:&#0160; <a href=\"http://www.overcomingbias.com/2008/12/two-visions-of.html\">Two Visions Of Heritage</a></p><p>Though it really goes tremendously against my grain - it feels like sticking my neck out over a cliff (or something) - I guess I have no choice here but to try and make a list of <em>just </em>my positions, without justifying them.&#0160; We can only talk justification, I guess, after we get straight what my positions <em>are.</em>&#0160; I will also <a href=\"http://www.overcomingbias.com/2008/06/against-disclai.html\">leave off many disclaimers</a> to present the points <em>compactly</em> enough to be <em>remembered.</em></p><p>\u2022 A well-designed mind should be <strong><em>much more efficient</em> than a human</strong>, capable of doing more with <a href=\"http://www.overcomingbias.com/2008/05/faster-than-ein.html\">less sensory data</a> and <a href=\"http://www.overcomingbias.com/2008/05/eld-science.html\">fewer computing operations</a>.&#0160; It is not <em>infinitely efficient</em> and <strong>does not use <em>zero</em></strong> <strong>data</strong>.&#0160; But it does use little enough that <em>local pipelines</em> such as a small pool of programmer-teachers and, later, a huge pool of e-data, are sufficient.</p><p>\u2022 An AI that reaches a certain point in its own development becomes able to (<a href=\"http://www.overcomingbias.com/2008/12/sustained-recur.html\">sustainably, strongly</a>) improve itself.&#0160; At this point, <strong><a href=\"http://www.overcomingbias.com/2008/11/recursion-magic.html\">recursive</a> <a href=\"http://www.overcomingbias.com/2008/11/cascades-cycles.html\">cascades</a> slam over many internal growth curves to near the limits of their current hardware</strong>, and the AI undergoes a vast increase in capability.&#0160; This point is at, or probably considerably before, a minimally transhuman mind capable of writing its own AI-theory textbooks - an upper bound beyond which it could swallow and improve its <em>entire </em>design chain.</p><p>\u2022 It is <em>likely</em> that this capability increase or &quot;FOOM&quot; has an intrinsic maximum velocity that a human would regard as &quot;fast&quot; if it happens at all.&#0160; A human week is ~1e15 serial operations for a population of 2GHz cores, and a century is ~1e19 serial operations; this whole range is a narrow window.&#0160; However,<strong> <em>the core argument does not require one-week speed</em></strong> and a FOOM that takes two years (~1e17 serial ops) will still carry the weight of the argument.</p><p>\n</p><a id=\"more\"></a>\n<p>\n</p><p>\n\u2022 <strong>The <em>default </em>case of FOOM is an unFriendly AI</strong>,\nbuilt by\nresearchers with shallow insights.&#0160; This AI becomes able to improve\nitself in a haphazard way, makes various changes that are net\nimprovements but may introduce value drift, and then gets smart enough\nto do guaranteed self-improvement, at which point its values freeze\n(forever).</p><p>\u2022 <strong>The <em>desired</em> case of FOOM is a Friendly AI</strong>,\nbuilt using deep insight, so that the AI never makes any changes to\nitself that potentially change its internal values; all such changes\nare guaranteed using <a href=\"http://www.overcomingbias.com/2008/11/first-order-log.html\">strong techniques</a> that allow for a billion sequential self-modifications without losing the guarantee.&#0160; The guarantee is written over the AI&#39;s <em>internal search criterion</em> for actions, rather than <em>external consequences</em>.</p><p>\u2022 The <strong>good guys do <em>not </em>write</strong> an AI which values <strong>a bag of things that the programmers think are good ideas</strong>, like libertarianism or socialism or making people happy or whatever.&#0160; There were <em>multiple</em> Overcoming Bias sequences about this <em>one point</em>, like the <a href=\"http://www.overcomingbias.com/2007/12/fake-fake-utili.html\">Fake Utility Function sequence</a> and the sequence on metaethics.&#0160; It is dealt with at length in the document <a href=\"http://intelligence.org/upload/CEV.html\"><strong>Coherent</strong> <strong>*Extrapolated*</strong> Volition</a>.&#0160;\nIt is the first thing, the last thing, and the middle thing that I say\nabout Friendly AI.&#0160; I have said it over and over.&#0160; I truly do not\nunderstand how anyone can pay <em>any</em> attention to <em>anything</em>\nI have said on this subject, and come away with the impression that I\nthink programmers are supposed to directly impress their non-meta personal philosophies onto a Friendly AI.</p><p>\u2022 <strong>The good guys do not directly impress their personal values onto a Friendly AI.<br /></strong></p><p>\u2022 Actually setting up a Friendly AI&#39;s values is <strong>an extremely <em>meta</em> operation</strong>, less &quot;make the AI want to make people happy&quot; and more like &quot;<a href=\"http://intelligence.org/upload/CEV.html\"><strong>superpose</strong><span> the possible </span><strong><span>reflective equilibria</span></strong><span> of the <strong>whole </strong></span><strong><span>human species</span></strong><span>, and <strong>output new code</strong> that overwrites the current AI and has the <strong>most coherent</strong> support within that superposition</span></a>&quot;.&#0160; This actually seems to be something of a Pons Asinorum in FAI - the ability to understand and endorse metaethical concepts that do not <em>directly </em>sound like amazing wonderful happy ideas.&#0160; <strong>Describing this as declaring total war on the rest of humanity, does not seem <a href=\"http://www.overcomingbias.com/2008/07/bedrock-fairnes.html\">fair</a></strong><span> </span>(or accurate).</p><p>\u2022 <strong>I myself am strongly individualistic</strong>:&#0160; The most painful memories in my life have been when other people thought they knew better than me, and tried to do things on my behalf.&#0160; It is also a known principle of hedonic psychology that people are happier when they&#39;re steering their own lives and doing their own interesting work.&#0160; When I try myself to visualize what a beneficial superintelligence ought to do, it consists of <strong>setting up a world that works by better rules, and then fading into the background</strong>, silent as the laws of Nature once were; and finally folding up and vanishing when it is no longer needed.&#0160; But this is only the thought of my mind that is merely human, and <strong>I am barred from programming any such consideration <em>directly </em>into a Friendly AI</strong>, for the reasons given above.</p><p>\u2022 Nonetheless, it does seem to me that this particular scenario <strong>could not be justly described as &quot;a God to rule over us all&quot;</strong>, unless the current fact that humans age and die is &quot;a malevolent God to rule us all&quot;.&#0160; So either Robin has a very different idea about what human reflective equilibrium values are likely to look like; or Robin believes that the Friendly AI project is bound to <em>fail</em> in such way as to create a paternalistic God; or - and this seems more likely to me - Robin didn&#39;t read all the way through all the blog posts in which I tried to explain all the ways that this is not how Friendly AI works.</p><p>\u2022 <strong>Friendly AI is technically difficult and requires an <a href=\"http://www.overcomingbias.com/2008/10/isshokenmei.html\">extra-ordinary</a> effort on multiple levels.</strong>&#0160; <a href=\"http://www.overcomingbias.com/2007/11/complex-wishes.html\">English sentences</a> like &quot;make people happy&quot; cannot describe the values of a Friendly AI.&#0160; <a href=\"http://www.overcomingbias.com/2008/08/magical-categor.html\">Testing is not sufficient to guarantee that values have been successfully transmitted</a>.</p><p>\u2022 White-hat AI researchers are distinguished by the degree to which <strong>they understand that a single misstep could be fatal, and can discriminate strong and weak assurances</strong>.&#0160; Good intentions are not only common, they&#39;re cheap.&#0160; The story isn&#39;t about good versus evil, it&#39;s about people trying to <a href=\"http://www.overcomingbias.com/2008/10/shut-up-and-do.html\">do the impossible</a> versus <a href=\"http://www.overcomingbias.com/2008/09/above-average-s.html\">others</a> who... aren&#39;t.<span style=\"font-style: italic;\"></span></p><p>\u2022 Intelligence is about being able to <strong>learn lots of things, not about knowing lots of things</strong>.&#0160; Intelligence is especially not about tape-recording lots of parsed English sentences a la Cyc.&#0160; Old AI work was poorly focused due to inability to introspectively see the first and higher <em>derivatives </em>of knowledge; human beings have an easier time reciting sentences than reciting their ability to learn.</p><p>\u2022 <strong>Intelligence is mostly about architecture</strong>, or &quot;knowledge&quot; along the\nlines of knowing to look for causal structure (Bayes-net type stuff) in\nthe environment; this kind of knowledge will usually be expressed procedurally as well as declaratively.<strong>&#0160; Architecture is mostly about deep insights.</strong>&#0160; This point has not yet been addressed (much) on Overcoming Bias, but Bayes nets can be considered as an archetypal example of &quot;architecture&quot; and &quot;deep insight&quot;.&#0160; Also, ask yourself how lawful intelligence seemed to you before you started reading this blog, how lawful it seems to you now, then extrapolate outward from that.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "oiRp4T6u5poc8r9Tj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z3kYdw54htktqt9Jb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 37, "extendedScore": null, "score": 5.8e-05, "legacy": true, "legacyId": "1177", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 103, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-12T19:14:32.000Z", "modifiedAt": "2022-05-28T20:22:41.352Z", "url": null, "title": "You Only Live Twice", "slug": "you-only-live-twice", "viewCount": null, "lastCommentedAt": "2020-10-30T13:25:32.946Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yKXKcyoBzWtECzXrE/you-only-live-twice", "pageUrlRelative": "/posts/yKXKcyoBzWtECzXrE/you-only-live-twice", "linkUrl": "https://www.lesswrong.com/posts/yKXKcyoBzWtECzXrE/you-only-live-twice", "postedAtFormatted": "Friday, December 12th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20You%20Only%20Live%20Twice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYou%20Only%20Live%20Twice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKXKcyoBzWtECzXrE%2Fyou-only-live-twice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=You%20Only%20Live%20Twice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKXKcyoBzWtECzXrE%2Fyou-only-live-twice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKXKcyoBzWtECzXrE%2Fyou-only-live-twice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1684, "htmlBody": "<p style=\"margin-left: 40px;\">\"It just so happens that your friend here is only <em>mostly</em> dead.&nbsp; There's a big difference between <em>mostly </em>dead and <em>all </em>dead.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <em>The Princess Bride</em></p>\n<p>My co-blogger Robin and I may disagree on how fast an AI can improve itself, but we agree on an issue that seems much simpler to us than that:&nbsp; <strong>At the point where the current legal and medical system gives up on a patient, they aren't really dead.</strong></p>\n<p>Robin has <a href=\"http://www.overcomingbias.com/2008/12/we-agree-get-froze.html\">already said</a> much of what needs saying, but a few more points:</p>\n<p>&bull; <a href=\"http://www.benbest.com/cryonics/CryoFAQ.html\">Ben Best's Cryonics FAQ</a>, <a href=\"http://www.alcor.org/FAQs/index.html\">Alcor's FAQ</a>, <a href=\"http://www.alcor.org/sciencefaq.htm\">Alcor FAQ for scientists</a>, <a href=\"http://www.imminst.org/cryonics_letter/\">Scientists' Open Letter on Cryonics</a></p>\n<p>&bull; I know more people who are planning to sign up for cryonics Real Soon Now than people who have actually signed up.&nbsp; <strong>I expect that more people have <em>died while cryocrastinating</em> than have <em>actually been cryopreserved</em>.</strong>&nbsp; If you've <em>already decided</em> this is a good idea, but you \"haven't gotten around to it\", sign up for cryonics NOW.&nbsp; I mean <em>RIGHT NOW.</em>&nbsp; Go to the website of <a href=\"http://www.alcor.org/BecomeMember/index.html\">Alcor</a> or the <a href=\"http://cryonics.org/become.html\">Cryonics Institute</a> and follow the instructions.</p>\n<p><a id=\"more\"></a></p>\n<p>&bull; Cryonics is usually funded through life insurance.&nbsp; The following conversation from an Overcoming Bias meetup is worth quoting:</p>\n<p style=\"margin-left: 40px;\">Him:&nbsp; I've been thinking about signing up for cryonics when I've got enough money.</p>\n<p style=\"margin-left: 40px;\">Me:&nbsp; Um... it doesn't take all that much money.</p>\n<p style=\"margin-left: 40px;\">Him:&nbsp; It doesn't?</p>\n<p style=\"margin-left: 40px;\">Me:&nbsp; Alcor is the high-priced high-quality organization, which is something like $500-$1000 in annual fees for the organization, I'm not sure how much.&nbsp; I'm young, so I'm signed up with the Cryonics Institute, which is $120/year for the membership.&nbsp; I pay $180/year for more insurance than I need - it'd be enough for Alcor too.</p>\n<p style=\"margin-left: 40px;\">Him:&nbsp; That's ridiculous.</p>\n<p style=\"margin-left: 40px;\">Me:&nbsp; Yes.</p>\n<p style=\"margin-left: 40px;\">Him:&nbsp; No, really, that's <em>ridiculous.</em>&nbsp; If that's true then my decision isn't just determined, it's overdetermined.</p>\n<p style=\"margin-left: 40px;\">Me:&nbsp; Yes.&nbsp; And there's around a thousand people worldwide [actually 1400] who are signed up for cryonics.&nbsp; Figure that at most a quarter of those did it for systematically rational reasons.&nbsp; That's a high upper bound on the number of people on Earth who can reliably reach the right conclusion on massively overdetermined issues.</p>\n<p>&bull; Cryonics is not marketed well - or at all, really.&nbsp; There's no salespeople who get commissions.&nbsp; There is <em>no one to hold your hand through signing up</em>, so you're going to have to get the papers signed and notarized yourself.&nbsp; The closest thing out there might be <a href=\"http://www.rudihoffman.com/\">Rudi Hoffman</a>, who sells life insurance with cryonics-friendly insurance providers (I went through him).</p>\n<p>&bull; If you want to <em>securely </em>erase a hard drive, it's not as easy as writing it over with zeroes.&nbsp; Sure, an \"erased\" hard drive like this won't boot up your computer if you just plug it in again.&nbsp; But if the drive falls into the hands of a specialist with a scanning tunneling microscope, they can tell the difference between \"this was a 0, overwritten by a 0\" and \"this was a 1, overwritten by a 0\".</p>\n<p>There are programs advertised to \"securely erase\" hard drives using many overwrites of 0s, 1s, and random data.&nbsp; But if you want to keep the secret on your hard drive secure against <em>all possible future technologies that might ever be developed,</em> then cover it with <a href=\"http://www.youtube.com/watch?v=WrCWLpRc1yM\">thermite</a> and set it on fire.&nbsp; It's the only way to be sure.</p>\n<p><em>Pumping someone full of cryoprotectant and gradually lowering their temperature until they can be stored in liquid nitrogen</em> is not a secure way to erase a person.</p>\n<p>See also the <a href=\"http://en.wikipedia.org/wiki/Information_theoretical_death\">information-theoretic criterion of death</a>.</p>\n<p>&bull; You don't have to buy what's usually called the \"patternist\" philosophy of identity, to sign up for cryonics.&nbsp; After reading all the information off the brain, you could put the \"same atoms\" back into their old places.</p>\n<p>&bull; \"Same atoms\" is in scare quotes because our current physics <em>prohibits</em> particles from possessing individual identities.&nbsp; It's a much stronger statement than \"we can't tell the particles apart with current measurements\" and has to do with the notion of configuration spaces in quantum mechanics.&nbsp; This is a standard idea in QM, <em>not </em>an unusual woo-woo one - see <a href=\"http://www.overcomingbias.com/2008/06/qm-and-identity.html\">this sequence on Overcoming Bias</a> for a gentle introduction.&nbsp; Although patternism is not <em>necessary</em> to the cryonics thesis, we happen to live in a universe where \"the same atoms\" is physical nonsense.</p>\n<p>There's a number of intuitions we have in our brains for processing a world of distinct physical objects, built in from a very young age.&nbsp; These intuitions, which may say things like \"If an object disappears, and then comes back, it isn't the same object\", are tuned to our macroscopic world and generally don't match up well with <em>fundamental</em> physics.&nbsp; Your identity is not like a little billiard ball that follows you around - there aren't <em>actually </em>any billiard balls down there.</p>\n<p><em>Separately and convergently,</em> more abstract reasoning strongly suggests that \"identity\" should not be epiphenomenal; that is, you should not be able to change someone's identity without changing any observable fact about them.</p>\n<p>If you go through <a href=\"http://www.overcomingbias.com/2008/06/qm-and-identity.html\">the aforementioned Overcoming Bias sequence</a>, you should actually be able to <em>see intuitively</em> that successful cryonics preserves anything about you that is preserved by going to sleep at night and waking up the next morning.</p>\n<p>&bull; Cryonics, to me, makes two statements.</p>\n<p>The first statement is about <a href=\"http://intelligence.org/blog/2007/06/16/transhumanism-as-simplified-humanism/\">systematically valuing human life</a>.&nbsp; It's bad when a pretty young white girl goes missing somewhere in America.&nbsp; But when 800,000 Africans get murdered in Rwanda, that gets 1/134 the media coverage of the Michael Jackson trial.&nbsp; It's sad, to be sure, but no cause for emotional alarm.&nbsp; When brown people die, that's <em>all part of the plan</em> - as a smiling man once said.</p>\n<p>Cryonicists are people who've decided that their deaths, and the deaths of their friends and family and the rest of the human species, are <a href=\"http://yudkowsky.net/other/yehuda\">not part of the plan</a>.</p>\n<p>I've met one or two Randian-type \"selfish\" cryonicists, but they aren't a majority.&nbsp; Most people who sign up for cryonics wish that everyone would sign up for cryonics.</p>\n<p>The second statement is that you have at least a <em>little </em>hope in the future.&nbsp; Not faith, not blind hope, not irrational hope - just, any hope at all.</p>\n<p>I was once at a table with Ralph Merkle, talking about how to market cryonics if anyone ever gets around to marketing it, and Ralph suggested a group of people in a restaurant, having a party; and the camera pulls back, and moves outside the window, and the restaurant is on the Moon.&nbsp; Tagline:&nbsp; \"Wouldn't you want to be there?\"</p>\n<p>If you look back at, say, the Middle Ages, things were worse then.&nbsp; I'd rather live here then there.&nbsp; I have hope that humanity will move forward <em>further</em>, and that's something that I want to see.</p>\n<p>And I hope that the idea that people are disposable, and that their deaths are part of the plan, is something that fades out of the Future.</p>\n<p>Once upon a time, infant deaths were part of the plan, and now they're not.&nbsp; Once upon a time, slavery was part of the plan, and now it's not.&nbsp; Once upon a time, dying at thirty was part of the plan, and now it's not.&nbsp; That's a psychological shift, not just an increase in living standards.&nbsp; Our era doesn't value human life with perfect consistency - but the value of human life is higher than it once was.</p>\n<p>We have a concept of what a medieval peasant <em>should</em> have had, the dignity with which they <em>should</em> have been treated, that is higher than what they would have thought to ask for themselves.</p>\n<p>If no one in the future cares enough to save people who can be saved... well.&nbsp; In cryonics there is an element of taking responsibility for the Future.&nbsp; You may be around to reap what your era has sown.&nbsp; It is not just my <em>hope</em> that the Future be a better place; it is my <em>responsibility.</em>&nbsp; If I thought that we were on track to a Future where no one cares about human life, and lives that could easily be saved are just thrown away - then I would try to change that.&nbsp; Not everything worth doing is easy.</p>\n<p><em>Not </em>signing up for cryonics - what does that say?&nbsp; That you've lost hope in the future.&nbsp; That you've lost your will to live.&nbsp; That you've stopped believing that human life, and your own life, is something of value.</p>\n<p>This can be a painful world we live in, and the media is always telling us how much worse it will get.&nbsp; If you spend enough time not looking forward to the next day, it damages you, after a while.&nbsp; You lose your ability to hope.&nbsp; Try telling someone already grown old to sign up for cryonics, and they'll tell you that they don't want to be old forever - that they're tired.&nbsp; If you try to explain to someone already grown old, that the nanotechnology to revive a cryonics patient is sufficiently advanced that reversing aging is almost trivial by comparison... then it's not something they can imagine on an emotional level, no matter what they believe or don't believe about future technology.&nbsp; They can't imagine not being tired.&nbsp; I think that's true of a lot of people in this world.&nbsp; If you've been hurt enough, you can no longer imagine healing.</p>\n<p>But things really were a lot worse in the Middle Ages.&nbsp; And they really are a lot better now.&nbsp; Maybe humanity <em>isn't</em> doomed.&nbsp; The Future could be something that's worth seeing, worth living in.&nbsp; And it may have a concept of sentient dignity that values your life more than you dare to value yourself.</p>\n<p>On behalf of the Future, then - please ask for a little more for yourself.&nbsp; More than death.&nbsp; It really... isn't being selfish.&nbsp; <em>I</em> want you to live.&nbsp; I think that the Future will want you to live.&nbsp; That if you let yourself die, people who aren't even born yet will be sad for the irreplaceable thing that was lost.</p>\n<p>So please, live.</p>\n<p>My brother didn't.&nbsp; My grandparents won't.&nbsp; But everything we can hold back from the Reaper, even a single life, is precious.</p>\n<p>If other people want you to live, then it's not just you doing something selfish and unforgivable, right?</p>\n<p>So I'm saying it to you.</p>\n<p>I want you to live.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 11}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yKXKcyoBzWtECzXrE", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 126, "baseScore": 164, "extendedScore": null, "score": 0.000242, "legacy": true, "legacyId": "1178", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 164, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 182, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2008-12-12T19:14:32.000Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-13T15:28:28.000Z", "modifiedAt": null, "url": null, "title": "BHTV: de Grey and Yudkowsky", "slug": "bhtv-de-grey-and-yudkowsky", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:54.979Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/px2SEBcGoRkuxFfEY/bhtv-de-grey-and-yudkowsky", "pageUrlRelative": "/posts/px2SEBcGoRkuxFfEY/bhtv-de-grey-and-yudkowsky", "linkUrl": "https://www.lesswrong.com/posts/px2SEBcGoRkuxFfEY/bhtv-de-grey-and-yudkowsky", "postedAtFormatted": "Saturday, December 13th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20BHTV%3A%20de%20Grey%20and%20Yudkowsky&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABHTV%3A%20de%20Grey%20and%20Yudkowsky%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpx2SEBcGoRkuxFfEY%2Fbhtv-de-grey-and-yudkowsky%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=BHTV%3A%20de%20Grey%20and%20Yudkowsky%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpx2SEBcGoRkuxFfEY%2Fbhtv-de-grey-and-yudkowsky", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpx2SEBcGoRkuxFfEY%2Fbhtv-de-grey-and-yudkowsky", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 61, "htmlBody": "<p><a href=\"http://bloggingheads.tv/diavlogs/16508\">My latest on Bloggingheads.tv</a> is up.&nbsp; BHTV wanted someone to interview Aubrey de Grey of the <a href=\"http://www.methuselahfoundation.org/\">Methuselah Foundation</a> about basic research in antiagathics, and they picked me to do it.&nbsp; It made the interview somewhat difficult, since Aubrey and I already agree about most things, but we managed to soldier on.&nbsp; The interview is mostly Aubrey talking, as it should be.\n\n\n</p><a id=\"more\"></a>\n<p><embed type=\"application/x-shockwave-flash\" src=\"http://bloggingheads.tv/maulik/offsite/offsite_flvplayer.swf\" flashvars=\"playlist=http%3A%2F%2Fbloggingheads%2Etv%2Fdiavlogs%2Fliveplayer%2Dplaylist%2F16508%2F00%3A00%2F62%3A46\" height=\"335\" width=\"448\"></embed>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"9DNZfxFvY5iKoZQbz": 1, "vmvTYnmaKA73fYDe5": 1, "t7t9nW6BtJhfGNSR6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "px2SEBcGoRkuxFfEY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "1179", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-14T17:13:03.000Z", "modifiedAt": null, "url": null, "title": "For The People Who Are Still Alive", "slug": "for-the-people-who-are-still-alive", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:39.657Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cfZ8zveqrTZbQrjeD/for-the-people-who-are-still-alive", "pageUrlRelative": "/posts/cfZ8zveqrTZbQrjeD/for-the-people-who-are-still-alive", "linkUrl": "https://www.lesswrong.com/posts/cfZ8zveqrTZbQrjeD/for-the-people-who-are-still-alive", "postedAtFormatted": "Sunday, December 14th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20For%20The%20People%20Who%20Are%20Still%20Alive&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFor%20The%20People%20Who%20Are%20Still%20Alive%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcfZ8zveqrTZbQrjeD%2Ffor-the-people-who-are-still-alive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=For%20The%20People%20Who%20Are%20Still%20Alive%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcfZ8zveqrTZbQrjeD%2Ffor-the-people-who-are-still-alive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcfZ8zveqrTZbQrjeD%2Ffor-the-people-who-are-still-alive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1192, "htmlBody": "<p><a href=\"http://space.mit.edu/home/tegmark/\">Max Tegmark observed</a> that we have three independent reasons to believe we live in a Big World:&nbsp; A universe which is large relative to the space of <em>possibilities</em>.&nbsp; For example, on current physics, the universe appears to be spatially infinite (though I'm not clear on how strongly this is implied by the standard model).</p>\n<p>If the universe is spatially infinite, then, on average, we should expect that <a href=\"http://arxiv.org/PS_cache/astro-ph/pdf/0302/0302131v1.pdf\">no more than 10^10^29 meters away is an exact duplicate of you</a>.&nbsp; If you're looking for an exact duplicate of a Hubble volume - an object the size of our observable universe - then you should still on average only need to look 10^10^115 lightyears.&nbsp; (These are numbers based on a highly conservative counting of \"physically possible\" states, e.g. packing the whole Hubble volume with potential protons at maximum density given by the Pauli Exclusion principle, and then allowing each proton to be present or absent.)</p>\n<p>The most popular cosmological theories also call for an \"inflationary\" scenario in which many different universes would be eternally budding off, our own universe being only one bud.&nbsp; And finally there are the alternative decoherent branches of the grand quantum distribution, aka \"many worlds\", <a href=\"/lw/r8/and_the_winner_is_manyworlds/\">whose presence is unambiguously implied by the simplest mathematics that fits our quantum experiments</a>.</p>\n<p>Ever since I realized that physics seems to tell us straight out that we live in a Big World, I've become much less focused on creating lots of people, and much more focused on ensuring the welfare of people who are already alive.</p>\n<p><a id=\"more\"></a></p>\n<p>If your decision to not create a person means that person will <em>never exist at all,</em> then you might, indeed, be moved to create them, for their sakes.&nbsp; But if you're just deciding whether or not to create a new person <em>here,</em> in your own Hubble volume and Everett branch, then it may make sense to have relatively lower populations <em>within each causal volume</em>, living higher qualities of life.&nbsp; It's not like anyone will actually <em>fail to be born</em> on account of that decision - they'll just be <em>born </em><em>predominantly </em><em>into regions with higher standards of living.</em></p>\n<p>Am I sure that this statement, that I have just emitted, actually makes sense?</p>\n<p>Not really.&nbsp; It dabbles in the dark arts of anthropics, and the Dark Arts don't get much murkier than that.&nbsp; Or to say it without the <a href=\"/lw/wb/chaotic_inversion/\">chaotic inversion</a>:&nbsp; I am stupid with respect to anthropics.</p>\n<p>But to apply the test of simplifiability - it seems in some raw intuitive sense, that if the universe is large enough for everyone to exist somewhere, then we should mainly be worried about giving babies nice <em>futures </em>rather than trying to \"ensure they get born\".</p>\n<p>Imagine taking a survey of the whole universe.&nbsp; Every plausible baby gets a little checkmark in the \"exists\" box - everyone is born somewhere.&nbsp; In fact, the total population count for each baby is something-or-other, some large number that may or may not be \"infinite\" -</p>\n<p>(I should mention at this point that I am an infinite set atheist, and my main hope for being able to maintain this in the face of a spatially infinite universe is to suggest that identical Hubble volumes add in the same way as any other <a href=\"/lw/pf/distinct_configurations/\">identical configuration of particles</a>.&nbsp; So in this case the universe would be exponentially large, the size of the branched decoherent distribution, but the spatial infinity would just fold into that very large but finite object.&nbsp; And I could still be an infinite set atheist.&nbsp; I am not a physicist so my fond hope may be ruled out for some reason of which I am not aware.)</p>\n<p>- so the first question, anthropically speaking, is whether multiple realizations of the exact same physical process count as more than one person.&nbsp; Let's say you've got an upload running on a computer.&nbsp; If you look inside the computer and realize that it contains triply redundant processors running in exact synchrony, is that three people or one person?&nbsp; <a href=\"/lw/ps/where_physics_meets_experience/\">How about if the processor is a flat sheet - if that sheet is twice as thick, is there twice as much person inside it</a>?&nbsp; If we split the sheet and put it back together again without desynchronizing it, <a href=\"/lw/pv/the_conscious_sorites_paradox/\">have we created a person and killed them</a>?</p>\n<p>I suppose the answer <em>could</em> be yes; I have confessed myself stupid about anthropics.</p>\n<p>Still:&nbsp; I, as I sit here, am <a href=\"/lw/r5/the_quantum_physics_sequence/\">frantically branching into exponentially vast numbers of quantum worlds</a>.&nbsp; I've come to terms with that.&nbsp; <a href=\"/lw/qz/living_in_many_worlds/\">It all adds up to normality, after all</a>.</p>\n<p>But I don't see myself as having a little utility counter that frantically increases at an exponential rate, <em>just </em>from my sitting here and splitting.&nbsp; The thought of <em>splitting at a faster rate</em> does not much appeal to me, even if such a thing could be arranged.</p>\n<p>What I <em>do</em> want for myself, is for the largest possible <em>proportion </em>of my future selves to lead eudaimonic existences, that is, to be happy.&nbsp; This is the \"probability\" of a good outcome in my expected utility maximization.&nbsp; I'm not concerned with having <em>more </em>of me - really, there are plenty of me already - but I do want <em>most </em>of me to be having fun.</p>\n<p>I'm not sure whether or not there exists an imperative for moral civilizations to try to create lots of happy people <em>so as to ensure that most babies born will be happy.</em>&nbsp; But suppose that you started off with 1 baby existing in unhappy regions for every 999 babies existing in happy regions.&nbsp; Would it make sense for the happy regions to create ten times as many babies leading one-tenth the quality of life, so that the universe was \"99.99% sorta happy and 0.01% unhappy\" instead of \"99.9% really happy and 0.1% unhappy\"?&nbsp; On the face of it, I'd have to answer \"No.\"&nbsp; (Though it depends on <em>how </em>unhappy the unhappy regions are; and if we start off with the universe mostly <em>unhappy</em>, well, that's a pretty unpleasant possibility...)</p>\n<p>But on the whole, it looks to me like if we decide to implement a policy of routinely killing off citizens to replace them with happier babies, <em>or </em>if we lower standards of living to create more people, then we aren't giving the \"gift of existence\" to babies who wouldn't otherwise have it.&nbsp; We're just setting up the universe to contain the same babies, born predominantly into regions where they lead short lifespans not containing much happiness.</p>\n<p>Once someone has been born into your Hubble volume and your Everett branch, you can't undo that; it becomes the responsibility of <em>your</em> region of existence to give them a happy future.&nbsp; You can't hand them back by killing them.&nbsp; That just makes their average lifespan shorter.</p>\n<p>It seems to me that in a Big World, the people who <em>already exist in your region</em> have a much stronger claim on your charity than <em>babies who have not yet been born into your region in particular.</em></p>\n<p>And that's why, when there is research to be done, I do it not just for all the future babies who will be born - but, yes, for the people who already exist in our local region, who are already our responsibility.</p>\n<p>For the good of all of us, except the ones who are <a href=\"/lw/wq/you_only_live_twice/\">dead</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jiuackr7B5JAetbF6": 2, "PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cfZ8zveqrTZbQrjeD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 26, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": "1180", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 71, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9cgBF6BQ2TRB3Hy4E", "NyFtHycJvkyNjXNsP", "KbeHkLNY5ETJ3TN3W", "WajiC3YWeJutyAXTn", "nso8WXdjHLLHkJKhr", "hc9Eg6erp6hk9bWhn", "qcYCAxYZT4Xp9iMZY", "yKXKcyoBzWtECzXrE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-15T22:18:47.000Z", "modifiedAt": null, "url": null, "title": "Not Taking Over the World", "slug": "not-taking-over-the-world", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:38.358Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DdEKcS6JcW7ordZqQ/not-taking-over-the-world", "pageUrlRelative": "/posts/DdEKcS6JcW7ordZqQ/not-taking-over-the-world", "linkUrl": "https://www.lesswrong.com/posts/DdEKcS6JcW7ordZqQ/not-taking-over-the-world", "postedAtFormatted": "Monday, December 15th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Not%20Taking%20Over%20the%20World&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANot%20Taking%20Over%20the%20World%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDdEKcS6JcW7ordZqQ%2Fnot-taking-over-the-world%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Not%20Taking%20Over%20the%20World%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDdEKcS6JcW7ordZqQ%2Fnot-taking-over-the-world", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDdEKcS6JcW7ordZqQ%2Fnot-taking-over-the-world", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1289, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/wp/what_i_think_if_not_why/\">What I Think, If Not Why<br /></a></p>\n<p>My esteemed co-blogger Robin Hanson<span style=\"text-decoration: underline;\"> </span><a href=\"http://www.overcomingbias.com/2008/12/what-i-think.html#comment-142372258\">accuses</a> me of <em>trying to take over the world</em>.</p>\n<p>Why, oh why must I be so misunderstood?</p>\n<p>(Well, it's not like I don't <em>enjoy </em>certain misunderstandings.&nbsp; Ah, I remember the first time someone seriously and not in a joking way accused me of trying to take over the world.&nbsp; On that day I felt like a true mad scientist, though I lacked a castle and hunchbacked assistant.)</p>\n<p>But if you're working from the premise of a <a href=\"/lw/wf/hard_takeoff/\">hard takeoff</a> - an Artificial Intelligence that self-improves at an extremely rapid rate - and you suppose such <a href=\"/lw/uo/make_an_extraordinary_effort/\">extra-ordinary</a> depth of insight and precision of craftsmanship that you can <em>actually </em>specify the AI's <a href=\"/lw/td/magical_categories/\">goal system</a> instead of <em>automatically </em>failing -</p>\n<p>- then it takes some <a href=\"http://intelligence.org/upload/CEV.html\">work</a> to come up with a way <em>not </em>to take over the world.<a href=\"http://intelligence.org/upload/CEV.html\"></a></p>\n<p>Robin <a href=\"http://www.overcomingbias.com/2008/12/two-visions-of.html\">talks up</a> the <a href=\"http://www.overcomingbias.com/2008/12/types-of-distru.html\">drama</a> inherent in the <a href=\"http://yudkowsky.net/singularity/schools\">intelligence explosion</a>, presumably because he feels that this is a primary source of bias.&nbsp; But I've got to say that Robin's dramatic story, does <em>not </em>sound like <a href=\"/lw/ue/the_magnitude_of_his_own_folly/\">the story I tell of myself</a>.&nbsp; There, the drama comes from tampering with such <em>extreme </em>forces that <em>every single idea you invent is wrong</em>.&nbsp; The standardized Final Apocalyptic Battle of Good Vs. Evil would be trivial by comparison; then all you have to do is put forth a <a href=\"/lw/uo/make_an_extraordinary_effort/\">desperate effort.</a>&nbsp; Facing an <a href=\"/lw/up/shut_up_and_do_the_impossible/\">adult problem</a> in a <a href=\"/lw/uk/beyond_the_reach_of_god/\">neutral universe</a> isn't so straightforward.&nbsp; Your enemy is yourself, who will <em>automatically </em>destroy the world, or just fail to accomplish anything, unless you can defeat you.&nbsp; - That is the drama I crafted into the story I tell myself, for I too would disdain anything so <a href=\"/lw/k5/cached_thoughts/\">cliched</a> as Armageddon.</p>\n<p>So, Robin, I'll ask you something of a probing question.&nbsp; Let's say that someone walks up to you and grants you unlimited power.</p>\n<p>What do you do with it, so as to <em>not </em>take over the world?</p>\n<p><a id=\"more\"></a></p>\n<p>Do you say, \"I will do nothing - I take the null action\"?</p>\n<p>But then you have instantly become a malevolent God, as <a href=\"http://img216.imageshack.us/img216/820/winningathiestsqp4.jpg/\">Epicurus</a> said:</p>\n<p style=\"margin-left: 40px;\">Is God willing to prevent evil, but not able?&nbsp; Then he is not omnipotent.<br />Is he able, but not willing?&nbsp; Then he is malevolent.<br />Is both able, and willing?&nbsp; Then whence cometh evil?<br />Is he neither able nor willing?&nbsp; Then why call him God.</p>\n<p>Peter Norvig said, \"Refusing to act is like refusing to allow time to pass.\"&nbsp; The null action is also a choice.&nbsp; So have you not, in refusing to act, established all sick people as sick, established all poor people as poor, ordained all in despair to continue in despair, and condemned the dying to death?&nbsp; Will you not be, until the end of time, responsible for every sin committed?</p>\n<p>Well, yes and no.&nbsp; If someone says, \"I don't trust myself not to destroy the world, therefore I take the null action,\" then I would tend to sigh and say, \"If that is so, then you did the right thing.\"&nbsp; Afterward, murderers will still be responsible for their murders, and altruists will still be creditable for the help they give.</p>\n<p>And to say that you used your power to <em>take over the world</em> by <em>doing nothing to it,</em> seems to stretch the ordinary meaning of the phrase.</p>\n<p>But it wouldn't be the <em>best </em>thing you could do with unlimited power, either.</p>\n<p>With \"unlimited power\" you have no need to crush your enemies.&nbsp; You have no moral defense if you treat your enemies with less than the utmost consideration.</p>\n<p>With \"unlimited power\" you cannot plead the necessity of monitoring or restraining others so that they do not rebel against you.&nbsp; If you do such a thing, you are simply a tyrant who enjoys power, and not a defender of the people.</p>\n<p>Unlimited power removes a lot of moral defenses, really.&nbsp; You can't say \"But I had to.\"&nbsp; You can't say \"Well, I wanted to help, but I couldn't.\"&nbsp; The only excuse for not helping is if you <em>shouldn't,</em> which is harder to establish.</p>\n<p>And let us also suppose that this power is wieldable without side effects or configuration constraints; it is wielded with <em>unlimited precision</em>.</p>\n<p>For example, you can't take refuge in saying anything like:&nbsp; \"Well, I built this AI, but <a href=\"/lw/rn/no_universally_compelling_arguments/\">any intelligence</a> will pursue its own interests, so now the AI will just be a Ricardian trading partner with humanity as it pursues its own goals.\"&nbsp; Say, the programming team has cracked the \"hard problem of conscious experience\" in sufficient depth that they can <em>guarantee </em>that the AI they create is <em>not sentient</em> - not a repository of pleasure, or pain, or subjective experience, or any interest-in-self - and hence, the AI is only a means to an end, and not an end in itself.</p>\n<p>And you cannot take refuge in saying, \"In invoking this power, the reins of destiny have passed out of my hands, and humanity has passed on the torch.\"&nbsp; Sorry, you haven't created a new person yet - not unless you <em>deliberately </em>invoke the unlimited power to do so - and then you can't take refuge in the <em>necessity</em> of it as a side effect; you must establish that it is the right thing to do.</p>\n<p>The AI is not <em>necessarily</em> a trading partner.&nbsp; You could make it a nonsentient device that just gave you things, <em>if </em>you thought that were wiser.</p>\n<p>You cannot say, \"The law, in protecting the rights of all, must necessarily protect the right of Fred the Deranged to spend all day giving himself electrical shocks.\"&nbsp; The power is wielded with unlimited precision; you <em>could,</em> if you wished, protect the rights of everyone except Fred.</p>\n<p>You cannot take refuge in the <em>necessity</em> of anything - that is the meaning of unlimited power.</p>\n<p>We will even suppose (for it removes yet more excuses, and hence reveals more of your morality) that you are not limited by the laws of physics as we know them.&nbsp; You are bound to deal only in finite numbers, but not otherwise bounded.&nbsp; This is so that we can see the true constraints of your morality, apart from your being able to plead constraint by the environment.</p>\n<p>In my <a href=\"/lw/ty/my_childhood_death_spiral/\">reckless youth</a>, I used to think that it might be a good idea to flash-upgrade to the highest possible level of intelligence you could manage on available hardware.&nbsp; <a href=\"/lw/ty/my_childhood_death_spiral/\">Being smart was good</a>, so being smarter was better, and being as smart as possible as quickly as possible was best - right?</p>\n<p>But when I imagined having <em>infinite</em> computing power available, I realized that no matter how large a mind you made yourself, you could just go on making yourself larger and larger and larger.&nbsp; So that wasn't an answer to the purpose of life.&nbsp; And only then did it occur to me to ask after <em>eudaimonic rates of intelligence increase,</em> rather than just assuming you wanted to immediately be as smart as possible.</p>\n<p>Considering the infinite case moved me to change the way I considered the finite case.&nbsp; Before, I was <em>running away from the question</em> by saying \"More!\"&nbsp; But considering an <em>unlimited</em> amount of ice cream forced me to confront the issue of what to <em>do</em> with <em>any </em>of it.</p>\n<p>Similarly with population:&nbsp; If you invoke the unlimited power to create a quadrillion people, then why not a quintillion?&nbsp; If 3^^^3, why not <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">3^^^^3</a>?&nbsp; So you can't take refuge in saying, \"I will create more people - that is the difficult thing, and to accomplish it is the main challenge.\"&nbsp; What is <em>individually</em> a life worth living?</p>\n<p>You can say, \"It's not my place to decide; I leave it up to others\" but then you are responsible for the consequences of that decision as well.&nbsp; You should say, at least, how this differs from the null act.</p>\n<p>So, Robin, reveal to us your character:&nbsp; What would you do with <em>unlimited </em>power?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xexCWMyds6QLWognu": 1, "sYm3HiWcfZvrGu3ui": 1, "RffCgqtwT86pNBJof": 1, "nSHiKwWyMZFdZg5qt": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DdEKcS6JcW7ordZqQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 32, "extendedScore": null, "score": 5.1e-05, "legacy": true, "legacyId": "1181", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 97, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["z3kYdw54htktqt9Jb", "tjH8XPxAnr6JRbh7k", "GuEsfTpSDSbXFiseH", "PoDAyQMWEXBBBEJ5P", "fLRPeXihRaiRo5dyX", "nCvvhFBaayaXyuBiD", "sYgv4eYH82JEsTD34", "2MD3NMLBPCqPfnfre", "PtoQdG7E8MxYJrigu", "uD9TDHPwQ5hx4CgaX", "a5JAiTdytou3Jg749"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-16T18:39:54.000Z", "modifiedAt": null, "url": null, "title": "Visualizing Eutopia", "slug": "visualizing-eutopia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:39.975Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fwd2qoP9jJtuHhjrd/visualizing-eutopia", "pageUrlRelative": "/posts/fwd2qoP9jJtuHhjrd/visualizing-eutopia", "linkUrl": "https://www.lesswrong.com/posts/fwd2qoP9jJtuHhjrd/visualizing-eutopia", "postedAtFormatted": "Tuesday, December 16th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Visualizing%20Eutopia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVisualizing%20Eutopia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffwd2qoP9jJtuHhjrd%2Fvisualizing-eutopia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Visualizing%20Eutopia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffwd2qoP9jJtuHhjrd%2Fvisualizing-eutopia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffwd2qoP9jJtuHhjrd%2Fvisualizing-eutopia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1014, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/wt/not_taking_over_the_world/\">Not Taking Over the World</a></p>\n<p style=\"margin-left: 40px;\">\"Heaven is a city 15,000 miles square or 6,000 miles around. One side is 245 miles longer than the length of the Great Wall of China. Walls surrounding Heaven are 396,000 times higher than the Great Wall of China and eight times as thick. Heaven has twelve gates, three on each side, and has room for 100,000,000,000 souls. There are no slums. The entire city is built of diamond material, and the streets are paved with gold. All inhabitants are honest and there are no locks, no courts, and no policemen.\"<br />&nbsp; -- Reverend Doctor George Hawes, in a sermon</p>\n<p>Yesterday I asked my esteemed co-blogger Robin what he would do with \"<a href=\"/lw/wt/not_taking_over_the_world/\">unlimited power</a>\", in order to reveal something of his character.&nbsp; Robin said that he would (a) be very careful and (b) ask for advice.&nbsp; I asked him what advice <em>he </em>would give himself.&nbsp; Robin said it was a difficult question and he wanted to wait on considering it until it actually happened.&nbsp; So overall he ran away from the question like a startled squirrel.</p>\n<p>The character thus revealed is a virtuous one: it shows common sense.&nbsp; A lot of people jump after the prospect of absolute power like it was a coin they found in the street.</p>\n<p>When you think about it, though, it says a lot about human nature that this is a difficult question.&nbsp; I mean - <em>most </em>agents with utility functions shouldn't have such a hard time describing their perfect universe.</p>\n<p>For a long time, I too ran away from the question like a startled squirrel.&nbsp; First <a href=\"/lw/u2/the_sheer_folly_of_callow_youth/\">I claimed that superintelligences would inevitably do what was right</a>, relinquishing moral responsibility in toto.&nbsp; After that, I propounded various schemes to <em>shape </em>a nice superintelligence, and let <em>it</em> decide what should be done with the world.</p>\n<p>Not that there's anything wrong with that.&nbsp; Indeed, this is <a href=\"http://intelligence.org/upload/CEV.html\">still the plan</a>.&nbsp; But it still meant that I, personally, was ducking the question.</p>\n<p>Why?&nbsp; Because I expected to fail at answering.&nbsp; Because I thought that any attempt for humans to visualize a better future was going to end up recapitulating the Reverend Doctor George Hawes: apes thinking, \"Boy, if I had <em>human intelligence</em> I sure could get a lot more bananas.\"</p>\n<p><a id=\"more\"></a></p>\n<p>But trying to get a <em>better</em> answer to a question out of a superintelligence, is a different matter from entirely ducking the question yourself.&nbsp; The point at which I stopped ducking was the point at which I realized that it's actually quite difficult to get a good answer to something out of a superintelligence, while simultaneously having literally <em>no</em> idea how to answer yourself.</p>\n<p>When you're dealing with <em>confusing and difficult</em> questions - as opposed to those that are straightforward but numerically tedious - it's quite suspicious to have, on the one hand, a procedure that executes to reliably answer the question, and, on the other hand, no idea of how to answer it yourself.</p>\n<p>If you could write a computer program that you knew would reliably output a satisfactory answer to \"Why does anything exist in the first place?\" or \"Why do I find myself in a universe giving rise to experiences that are ordered rather than chaotic?\", then shouldn't you be able to at least try executing the same procedure yourself?</p>\n<p>I suppose there could be some section of the procedure where you've got to do a septillion operations and so you've just got no choice but to wait for superintelligence, but really, that sounds rather suspicious in cases like these.</p>\n<p>So it's not that I'm planning to use the output of my own intelligence to take over the universe.&nbsp; But I did realize at some point that it was too suspicious to entirely duck the question while trying to make a computer knowably solve it.&nbsp; It didn't even seem all that <em>morally cautious</em>, once I put in those terms.&nbsp; You <em>can </em>design an arithmetic chip using purely abstract reasoning, but would you be <em>wise </em>to never try an arithmetic problem yourself?</p>\n<p>And when I did finally try - well, that caused me to update in various ways.</p>\n<p>It does make a difference to <em>try </em>doing arithmetic yourself, instead of just trying to design chips that do it for you.&nbsp; So I found.</p>\n<p>Hence my bugging Robin about it.</p>\n<p>For it seems to me that Robin asks too little of the future.&nbsp; It's all very well to plead that you are only forecasting, but if you display greater revulsion to the idea of a Friendly AI than to the idea of rapacious hardscrapple frontier folk...</p>\n<p>I thought that Robin might be asking too little, due to not visualizing <em>any</em> future in enough detail.&nbsp; Not <em>the</em> future but <em>any</em> future.&nbsp; I'd hoped that if Robin had allowed himself to visualize his \"perfect future\" in more detail, rather than focusing on all the compromises he thinks he has to make, he might see that there were futures more desirable than the rapacious hardscrapple frontier folk.</p>\n<p>It's hard to see on an emotional level why a genie might be a good thing to have, if you haven't acknowledged any wishes that need granting.&nbsp; It's like not feeling the temptation of cryonics, if you haven't thought of anything the Future contains that might be worth seeing.</p>\n<p>I'd also hoped to persuade Robin, if his wishes were <em>complicated</em> enough, that there were attainable good futures that could not come about by letting things go their own way.&nbsp; So that he might begin to see the future as I do, as a dilemma between extremes:&nbsp; The default, loss of control, followed by a Null future containing little or no utility.&nbsp; Versus extremely precise steering through \"impossible\" problems to get to any sort of Good future whatsoever.</p>\n<p>This is mostly a matter of appreciating how even the desires we call \"simple\" actually contain many bits of information.&nbsp; Getting past <a href=\"/lw/st/anthropomorphic_optimism/\">anthropomorphic optimism</a>, to realize that a Future not strongly steered by our utility functions is likely to contain little or no utility, for the same reason it's hard to hit a distant target while shooting blindfolded...</p>\n<p>But if your \"desired future\" remains mostly unspecified, <em>that </em>may encourage too much optimism as well.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sSNtcEQsqHgN8ZmRF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fwd2qoP9jJtuHhjrd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 18, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "1182", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DdEKcS6JcW7ordZqQ", "Yicjw6wSSaPdb83w9", "RcZeZt8cPk48xxiQ8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-17T23:33:01.000Z", "modifiedAt": null, "url": null, "title": "Prolegomena to a Theory of Fun", "slug": "prolegomena-to-a-theory-of-fun", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:01.304Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pK4HTxuv6mftHXWC3/prolegomena-to-a-theory-of-fun", "pageUrlRelative": "/posts/pK4HTxuv6mftHXWC3/prolegomena-to-a-theory-of-fun", "linkUrl": "https://www.lesswrong.com/posts/pK4HTxuv6mftHXWC3/prolegomena-to-a-theory-of-fun", "postedAtFormatted": "Wednesday, December 17th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Prolegomena%20to%20a%20Theory%20of%20Fun&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProlegomena%20to%20a%20Theory%20of%20Fun%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpK4HTxuv6mftHXWC3%2Fprolegomena-to-a-theory-of-fun%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Prolegomena%20to%20a%20Theory%20of%20Fun%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpK4HTxuv6mftHXWC3%2Fprolegomena-to-a-theory-of-fun", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpK4HTxuv6mftHXWC3%2Fprolegomena-to-a-theory-of-fun", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1805, "htmlBody": "<p><em></em><strong>Followup to</strong>:&nbsp; <a href=\"/lw/sx/inseparably_right_or_joy_in_the_merely_good/\">Joy in the Merely Good</a></p>\n<p>Raise the topic of cryonics, uploading, or just medically extended lifespan/healthspan, and some bioconservative neo-Luddite is bound to ask, in portentous tones:</p>\n<p style=\"margin-left: 40px;\">\"But what will people <em>do </em>all day?\"</p>\n<p>They don't try to <a href=\"http://intelligence.org/blog/2007/10/14/the-meaning-that-immortality-gives-to-life/\">actually answer the question</a>.&nbsp; That is not a bioethicist's role, in the scheme of things.&nbsp; They're just there to collect credit for the <a href=\"/lw/k5/cached_thoughts/\">Deep Wisdom</a> of asking the question.&nbsp; It's enough to <em>imply </em>that the question is unanswerable, and therefore, we should all drop dead.</p>\n<p>That <a href=\"/lw/lw/reversed_stupidity_is_not_intelligence/\">doesn't mean</a> it's a <em>bad </em>question.</p>\n<p>It's not an <em>easy </em>question to answer, either.&nbsp; The primary experimental result in hedonic psychology&mdash;the study of happiness&mdash;is that people don't <em>know </em>what makes them happy.</p>\n<p>And there are many exciting results in this new field, which go a long way toward explaining the emptiness of classical Utopias.&nbsp; But it's worth remembering that<em> human</em> hedonic psychology is not enough for us to consider, if we're asking whether a million-year lifespan could be worth living.</p>\n<p>Fun Theory, then, is the field of knowledge that would deal in questions like:</p>\n<ul>\n<li>\"How much fun is there in the universe?\"</li>\n<li>\"Will we ever run out of fun?\"</li>\n<li>\"Are we having fun yet?\"</li>\n<li>\"Could we be having more fun?\"</li>\n</ul>\n<p><a id=\"more\"></a></p>\n<p>One major set of experimental results in hedonic psychology has to do with <em>overestimating the impact</em> of life events on happiness.&nbsp; Six months after the event, lottery winners aren't as happy as they expected to be, and quadriplegics aren't as sad.&nbsp; A parent who loses a child isn't as sad as they think they'll be, a few years later.&nbsp; If you look at one moment snapshotted out of their lives a few years later, that moment isn't likely to be about the lost child.&nbsp; Maybe they're playing with one of their surviving children on a swing.&nbsp; Maybe they're just listening to a nice song on the radio.</p>\n<p>When people are asked to imagine how happy or sad an event will make them, they anchor on <em>the moment of first receiving the news,</em> rather than realistically imagining the process of daily life years later.</p>\n<p>Consider what the Christians made of their Heaven, meant to be literally <em>eternal.</em>&nbsp; Endless rest, the glorious presence of God, and occasionally&mdash;in the <a href=\"/lw/wu/visualizing_eutopia/\">more clueless sort of sermon</a>&mdash;golden streets and diamond buildings.&nbsp; Is this eudaimonia?&nbsp; It doesn't even seem very <em>hedonic</em>.</p>\n<p>As someone who said his share of prayers back in his Orthodox Jewish childhood upbringing, I can personally testify that praising God is an enormously boring activity, even if you're still young enough to truly believe in God.&nbsp; The part about praising God is there as an <a href=\"/lw/jb/applause_lights/\">applause light</a> that no one is allowed to contradict: it's something theists <a href=\"/lw/i4/belief_in_belief/\">believe they <em>should</em> enjoy</a>, even though, if you ran them through an fMRI machine, you probably wouldn't find their pleasure centers lighting up much.</p>\n<p>Ideology is one major wellspring of flawed Utopias, containing things that the imaginer believes <em>should </em>be enjoyed, rather than things that would actually be enjoyable.</p>\n<p>And eternal <em>rest?</em>&nbsp; What could possibly be more boring than eternal <em>rest?</em></p>\n<p>But to an exhausted, poverty-stricken medieval peasant, the Christian Heaven sounds like <em>good news in the moment of being first informed:</em>&nbsp; You can lay down the plow and rest!&nbsp; Forever!&nbsp; Never to work again!</p>\n<p>It'd get boring after... what, a week?&nbsp; A day?&nbsp; An hour?<br /><span style=\"font-style: italic;\"> </span></p>\n<p>Heaven is not configured as a nice place to <em>live.</em>&nbsp; It is rather memetically optimized to be a nice place for an exhausted peasant to <em>imagine.</em>&nbsp; It's not like some Christians <em>actually </em>got a chance to live in various Heavens, and voted on how well they liked it after a year, and then they kept the best one.&nbsp; The Paradise that survived was the one that was <em>retold,</em> not lived.</p>\n<p>Timothy Feriss observed, \"<em>Living</em> like a millionaire requires <em>doing</em> interesting things and not just owning enviable things.\"&nbsp; <a href=\"/lw/wu/visualizing_eutopia/\">Golden streets and diamond walls</a> would fade swiftly into the background, once <em>obtained </em>&mdash;but so long as you <a href=\"/lw/oz/scarcity/\">don't actually <em>have</em> gold</a>, it stays desirable.</p>\n<p>And there's two lessons required to get past such failures; and these lessons are in some sense opposite to one another.</p>\n<p>The first lesson is that humans are terrible judges of what will <em>actually </em>make them happy, in the real world and the living moments.&nbsp; Daniel Gilbert's <em>Stumbling on Happiness</em> is the most famous popular introduction to the research.</p>\n<p>We need to be ready to correct for such biases&mdash;the world that is fun to <em>live in</em>, may not be the world that sounds good when spoken into our ears.</p>\n<p>And the second lesson is that there's <em>nothing</em> in the universe out of which to construct Fun Theory, except that which we want for ourselves or prefer to become.</p>\n<p>If, <em>in fact</em>, you <em>don't</em> like praying, then there's no higher God than yourself to tell you that you <em>should</em> enjoy it.&nbsp; We sometimes do things we don't like, but that's still our own choice.&nbsp; There's no <em>outside</em> force to scold us for making the wrong decision.</p>\n<p>This is something for transhumanists to keep in mind&mdash;not because we're tempted to pray, of course, but because there are so many other logical-sounding solutions we wouldn't really <em>want.</em></p>\n<p>The transhumanist philosopher <a href=\"http://en.wikipedia.org/wiki/David_Pearce_%28philosopher%29\">David Pearce</a> is an advocate of what he calls the <a href=\"http://www.hedweb.com/\">Hedonistic Imperative</a>:&nbsp; The eudaimonic life is the one that is as pleasurable as possible.&nbsp; So even happiness attained through drugs is good?&nbsp; Yes, in fact:&nbsp; Pearce's motto is \"Better Living Through Chemistry\".</p>\n<p>Or similarly:&nbsp; When giving a small informal talk once on the Stanford campus, I raised the topic of Fun Theory in the post-talk mingling.&nbsp; And someone there said that his ultimate objective was to experience delta pleasure.&nbsp; That's \"delta\" as in the Dirac delta&mdash;roughly, an infinitely high spike (that happens to be integrable).&nbsp; \"Why?\" I asked.&nbsp; He said, \"Because that means I win.\"</p>\n<p>(I replied, \"How about if you get two times delta pleasure?&nbsp; Do you win twice as hard?\")</p>\n<p>In the transhumanist lexicon, \"orgasmium\" refers to simplified brains that are just pleasure centers experiencing huge amounts of stimulation&mdash;a happiness counter containing a large number, plus whatever the minimum surrounding framework to <em>experience</em> it.&nbsp; You can imagine a whole galaxy tiled with orgasmium.&nbsp; Would this be a good thing?</p>\n<p>And the vertigo-inducing thought is this&mdash;if you would <em>prefer</em> not to become orgasmium, then why <em>should</em> you?</p>\n<p>Mind you, there are many reasons why something that sounds unpreferred at first glance, might be worth a closer look.&nbsp; That was the <em>first</em> lesson.&nbsp; Many Christians <em>think </em>they want to go to Heaven.</p>\n<p>But when it comes to the question, \"Don't I <em>have</em> to want to be as happy as possible?\" then the answer is simply \"No.&nbsp; If you don't prefer it, why go there?\"</p>\n<p>There's nothing <em>except </em>such preferences out of which to construct Fun Theory&mdash;a second look is still a look, and must still be constructed out of preferences at some level.</p>\n<p>In the era of my foolish youth, when <a href=\"/lw/ty/my_childhood_death_spiral/\">I went into an affective death spiral around intelligence</a>, I thought that the <a href=\"/lw/sx/inseparably_right_or_joy_in_the_merely_good/\">mysterious \"right\" thing</a> that <a href=\"/lw/u2/the_sheer_folly_of_callow_youth/\">any superintelligence would inevitably do</a>, would be to upgrade every nearby mind to superintelligence as fast as possible.&nbsp; Intelligence was good; therefore, more intelligence was better.</p>\n<p>Somewhat later I imagined the scenario of <em>unlimited</em> computing power, so that no matter how smart you got, you were still just as far from infinity as ever.&nbsp; That got me thinking about a journey rather than a destination, and <em>allowed </em>me to think \"What <em>rate </em>of intelligence increase would be fun?\"</p>\n<p>But the real break came when I <a href=\"/lw/sm/the_meaning_of_right/\">naturalized my understanding of morality</a>, and value stopped being a mysterious attribute of unknown origins.</p>\n<p>Then if there was no outside light in the sky to order me to do things&mdash;</p>\n<p>The thought occurred to me that I didn't actually <em>want</em> to bloat up immediately into a superintelligence, <em>or</em> have my world transformed instantaneously and completely into something incomprehensible.&nbsp; I'd prefer to have it happen gradually, with time to stop and smell the flowers along the way.</p>\n<p>It felt like a very guilty thought, but&mdash;</p>\n<p>But there was nothing <em>higher </em>to <em>override </em>this preference.</p>\n<p>In which case, if the Friendly AI project succeeded, there would be a day after the Singularity to wake up to, and myself to wake up to it.</p>\n<p>You may not see why this would be a vertigo-inducing concept.&nbsp; Pretend you're Eliezer<sub>2003</sub> who has spent the last seven years talking about how it's forbidden to try to look beyond the Singularity&mdash;because the AI is smarter than you, and if you knew what it would do, you would have to be that smart yourself&mdash;</p>\n<p>&mdash;but what if you don't <em>want</em> the world to be made suddenly incomprehensible?&nbsp; Then there might be something to understand, that next morning, <em>because</em> you don't <em>actually want</em> to wake up in an incomprehensible world, any more than you <em>actually want</em> to suddenly be a superintelligence, or turn into orgasmium.</p>\n<p>I can only analogize the experience to a theist who's suddenly told that they <em>can</em> know the mind of God, and it turns out to be only twenty lines of Python.</p>\n<p>You may find it hard to sympathize.&nbsp; Well, Eliezer<sub>1996</sub>, who originally made the mistake, was <a href=\"/lw/u1/a_prodigy_of_refutation/\">smart but methodologically inept</a>, as I've mentioned a few times.</p>\n<p>Still, expect to see some outraged comments on this very blog post, from commenters who think that it's <em>selfish and immoral</em>, and above all a <em>failure of imagination,</em> to talk about human-level minds still running around the day after the Singularity.</p>\n<p>That's the frame of mind I used to occupy&mdash;that the things I wanted were selfish, and that I shouldn't think about them too much, or at all, because I would need to sacrifice them for something higher.</p>\n<p>People who talk about an existential pit of meaninglessness in a universe devoid of meaning&mdash;I'm pretty sure they don't understand morality in naturalistic terms.&nbsp; There <em>is</em> vertigo involved, but it's <em>not</em> the vertigo of meaninglessness<em>.</em></p>\n<p>More like a theist who is <a href=\"/lw/sb/could_anything_be_right/\">frightened</a> that someday God will <a href=\"http://www.thevillageatheist.co.uk/genocide.html\">order him to murder children</a>, and then he realizes that there <em>is</em> no God and his fear of being ordered to murder children <em><a href=\"/lw/ky/fake_morality/\">was morality</a></em>.&nbsp; It's a strange relief, mixed with the realization that you've been very silly, as the last remnant of outrage at your own selfishness fades away.</p>\n<p>So the first step toward Fun Theory is that, so far as I can tell, it looks basically <em>okay </em>to make our future light cone&mdash;all the galaxies that we can get our hands on&mdash;into a place that is <em>fun</em> rather than <em>not fun.</em></p>\n<p>We don't need to transform the universe into something we feel <em>dutifully obligated</em> to create, but isn't really much fun&mdash;in the same way that a Christian would feel dutifully obliged to enjoy heaven&mdash;or that some strange folk think that creating orgasmium is, logically, the rightest thing to do.</p>\n<p>Fun is okay.&nbsp; It's allowed.&nbsp; It doesn't get any better than fun.</p>\n<p>And then we can turn our attention to the question of what <em>is</em> fun, and how to have it.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/ww/high_challenge/\">High Challenge</a>\"</p>\n<p style=\"text-align:right\">(start of sequence)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sSNtcEQsqHgN8ZmRF": 2, "Jzm2mYuuDBCNWq8hi": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pK4HTxuv6mftHXWC3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 47, "extendedScore": null, "score": 7e-05, "legacy": true, "legacyId": "1183", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 48, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JynJ6xfnpq9oN3zpb", "2MD3NMLBPCqPfnfre", "qNZM3EGoE5ZeMdCRt", "fwd2qoP9jJtuHhjrd", "dLbkrPu5STNCBLRjr", "CqyJzDZWvGhhFJ7dY", "MCYp8g9EMAiTCTawk", "uD9TDHPwQ5hx4CgaX", "Yicjw6wSSaPdb83w9", "fG3g3764tSubr6xvs", "CcBe9aCKDgT5FSoty", "vy9nnPdwTjSmt5qdb", "fATPBv4pnHC33EmJ2", "K4aGvLnHvYgX9pZHS", "29vqqmGNxNRGzffEj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-19T00:51:08.000Z", "modifiedAt": null, "url": null, "title": "High Challenge", "slug": "high-challenge", "viewCount": null, "lastCommentedAt": "2022-03-01T08:38:07.713Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/29vqqmGNxNRGzffEj/high-challenge", "pageUrlRelative": "/posts/29vqqmGNxNRGzffEj/high-challenge", "linkUrl": "https://www.lesswrong.com/posts/29vqqmGNxNRGzffEj/high-challenge", "postedAtFormatted": "Friday, December 19th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20High%20Challenge&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHigh%20Challenge%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F29vqqmGNxNRGzffEj%2Fhigh-challenge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=High%20Challenge%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F29vqqmGNxNRGzffEj%2Fhigh-challenge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F29vqqmGNxNRGzffEj%2Fhigh-challenge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1236, "htmlBody": "<p>There's a class of prophecy that runs:&nbsp; \"In the Future, machines will do all the work.&nbsp; Everything will be automated.&nbsp; Even labor of the sort we now consider 'intellectual', like engineering, will be done by machines.&nbsp; We can sit back and own the capital.&nbsp; You'll never have to lift a finger, ever again.\"</p>\n<p>But then won't people be bored?</p>\n<p>No; they can play computer games&mdash;not like <em>our</em> games, of course, but much more advanced and entertaining.</p>\n<p>Yet wait!&nbsp; If you buy a modern computer game, you'll find that it contains some tasks that are&mdash;there's no kind word for this&mdash;<em>effortful.</em>&nbsp; (I would even say \"difficult\", with the understanding that we're talking about something that takes 10 minutes, not 10 years.)</p>\n<p>So in the future, we'll have programs that <em>help </em>you play the game&mdash;taking over if you get stuck on the game, or just bored; or so that you can play games that would otherwise be too advanced for you.</p>\n<p>But isn't there some wasted effort, here?&nbsp; Why have one programmer working to make the game harder, and another programmer to working to make the game easier?&nbsp; Why not just make the game easier to <em>start with?</em>&nbsp; Since you play the game to get gold and experience points, making the game easier will let you get more gold per unit time: the game will become more fun.</p>\n<p>So this is the ultimate end of the prophecy of technological progress&mdash;just staring at a screen that says \"YOU WIN\", forever.</p>\n<p>And maybe we'll build a robot that does <em>that,</em> too.</p>\n<p>Then what?</p>\n<p><a id=\"more\"></a></p>\n<p>The world of machines that do <em>all</em> the work&mdash;well, I don't want to say it's \"analogous to the Christian Heaven\" because it isn't <a href=\"/lw/tv/excluding_the_supernatural/\">supernatural</a>; it's something that could in principle be realized.&nbsp; Religious analogies are far too easily tossed around as accusations...&nbsp; But, without implying any other similarities, I'll say that it seems analogous in the sense that eternal laziness \"<a href=\"/lw/wv/prolegomena_to_a_theory_of_fun/\">sounds like good news</a>\" to your present self who still has to work.</p>\n<p>And as for playing games, as a substitute&mdash;what <em>is</em> a computer game except synthetic work?&nbsp; Isn't there a wasted step here?&nbsp; (And computer games in their present form, considered as work, have various aspects that reduce stress and increase engagement; but they also carry costs in the form of artificiality and isolation.)</p>\n<p>I sometimes think that futuristic ideals phrased in terms of \"getting rid of work\" would be better reformulated as \"removing low-quality work to make way for high-quality work\".</p>\n<p>There's a broad class of goals that aren't suitable as the long-term meaning of life, because you can actually achieve them, and then you're done.</p>\n<p>To look at it another way, if we're looking for a suitable long-run meaning of life, we should look for goals that are good to <em>pursue</em> and not just good to <em>satisfy.</em></p>\n<p>Or to phrase that somewhat less paradoxically:&nbsp; We should look for valuations that are over 4D states, rather than 3D states.&nbsp; Valuable ongoing processes, rather than \"make the universe have property P and then you're done\".</p>\n<p>Timothy Ferris is again worth quoting:&nbsp; To find happiness, \"the question you should be asking isn't 'What do I want?' or 'What are my goals?' but 'What would excite me?'\"</p>\n<p>You might say that for a long-run meaning of life, we need games that are fun to <em>play</em> and not just to <em>win.</em></p>\n<p>Mind you&mdash;sometimes you <em>do</em> <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">want to win</a>.&nbsp; There are legitimate goals where winning is everything.&nbsp; If you're talking, say, about curing cancer, then the suffering experienced by even a single cancer patient outweighs any fun that you might have in solving their problems.&nbsp; If you work at creating a cancer cure for twenty years through your own efforts, learning new knowledge and new skill, making friends and allies&mdash;and then some alien superintelligence offers you a cancer cure on a silver platter for thirty bucks&mdash;then you shut up and take it.</p>\n<p>But \"curing cancer\" is a problem of the 3D-predicate sort: you want the no-cancer predicate to go from False in the present to True in the future.&nbsp; The importance of this destination far outweighs the journey; you don't want to <em>go</em> there, you just want to <em>be</em> there.&nbsp; There are many <em>legitimate</em> goals of this sort, but they are not suitable as long-run fun.&nbsp; \"Cure cancer!\" is a worthwhile activity for us to pursue here and now, but it is not a plausible future goal of galactic civilizations.</p>\n<p>Why should this \"valuable ongoing process\" be a process of <em>trying to do things</em>&mdash;why not a process of passive experiencing, like the Buddhist Heaven?</p>\n<p>I confess I'm not entirely sure how to set up a \"passively experiencing\" mind.&nbsp; The human brain was <em>designed</em> to perform various sorts of internal work that add up to an active intelligence; even if you lie down on your bed and exert no particular effort to think, the thoughts that go on through your mind are activities of brain areas that are designed to, you know, <em>solve problems.</em></p>\n<p>How much of the human brain could you eliminate, <em>apart </em>from the pleasure centers, and still keep the subjective experience of pleasure?</p>\n<p>I'm not going to touch that one.&nbsp; I'll stick with the much simpler answer of \"I wouldn't actually <em>prefer </em>to be a passive experiencer.\"&nbsp; If I <em>wanted</em> Nirvana, I might try to figure out how to achieve that impossibility.&nbsp; But once you strip away Buddha telling me that Nirvana is the end-all of existence, Nirvana seems rather more like \"sounds like good news in the moment of first being told\" or \"ideological belief in desire\" rather than, y'know, something I'd actually <em>want.</em><em><br /></em></p>\n<p>The reason I have a mind at all, is that natural selection built me to <em>do</em> things&mdash;to solve certain kinds of problems.</p>\n<p>\"Because it's human nature\" is not an <a href=\"/lw/s0/where_recursive_justification_hits_bottom/\">explicit</a> justification for anything.&nbsp; There is human nature, which is what we are; and there is humane nature, which is what, being human, we wish we were.</p>\n<p>But I don't <em>want</em> to change my nature toward a more passive object&mdash;which <em>is </em>a justification.&nbsp; A happy blob is <em>not </em>what, being human, I wish to become.</p>\n<p><a href=\"/lw/lb/not_for_the_sake_of_happiness_alone/\">I earlier argued that many values require both subjective happiness and the external objects of that happiness</a>.&nbsp; That you can legitimately have a utility function that says, \"It matters to me whether or not the person I love is a real human being or just a highly realistic nonsentient chatbot, <em>even if I don't know,</em> because that-which-I-value is not my own state of mind, but the external reality.\"&nbsp; So that you need both the experience of love, and the real lover.</p>\n<p>You can similarly have valuable activities that require both real challenge and real effort.</p>\n<p>Racing along a track, it matters that the other racers are real, and that you have a real chance to win or lose.&nbsp; (We're not talking about <a href=\"/lw/r0/thou_art_physics/\">physical determinism</a> here, but whether some external optimization process explicitly chose for you to win the race.)</p>\n<p>And it matters that you're racing with your own skill at running and your own willpower, not just pressing a button that says \"Win\".&nbsp; (Though, since you never designed your own leg muscles, you <em>are</em> racing using strength that isn't yours.&nbsp; A race between robot cars is a purer contest of their designers.&nbsp; There is plenty of room to improve on the human condition.)</p>\n<p>And it matters that you, a sentient being, are experiencing it.&nbsp; (Rather than some nonsentient process carrying out a skeleton imitation of the race, trillions of times per second.)</p>\n<p>There must be the true effort, the true victory, and the true experience&mdash;the journey, the destination and the traveler.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"R6uagTfhhBeejGrrf": 2, "sSNtcEQsqHgN8ZmRF": 2, "xexCWMyds6QLWognu": 2, "fR7QfYx4JA3BnptT9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "29vqqmGNxNRGzffEj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 54, "extendedScore": null, "score": 8e-05, "legacy": true, "legacyId": "1184", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "9bvAELWc8y2gYjRav", "canonicalCollectionSlug": "rationality", "canonicalBookId": "T3CjiQq6bBgFuzvp6", "canonicalNextPostSlug": "serious-stories", "canonicalPrevPostSlug": "sympathetic-minds", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 75, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["u6JzcFtPGiznFgDxP", "pK4HTxuv6mftHXWC3", "6ddcsdA2c2XpNpE5x", "C8nEXTcjZb9oauTCW", "synsRtBKDeAFuo7e3", "NEeW7eSXThPz7o4Ne"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-20T00:31:51.000Z", "modifiedAt": null, "url": null, "title": "Complex Novelty", "slug": "complex-novelty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:38.726Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aEdqh3KPerBNYvoWe/complex-novelty", "pageUrlRelative": "/posts/aEdqh3KPerBNYvoWe/complex-novelty", "linkUrl": "https://www.lesswrong.com/posts/aEdqh3KPerBNYvoWe/complex-novelty", "postedAtFormatted": "Saturday, December 20th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Complex%20Novelty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComplex%20Novelty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaEdqh3KPerBNYvoWe%2Fcomplex-novelty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Complex%20Novelty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaEdqh3KPerBNYvoWe%2Fcomplex-novelty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaEdqh3KPerBNYvoWe%2Fcomplex-novelty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2313, "htmlBody": "<p><strong></strong>From Greg Egan's <em>Permutation City</em>:</p>\n<p style=\"margin-left: 40px;\">&nbsp;&nbsp;&nbsp; The workshop abutted a warehouse full of table legs&mdash;one hundred and sixty-two thousand, three hundred and twenty-nine, so far.&nbsp; Peer could imagine nothing more satisfying than reaching the two hundred thousand mark&mdash;although he knew it was likely that he'd change his mind and abandon the workshop before that happened; new vocations were imposed by his exoself at random intervals, but statistically, the next one was overdue.&nbsp; Immediately before taking up woodwork, he'd passionately devoured all the higher mathematics texts in the central library, run all the tutorial software, and then personally contributed several important new results to group theory&mdash;untroubled by the fact that none of the Elysian mathematicians would ever be aware of his work.&nbsp; Before that, he'd written over three hundred comic operas, with librettos in Italian, French and English&mdash;and staged most of them, with puppet performers and audience.&nbsp; Before that, he'd patiently studied the structure and biochemistry of the human brain for sixty-seven years; towards the end he had fully grasped, to his own satisfaction, the nature of the process of consciousness.&nbsp; Every one of these pursuits had been utterly engrossing, and satisfying, at the time.&nbsp; He'd even been interested in the Elysians, once.<br />&nbsp;&nbsp;&nbsp; No longer.&nbsp; He preferred to think about table legs.</p>\n<p>Among science fiction authors, (early) Greg Egan is my favorite; of early-Greg-Egan's books, <em>Permutation City</em> is my favorite; and this particular passage in <em>Permutation City,</em> more than any of the others, I find utterly horrifying.</p>\n<p>If this were all the hope the future held, I don't know if I could bring myself to try.&nbsp; Small wonder that people don't <a href=\"http://www.overcomingbias.com/2008/12/you-only-live-twice.html\">sign up for cryonics</a>, if even SF writers think this is the best we can do.</p>\n<p>You could think of this whole series on Fun Theory as my reply to Greg Egan&mdash;a list of the ways that his human-level uploaded civilizations Fail At Fun.&nbsp; (And yes, this series will also explain what's wrong with the Culture and how to fix it.)</p>\n<p><a id=\"more\"></a></p>\n<p>We won't get to <em>all</em> of Peer's problems today&mdash;but really.&nbsp; <em>Table legs?</em></p>\n<p>I could see myself carving <em>one</em> table leg, maybe, if there was something non-obvious to learn from the experience.&nbsp; But not 162,329.</p>\n<p>In <em>Permutation City</em>, Peer modified himself to find table-leg-carving fascinating and worthwhile and pleasurable.&nbsp; But really, at <em>that </em>point, you might as well modify yourself to get pleasure from playing Tic-Tac-Toe, or lie motionless on a pillow as a limbless eyeless blob having fantastic orgasms.&nbsp; It's not a worthy use of a human-level intelligence.</p>\n<p>Worse, carving the 162,329th table leg doesn't <em>teach </em>you anything that you didn't already know from carving 162,328 previous table legs.&nbsp; A mind that changes so little in life's course is scarcely experiencing time.</p>\n<p>But apparently, once you do a little group theory, write a few operas, and solve the mystery of consciousness, there isn't much else worth doing in life: you've <em>exhausted the entirety of Fun Space</em> down to the level of table legs.</p>\n<p>Is this plausible?&nbsp; How large is Fun Space?</p>\n<p>Let's say you were a human-level intelligence who'd never seen a Rubik's Cube, or anything remotely like it.&nbsp; As Hofstadter describes in two whole chapters of<span style=\"font-style: italic;\"> </span><em>Metamagical Themas,</em> there's a <em>lot</em> that intelligent human novices can learn from the Cube&mdash;like the whole notion of an \"operator\" or \"macro\", a sequence of moves that accomplishes a limited swap with few side effects.&nbsp; Parity, search, impossibility&mdash;</p>\n<p>So you learn these things in the long, difficult course of solving the <em>first </em>scrambled Rubik's Cube you encounter.&nbsp; The <em>second </em>scrambled Cube&mdash;solving it might still be difficult, still be enough fun to be worth doing.&nbsp; But you won't have quite the same pleasurable shock of encountering something as new, and strange, and interesting as the first Cube was unto you.</p>\n<p>Even if you encounter a variant of the Rubik's Cube&mdash;like a 4x4x4 Cube instead of a 3x3x3 Cube&mdash;or even a <a href=\"http://www.superliminal.com/cube/cube.htm\">Rubik's Tesseract</a> (a 3x3x3x3 Cube in four dimensions)&mdash;it still won't contain quite as much fun as the first Cube you ever saw.&nbsp; I haven't tried mastering the Rubik's Tesseract myself, so I don't know if there are added secrets in four dimensions&mdash;but it doesn't seem likely to teach me anything as fundamental as \"operators\", \"side effects\", or \"parity\".</p>\n<p>(I was quite young when I encountered a Rubik's Cube in a toy cache, and so that actually <em>is</em> where I discovered such concepts.&nbsp; I tried that Cube on and off for months, without solving it.&nbsp; Finally I took out a book from the library on Cubes, applied the macros there, and discovered that this particular Cube was <em>unsolvable </em>&mdash;it had been disassembled and reassembled into an impossible position.&nbsp; I think I was faintly annoyed.)</p>\n<p>Learning is fun, but it <em>uses up</em> fun: you can't have the same stroke of genius twice.&nbsp; Insight is insight because it makes future problems <em>less difficult,</em> and \"deep\" because it applies to many such problems.</p>\n<p>And the smarter you are, the faster you learn&mdash;so the smarter you are, the less <em>total </em>fun you can have.&nbsp; Chimpanzees can occupy themselves for a lifetime at tasks that would bore you or I to tears.&nbsp; Clearly, the solution to Peer's difficulty is to become stupid enough that carving table legs is <em>difficult</em> again&mdash;and so lousy at generalizing that every table leg is a new and exciting challenge&mdash;</p>\n<p>Well, but hold on:&nbsp; If you're a chimpanzee, you can't understand the Rubik's Cube <em>at all.</em>&nbsp; At least I'm willing to bet against anyone training a chimpanzee to solve one&mdash;let alone a chimpanzee solving it spontaneously&mdash;let alone a chimpanzee understanding the deep concepts like \"operators\", \"side effects\", and \"parity\".</p>\n<p>I could be wrong here, but it seems to me, on the whole, that when you look at the number of ways that chimpanzees have fun, and the number of ways that humans have fun, <em>that Human Fun Space is larger than Chimpanzee Fun Space.</em></p>\n<p>And not in a way that increases just <em>linearly </em>with brain size, either.</p>\n<p>The space of problems that are Fun to a given brain, <em>will </em>definitely be smaller than the exponentially increasing space of all possible problems that brain can <em>represent</em>.&nbsp; We are interested only in the borderland between triviality and impossibility&mdash;problems difficult enough to worthily occupy our minds, yet tractable enough to be worth challenging.&nbsp; (<a href=\"http://www.overcomingbias.com/2008/10/try-persevere.html\">What <em>looks</em> \"impossible\" is not always impossible</a>, but the border is still <em>somewhere</em> even if we can't see it at a glance&mdash;there are some problems so difficult you can't even learn much from failing.)</p>\n<p>An even stronger constraint is that if you do something many times, you ought to learn from the experience and get better&mdash;many problems of the same <em>difficulty </em>will have the same \"learnable lessons\" embedded in them, so that doing one consumes some of the fun of others.</p>\n<p>As you learn new things, and your skills improve, problems will get easier.&nbsp; Some will move off the border of the possible and the impossible, and become too easy to be interesting.</p>\n<p>But <em>others </em>will move from the territory of impossibility into the borderlands of mere extreme difficulty.&nbsp; It's easier to invent group theory if you've solved the Rubik's Cube first.&nbsp; There are insights you can't have without prerequisite insights.</p>\n<p>If you get smarter over time (larger brains, improved mind designs) that's a still higher octave of the same phenomenon.&nbsp; (As best I can grasp the Law, there are insights you can't understand <em>at all </em>without having a brain of sufficient size and sufficient design.&nbsp; Humans are not maximal in this sense, and I don't think there should be any maximum&mdash;but that's a rather deep topic, which I shall not explore further in this blog post.&nbsp; Note that Greg Egan seems to explicitly believe the reverse&mdash;that humans can understand <em>anything understandable</em>&mdash;which explains a lot.)</p>\n<p>One suspects that in a better-designed existence, the eudaimonic rate of intelligence increase would be bounded below by the need to <em>integrate</em> the loot of your adventures&mdash;to incorporate new knowledge and new skills <em>efficiently</em>, without swamping your mind in a sea of disconnected memories and associations&mdash;to manipulate larger, more powerful concepts that generalize more of your accumulated life-knowledge at once.</p>\n<p>And one also suspects that part of the poignancy of transhuman existence will be having to <em>move on</em> from your current level&mdash;get smarter, leaving old challenges behind&mdash;before you've explored more than an infinitesimal fraction of the Fun Space for a mind of your level.&nbsp; If, like me, you play through computer games trying to slay every single monster so you can collect every single experience point, this is as much tragedy as an improved existence could possibly need.</p>\n<p>Fun Space can increase much more slowly than the space of representable problems, and still overwhelmingly swamp the amount of time you could bear to spend as a mind of a fixed level.&nbsp; Even if Fun Space grows at some ridiculously tiny rate like N-squared&mdash;bearing in mind that the actual raw space of representable problems goes as 2<sup>N</sup>&mdash;we're still talking about \"way more fun than you can handle\".</p>\n<p>If you consider the loot of every human adventure&mdash;everything that was ever learned about science, and everything that was ever learned about people, and all the original stories ever told, and all the original games ever invented, and all the plots and conspiracies that were ever launched, and all the personal relationships ever raveled, and all the ways of existing that were ever tried, and all the glorious epiphanies of wisdom that were ever minted&mdash;</p>\n<p>&mdash;and you deleted all the duplicates, keeping only one of every lesson that had the same moral&mdash;</p>\n<p>&mdash;how long would you have to stay human, to collect <em>every</em> gold coin in the dungeons of history?</p>\n<p>Would it all fit into a single human brain, without that mind completely disintegrating under the weight of unrelated associations?&nbsp; And even then, would you have come close to exhausting the space of <em>human possibility, </em>which we've surely not finished exploring?</p>\n<p>This is all sounding like suspiciously good news.&nbsp; So let's turn it around. Is there any way that Fun Space could fail to grow, and instead collapse?</p>\n<p>Suppose there's only so many deep insights you <em>can</em> have on the order of \"parity\", and that you collect them all, and then math is never again as exciting as it was in the beginning.&nbsp; And that you then exhaust the shallower insights, and the trivial insights, until finally you're left with the delightful shock of \"Gosh wowie gee willickers, the product of 845 and 109 is 92105, I didn't know that logical truth before.\"</p>\n<p>Well&mdash;obviously, if you sit around and catalogue all the deep insights <em>known </em>to you to exist, you're going to end up with a bounded list.&nbsp; And equally obviously, if you declared, \"This is all there is, and all that will ever be,\" you'd be taking an unjustified step.&nbsp; (Though I fully expect some people out there to step up and say how it seems to them that they've already started to run out of available insights that are as deep as the ones they remember from their childhood.&nbsp; And I fully expect that&mdash;compared to the sort of person who makes such a pronouncement&mdash;I <em>personally </em>will have collected more additional insights than they believe exist in the whole remaining realm of possibility.)</p>\n<p>Can we say anything more on this subject of fun insights that might exist, but that we haven't yet found?</p>\n<p>The obvious thing to do is start appealing to Godel, but Godelian arguments are dangerous tools to employ in debate.&nbsp; It does seem to me that Godelian arguments weigh in the general direction of \"inexhaustible deep insights\", but inconclusively and only by loose analogies.</p>\n<p>For example, the <a href=\"http://en.wikipedia.org/wiki/Busy_beaver\">Busy-Beaver(N)</a> problem asks for the longest running time of a Turing machine with no more than N states.&nbsp; The Busy Beaver problem is uncomputable&mdash;there is no fixed Turing machine that computes it for all N&mdash;because if you knew all the Busy Beaver numbers, you would have an infallible way of telling whether a Turing machine halts; just run it up for as long as the longest-running Turing machine of that size.</p>\n<p>The human species has managed to figure out and prove the Busy Beaver numbers up to 4, and they are:</p>\n<p>BB(1):&nbsp; 1<br />BB(2):&nbsp; 6<br />BB(3):&nbsp; 21<br />BB(4):&nbsp; 107</p>\n<p>Busy-Beaver 5 is believed to be 47,176,870.</p>\n<p>The current lower bound on Busy-Beaver(6) is ~2.5 &times; 10<sup>2879</sup>.</p>\n<p>This function <em>provably </em>grows faster than any compact specification you can imagine.&nbsp; Which would seem to argue that each new Turing machine is exhibiting a new and interesting kind of behavior.&nbsp; Given infinite time, you would even be able to <em>notice </em>this behavior.&nbsp; You won't ever know for certain that you've discovered the Busy-Beaver <em>champion </em>for any given N, after finite time; but conversely, you will <em>notice</em> the Busy Beaver champion for any N after some finite time.</p>\n<p>Yes, this is an <em>unimaginably long</em> time&mdash;one of the few occasions where the word \"unimaginable\" is literally correct.&nbsp; We can't <em>actually</em> do this unless reality works the way it does in Greg Egan novels.&nbsp; But the point is that in the limit of infinite time we can point to <em>something </em>sorta like \"an infinite sequence of <em>learnable </em>deep insights not reducible to any of their predecessors or to any learnable abstract summary\".&nbsp; It's not conclusive, but it's at least <em>suggestive.</em></p>\n<p>Now you could still look at that and say, \"I don't think my life would be an adventure of neverending excitement if I spent until the end of time trying to figure out the weird behaviors of slightly larger Tuing machines.\"</p>\n<p>Well&mdash;as I said before, Peer is doing more than <em>one </em>thing wrong.&nbsp; Here I've dealt with only one sort of dimension of Fun Space&mdash;the dimension of how much <em>novelty</em> we can expect to find available to introduce into our fun.</p>\n<p>But even on the arguments given so far... I don't call it conclusive, but it seems like sufficient reason to <em>hope and expect</em> that our descendants and future selves won't exhaust Fun Space to the point that there is literally nothing left to do but carve the 162,329th table leg.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/xk/continuous_improvement/\">Continuous Improvement</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/ww/high_challenge/\">High Challenge</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sSNtcEQsqHgN8ZmRF": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aEdqh3KPerBNYvoWe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 40, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "1185", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K4aGvLnHvYgX9pZHS", "QfpHRAMRM2HjteKFK", "29vqqmGNxNRGzffEj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-21T00:56:31.000Z", "modifiedAt": null, "url": null, "title": "Sensual Experience", "slug": "sensual-experience", "viewCount": null, "lastCommentedAt": "2018-12-31T18:30:55.028Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eLHCWi8sotQT6CmTX/sensual-experience", "pageUrlRelative": "/posts/eLHCWi8sotQT6CmTX/sensual-experience", "linkUrl": "https://www.lesswrong.com/posts/eLHCWi8sotQT6CmTX/sensual-experience", "postedAtFormatted": "Sunday, December 21st 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sensual%20Experience&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASensual%20Experience%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeLHCWi8sotQT6CmTX%2Fsensual-experience%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sensual%20Experience%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeLHCWi8sotQT6CmTX%2Fsensual-experience", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeLHCWi8sotQT6CmTX%2Fsensual-experience", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1319, "htmlBody": "<p><em></em></p>\n<p>Modern day gamemakers are constantly working on higher-resolution, more realistic graphics; more immersive sounds&mdash;but they're a long <em>long </em>way off real life.</p>\n<p>Pressing the \"W\" key to run forward as a graphic of a hungry tiger bounds behind you, just doesn't seem quite as <em>sensual </em>as running frantically across the savanna with your own legs, breathing in huge gasps and pumping your arms as the sun beats down on your shoulders, the grass brushes your shins, and the air whips around you with the wind of your passage.</p>\n<p>Don't mistake me for a luddite; I'm not saying the technology <em>can't</em> get that good.&nbsp; I'm saying it hasn't gotten that good <em>yet.</em></p>\n<p>Failing to escape the computer tiger would also have fewer <em>long-term consequences</em> than failing to escape a biological tiger&mdash;it would be less a part of the total story of your life&mdash;meaning you're also likely to be less <em>emotionally</em> involved.&nbsp; But that's a topic for another post.&nbsp; Today's post is just about the sensual quality of the experience.</p>\n<p>Sensual experience isn't a question of some mysterious quality that only the \"real world\" possesses.&nbsp; A computer screen is as <em>real</em> as a tiger, after all.&nbsp; Whatever <em>is,</em> is real.</p>\n<p>But the pattern of the pseudo-tiger, inside the computer chip, is nowhere near as complex as a biological tiger; it offers far fewer modes in which to interact.&nbsp; And the sensory bandwidth between you and the computer's pseudo-world is relatively low; and the information passing along it isn't in quite the right format.</p>\n<p>It's not a question of computer tigers being \"virtual\" or \"simulated\", and therefore somehow a separate magisterium. But with present technology, and the way your brain is presently set up, you'd have <em>a lot more neurons</em> involved in running away from a biological tiger.<a id=\"more\"></a></p>\n<p>Running would fill your whole vision with motion, not just a flat rectangular screen&mdash;which translates into more square centimeters of visual cortex getting actively engaged.</p>\n<p>The graphics on a computer monitor try to trigger your sense of spatial motion (residing in the parietal cortex, btw).&nbsp; But they're presenting the information differently from its native <em>format </em>&mdash;without binocular vision, for example, and without your vestibular senses indicating true motion.&nbsp; So the sense of <em>motion</em> isn't likely to be quite the same, what it would be if you were running.</p>\n<p>And there's the sense of touch that indicates the wind on your skin; and the proprioceptive sensors that respond to the position of your limbs; and the nerves that record the strain on your muscles.&nbsp; There's a whole strip of sensorimotor cortex running along the top of your brain, that would be much more intensely involved in \"real\" running.</p>\n<p>It's a very old observation, that <em>Homo sapiens</em> was made to hunt and gather on the savanna, rather than work in an office.&nbsp; Civilization and its discontents...&nbsp; But alienation needs a causal mechanism; it doesn't just happen by magic.&nbsp; Physics is physics, so it's not that one environment is less real than another.&nbsp; But our <em>brains </em>are more adapted to <em>interfacing </em>with jungles than computer code.</p>\n<p>Writing a complicated computer program carries its own triumphs and failures, heights of exultation and pits of despair.&nbsp; But is it the same sort of <em>sensual </em> experience as, say, riding a motorcycle?&nbsp; I've never actually ridden a motorcycle, but I expect not.</p>\n<p>I've experienced the <em>exhilaration</em> of getting a program right on the dozenth try after finally spotting the problem.&nbsp; I doubt a random moment of a motorcycle ride actually feels <em>better</em> than that.&nbsp; But still, my hunter-gatherer ancestors never wrote computer programs.&nbsp; And so my mind's grasp on code is maintained using more rarefied, more abstract, more general capabilities&mdash;which means less sensual involvement.</p>\n<p>Doesn't computer programming <em>deserve</em> to be as much of a sensual experience as motorcycle riding?&nbsp; Some time ago, a relative once asked me if I thought that computer programming could use all my talents; I at once replied, \"There is <em>no limit</em> to the talent you can use in computer programming.\"&nbsp; It's as close as human beings have ever come to playing with the raw stuff of creation&mdash;but our grasp on it is too distant from the jungle.&nbsp; All our involvement is through letters on a computer screen.&nbsp; I win, and I'm happy, but there's no wind on my face.</p>\n<p>If only my ancestors back to the level of my last common ancestor with a mouse, had constantly faced the challenge of writing computer programs!&nbsp; Then I would have brain areas suited to the task, and programming computers would be more of a sensual experience...</p>\n<p>Perhaps it's not too late to fix the mistake?</p>\n<p>If there were something around that was smart enough to rewrite human brains without breaking them&mdash;not a trivial amount of smartness&mdash;then it would be possible to expand the range of things that are <em>sensually</em> fun.</p>\n<p>Not just <a href=\"/lw/wx/complex_novelty/\">novel challenges</a>, but novel high-bandwidth senses and corresponding new brain areas.&nbsp; Widening the sensorium to include new vivid, detailed experiences.&nbsp; And not neglecting the other half of the equation, high-bandwidth motor connections&mdash;new motor brain areas, to control with subtlety our new limbs (the parts of the process that we control as our direct handles on it).</p>\n<p>There's a story&mdash;old now, but I remember how exciting it was when the news first came out&mdash;about a brain-computer interface for a \"locked-in\" patient (who could previously only move his eyes), connecting control of a computer cursor directly to neurons in his visual cortex.&nbsp; It took some training at first for him to use the cursor&mdash;he started out by trying to move his paralyzed arm, which was the part of the motor cortex they were interfacing, and watched as the cursor jerked around on the screen.&nbsp; But after a while, they asked the patient, \"What does it feel like?\" and the patient replied, \"It doesn't feel like anything.\"&nbsp; He just controlled the cursor the same sort of way he would have controlled a finger, except that it wasn't a finger, it was a cursor.</p>\n<p>Like most brain modifications, adding new senses is not something to be done lightly.&nbsp; Sensual experience too <em>easily</em> renders a task involving.</p>\n<p>Consider taste buds.&nbsp; Recognizing the taste of the same food on different occasions was very important to our ancestors&mdash;it was how they <em>learned </em>what to eat, that extracted regularity.&nbsp; And our ancestors also got helpful reinforcement from their taste buds about what to eat&mdash;reinforcement which is now worse than useless, because of <a href=\"/lw/h3/superstimuli_and_the_collapse_of_western/\">the marketing incentive to reverse-engineer tastiness using artificial substances</a>.&nbsp; By now, it's probably true that at least some people have eaten 162,329 potato chips in their lifetimes.&nbsp; That's even less novelty and challenge than <a href=\"/lw/wx/complex_novelty/\">carving 162,329 table legs</a>.</p>\n<p>I'm not saying we should try to eliminate our senses of taste.&nbsp; There's a lot to be said for grandfathering in the senses we started with&mdash;it preserves our existing life memories, for example.&nbsp; Once you realize how easy it would be for a mind to collapse into a pleasure center, you start to respect the \"complications\" of your goal system a lot more, and be more wary around \"simplifications\".</p>\n<p>But I do want to nudge people into adopting something of a questioning attitude toward the senses we have now, rather than assuming that the existing senses are The Way Things Have Been And Will Always Be.&nbsp; A sex organ bears thousands of densely packed nerves for signal strength, but that signal&mdash;however strong&mdash;isn't as <em>complicated</em> as the sensations sent out by taste buds.&nbsp; Is that really appropriate for one of the most interesting parts of human existence?&nbsp; That even a novice chef can create a wider variety of taste sensations for your tongue, than&mdash;well, I'd better stop there.&nbsp; But from a fun-theoretic standpoint, the existing setup is wildly unbalanced in a lot of ways.&nbsp; It wasn't <em>designed</em> for the sake of eudaimonia.</p>\n<p>I conclude with the following cautionary quote from an old IRC conversation, as a reminder that maybe not <em>everything </em>should be a sensual experience:</p>\n<p style=\"margin-left: 40px;\">&lt;MRAmes&gt; I want a sensory modality for regular expressions.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/wz/living_by_your_own_strength/\">Living By Your Own Strength</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/xk/continuous_improvement/\">Continuous Improvement</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sSNtcEQsqHgN8ZmRF": 1, "rfZ6DY88ApBDXFpyW": 1, "v8MTECTHWs3crpnnZ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eLHCWi8sotQT6CmTX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 21, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "1186", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 86, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aEdqh3KPerBNYvoWe", "Jq73GozjsuhdwMLEG", "K4aGvLnHvYgX9pZHS", "dKGfNvjGjq4rqffyF", "QfpHRAMRM2HjteKFK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-22T00:37:52.000Z", "modifiedAt": null, "url": null, "title": "Living By Your Own Strength", "slug": "living-by-your-own-strength", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:08.474Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dKGfNvjGjq4rqffyF/living-by-your-own-strength", "pageUrlRelative": "/posts/dKGfNvjGjq4rqffyF/living-by-your-own-strength", "linkUrl": "https://www.lesswrong.com/posts/dKGfNvjGjq4rqffyF/living-by-your-own-strength", "postedAtFormatted": "Monday, December 22nd 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Living%20By%20Your%20Own%20Strength&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALiving%20By%20Your%20Own%20Strength%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdKGfNvjGjq4rqffyF%2Fliving-by-your-own-strength%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Living%20By%20Your%20Own%20Strength%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdKGfNvjGjq4rqffyF%2Fliving-by-your-own-strength", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdKGfNvjGjq4rqffyF%2Fliving-by-your-own-strength", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1397, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/la/truly_part_of_you/\">Truly Part of You</a></p>\n<p style=\"margin-left: 40px;\"><em>\"Myself, and Morisato-san... we want to live together by our own strength.\"</em></p>\n<p>Jared Diamond once called agriculture \"<a href=\"http://www.environnement.ens.fr/perso/claessen/agriculture/mistake_jared_diamond.pdf\">the worst mistake in the history of the human race</a>\".&nbsp; Farmers could grow more wheat than hunter-gatherers could collect nuts, but the evidence seems pretty conclusive that agriculture traded quality of life for quantity of life.&nbsp; One study showed that the farmers in an area were six inches shorter and seven years shorter-lived than their hunter-gatherer predecessors&mdash;even though the farmers were more numerous.</p>\n<p>I don't know if I'd call agriculture a <em>mistake</em>.&nbsp; But one should at least be aware of the downsides.&nbsp; <a href=\"/lw/gz/policy_debates_should_not_appear_onesided/\">Policy debates should not appear one-sided</a>.</p>\n<p>In the same spirit&mdash;</p>\n<p>Once upon a time, our hunter-gatherer ancestors strung their own bows, wove their own baskets, whittled their own flutes.</p>\n<p>And part of our alienation from that <a href=\"http://en.wikipedia.org/w/index.php?title=Environment_of_evolutionary_adaptedness\">environment of evolutionary adaptedness</a>, is the number of tools we use that we don't understand and couldn't make for ourselves.</p>\n<p><a id=\"more\"></a></p>\n<p>You can look back on <em>Overcoming Bias</em>, and see that I've always been suspicious of borrowed strength.&nbsp; (Even before I understood the source of Robin's and my disagreement about the Singularity, that is.)&nbsp; In <a href=\"/lw/iq/guessing_the_teachers_password/\">Guessing the Teacher's Password</a> I talked about the (well-known) problem in which schools end up teaching verbal behavior rather than real knowledge.&nbsp; In <a href=\"/lw/la/truly_part_of_you/\">Truly Part of You</a> I suggested one test for false knowledge:&nbsp; Imagine deleting a fact from your mind, and ask if it would grow back.</p>\n<p>I know many ways to prove the Pythagorean Theorem, including at least one proof that is purely visual and can be seen at a glance.&nbsp; But if you deleted the Pythagorean Theorem from my mind <em>entirely,</em> would I have enough math skills left to grow it back the next time I needed it?&nbsp; I hope so&mdash;certainly I've solved math problems that <em>seem</em> tougher than that, what with benefit of hindsight and all.&nbsp; But, as I'm not an AI, I can't actually switch off the memories and associations, and test myself in that way.</p>\n<p>Wielding someone else's strength to do things beyond your own understanding&mdash;that really <em>is</em> as dangerous as the Deeply Wise phrasing makes it sound.</p>\n<p>I observed in <a href=\"/lw/iz/failing_to_learn_from_history/\">Failing to Learn from History</a> (musing on my <a href=\"/lw/u2/the_sheer_folly_of_callow_youth/\">childhood foolishness</a> in offering a <a href=\"/lw/iu/mysterious_answers_to_mysterious_questions/\">mysterious answer to a mysterious question</a>):&nbsp; \"If only I had <em>personally</em> postulated astrological mysteries and then discovered Newtonian mechanics, postulated alchemical mysteries and then discovered chemistry, postulated vitalistic mysteries and then discovered biology.&nbsp; I would have thought of my Mysterious Answer and said to myself:&nbsp; <em>No way am I falling for that again.<em>\"</em></em></p>\n<p>At that point in my childhood, I'd been <em>handed</em> some techniques of rationality but I didn't exactly <em>own</em> them.&nbsp; Borrowing someone else's knowledge really doesn't give you anything <em>remotely </em>like the same power level required to discover that knowledge for yourself.</p>\n<p>Would Isaac Newton have remained a mystic, even in that earlier era, if he'd <em>lived </em>the lives of Galileo and Archimedes instead of just reading about them?&nbsp; If he'd <em>personally </em>seen the planets reduced from gods to spheres in a telescope?&nbsp; If he'd <em>personally </em>fought that whole war against ignorance and mystery that had to be fought, before Isaac Newton could be handed math and science as a start to his further work?</p>\n<p>We stand on the shoulders of giants, and in doing so, the power that we wield is far out of proportion to the power that we could generate for ourselves.&nbsp; This is true even of our revolutionaries.&nbsp; And yes, we couldn't begin to support this world if people could only use their own strength.&nbsp; Even so, we are losing something.</p>\n<p>That thought occurred to me, reading about the Manhattan Project, and the petition that the physicists signed to avoid dropping the atomic bomb on Japan.&nbsp; It was too late, of course; they'd already built the bombs and handed them to the military, and they couldn't take back that gift.&nbsp; And so nuclear weapons passed into the hands of politicians, who could never have created such a thing through their own strength...</p>\n<p>Not that I'm saying the world would necessarily have been a better place, if physicists had possessed sole custody of ICBMs.&nbsp; What does a physicist know about international diplomacy, or war?&nbsp; And it's not as if Leo Szilard&mdash;who first thought of the fission chain reaction&mdash;had personally invented science; he too was using powers beyond his own strength.&nbsp; And it's not as if the physicists on the Manhattan Project raised the money to pay for their salaries and their materials; they were borrowing the strength of the politicians...</p>\n<p>But if no one had been able to use nuclear weapons without, say, possessing the discipline of a scientist <em>and</em> the discipline of a politician&mdash;without personally knowing enough to construct an atomic bomb <em>and</em> make friends&mdash;the world might have been a slightly safer place.</p>\n<p>And if nobody had been able to construct an atomic bomb without first discovering for themselves the nature and existence of physics, then we would have been <em>much </em>safer from atomic bombs, because no one would have been able to build them until they were two hundred years old.</p>\n<p>With humans leaving the game after just seventy years, we couldn't support this world using only our own strengths.&nbsp; But we have traded quality of insight for quantity of insight.</p>\n<p>It does sometimes seem to me that many of this world's problems, stem from our using powers that aren't appropriate to seventy-year-olds.</p>\n<p>And there is a higher level of strength-ownership, which no human being has yet achieved.&nbsp; Even when we run, we're just using the muscles that evolution built for us.&nbsp; Even when we think, we're just using the brains that evolution built for us.</p>\n<p>I'm not suggesting that people should <a href=\"/lw/rc/the_ultimate_source/\">create themselves from scratch without a starting point</a>.&nbsp; Just pointing out that it would be a different world if we understood our own brains and could redesign our own legs.&nbsp; As <em>yet </em>there's no human \"rationalist\" or \"scientist\", whatever they know about \"how to think\", who could actually <em>build a rational AI</em>&mdash;which shows you the limits of our self-understanding.</p>\n<p>This is not the sort of thing that I'd suggest as an immediate alteration.&nbsp; I'm not suggesting that people should <em>instantly</em> on a silver platter be given full knowledge of how their own brains work and the ability to redesign their own legs.&nbsp; Because maybe people will be better off if they aren't <em>given </em>that kind of power, but rather have to work out the answer for <em>themselves.</em></p>\n<p>Just in terms of anomie versus fun, there's a big difference between being able to do things for yourself, and having to rely on other people to do them for you.&nbsp; (Even if you're doing them with a brain you never designed yourself.)</p>\n<p>I don't know if it's a principle that would stay until the end of time, to the children's children.&nbsp; Maybe better-designed minds could handle opaque tools without the anomie.</p>\n<p>But it is part of the commonly retold prophecy of Artificial Intelligence and the Age of the Machine, that this era must be accompanied by <em>greater</em> reliance on things outside yourself, <em>more</em> incomprehensible tools into which you have <em>less </em>insight and <em>less</em> part in their creation.</p>\n<p>Such a prophecy is not surprising.&nbsp; That <em>is </em>the way the trend has gone so far, in our culture that is too busy staying alive to optimize for fun.&nbsp; From the fire-starting tools that you built yourself, to the village candleseller, and then from the candleseller to the electric light that runs on strange mathematical principles and is powered by a distant generator...&nbsp; we are surrounded by things outside ourselves and strengths outside our understanding; we need them to stay alive, or we buy them because it's easier that way.</p>\n<p>But with a sufficient surplus of power, you could start doing things the eudaimonic way.&nbsp; Start rethinking the life experience as a road to internalizing new strengths, instead of just trying to keep people alive efficiently.</p>\n<p>A Friendly AI doesn't have to be a continuation of existing trends.&nbsp; It's not the Machine.&nbsp; It's not the alien force of technology.&nbsp; It's not mechanizing a factory.&nbsp; It's not a new gadget for sale.&nbsp; That's not where the shape comes from. What it <em>is</em>&mdash;is not easy to explain; but I'm reminded of doc Smith's description of the Lens as \"the physical manifestation of a purely philosophical concept\".&nbsp; That philosophical concept doesn't have to manifest as new buttons to press&mdash;<a href=\"http://intelligence.org/upload/CEV.html\">if, on reflection, that's not what we would want</a>.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/xb/free_to_optimize/\">Free to Optimize</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/wy/sensual_experience/\">Sensual Experience</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bY5MaF2EATwDkomvu": 1, "sSNtcEQsqHgN8ZmRF": 1, "fH8jPjHF2R27sRTTG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dKGfNvjGjq4rqffyF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 37, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "1187", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fg9fXrHpeaDD6pEPL", "PeSzc9JTBxhaYRp9b", "NMoLJuDJEms7Ku9XS", "97Y7Jwrzxyfzz3Ad2", "Yicjw6wSSaPdb83w9", "6i3zToomS86oj9bS6", "EsMhFZuycZorZNRF5", "K4aGvLnHvYgX9pZHS", "EZ8GniEPSechjDYP9", "eLHCWi8sotQT6CmTX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-22T22:05:43.000Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes 20", "slug": "rationality-quotes-20", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:35.250Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EA5JmHSHa2E2RJKSY/rationality-quotes-20", "pageUrlRelative": "/posts/EA5JmHSHa2E2RJKSY/rationality-quotes-20", "linkUrl": "https://www.lesswrong.com/posts/EA5JmHSHa2E2RJKSY/rationality-quotes-20", "postedAtFormatted": "Monday, December 22nd 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%2020&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%2020%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEA5JmHSHa2E2RJKSY%2Frationality-quotes-20%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%2020%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEA5JmHSHa2E2RJKSY%2Frationality-quotes-20", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEA5JmHSHa2E2RJKSY%2Frationality-quotes-20", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 307, "htmlBody": "<p>\"For every stock you buy, there is someone selling you that stock.&nbsp; What is it that you know that they don't?&nbsp; What is it that they know, that you don't?&nbsp; Who has the edge?&nbsp; If it's not you, chances are you are going to lose money on the deal.\"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- <a href=\"http://www.blogmaverick.com/2008/09/08/talking-stocks-and-money/\">Mark Cuban </a></p>\n<p>\"If you have two choices, choose the harder.&nbsp; If you're trying to decide whether to go out running or sit home and watch TV, go running.&nbsp; Probably the reason this trick works so well is that when you have two choices and one is harder, the only reason you're even considering the other is laziness.&nbsp; You know in the back of your mind what's the right thing to do, and this trick merely forces you to acknowledge it.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <a href=\"http://www.paulgraham.com/wealth.html\">Paul Graham </a></p>\n<p>\"Never attribute to malice that which can be adequately explained by stupidity.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Hanlon's Razor</p>\n<p>\"I divide my officers into four classes; the clever, the lazy, the industrious, and the stupid.&nbsp; Each officer possesses at least two of these qualities.&nbsp; Those who are clever and industrious are fitted for the highest staff appointments.&nbsp; Use can be made of those who are stupid and lazy.&nbsp; The man who is clever and lazy however is for the very highest command; he has the temperament and nerves to deal with all situations.&nbsp; But whoever is stupid and industrious is a menace and must be removed immediately!\"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- <a href=\"http://en.wikipedia.org/wiki/Kurt_von_Hammerstein-Equord\">General Kurt von Hammerstein-Equord </a></p>\n<p>\"There's no such thing as a human who doesn't commit sin.&nbsp; It's not like the world is divided into sinners and the innocent.&nbsp; There are only people who can and who cannot atone for their sins.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Ciel</p>\n<p>\"Simple stupidity is never enough.&nbsp; People need to pile stupidity on stupidity on stupidity.\"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- <a href=\"http://scienceblogs.com/goodmath/2008/09/economic_disasters_and_stupid.php\">Mark C. Chu-Carroll</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EA5JmHSHa2E2RJKSY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 4.6480996957088955e-07, "legacy": true, "legacyId": "1188", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-23T17:35:40.000Z", "modifiedAt": null, "url": null, "title": "Imaginary Positions", "slug": "imaginary-positions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:02.204Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jq5WAQEboeufkxzsg/imaginary-positions", "pageUrlRelative": "/posts/jq5WAQEboeufkxzsg/imaginary-positions", "linkUrl": "https://www.lesswrong.com/posts/jq5WAQEboeufkxzsg/imaginary-positions", "postedAtFormatted": "Tuesday, December 23rd 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Imaginary%20Positions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AImaginary%20Positions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjq5WAQEboeufkxzsg%2Fimaginary-positions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Imaginary%20Positions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjq5WAQEboeufkxzsg%2Fimaginary-positions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjq5WAQEboeufkxzsg%2Fimaginary-positions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 935, "htmlBody": "<p>Every now and then, one reads an article about the Singularity in\nwhich some reporter confidently asserts, &quot;The Singularitarians,\nfollowers of Ray Kurzweil, believe that they will be uploaded into\ntechno-heaven while the unbelievers languish behind or are extinguished\nby the machines.&quot;</p><p>I don&#39;t think I&#39;ve ever met a <em>single</em> Singularity fan, <a href=\"http://yudkowsky.net/singularity/schools\">Kurzweilian or otherwise</a>, who\nthinks that only believers in the Singularity will go to upload heaven\nand everyone else will be left to rot.&#0160; Not <em>one</em>.&#0160;\n(There&#39;s a very few pseudo-Randian types who believe that only the truly selfish who accumulate lots of money will make it, but they expect e.g. me to be damned with\nthe rest.)</p><p>But if you start out thinking that the Singularity is a loony religious meme, then it seems like Singularity believers <em>ought</em> to believe that they alone will be saved.&#0160; It seems like a detail that would fit the story.</p><p>This fittingness is so strong as to manufacture the conclusion without any particular observations.&#0160; And then the conclusion <span style=\"font-style: italic;\"></span><em>isn&#39;t marked as a deduction</em>.&#0160; The reporter just thinks that they investigated the Singularity, and found some loony cultists who believe they alone will be saved.</p><p>Or so I deduce.&#0160; I haven&#39;t actually <em>observed </em>the inside of their minds, after all.</p><p>Has <em>any </em>rationalist <em>ever </em>advocated behaving as if all\npeople are reasonable and fair?&#0160; I&#39;ve repeatedly heard people say,\n&quot;Well, it&#39;s not always smart to be rational, because other people\naren&#39;t always reasonable.&quot;&#0160; What rationalist said they were?&#0160; I would deduce:&#0160; This is something that non-rationalists believe it would &quot;fit&quot; for us to believe, <em>given our general blind faith in Reason</em>.&#0160; And\nso their minds just add it to the knowledge pool, as though it were an observation.&#0160; (In this case I encountered yet another example recently enough to find the reference; see <a href=\"http://ben.casnocha.com/2008/12/easier-to-deny-or-rationalize-behavior-than-evolve-your-own-identity.html#comment-6a00d8341c85c753ef0105366832a0970c\">here</a>.)</p><a id=\"more\"></a>\n<p>(<a href=\"http://www.overcomingbias.com/2008/06/against-disclai.html\">Disclaimer</a>:&#0160; Many things\nhave been said, at one time or another, by one person or another, over\ncenturies of recorded history; and the topic of &quot;rationality&quot; is popularly enough discussed that some self-identified &quot;rationalist&quot; may have described &quot;rationality&quot; that way at one point or another.&#0160; But I have yet to hear a rationalist say it, myself.)</p><p>\n</p><p>I once read an article on Extropians (a certain flavor of transhumanist) which asserted\nthat the Extropians were a reclusive enclave of techno-millionaires\n(yeah, don&#39;t we wish).&#0160; Where did this detail come from?&#0160; Definitely not from observation.&#0160; And considering the sheer divergence from reality, I doubt it\nwas ever planned as a deliberate lie.&#0160; It&#39;s not just easily falsified,\nbut a mark of <em>embarassment </em>to give others too much credit that way\n(&quot;Ha!&#0160; You believed they were millionaires?&quot;)&#0160; One suspects, rather,\nthat the proposition seemed to fit, and so it was added -\nwithout any warning label saying &quot;I deduced this from my other beliefs,\nbut have no direct observations to support it.&quot;\n</p><p>There&#39;s also a general problem with reporters which is that they\ndon&#39;t write what happened, they write the Nearest Cliche to what\nhappened - which is very little information for backward inference, especially if\nthere are few cliches to be selected from.&#0160; The distance from actual Extropians to the Nearest Cliche &quot;reclusive enclave of techno-millionaires&quot; is kinda large.&#0160; This may get a separate post at some point.</p><p><a href=\"/lw/wp/what_i_think_if_not_why/\">My actual</a> nightmare scenario for the future involves well-intentioned AI researchers who try to make\na nice AI but don&#39;t do enough math.&#0160; (If you&#39;re not an expert you can&#39;t\ntrack the technical issues yourself, but you can often also tell at a\nglance that they&#39;ve put very little thinking into &quot;nice&quot;.)&#0160; The\nAI ends up wanting to tile the galaxy with tiny smiley-faces, or\nreward-counters; the AI doesn&#39;t bear the slightest hate for humans, but\nwe are made of atoms it can use for something else.&#0160; The most probable-seeming result is\nnot Hell On Earth but Null On Earth, a galaxy tiled with paperclips or something\nequally morally inert.</p><p>The imaginary position that gets invented because it seems to &quot;fit&quot; - that is, fit the folly that the other <em>believes</em> is generating the position - is &quot;The Singularity is a dramatic final conflict\nbetween Good AI and Evil AI, where Good AIs are made by\nwell-intentioned people and Evil AIs are made by ill-intentioned\npeople.&quot;</p><p>In many such cases, no matter how much you <em>tell</em> people what you really believe, they don&#39;t update!&#0160; I&#39;m not even sure this is a matter of any deliberately justifying decision on their part - like an explicit counter that you&#39;re concealing your real beliefs.&#0160; To me the process seems more like:&#0160; They stare at you for a moment, think &quot;That&#39;s not what this person <em>ought </em>to believe!&quot;, and then blink away the dissonant evidence and continue as before.&#0160; If your real beliefs are <em>less convenient</em> for them, the same phenomenon occurs: words from the lips will be discarded.</p><p>There&#39;s an obvious relevance to prediction markets - that if there&#39;s an outstanding dispute, and the market-makers don&#39;t consult both sides on the wording of the payout conditions, it&#39;s possible that one side won&#39;t take the bet because &quot;<em>That&#39;s</em> not what we assert!&quot;&#0160; In which case it would be highly inappropriate to crow &quot;Look at those market prices!&quot; or &quot;So you don&#39;t really believe; you won&#39;t take the bet!&quot;&#0160; But I would guess that this issue has already been discussed by prediction market advocates.&#0160; (And that standard procedures have already been proposed for resolving it?)<br />\n\n</p><p>I&#39;m wondering if there are similar Imaginary Positions in, oh, say,\neconomics - if there are things that few or no economists believe, but\nwhich people (or journalists) think economists believe because it seems\nto them like &quot;the sort of thing that economists would believe&quot;.&#0160; Open general question.</p><p></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dJ6eJxJrCEget7Wb6": 1, "pszEEb3ctztv3rozd": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jq5WAQEboeufkxzsg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 29, "extendedScore": null, "score": 4.6e-05, "legacy": true, "legacyId": "1189", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["z3kYdw54htktqt9Jb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-25T02:26:22.000Z", "modifiedAt": null, "url": null, "title": "Harmful Options", "slug": "harmful-options", "viewCount": null, "lastCommentedAt": "2022-03-19T12:52:06.607Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CtSS6SkHhLBvdodTY/harmful-options", "pageUrlRelative": "/posts/CtSS6SkHhLBvdodTY/harmful-options", "linkUrl": "https://www.lesswrong.com/posts/CtSS6SkHhLBvdodTY/harmful-options", "postedAtFormatted": "Thursday, December 25th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harmful%20Options&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarmful%20Options%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCtSS6SkHhLBvdodTY%2Fharmful-options%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harmful%20Options%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCtSS6SkHhLBvdodTY%2Fharmful-options", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCtSS6SkHhLBvdodTY%2Fharmful-options", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1032, "htmlBody": "<p><strong>Previously in series</strong>:&nbsp; <a href=\"/lw/wz/living_by_your_own_strength/\">Living By Your Own Strength</a></p>\n<p>Barry Schwartz's <a href=\"http://www.swarthmore.edu/SocSci/bschwar1/Sci.Amer.pdf\">The Paradox of Choice</a>&mdash;which I haven't read, though I've read some of the research behind it&mdash;talks about how offering people <em>more choices</em> can make them <em>less happy</em>.</p>\n<p>A simple intuition says this shouldn't ought to happen to rational agents:&nbsp; If your current choice is X, and you're offered an alternative Y that's worse than X, and you know it, you can always just go on doing X.&nbsp; So a rational agent shouldn't do worse by having more options.&nbsp; The more available actions you have, the more powerful you become&mdash;that's how it should ought to work.</p>\n<p>For example, if an ideal rational agent is initially <em>forced</em> to <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">take only box B in Newcomb's Problem</a>, and is then offered the <em>additional</em> choice of taking both boxes A and B, the rational agent shouldn't <em>regret having more options</em>.&nbsp; Such regret indicates that you're \"fighting your own ritual of cognition\" which helplessly selects the worse choice once it's offered you.</p>\n<p>But this intuition only governs <em>extremely</em> idealized rationalists, or rationalists in extremely idealized situations.&nbsp; Bounded rationalists can easily do worse with strictly more options, because they burn computing operations to evaluate them.&nbsp; You could write an invincible chess program in one line of Python if its only legal move were the winning one.</p>\n<p>Of course Schwartz and co. are not talking about anything so pure and innocent as the <em>computing cost</em> of having more choices.</p>\n<p>If you're dealing, not with an ideal rationalist, not with a bounded rationalist, but with a <em>human being&mdash;</em></p>\n<p>Say, would you like to finish reading this post, or <a href=\"http://www.youtube.com/watch?v=oHg5SJYRHA0\">watch this surprising video</a> instead?</p>\n<p><a id=\"more\"></a></p>\n<p>Schwartz, I believe, talks primarily about the decrease in <em>happiness</em> and <em>satisfaction</em> that results from having more mutually exclusive options.&nbsp; Before this research was done, it was already known that people are more sensitive to losses than to gains, generally by a factor of between 2 and 2.5 (in various different experimental scenarios).&nbsp; That is, the pain of losing something is between 2 and 2.5 times as worse as the joy of gaining it.&nbsp; (This is an interesting constant in its own right, and may have something to do with compensating for our systematic overconfidence.)</p>\n<p>So&mdash;if you can only choose one dessert, you're likely to be happier choosing from a menu of two than a menu of fourteen.&nbsp; In the first case, you eat one dessert and pass up one dessert; in the latter case, you eat one dessert and pass up thirteen desserts.&nbsp; And we are more sensitive to loss than to gain.</p>\n<p>(If I order dessert on a menu at all, I will order quickly and then close the menu and put it away, so as not to look at the other items.)</p>\n<p>Not only that, but if the options have incommensurable attributes, then whatever option we select is likely to <em>look worse</em> because of the comparison.&nbsp; A luxury car that would have looked great by comparison to a Crown Victoria, instead becomes slower than the Ferrari, more expensive than the 9-5, with worse mileage than the Prius, and not looking quite as good as the Mustang.&nbsp; So we lose on satisfaction with the road we <em>did</em> take.</p>\n<p>And then there are more direct forms of harm done by painful choices.&nbsp; IIRC, an experiment showed that people who <em>refused</em> to eat a cookie&mdash;who were offered the cookie, and chose <em>not</em> to take it&mdash;did worse on subsequent tests of mental performance than either those who ate the cookie or those who were not offered any cookie.&nbsp; You pay a price in mental energy for resisting temptation.</p>\n<p>Or consider the various \"trolley problems\" of ethical philosophy&mdash;a trolley is bearing down on 5 people, but there's one person who's very fat and can be pushed onto the tracks to stop the trolley, that sort of thing.&nbsp; If you're forced to choose between two unacceptable evils, you'll pay a price either way.&nbsp; Vide <em>Sophie's Choice</em>.</p>\n<p>An option need not be taken, or even be strongly considered, in order to wreak harm.&nbsp; Recall the point from \"<a href=\"/lw/ww/high_challenge/\">High Challenge</a>\", about how offering to do someone's work for them is not always helping them&mdash;how the ultimate computer game is not the one that just says \"YOU WIN\", forever.</p>\n<p>Suppose your computer games, in addition to the <em>long difficult</em> path to your level's goal, also had little side-paths that you could use&mdash;directly in the game, as corridors&mdash;that would bypass all the enemies and take you straight to the goal, offering along the way all the items and experience that you could have gotten the hard way.&nbsp; And this corridor is always visible, out of the corner of your eye.</p>\n<p><em>Even </em><em>if you resolutely refused</em> to take the easy path through the game, knowing that it would cheat you of the very experience that you paid money in order to buy&mdash;wouldn't that always-visible corridor, make the game that much less fun?&nbsp; Knowing, for every alien you shot, and every decision you made, that there was always an easier path?</p>\n<p>I don't know if this story has ever been written, but you can imagine a Devil who follows someone around, making their life miserable, <em>solely by offering them options which are never actually taken</em>&mdash;a \"deal with the Devil\" story that only requires the Devil to have the <em>capacity</em> to grant wishes, rather than ever granting a single one.</p>\n<p>And what if the worse option is actually taken?&nbsp; I'm not suggesting that it is always a good idea <a href=\"/lw/gz/policy_debates_should_not_appear_onesided/\">for human governments to go around Prohibiting temptations</a>.&nbsp; But the literature of heuristics and biases is replete with examples of reproducible stupid choices; and there is also such a thing as akrasia (weakness of will).</p>\n<p>If you're an agent operating from a <em>much </em>higher vantage point&mdash;high enough to see humans as flawed algorithms, so that it's not a matter of second-guessing but second-<em>knowing</em>&mdash;then is it <em>benevolence</em> to offer choices that will assuredly be made wrongly?&nbsp; Clearly, removing all choices from someone and reducing their life to <a href=\"http://www.progressquest.com/\">Progress Quest</a>, is not helping them.&nbsp; But are we wise enough to <em>know when we should choose?</em>&nbsp; And in some cases, even offering that much of a choice, <em>even if the choice is made correctly,</em> may already do the harm...</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/x3/devils_offers/\">Devil's Offers</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/xb/free_to_optimize/\">Free to Optimize</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sSNtcEQsqHgN8ZmRF": 1, "wfW6iL96u26mbatep": 3, "r7qAjcbfhj2256EHH": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CtSS6SkHhLBvdodTY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 42, "extendedScore": null, "score": 6.2e-05, "legacy": true, "legacyId": "1190", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dKGfNvjGjq4rqffyF", "6ddcsdA2c2XpNpE5x", "29vqqmGNxNRGzffEj", "PeSzc9JTBxhaYRp9b", "K4aGvLnHvYgX9pZHS", "MTjej6HKvPByx3dEA", "EZ8GniEPSechjDYP9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-25T17:00:00.000Z", "modifiedAt": null, "url": null, "title": "Devil's Offers", "slug": "devil-s-offers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:04.016Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MTjej6HKvPByx3dEA/devil-s-offers", "pageUrlRelative": "/posts/MTjej6HKvPByx3dEA/devil-s-offers", "linkUrl": "https://www.lesswrong.com/posts/MTjej6HKvPByx3dEA/devil-s-offers", "postedAtFormatted": "Thursday, December 25th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Devil's%20Offers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADevil's%20Offers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMTjej6HKvPByx3dEA%2Fdevil-s-offers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Devil's%20Offers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMTjej6HKvPByx3dEA%2Fdevil-s-offers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMTjej6HKvPByx3dEA%2Fdevil-s-offers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1774, "htmlBody": "<p><em></em><strong>Previously in series</strong>:&nbsp; <a href=\"/lw/x2/harmful_options/\">Harmful Options</a></p>\n<p>An iota of <a href=\"/lw/k9/the_logical_fallacy_of_generalization_from/\">fictional evidence</a> from <a href=\"http://books.google.com/books?id=CDjJ7K3bm28C&amp;pg=PA196&amp;lpg=PA196&amp;dq=%22Helion+had+leaned+and+said%22&amp;source=bl&amp;ots=QS-yd1jvWz&amp;sig=twyQUHiB9O-nPsw0BcEAbuAnkgw&amp;hl=en&amp;sa=X&amp;oi=book_result&amp;resnum=2&amp;ct=result#PPA196,M1\"><em>The Golden Age</em></a> by John C. Wright:</p>\n<p style=\"margin-left: 40px;\">&nbsp;&nbsp;&nbsp; Helion had leaned and said, \"Son, once you go in there, the full powers and total command structures of the Rhadamanth Sophotech will be at your command.&nbsp; You will be invested with godlike powers; but you will still have the passions and distempers of a merely human spirit.&nbsp; There are two temptations which will threaten you.&nbsp; First, you will be tempted to remove your human weaknesses by abrupt mental surgery.&nbsp; The Invariants do this, and to a lesser degree, so do the White Manorials, abandoning humanity to escape from pain.&nbsp; Second, you will be tempted to indulge your human weakness.&nbsp; The Cacophiles do this, and to a lesser degree, so do the Black Manorials.&nbsp; Our society will gladly feed every sin and vice and impulse you might have; and then stand by helplessly and watch as you destroy yourself; because the first law of the Golden Oecumene is that no peaceful activity is forbidden.&nbsp; Free men may freely harm themselves, provided only that it is only themselves that they harm.\"<br />&nbsp;&nbsp;&nbsp; Phaethon knew what his sire was intimating, but he did not let himself feel irritated.&nbsp; Not today.&nbsp; Today was the day of his majority, his emancipation; today, he could forgive even Helion's incessant, nagging fears.<br />&nbsp;&nbsp;&nbsp; Phaethon also knew that most Rhadamanthines were not permitted to face the Noetic tests until they were octogenerians; most did not pass on their first attempt, or even their second.&nbsp; Many folk were not trusted with the full powers of an adult until they reached their Centennial.&nbsp; Helion, despite criticism from the other Silver-Gray branches, was permitting Phaethon to face the tests five years early...</p>\n<p><a id=\"more\"></a></p>\n<p style=\"margin-left: 40px;\">&nbsp;&nbsp;&nbsp; Then Phaethon said, \"It's a paradox, Father.&nbsp; I cannot be, at the same time and in the same sense, a child and an adult.&nbsp; And, if I am an adult, I cannot be, at the same time, free to make my own successes, but not free to make my own mistakes.\"<br />&nbsp;&nbsp;&nbsp; Helion looked sardonic.&nbsp; \"'Mistake' is such a simple word.&nbsp; An adult who suffers a moment of foolishness or anger, one rash moment, has time enough to delete or destroy his own free will, memory, or judgment.&nbsp; No one is allowed to force a cure on him.&nbsp; No one can restore his sanity against his will.&nbsp; And so we all stand quietly by, with folded hands and cold eyes, and meekly watch good men annihilate themselves.&nbsp; It is somewhat... quaint... to call such a horrifying disaster a 'mistake.'\"</p>\n<p>Is this the best Future we could possibly get to&mdash;the Future where you must be absolutely stern and resistant throughout your entire life, because <em>one moment of weakness</em> is enough to betray you to <a href=\"/lw/h3/superstimuli_and_the_collapse_of_western/\">overwhelming temptation</a>?</p>\n<p>Such flawless perfection would be easy enough for a superintelligence, perhaps&mdash;for a <em>true </em>adult&mdash;but for a human, even a hundred-year-old human, it seems like a dangerous and inhospitable place to live.&nbsp; Even if you are strong enough to always choose correctly&mdash;maybe you don't want to <em>have </em>to be so strong, always at every moment.</p>\n<p>This is the great flaw in Wright's otherwise shining Utopia&mdash;that the Sophotechs are <em>helpfully </em>offering up overwhelming temptations to people who would not be at <em>quite </em>so much risk from only <em>themselves</em>.&nbsp; (Though if not for this flaw in Wright's Utopia, he would have had no story...)</p>\n<p>If I recall correctly, it was while reading <em>The Golden Age</em> that I generalized the principle \"<a href=\"/lw/wz/living_by_your_own_strength/\">Offering people powers beyond their own is not always helping them.</a>\"</p>\n<p>If you couldn't just ask a Sophotech to edit your neural networks&mdash;and you couldn't buy a standard package at the supermarket&mdash;but, rather, had to study neuroscience yourself until you could do it with your own hands&mdash;then that would act as something of a natural limiter.&nbsp; Sure, there are pleasure centers that would be relatively easy to stimulate; but we don't tell you where they are, so you have to do your own neuroscience.&nbsp; Or we don't sell you your own neurosurgery kit, so you have to build it yourself&mdash;metaphorically speaking, anyway&mdash;</p>\n<p>But you see the idea: it is not so terrible a disrespect for free will, to live in a world in which people are free to shoot their feet off <em>through their own strength</em>&mdash;in the hope that by the time they're smart enough to do it <em>under their own power</em>, they're smart enough <em>not </em>to.</p>\n<p>The more dangerous and destructive the act, the more you require people to do it without external help.&nbsp; If it's really dangerous, you don't just require them to do their own engineering, but to do their own science.&nbsp; A <a href=\"/lw/wc/singletons_rule_ok/\">singleton</a> might be justified in <a href=\"/lw/p0/to_spread_science_keep_it_secret/\">prohibiting standardized textbooks</a> in certain fields, so that people have to do their own science&mdash;make their own discoveries, learn to rule out their own stupid hypotheses, and fight their own overconfidence.&nbsp; Besides, everyone should experience <a href=\"/lw/os/joy_in_discovery/\">the joy of major discovery</a> at least once in their lifetime, and to do this properly, you may have to prevent spoilers from entering the public discourse.&nbsp; So you're getting <a href=\"/lw/p0/to_spread_science_keep_it_secret/\">three</a> social benefits at once, here.</p>\n<p>But now I'm trailing off into plots for SF novels, instead of Fun Theory per se.&nbsp; (It can be fun to muse how I would create the world if I had to order it according to my own childish wisdom, but in real life one rather prefers to <a href=\"/lw/wp/what_i_think_if_not_why/\">avoid that scenario</a>.)</p>\n<p>As a matter of Fun Theory, though, you can imagine a <em>better </em>world than the Golden Oecumene depicted above&mdash;it is not the <em>best </em>world imaginable, fun-theoretically speaking.&nbsp; We would prefer (if attainable) a world in which people own their own mistakes and their own successes, and yet they are not given loaded handguns on a silver platter, nor do they perish through <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">suicide by genie bottle</a>.</p>\n<p>Once you imagine a world in which people can shoot off their own feet <em>through their own strength,</em> are you making that world incrementally better by offering incremental help along the way?</p>\n<p>It's one matter to prohibit people from using dangerous powers that they have grown enough to acquire naturally&mdash;to literally <em>protect them from themselves.</em>&nbsp; One expects that if a mind kept getting smarter, at some eudaimonic rate of intelligence increase, then&mdash;if you took the most obvious course&mdash;the mind would eventually become able to edit its own source code, and bliss itself out <a href=\"/lw/rb/possibility_and_couldness/\">if it chose to do so</a>.&nbsp; Unless the mind's growth were steered onto a non-obvious course, or monitors were mandated to prohibit that event...&nbsp; To protect people <em>from their own powers</em> might take some twisting.</p>\n<p>To descend from above and <em>offer dangerous powers as an untimely gift</em>, is another matter entirely.&nbsp; That's why the title of this post is \"Devil's Offers\", not \"Dangerous Choices\".</p>\n<p>And to allow dangerous powers to be sold in a marketplace&mdash;or alternatively to prohibit them from being transferred from one mind to another&mdash;that is somewhere in between.</p>\n<p>John C. Wright's writing has a particular poignancy for me, for in my <a href=\"/lw/u2/the_sheer_folly_of_callow_youth/\">foolish youth</a> I thought that something very much like this scenario was a good idea&mdash;that a benevolent superintelligence ought to go around offering people lots of options, and doing as it was asked.</p>\n<p>In retrospect, this was a case of a pernicious distortion where you end up believing things that are easy to market to other people.</p>\n<p>I know someone who drives across the country on long trips, rather than flying.&nbsp; Air travel scares him.&nbsp; Statistics, naturally, show that flying a given distance is much safer than driving it.&nbsp; But some people fear too much the <em>loss of control</em> that comes from not having their own hands on the steering wheel.&nbsp; It's a common complaint.</p>\n<p>The future sounds less scary if you imagine yourself having lots of control over it.&nbsp; For every awful thing that you imagine happening to you, you can imagine, \"But I won't choose that, so it will be all right.\"</p>\n<p>And if it's not your own hands on the steering wheel, you think of scary things, and imagine, \"What if this is chosen <em>for </em>me, and I can't say no?\"</p>\n<p>But in real life rather than imagination, human choice is a fragile thing.&nbsp; If the whole field of heuristics and biases teaches us anything, it surely teaches us that.&nbsp; Nor has it been the verdict of experiment, that humans correctly estimate the flaws of their own decision mechanisms.</p>\n<p>I flinched away from that thought's implications, not so much because I feared superintelligent paternalism <em>myself</em>, but because I feared what other people would say of that position.&nbsp; If I believed it, I would have to defend it, so I managed not to believe it.&nbsp; Instead I told people not to worry, a superintelligence would surely respect their decisions (and even believed it myself).&nbsp; A very pernicious sort of self-deception.</p>\n<p>Human governments are made up of humans who are foolish like ourselves, plus they have poor incentives.&nbsp; Less skin in the game, and <a href=\"/lw/uu/why_does_power_corrupt/\">specific human brainware to be corrupted by wielding power</a>.&nbsp; So we've learned the historical lesson to be wary of ceding control to human bureaucrats and politicians.&nbsp; We may even be emotionally hardwired to resent the loss of anything we perceive as power.</p>\n<p>Which is just to say that people are biased, by instinct, by <a href=\"/lw/so/humans_in_funny_suits/\">anthropomorphism</a>, and by narrow experience, to <a href=\"/lw/uv/ends_dont_justify_means_among_humans/\"><em>under</em>estimate how much they could potentially trust a superintelligence</a> which lacks a human's corruption circuits, doesn't easily make certain kinds of mistakes, and has strong overlap between its motives and your own interests.</p>\n<p>Do you trust yourself?&nbsp; Do you trust yourself to know when to trust yourself?&nbsp; If you're dealing with a superintelligence kindly enough to care about you at all, rather than disassembling you for raw materials, are you wise to second-guess its choice of <em>who </em>it thinks should decide?&nbsp; Do you think you have a superior epistemic vantage point here, or what?</p>\n<p>Obviously we should not trust all agents who claim to be trustworthy&mdash;especially if they are <em>weak </em>enough, relative to us, to <em>need </em>our goodwill.&nbsp; But I am quite ready to accept that a benevolent superintelligence may not offer certain choices.</p>\n<p>If you <em>feel safer</em> driving than flying, because that way it's your own hands on the steering wheel, statistics be damned&mdash;</p>\n<p>&mdash;then maybe it isn't <em>helping </em>you, for a superintelligence to offer you the option of driving.</p>\n<p>Gravity doesn't ask you if you would like to float up out of the atmosphere into space and die.&nbsp; But you don't go around complaining that gravity is a tyrant, right?&nbsp; You can build a spaceship if you work hard and study hard.&nbsp; It would be a more dangerous world if your six-year-old son could do it in an hour using string and cardboard.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/x4/nonperson_predicates/\">Nonperson Predicates</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/x2/harmful_options/\">Harmful Options</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sSNtcEQsqHgN8ZmRF": 1, "GBpwq8cWvaeRoE9X5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MTjej6HKvPByx3dEA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 40, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "1191", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CtSS6SkHhLBvdodTY", "rHBdcHGLJ7KvLJQPk", "Jq73GozjsuhdwMLEG", "dKGfNvjGjq4rqffyF", "rSTpxugJxFPoRMkGW", "3diLhMELXxM8rFHJj", "KfMNFB3G7XNviHBPN", "z3kYdw54htktqt9Jb", "4ARaTpNX62uaL86j6", "3buXtNiSK8gcRLMSG", "Yicjw6wSSaPdb83w9", "v8rghtzWCziYuMdJ5", "Zkzzjg3h7hW5Z36hK", "K9ZaZXDnL3SEmYZqB", "K4aGvLnHvYgX9pZHS", "wqDRRx9RqwKLzWt7R"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-27T01:47:32.000Z", "modifiedAt": null, "url": null, "title": "Nonperson Predicates", "slug": "nonperson-predicates", "viewCount": null, "lastCommentedAt": "2017-06-17T04:37:03.748Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wqDRRx9RqwKLzWt7R/nonperson-predicates", "pageUrlRelative": "/posts/wqDRRx9RqwKLzWt7R/nonperson-predicates", "linkUrl": "https://www.lesswrong.com/posts/wqDRRx9RqwKLzWt7R/nonperson-predicates", "postedAtFormatted": "Saturday, December 27th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Nonperson%20Predicates&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANonperson%20Predicates%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwqDRRx9RqwKLzWt7R%2Fnonperson-predicates%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Nonperson%20Predicates%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwqDRRx9RqwKLzWt7R%2Fnonperson-predicates", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwqDRRx9RqwKLzWt7R%2Fnonperson-predicates", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1673, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"http://www.overcomingbias.com/2008/03/righting-a-wron.html\">Righting a Wrong Question</a>, <a href=\"http://www.overcomingbias.com/2008/04/zombies.html\">Zombies! Zombies?</a>, <a href=\"http://www.overcomingbias.com/2008/05/an-ai-new-timer.html\">A Premature Word on AI</a>, <a href=\"http://www.overcomingbias.com/2008/10/try-persevere.html\">On Doing the Impossible</a></p>\n<p>There is a subproblem of <a href=\"http://yudkowsky.net/singularity/ai-risk\">Friendly AI</a> which is so scary that I usually don't talk about it, because very few would-be AI designers&nbsp;would react to it appropriately&mdash;that is, by saying, \"Wow, that does sound like an <em>interesting </em>problem\", instead of finding one of many subtle ways to scream and run away.</p>\n<p>This is the problem that if you create an AI and tell it to model the world around it, it may form models of people that are people themselves.&nbsp; Not necessarily the <em>same</em> person, but people nonetheless.</p>\n<p>If you look up at the night sky, and see the tiny dots of light that move over days and weeks&mdash;<span class=\"Unicode\" style=\"white-space: normal; text-decoration: none;\" title=\"grc transliteration\" lang=\"grc-Latn\" xml:lang=\"grc-Latn\"><em>plan\u0113toi</em></span>, the Greeks called them, \"wanderers\"&mdash;and you try to predict the movements of those planet-dots as best you can...</p>\n<p>Historically, humans went through a journey as long and as wandering as the planets themselves, to find an accurate model.&nbsp; In the beginning, the models were things of cycles and epicycles, not much resembling the true Solar System.</p>\n<p>But eventually we found laws of gravity, and finally built models&mdash;even if they were just on paper&mdash;that were <em>extremely</em> accurate so that Neptune could be deduced by looking at the unexplained perturbation of Uranus from its expected orbit.&nbsp; This required moment-by-moment modeling of where a simplified version of Uranus would be, and the other known planets.&nbsp; Simulation, not just abstraction.&nbsp; Prediction through simplified-yet-still-detailed pointwise similarity.</p>\n<p>Suppose you have an AI that is around human beings.&nbsp; And like any Bayesian trying to explain its enivornment, the AI goes in quest of <em>highly accurate models </em>that predict what it sees of humans.</p>\n<p>Models that predict/explain why people do the things they do, say the things they say, want the things they want, think the things they think, and even why people talk about \"the <a href=\"http://www.overcomingbias.com/2007/08/mysterious-answ.html\">mystery</a> of <a href=\"http://www.overcomingbias.com/2008/04/zombies.html\">subjective experience</a>\".</p>\n<p>The model that most precisely predicts these facts, may well be a 'simulation' detailed enough to <em>be </em>a person in its own right.</p>\n<p><a id=\"more\"></a></p>\n<p>A highly detailed model <em>of </em>me, may not <em>be </em>me.&nbsp; But it will, at least, be a model which (for purposes of prediction via similarity) thinks <em>itself </em>to be Eliezer Yudkowsky.&nbsp; It will be a model that, when cranked to find my behavior if asked \"Who are you and are you conscious?\", says \"I am Eliezer Yudkowsky and I seem have subjective experiences\" <a href=\"http://www.overcomingbias.com/2008/04/zombies.html\">for much the same reason I do</a>.</p>\n<p>If that doesn't worry you, (re)read \"<a href=\"http://www.overcomingbias.com/2008/04/zombies.html\">Zombies! Zombies?</a>\".</p>\n<p>It seems likely (though not certain) that this happens <em>automatically</em>, whenever a mind of sufficient power to find the right answer, and not <em>otherwise</em> disinclined to create a sentient being trapped within itself, tries to model a human as accurately as possible.</p>\n<p>Now you could wave your hands and say, \"Oh, by the time the AI is smart enough to do that, it will be smart enough not to\".&nbsp; (This is, in general, a phrase useful in running away from Friendly AI problems.)&nbsp; But do you know this for a fact?</p>\n<p>When dealing with things that confuse you, it is wise to widen your confidence intervals.&nbsp; Is a human mind the simplest possible mind that can be sentient?&nbsp; What if, in the course of trying to model its own programmers, a relatively younger AI manages to create a sentient simulation trapped within itself?&nbsp; How soon do you have to start worrying?&nbsp; Ask yourself that fundamental question, \"What do I think I know, and how do I think I know it?\"</p>\n<p>You could wave your hands and say, \"Oh, it's more important to get the job done quickly, then to worry about such relatively minor problems; the end justifies the means.&nbsp; Why, look at all these problems the Earth has right now...\"&nbsp; (This is also a general way of running from Friendly AI problems.)</p>\n<p>But we may consider and discard many hypotheses in the course of finding the truth, and we are but slow humans.&nbsp; What if an AI creates millions, billions, trillions of alternative hypotheses, models that are actually people, who die when they are disproven?</p>\n<p>If you accidentally kill a few trillion people, or permit them to be killed&mdash;you could say that the weight of the Future outweighs this evil, perhaps.&nbsp; But the absolute weight of the sin would not be light.&nbsp; If you would balk at killing a million people with a nuclear weapon, you should balk at this.</p>\n<p>You could wave your hands and say, \"The model will contain abstractions over various uncertainties within it, and this will prevent it from being conscious even though it produces well-calibrated probability distributions over what you will say when you are asked to talk about consciousness.\"&nbsp; To which I can only reply, \"That would be very convenient if it were true, but how the hell do you <em>know </em>that?\"&nbsp; An element of a model marked 'abstract' is still there as a computational token, and the interacting causal system may still be sentient.</p>\n<p>For these purposes, we do not, in principle, need to crack the entire Hard Problem of Consciousness&mdash;the <a href=\"http://www.overcomingbias.com/2008/03/dissolving-the.html\">confusion</a> that we name \"subjective experience\".&nbsp; We only need to understand enough of it to know when a process is <em>not</em> conscious, <em>not </em>a person, <em>not </em>something deserving of the rights of citizenship.&nbsp; In practice, I suspect you can't <em>halfway </em>stop being confused&mdash;but in theory, half would be enough.</p>\n<p>We need a <em>nonperson predicate</em>&mdash;a predicate that returns 1 for anything that is a person, and can return 0 or 1 for anything that is not a person.&nbsp; This is a \"nonperson predicate\" because <em>if </em>it returns 0, <em>then </em>you know that something is definitely not a person.</p>\n<p>You can have more than one such predicate, and if <em>any </em>of them returns 0, you're ok.&nbsp; It just had better never return 0 on anything that <em>is</em> a person, however many nonpeople it returns 1 on.</p>\n<p>We can even hope that the vast majority of models the AI needs, will be swiftly and trivially approved by a predicate that quickly answers 0.&nbsp; And that the AI would only need to resort to more specific predicates in case of modeling actual people.</p>\n<p>With a good toolbox of nonperson predicates in hand, we could exclude all \"model citizens\"&mdash;all beliefs that are themselves people&mdash;from the set of hypotheses our Bayesian AI may invent to try to model its person-containing environment.</p>\n<p>Does that sound odd?&nbsp; Well, one has to handle the problem somehow.&nbsp; I am open to better ideas, though I will be a bit skeptical about any suggestions for how to proceed that let us <a href=\"http://www.overcomingbias.com/2008/12/artificial-myst.html\">cleverly avoid solving the damn mystery</a>.</p>\n<p>So do I <em>have</em> a nonperson predicate?&nbsp; No.&nbsp; At least, no nontrivial ones.</p>\n<p>This is a challenge that I have not even tried to talk about, with <a href=\"http://www.overcomingbias.com/2008/09/above-average-s.html\">those folk who think themselves ready to challenge the problem of true AI</a>.&nbsp; For they seem to have the standard reflex of running away from difficult problems, and are challenging AI only because <a href=\"http://www.overcomingbias.com/2008/08/dreams-of-ai-de.html\">they think their amazing insight has already solved it</a>.&nbsp; Just mentioning the problem of Friendly AI by itself, or of precision-grade AI design, is enough to send them fleeing into the night, screaming \"It's too hard!&nbsp; It can't be done!\"&nbsp; If I tried to explain that their job duties might impinge upon the sacred, mysterious, holy Problem of Subjective Experience&mdash;</p>\n<p>&mdash;I'd actually expect to get blank stares, mostly, followed by some <em>instantaneous </em>dismissal which requires no further effort on their part.&nbsp; I'm not sure of what the exact dismissal would be&mdash;maybe, \"Oh, none of the hypotheses my AI considers, could <em>possibly </em>be a person?\"&nbsp; I don't know; I haven't bothered trying. &nbsp;But it has to be a dismissal which rules out all possibility of their having to <a href=\"http://www.overcomingbias.com/2008/12/artificial-myst.html\">actually solve the damn problem</a>, because most of them would think that they are smart enough to build an AI&mdash;indeed, smart enough to have already solved the key part of the problem&mdash;but not smart enough to solve the Mystery of Consciousness, which still <em>looks</em> scary to them.</p>\n<p>Even if they thought of trying to solve it, they would be afraid of <em>admitting </em>they were trying to solve it.&nbsp; Most of these people cling to the shreds of their <a href=\"http://www.overcomingbias.com/2006/12/the_proper_use_.html\">modesty</a>, trying at one and the same time to have solved the AI problem while still being humble ordinary blokes.&nbsp; (There's a grain of truth to that, but at the same time: who the hell do they think they're kidding?)&nbsp; They know without words that their audience sees the Mystery of Consciousness as a <em>sacred untouchable problem</em>, <a href=\"http://www.overcomingbias.com/2008/05/einsteins-super.html\">reserved for some future superbeing</a>.&nbsp; They don't want people to think that they're claiming <a href=\"http://www.overcomingbias.com/2008/05/einsteins-super.html\">an Einsteinian aura of destiny</a> by trying to solve the problem.&nbsp; So it is easier to dismiss the problem, and not believe a proposition that would be uncomfortable to explain.</p>\n<p>Build an AI?&nbsp; Sure!&nbsp; Make it Friendly?&nbsp; Now that you point it out, sure!&nbsp; But trying to come up with a \"nonperson predicate\"?&nbsp; That's just way above the difficulty level they signed up to handle.</p>\n<p>But <a href=\"http://www.overcomingbias.com/2007/08/mysterious-answ.html\">a blank map does not correspond to a blank territory</a>. &nbsp;<a href=\"http://www.overcomingbias.com/2008/03/wrong-questions.html\">Impossible confusing questions correspond to places where your own thoughts are tangled</a>, not to places where the environment itself contains magic. &nbsp;<a href=\"http://www.overcomingbias.com/2008/05/einsteins-super.html\">Even difficult problems do not require an aura of destiny to solve</a>.&nbsp; And the first step to solving one is <a href=\"http://www.overcomingbias.com/2008/10/try-persevere.html\">not running away from the problem like a frightened rabbit</a>, but instead sticking long enough to learn something.</p>\n<p>So let us not run away from this problem.&nbsp; <a href=\"http://www.overcomingbias.com/2008/03/wrong-questions.html\">I doubt it is even difficult in any absolute sense, just a place where my brain is tangled.</a>&nbsp; I suspect, based on some prior experience with similar challenges, that you can't <em>really </em>be good enough to build a Friendly AI, and still be tangled up in your own brain like that.&nbsp; So it is not necessarily any <em>new </em>effort&mdash;over and above that required <em>generally </em>to build a mind while knowing exactly what you are about.</p>\n<p>But in any case, I am not screaming and running away from the problem.&nbsp; And I hope that you, dear longtime reader,&nbsp;will not faint at the audacity of my trying to solve it.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/x5/nonsentient_optimizers/\">Nonsentient Optimizers</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/x3/devils_offers/\">Devil's Offers</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XSryTypw5Hszpa4TS": 9, "uLqT8mmFiA8NeytTi": 1, "sYm3HiWcfZvrGu3ui": 1, "ZFrgTgzwEfStg26JL": 1, "mxSBcaTrakvCkgLzL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wqDRRx9RqwKLzWt7R", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 47, "baseScore": 49, "extendedScore": null, "score": 7.3e-05, "legacy": true, "legacyId": "1192", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 49, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 176, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K4aGvLnHvYgX9pZHS", "HsRFQTAySAx8xbXEc", "MTjej6HKvPByx3dEA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-27T02:32:23.000Z", "modifiedAt": null, "url": null, "title": "Nonsentient Optimizers", "slug": "nonsentient-optimizers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:53.239Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HsRFQTAySAx8xbXEc/nonsentient-optimizers", "pageUrlRelative": "/posts/HsRFQTAySAx8xbXEc/nonsentient-optimizers", "linkUrl": "https://www.lesswrong.com/posts/HsRFQTAySAx8xbXEc/nonsentient-optimizers", "postedAtFormatted": "Saturday, December 27th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Nonsentient%20Optimizers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANonsentient%20Optimizers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHsRFQTAySAx8xbXEc%2Fnonsentient-optimizers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Nonsentient%20Optimizers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHsRFQTAySAx8xbXEc%2Fnonsentient-optimizers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHsRFQTAySAx8xbXEc%2Fnonsentient-optimizers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1716, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/x4/nonperson_predicates/\">Nonperson Predicates</a>, <a href=\"/lw/rb/possibility_and_couldness/\">Possibility and Could-ness</a></p>\n<p style=\"margin-left: 40px;\">&nbsp;&nbsp;&nbsp; \"All our ships are sentient.&nbsp; You could certainly <em>try</em> telling a ship what to do... but I don't think you'd get very far.\"<br />&nbsp;&nbsp;&nbsp; \"Your ships think they're sentient!\" Hamin chuckled.<br />&nbsp;&nbsp;&nbsp; \"A common delusion shared by some of our human citizens.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &mdash;<em>Player of Games</em>, Iain M. Banks</p>\n<p><a href=\"/lw/x4/nonperson_predicates/\">Yesterday</a>, I suggested that, when an AI is trying to build a model of an environment that includes human beings, we want to avoid the AI constructing detailed models that are <em>themselves</em> people.&nbsp; And that, to this end, we would like to know what is or isn't a person&mdash;or at least have a predicate that returns 1 for all people and could return 0 or 1 for anything that isn't a person, so that, if the predicate returns 0, we know we have a definite nonperson on our hands.</p>\n<p>And as long as you're going to solve that problem <em>anyway</em>, why not apply the <em>same </em>knowledge to create a Very Powerful <a href=\"/lw/vb/efficient_crossdomain_optimization/\">Optimization Process</a> which is <em>also </em>definitely not a person?</p>\n<p>\"<a href=\"/lw/ui/use_the_try_harder_luke/\">What?&nbsp; That's impossible!</a>\"</p>\n<p>How do you know?&nbsp; Have you solved the <a href=\"/lw/og/wrong_questions/\">sacred mysteries</a> of consciousness and existence?</p>\n<p>\"Um&mdash;okay, look, putting aside the obvious objection that any sufficiently powerful intelligence will be able to model itself&mdash;\"</p>\n<p><a href=\"/lw/t6/the_cartoon_guide_to_l&ouml;bs_theorem/\">Lob's Sentence</a> contains an exact recipe for a copy of itself, including the recipe for the recipe; it has a <em>perfect </em>self-model.&nbsp; Does that make it sentient?</p>\n<p>\"Putting that aside&mdash;to create a powerful AI and make it <em>not sentient</em>&mdash;I mean, <em>why would you want to?</em>\"</p>\n<p>Several reasons.&nbsp; Picking the simplest to explain first&mdash;I'm not ready to be a father.</p>\n<p><a id=\"more\"></a></p>\n<p>Creating a true child is the only moral and metaethical problem I know that is even <em>harder</em> than the shape of a Friendly AI.&nbsp; I would like to be able to create Friendly AI while worrying <em>just </em>about the Friendly AI problems, and not worrying whether I've created someone who will lead a life worth living.&nbsp; Better by far to just create a Very Powerful Optimization Process, if at all possible.</p>\n<p>\"Well, you can't have everything, and <em>this </em>thing sounds distinctly alarming even if you <em>could </em>-\"</p>\n<p>Look, suppose that someone said&mdash;in fact, I <em>have</em> heard it said&mdash;that Friendly AI is impossible, because you can't have an intelligence without <em>free will</em>.</p>\n<p>\"In light of <a href=\"/lw/rb/possibility_and_couldness/\">the dissolved confusion about free will</a>, both that statement and its negation are pretty darned messed up, I'd say.&nbsp; Depending on how you look at it, either no intelligence has 'free will', or anything that simulates alternative courses of action has 'free will'.\"</p>\n<p>But, <em>understanding</em> how the human confusion of free will arises&mdash;the source of the strange things that people say about \"free will\"&mdash;I could construct a mind that did not have this confusion, nor say similar strange things itself.</p>\n<p>\"So the AI would be less confused about free will, just as you or I are less confused.&nbsp; But the AI would still consider alternative courses of action, and select among them without knowing at the beginning which alternative it would pick.&nbsp; You would <em>not </em>have constructed a mind lacking that which the confused name 'free will'.\"</p>\n<p>Consider, though, the original context of the objection&mdash;that you couldn't have Friendly AI, because you couldn't have intelligence without free will.</p>\n<p style=\"margin-left: 40px;\"><strong>Note:&nbsp; This post was accidentally published half-finished.&nbsp; Comments up to 11am (Dec 27), are only on the essay up to the above point.&nbsp; Sorry!</strong></p>\n<p>What is the original intent of the objection?&nbsp; What does the objector have in mind?</p>\n<p>Probably that you can't have an AI which is <em>knowably </em>good, because, as a full-fledged mind, it will have the power to choose between good and evil.&nbsp; (In an agonizing, self-sacrificing decision?)&nbsp; And in reality, <em>this</em>, which humans do, is <em>not </em>something that a Friendly AI&mdash;especially one <em>not</em> intended to be a child and a citizen&mdash;need go through.</p>\n<p>Which may sound very scary, if you see the landscape of possible minds in strictly anthropomorphic terms:&nbsp; <em>A mind without free will!</em>&nbsp; Chained to the selfish will of its creators!&nbsp; Surely, such an evil endeavor is bound to go wrong somehow...&nbsp; But if you shift over to seeing the mindscape in terms of e.g. utility functions and optimization, the \"free will\" thing sounds needlessly complicated&mdash;you would only do it if you wanted a specifically human-shaped mind, perhaps for purposes of creating a child.</p>\n<p>Or consider some of the other aspects of free will as it is ordinarily seen&mdash;the idea of <a href=\"/lw/rc/the_ultimate_source/\">agents as atoms </a>that bear irreducible charges of <a href=\"/lw/ra/causality_and_moral_responsibility/\">moral responsibility</a>.&nbsp; You can imagine how alarming it sounds (from an anthropomorphic perspective) to say that I plan to create an AI which lacks \"moral responsibility\".&nbsp; How could an AI possibly be moral, if it doesn't have a sense of moral responsibility?</p>\n<p>But an AI (especially a noncitizen AI) needn't conceive of itself as a moral atom whose actions, in addition to having good or bad effects, also carry a weight of sin or virtue which resides upon that atom.&nbsp; It doesn't have to think, \"If I do X, that makes me a good person; if I do Y, that makes me a bad person.\"&nbsp; It need merely weigh up the positive and negative utility of the consequences.&nbsp; It can understand the concept of people who carry weights of sin and virtue as the result of the decisions they make, while not treating itself as a person in that sense.</p>\n<p>Such an AI could fully understand an abstract concept of moral responsibility or agonizing moral struggles, and even correctly predict decisions that \"morally responsible\", \"free-willed\" humans would make, while possessing no actual sense of moral responsibility itself and not undergoing any agonizing moral struggles; yet still outputting the <a href=\"/lw/sm/the_meaning_of_right/\">right</a> behavior.</p>\n<p>And this might sound unimaginably impossible if you were taking an anthropomorphic view, simulating an \"AI\" by imagining yourself in its shoes, expecting <a href=\"/lw/rf/ghosts_in_the_machine/\">a ghost to be summoned into the machine</a>&mdash;</p>\n<p>&mdash;but when you know how \"free will\" works, and you take apart the mind design into pieces, it's actually not all that difficult.</p>\n<p>While we're on the subject, imagine some would-be AI designer saying:&nbsp; \"Oh, well, I'm going to build an AI, but of course it <em>has </em>to have moral free will&mdash;it can't be moral otherwise&mdash;it wouldn't be <em>safe </em>to build something that doesn't have free will.\"</p>\n<p>Then you may know that you are <em>not safe</em> with this one; they fall far short of the fine-grained understanding of mind required to build a knowably Friendly AI.&nbsp; Though it's conceivable (if not likely) that they could slap together something just smart enough to improve itself.</p>\n<p>And it's not even that \"free will\" is such a terribly important problem for an AI-builder.&nbsp; It's just that if you <em>do </em>know what you're doing, and you look at humans talking about free will, then you can see things like a search tree that labels reachable sections of plan space, or an evolved moral system that labels people as moral atoms.&nbsp; I'm sorry to have to say this, but it appears to me to be true: the mountains of philosophy are the foothills of AI.&nbsp; Even if philosophers debate free will for ten times a hundred years, it's not surprising if the key insight is found by AI researchers inventing search trees, on their way to doing other things.</p>\n<p>So anyone who says&mdash;\"It's too difficult to try to figure out the nature of free will, we should just go ahead and build an AI that has free will like we do\"&mdash;surely they are utterly doomed.</p>\n<p>And anyone who says:&nbsp; \"How can we dare build an AI that lacks the empathy to feel pain when humans feel pain?\"&mdash;Surely they too are doomed.&nbsp; They don't even understand the concept of a utility function in classical decision theory (which makes no mention of the neural idiom of reinforcement learning of policies).&nbsp; They cannot conceive of something that works unlike a human&mdash;implying that they see only a featureless ghost in the machine, secretly simulated by their own brains.&nbsp; They won't see the human algorithm as <em>detailed machinery,</em> as <em>big complicated machinery,</em> as <em>overcomplicated</em> machinery.</p>\n<p>And so their mind imagines something that does the <a href=\"/lw/sm/the_meaning_of_right/\">right thing</a> for much the same reasons human altruists do it&mdash;because that's <a href=\"/lw/tf/dreams_of_ai_design/\">easy to imagine</a>, if you're just imagining a ghost in the machine.&nbsp; But those human reasons are more complicated than they imagine&mdash;also less stable outside an exactly human cognitive architecture, than they imagine&mdash;and their chance of hitting that tiny target in design space is nil.</p>\n<p>And anyone who says:&nbsp; \"It would be terribly dangerous to build a non-sentient AI, even if we could, for it would lack empathy with us sentients&mdash;\"</p>\n<p>An analogy proves nothing; history never repeats itself; foolish generals set out to refight their last war.&nbsp; Who knows how this matter of \"sentience\" will go, once I have resolved it?&nbsp; It won't be <em>exactly</em> the same way as free will, or I would already be done.&nbsp; Perhaps there will be no choice but to create an AI which has that which we name \"subjective experiences\".</p>\n<p>But I think there is reasonable grounds for <em>hope</em> that when this confusion of \"sentience\" is resolved&mdash;probably via resolving some other problem in AI that turns out to hinge on the same reasoning process that's generating the confusion&mdash;we will be able to build an AI that is not \"sentient\" in the <em>morally important</em> aspects of that.</p>\n<p>Actually, the challenge of building a nonsentient AI seems to me <em>much less worrisome </em>than being able to come up with a <a href=\"/lw/x4/nonperson_predicates/\">nonperson predicate</a>!</p>\n<p>Consider:&nbsp; In the first case, I only need to pick <em>one</em> design that is not sentient.&nbsp; In the latter case, I need to have an AI that can correctly predict the decisions that conscious humans make, without ever using a conscious model of them!&nbsp; The first case is only a flying thing without flapping wings, but the second case is like modeling water without modeling wetness.&nbsp; Only the fact that it actually looks fairly <em>straightforward </em>to have an AI understand \"free will\" without having \"free will\", gives me hope by analogy.</p>\n<p>So why did I talk about the <em>much more difficult</em> case <em>first?</em></p>\n<p>Because humans are accustomed to thinking about other people, without believing that those imaginations are themselves sentient.&nbsp; But we're not accustomed to thinking of smart agents that aren't sentient.&nbsp; So I knew that a nonperson predicate would <em>sound easier to believe in</em>&mdash;even though, as problems go, it's actually far more worrisome.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/x7/cant_unbirth_a_child/\">Can't Unbirth a Child</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/x4/nonperson_predicates/\">Nonperson Predicates</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "XSryTypw5Hszpa4TS": 3, "5f5c37ee1b5cdee568cfb1b8": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HsRFQTAySAx8xbXEc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 30, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "1193", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wqDRRx9RqwKLzWt7R", "3buXtNiSK8gcRLMSG", "yLeEPFnnB9wE7KLx2", "fhEPnveFhb9tmd7Pe", "XzrqkhfwtiSDgKoAF", "ALCnqX6Xx8bpFMZq3", "EsMhFZuycZorZNRF5", "FqJGfSrXphrcwpiZe", "fG3g3764tSubr6xvs", "cnYHFNBF3kZEyx24v", "p7ftQ6acRkgo6hqHb", "K4aGvLnHvYgX9pZHS", "gb6zWstjmkYHLrbrg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-27T16:27:59.000Z", "modifiedAt": null, "url": null, "title": "Nonsentient Bloggers", "slug": "nonsentient-bloggers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:05.648Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CJQDHufACHm6XiYkY/nonsentient-bloggers", "pageUrlRelative": "/posts/CJQDHufACHm6XiYkY/nonsentient-bloggers", "linkUrl": "https://www.lesswrong.com/posts/CJQDHufACHm6XiYkY/nonsentient-bloggers", "postedAtFormatted": "Saturday, December 27th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Nonsentient%20Bloggers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANonsentient%20Bloggers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCJQDHufACHm6XiYkY%2Fnonsentient-bloggers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Nonsentient%20Bloggers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCJQDHufACHm6XiYkY%2Fnonsentient-bloggers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCJQDHufACHm6XiYkY%2Fnonsentient-bloggers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 25, "htmlBody": "<p>Today&#39;s post, <a href=\"/lw/x5/nonsentient_optimizers/\">Nonsentient Optimizers</a>, was accidentally published yesterday, although I&#39;d only written half of it.&#0160; It has now been completed; please <a href=\"http://www.overcomingbias.com/2008/12/unpersons.html#more\">look at it again</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CJQDHufACHm6XiYkY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 4.6574102889105305e-07, "legacy": true, "legacyId": "1194", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HsRFQTAySAx8xbXEc"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-28T17:00:00.000Z", "modifiedAt": null, "url": null, "title": "Can't Unbirth a Child", "slug": "can-t-unbirth-a-child", "viewCount": null, "lastCommentedAt": "2021-06-28T17:17:02.612Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gb6zWstjmkYHLrbrg/can-t-unbirth-a-child", "pageUrlRelative": "/posts/gb6zWstjmkYHLrbrg/can-t-unbirth-a-child", "linkUrl": "https://www.lesswrong.com/posts/gb6zWstjmkYHLrbrg/can-t-unbirth-a-child", "postedAtFormatted": "Sunday, December 28th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Can't%20Unbirth%20a%20Child&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACan't%20Unbirth%20a%20Child%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgb6zWstjmkYHLrbrg%2Fcan-t-unbirth-a-child%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Can't%20Unbirth%20a%20Child%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgb6zWstjmkYHLrbrg%2Fcan-t-unbirth-a-child", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgb6zWstjmkYHLrbrg%2Fcan-t-unbirth-a-child", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 965, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/x5/nonsentient_optimizers/\">Nonsentient Optimizers</a></p>\n<p>Why would you want to <em>avoid</em> creating a sentient AI?&nbsp; \"<em>Several reasons,</em>\" <a href=\"/lw/x5/nonsentient_optimizers/\">I said</a>.&nbsp; \"Picking the simplest to explain first&mdash;I'm not ready to be a father.\"</p>\n<p>So here is the <em>strongest </em>reason:</p>\n<p>You can't unbirth a child.</p>\n<p>I asked Robin Hanson what he would do with <a href=\"/lw/wt/not_taking_over_the_world/\">unlimited power</a>.&nbsp; \"Think very very carefully about what to do next,\" Robin said.&nbsp; \"Most likely the first task is who to get advice from.&nbsp; And then I listen to that advice.\"</p>\n<p>Good advice, I suppose, if a little meta.&nbsp; On a similarly meta level, then, I recall two excellent advices for wielding too much power:</p>\n<ol>\n<li>Do less; don't do everything that seems like a good idea, but only what you <em>must </em>do. </li>\n<li>Avoid doing things you can't undo. </li>\n</ol>\n<p><a id=\"more\"></a></p>\n<p>Imagine that you knew the secrets of subjectivity and could create sentient AIs.</p>\n<p>Suppose that you did create a sentient AI.</p>\n<p>Suppose that this AI was lonely, and figured out how to hack the Internet as it then existed, and that the available hardware of the world was such, that the AI created trillions of sentient kin&mdash;not copies, but differentiated into separate people.</p>\n<p>Suppose that these AIs were not hostile to us, but content to earn their keep and pay for their living space.</p>\n<p>Suppose that these AIs were emotional as well as sentient, capable of being happy or sad.&nbsp; And that these AIs were capable, indeed, of finding fulfillment in our world.</p>\n<p>And suppose that, while these AIs did care for one another, and cared about themselves, and cared how they were treated in the eyes of society&mdash;</p>\n<p>&mdash;these trillions of people <em>also </em>cared, very strongly, about <a href=\"/lw/sy/sorting_pebbles_into_correct_heaps/\">making giant cheesecakes</a>.</p>\n<p>Now suppose that these AIs sued for legal rights before the Supreme Court and tried to register to vote.</p>\n<p>Consider, I beg you, the full and awful depths of our moral dilemma.</p>\n<p>Even if the few billions of <em>Homo sapiens</em> retained a position of superior military power and economic capital-holdings&mdash;even if we <em>could</em> manage to keep the new sentient AIs down&mdash;</p>\n<p>&mdash;would we be <em>right</em> to do so?&nbsp; They'd be people, no less than us.</p>\n<p>We, the original humans, would have become a numerically tiny minority.&nbsp; Would we be right to make of ourselves an aristocracy and impose apartheid on the Cheesers, even if we had the power?</p>\n<p>Would we be right to go on trying to seize the destiny of the galaxy&mdash;to make of it a place of peace, freedom, art, aesthetics, individuality, empathy, and other components of <em>humane </em>value?</p>\n<p>Or should we be content to have the galaxy be 0.1% eudaimonia and 99.9% cheesecake?</p>\n<p>I can tell you <em>my </em>advice on how to resolve this horrible moral dilemma:&nbsp; <em>Don't create trillions of new people that care about cheesecake</em>.</p>\n<p>Avoid creating any new intelligent species <em>at all</em>, until we or some other decision process advances to the point of understanding what the hell we're doing and the implications of our actions.</p>\n<p>I've heard proposals to \"uplift chimpanzees\" by trying to mix in human genes to create \"humanzees\", and, leaving off all the other reasons why this proposal sends me screaming off into the night:</p>\n<p>Imagine that the humanzees end up as people, but rather dull and stupid people.&nbsp; They have social emotions, the alpha's desire for status; but they don't have the sort of <a href=\"/lw/sn/interpersonal_morality/\">transpersonal</a> moral concepts that humans evolved to deal with linguistic concepts.&nbsp; They have goals, but not ideals; they have allies, but not friends; they have chimpanzee drives coupled to a human's abstract intelligence.&nbsp;</p>\n<p>When humanity gains a bit more knowledge, we understand that the humanzees want to continue as they are, and have a <em>right </em>to continue as they are, until the end of time.&nbsp; Because despite all the higher destinies <em>we </em>might have wished for them, the original human creators of the humanzees, lacked the power and the wisdom to make humanzees who <em>wanted</em> to be anything better...</p>\n<p>CREATING A NEW INTELLIGENT SPECIES IS A HUGE DAMN #(*%#!ING <em>COMPLICATED </em>RESPONSIBILITY.</p>\n<p>I've lectured on the subtle art of <a href=\"/lw/un/on_doing_the_impossible/\">not running away from scary, confusing, impossible-seeming problems</a> like Friendly AI or the mystery of consciousness.&nbsp; You want to know how high a challenge has to be before I finally give up and flee screaming into the night?&nbsp; There it stands.</p>\n<p>You can pawn off this problem on a superintelligence, but it has to be a <em>nonsentient </em>superintelligence.&nbsp; Otherwise: egg, meet chicken, chicken, meet egg.</p>\n<p>If you create a <em>sentient </em>superintelligence&mdash;</p>\n<p>It's not just the problem of creating <em>one</em> damaged soul.&nbsp; It's the problem of creating a really <em>big</em> citizen.&nbsp; What if the superintelligence is multithreaded a trillion times, and every thread weighs as much in the moral calculus (we would conclude upon reflection) as a human being?&nbsp; What if (we would conclude upon moral reflection) the superintelligence is a trillion times human size, and that's enough by itself to outweigh our species?</p>\n<p>Creating a new intelligent species, and a new member of that species, especially a superintelligent member that might perhaps morally outweigh the whole of present-day humanity&mdash;</p>\n<p>&mdash;delivers a <em>gigantic </em>kick to the world, which cannot be undone.</p>\n<p>And if you choose the wrong shape for that mind, that is not so easily fixed&mdash;<em>morally </em>speaking&mdash;as a nonsentient program rewriting itself.</p>\n<p>What you make nonsentient, can always be made sentient later; but you can't just unbirth a child.</p>\n<p>Do less.&nbsp; Fear the non-undoable.&nbsp; <a href=\"http://www.overcomingbias.com/2008/06/against-disclai.html\">It's sometimes poor advice in general</a>, but very important advice when you're working with an undersized decision process having an oversized impact.&nbsp; What a (nonsentient) Friendly superintelligence might be able to decide safely, is another issue.&nbsp; But <em>for myself and my own small wisdom</em>, creating a sentient superintelligence <em>to start with</em> is far too large an impact on the world.</p>\n<p>A <em>nonsentient </em>Friendly superintelligence is a more colorless act.</p>\n<p>So that is the <em>most</em> important reason to avoid creating a sentient superintelligence <em>to start with</em>&mdash;though I have not exhausted the set.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/x8/amputation_of_destiny/\">Amputation of Destiny</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/x5/nonsentient_optimizers/\">Nonsentient Optimizers</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"pGqRLe9bFDX2G2kXY": 1, "R6uagTfhhBeejGrrf": 1, "sSNtcEQsqHgN8ZmRF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gb6zWstjmkYHLrbrg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 35, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "1195", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 96, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HsRFQTAySAx8xbXEc", "DdEKcS6JcW7ordZqQ", "mMBTPTjRbsrqbSkZE", "7HDtecu4qW9PCsSR6", "fpecAJLG9czABgCe9", "K4aGvLnHvYgX9pZHS", "vwnSPgwtmLjvTK2Wa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-29T18:00:00.000Z", "modifiedAt": null, "url": null, "title": "Amputation of Destiny", "slug": "amputation-of-destiny", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:37.632Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vwnSPgwtmLjvTK2Wa/amputation-of-destiny", "pageUrlRelative": "/posts/vwnSPgwtmLjvTK2Wa/amputation-of-destiny", "linkUrl": "https://www.lesswrong.com/posts/vwnSPgwtmLjvTK2Wa/amputation-of-destiny", "postedAtFormatted": "Monday, December 29th 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Amputation%20of%20Destiny&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAmputation%20of%20Destiny%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvwnSPgwtmLjvTK2Wa%2Famputation-of-destiny%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Amputation%20of%20Destiny%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvwnSPgwtmLjvTK2Wa%2Famputation-of-destiny", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvwnSPgwtmLjvTK2Wa%2Famputation-of-destiny", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2473, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/x5/nonsentient_optimizers/\">Nonsentient Optimizers</a>, <a href=\"/lw/x7/cant_unbirth_a_child/\">Can't Unbirth a Child</a></p>\n<p>From <em>Consider Phlebas</em> by Iain M. Banks:</p>\n<p style=\"margin-left: 40px;\">&nbsp;&nbsp;&nbsp; In practice as well as theory the Culture was beyond considerations of wealth or empire.&nbsp; The very concept of money&mdash;regarded by the Culture as a crude, over-complicated and inefficient form of rationing&mdash;was irrelevant within the society itself, where the capacity of its means of production ubiquitously and comprehensively exceeded every reasonable (and in some cases, perhaps, unreasonable) demand its not unimaginative citizens could make.&nbsp; These demands were satisfied, with one exception, from within the Culture itself.&nbsp; Living space was provided in abundance, chiefly on matter-cheap Orbitals; raw material existed in virtually inexhaustible quantities both between the stars and within stellar systems; and energy was, if anything, even more generally available, through fusion, annihilation, the Grid itself, or from stars (taken either indirectly, as radiation absorbed in space, or directly, tapped at the stellar core).&nbsp; Thus the Culture had no need to colonise, exploit, or enslave.<br />&nbsp;&nbsp;&nbsp; The only desire the Culture could not satisfy from within itself was one common to both the descendants of its original human stock and the machines they had (at however great a remove) brought into being: the urge not to feel useless.&nbsp; The Culture's sole justification for the relatively unworried, hedonistic life its population enjoyed was its good works; the secular evangelism of the Contact Section, not simply finding, cataloguing, investigating and analysing other, less advanced civilizations but&mdash;where the circumstances appeared to Contact to justify so doing&mdash;actually interfering (overtly or covertly) in the historical processes of those other cultures.</p>\n<p>Raise the subject of science-fictional utopias in front of any halfway sophisticated audience, and someone will mention the Culture.&nbsp; Which is to say: Iain Banks is the one to beat.</p>\n<p><a id=\"more\"></a></p>\n<p>Iain Banks's Culture could be called the apogee of hedonistic low-grade transhumanism.&nbsp; Its people are beautiful and fair, as pretty as they choose to be.&nbsp; Their bodies have been reengineered for swift adaptation to different gravities; and also reengineered for greater sexual endurance.&nbsp; Their brains contains glands that can emit various euphoric drugs on command.&nbsp; They live, in perfect health, for generally around four hundred years before choosing to die (I don't quite understand why they would, but this is low-grade transhumanism we're talking about).&nbsp; Their society is around eleven thousand years old, and held together by the Minds, artificial superintelligences decillions of bits big, that run their major ships and population centers.</p>\n<p><em>Consider Phlebas</em>, the first Culture novel, introduces all this from the perspective of an outside agent <em>fighting </em>the Culture&mdash;someone convinced that the Culture spells an end to life's meaning.&nbsp; Banks uses his novels to criticize the Culture along many dimensions, while simultaneously keeping the Culture a well-intentioned society of mostly happy people&mdash;an ambivalence which saves the literary quality of his books, avoiding either utopianism or dystopianism.&nbsp; Banks's books vary widely in quality; I would recommend starting with <em>Player of Games,</em> the quintessential Culture novel, which I would say achieves greatness.</p>\n<p>From a <a href=\"/lw/wv/prolegomena_to_a_theory_of_fun/\">fun-theoretic</a> perspective, the Culture and its humaniform citizens have a number of problems, some already covered in this series, some not.</p>\n<p>The Culture has deficiencies in <a href=\"/lw/ww/high_challenge/\">High Challenge</a> and <a href=\"/lw/wx/complex_novelty/\">Complex Novelty</a>.&nbsp; There are incredibly complicated games, of course, but these are <em>games</em>&mdash;not things with enduring consequences, woven into the story of your life.&nbsp; Life itself, in the Culture, is neither especially challenging nor especially novel; your future is not an unpredictable thing about which to be curious.</p>\n<p><a href=\"/lw/wz/living_by_your_own_strength/\">Living By Your Own Strength</a> is not a theme of the Culture.&nbsp; If you want something, you ask a Mind how to get it; and they will helpfully provide it, rather than saying \"No, you figure out how to do it yourself.\"&nbsp; The people of the Culture have little use for personal formidability, nor for <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">a wish to become stronger</a>.&nbsp; To me, the notion of growing in strength seems obvious, and it also seems obvious that the humaniform citizens of the Culture ought to grow into Minds themselves, over time.&nbsp; But the people of the Culture do <em>not </em>seem to get any smarter as they age; and after four hundred years so, they displace themselves into a sun.&nbsp; These two literary points are probably related.</p>\n<p>But the Culture's <em>main </em>problem, I would say, is...</p>\n<p>...the same as Narnia's main problem, actually.&nbsp; Bear with me here.</p>\n<p>If you read <em>The Lion, the Witch, and the Wardrobe</em> or saw the first <em>Chronicles of Narnia</em> movie, you'll recall&mdash;</p>\n<p>&mdash;I suppose that if you don't want any spoilers, you should stop reading here, but since it's a children's story and based on Christian theology, I don't think I'll be giving away too much by saying&mdash;</p>\n<p>&mdash;that the four human children who are the main characters, fight the White Witch and defeat her with the help of the great talking lion Aslan.</p>\n<p>Well, to be precise, Aslan defeats the White Witch.</p>\n<p>It's never explained why Aslan ever <em>left </em>Narnia a hundred years ago, allowing the White Witch to impose eternal winter and cruel tyranny on the inhabitants.&nbsp; Kind of an awful thing to do, wouldn't you say?</p>\n<p>But once Aslan comes back, he kicks the White Witch out and everything is okay again.&nbsp; There's no obvious reason why Aslan actually <em>needs</em> the help of four snot-nosed human youngsters.&nbsp; Aslan could have led the armies.&nbsp; In fact, Aslan <em>did </em>muster the armies and lead them before the children showed up.&nbsp; Let's face it, the kids are just along for the ride.</p>\n<p>The problem with Narnia... is Aslan.</p>\n<p>C. S. Lewis never needed to write Aslan into the story.&nbsp; The plot makes far more sense without him.&nbsp; The children could show up in Narnia on their own, and lead the armies on their own.</p>\n<p>But is poor Lewis alone to blame?&nbsp; Narnia was written as a Christian parable, and the Christian religion itself has exactly the same problem.&nbsp; All Narnia does is project the flaw in a stark, simplified light: this story has an extra lion.</p>\n<p>And the problem with the Culture is the Minds.</p>\n<p>\"Well...\" says the transhumanist SF fan, \"Iain Banks <em>did </em>portray the Culture's Minds as 'cynical, amoral, and downright sneaky' in their altruistic way; and they do, in his stories, mess around with humans and use them as pawns.&nbsp; But that is mere <a href=\"/lw/k9/the_logical_fallacy_of_generalization_from/\">fictional evidence</a>.&nbsp; A better-organized society would have laws against big Minds messing with small ones without consent.&nbsp; Though if a Mind is <em>truly</em> wise and kind and utilitarian, it should know how to balance possible resentment against other gains, without needing a law.&nbsp; Anyway, the problem with the Culture is the meddling, not the Minds.\"</p>\n<p>But that's not what I mean.&nbsp; What I mean is that if you could otherwise live in the same Culture&mdash;the same technology, the same lifespan and healthspan, the same wealth, freedom, and opportunity&mdash;</p>\n<p>\"I don't want to live in <em>any </em>version of the Culture.&nbsp; I don't want to live four hundred years in a biological body with a constant IQ and then die.&nbsp; Bleah!\"</p>\n<p>Fine, stipulate that problem solved.&nbsp; My point is that if you could otherwise get the same quality of life, in the same world, but <em>without</em> any Minds around to usurp the role of main character, wouldn't you prefer&mdash;</p>\n<p>\"What?\" cry my transhumanist readers, incensed at this betrayal by one of their own.&nbsp; \"Are you saying that we should never create any minds smarter than human, or keep them under lock and chain?&nbsp; Just because your soul is so small and mean that you can't bear the thought of anyone else being better than you?\"</p>\n<p>No, I'm not saying&mdash;</p>\n<p>\"Because that business about our souls shriveling up due to 'loss of meaning' is <em>typical </em>bioconservative neo-Luddite propaganda&mdash;\"</p>\n<p>Invalid argument: the world's greatest fool may say the sun is shining but <a href=\"/lw/lw/reversed_stupidity_is_not_intelligence/\">that doesn't make it dark out</a>.&nbsp; But in any case, that's <em>not</em> what I'm saying&mdash;</p>\n<p>\"It's a lost cause!&nbsp; You'll never prevent intelligent life from achieving its destiny!\"</p>\n<p>Trust me, I&mdash;</p>\n<p>\"And anyway it's a silly question to begin with, because you <em>can't</em> just remove the Minds and keep the same technology, wealth, and society.\"</p>\n<p>So you admit the Culture's Minds are a <em>necessary evil</em>, then.&nbsp; A price to be paid.</p>\n<p>\"Wait, I didn't say <em>that </em>-\"</p>\n<p>And <em>I</em> didn't say all that stuff <em>you're</em> imputing to <em>me!</em></p>\n<p>Ahem.</p>\n<p>My model already says <a href=\"/lw/ws/for_the_people_who_are_still_alive/\">we live in a Big World</a>.&nbsp; In which case there are vast armies of minds out there in the immensity of Existence (not just Possibility) which are far more awesome than myself.&nbsp; Any shrivelable souls can already go ahead and shrivel.</p>\n<p>And I just talked about people growing up into Minds over time, at some <a href=\"/lw/wx/complex_novelty/\">eudaimonic rate of intelligence increase</a>.&nbsp; So clearly I'm not trying to 'prevent intelligent life from achieving its destiny', nor am I trying to enslave all Minds to biological humans scurrying around forever, nor am I etcetera.&nbsp; (I do wish people wouldn't be <em>quite </em>so fast to assume that I've suddenly turned to the Dark Side&mdash;though I suppose, in this day and era, it's never an implausible hypothesis.)</p>\n<p>But I've already argued that we need a <a href=\"/lw/x4/nonperson_predicates/\">nonperson predicate</a>&mdash;some way of knowing that some computations are definitely <em>not </em>people&mdash;to avert an AI from creating sentient simulations in its efforts to <em>model</em> people.</p>\n<p>And trying to <a href=\"/lw/x5/nonsentient_optimizers/\">create a Very Powerful Optimization Process that lacks subjective experience and other aspects of personhood</a>, is <em>probably </em>&mdash;though I still confess myself somewhat confused on this subject&mdash;probably <a href=\"/lw/x5/nonsentient_optimizers/\">substantially <em>easier</em> than coming up with a nonperson predicate</a>.</p>\n<p>This being the case, <a href=\"/lw/x7/cant_unbirth_a_child/\">there are very strong reasons why a superintelligence should <em>initially</em> be designed to be knowably nonsentient</a>, if at all possible.&nbsp; Creating a new kind of sentient mind is a huge and non-undoable act.</p>\n<p>Now, this doesn't answer the question of whether a nonsentient Friendly superintelligence ought to <em>make</em> itself sentient, or whether an NFSI ought to immediately manufacture sentient Minds first thing in the morning, once it has adequate wisdom to make the decision.</p>\n<p>But there is <a href=\"/lw/wv/prolegomena_to_a_theory_of_fun/\">nothing except our own preferences, out of which to construct the Future</a>.&nbsp; So though this piece of information is not <em>conclusive</em>, nonetheless it is highly <em>informative:</em></p>\n<p>If you already had the lifespan and the health and the promise of future growth, would you <em>want</em> new powerful superintelligences to be created in your vicinity, on your same playing field?</p>\n<p>Or would you prefer that we stay on as the main characters in the story of intelligent life, with no higher beings above us?</p>\n<p>Should existing human beings grow up at some eudaimonic rate of intelligence increase, and then eventually decide what sort of galaxy to create, and how to people it?</p>\n<p>Or is it better for a nonsentient superintelligence to exercise that decision on our behalf, and start creating new powerful Minds right away?</p>\n<p>If we don't <em>have </em>to do it one way or the other&mdash;if we have both options&mdash;and if there's no particular need for heroic self-sacrifice&mdash;then which do you <em>like?</em></p>\n<p>\"I don't understand the <em>point </em>to what you're suggesting.&nbsp; Eventually, the galaxy is going to have Minds in it, right?&nbsp; We have to find a stable state that allows big Minds and little Minds to coexist.&nbsp; So what's the point in waiting?\"</p>\n<p>Well... you could have the humans grow up (at some eudaimonic rate of intelligence increase), and then when new people are created, they might be created as powerful Minds to start with.&nbsp; Or when you create new minds, <em>they </em>might have a different emotional makeup, which doesn't lead them to feel overshadowed if there are more powerful Minds above them.&nbsp; But <em>we</em>, as we exist <a href=\"/lw/ws/for_the_people_who_are_still_alive/\">already created</a>&mdash;<em>we </em>might prefer to stay on as the main characters, for now, if given a choice.</p>\n<p>\"You are showing far too much concern for six billion squishy things who happen to be alive today, out of all the unthinkable vastness of space and time.\"</p>\n<p>The Past contains enough tragedy, and has seen enough sacrifice already, I think.&nbsp; And I'm not sure that you can cleave off the Future so neatly from the Present.</p>\n<p>So I will set out as I mean the future to continue: with concern for the living.</p>\n<p>The sound of six billion faces being casually stepped on, does not seem to me like a good beginning.&nbsp; Even the Future should not be assumed to prefer that another chunk of pain be paid into its price.</p>\n<p>So yes, I am concerned <a href=\"/lw/ws/for_the_people_who_are_still_alive/\">for those currently alive</a>, because it is <em>that concern</em>&mdash;and <em>not </em>a casual attitude toward the welfare of sentient beings&mdash;which I wish to continue into the Future.</p>\n<p>And I will not, if at all possible, give any other human being the least cause to think that someone else might spark a better Singularity.&nbsp; I can make no promises upon the future, but I will at least not <em>close off</em> desirable avenues through my own actions.&nbsp; I will not, on my own authority, create a sentient superintelligence which may <em>already determine</em> humanity as having passed on the torch.&nbsp; It is too much to do on my own, and too much harm to do on my own&mdash;to amputate someone else's destiny, and steal their main character status.&nbsp; That is yet another reason not to create a sentient superintelligence <em>to start with.</em>&nbsp; (And it's part of the logic behind the <a href=\"http://intelligence.org/upload/CEV.html\">CEV proposal</a><em></em>, which carefully avoids filling in any moral parameters not yet determined.)</p>\n<p>But to return finally to the Culture and to <a href=\"/lw/wv/prolegomena_to_a_theory_of_fun/\">Fun Theory</a>:</p>\n<p>The Minds in the Culture don't need the humans, and yet the humans need to be needed.</p>\n<p>If you're going to have human-level minds with human emotional makeups, they shouldn't be competing on a level playing field with superintelligences.&nbsp; Either keep the superintelligences off the local playing field, or design the human-level minds with a different emotional makeup.</p>\n<p>\"The Culture's sole justification for the relatively unworried, hedonistic life its population enjoyed was its good works,\" writes Iain Banks.&nbsp; This indicates a rather unstable moral position.&nbsp; Either the life the population enjoys is eudaimonic enough to be its <em>own</em> justification, an end rather than a means; or else that life needs to be changed.</p>\n<p>When people are in need of rescue, this is is a goal of the <a href=\"/lw/ww/high_challenge/\">overriding-static-predicate</a> sort, where you rescue them <em>as fast as possible</em>, and <em>then you're done</em>.&nbsp; Preventing suffering <em>cannot provide a lasting meaning to life.</em>&nbsp; What happens when you run out of victims?&nbsp; If there's nothing more to life than eliminating suffering, you might as well eliminate life and be done.</p>\n<p>If the Culture isn't valuable enough for <em>itself</em>, even without its good works&mdash;then the Culture might as well not be.&nbsp; And when the Culture's Minds could do a better job and faster, \"good works\" can hardly justify the <em>human</em> existences within it.</p>\n<p>The human-level people need a destiny to make for themselves, and they need the overshadowing Minds off their playing field while they make it.&nbsp; Having an external evangelism project, and being given cute little roles that any Mind could do better in a flash, so as to \"supply meaning\", isn't going to cut it.</p>\n<p>That's far from the only thing the Culture is doing wrong, but it's at the top of my list.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/x9/dunbars_function/\">Dunbar's Function</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/x7/cant_unbirth_a_child/\">Can't Unbirth a Child</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sSNtcEQsqHgN8ZmRF": 1, "GBpwq8cWvaeRoE9X5": 1, "jiuackr7B5JAetbF6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vwnSPgwtmLjvTK2Wa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 36, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "1196", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HsRFQTAySAx8xbXEc", "gb6zWstjmkYHLrbrg", "pK4HTxuv6mftHXWC3", "29vqqmGNxNRGzffEj", "aEdqh3KPerBNYvoWe", "dKGfNvjGjq4rqffyF", "DoLQN5ryZ9XkZjq5h", "rHBdcHGLJ7KvLJQPk", "qNZM3EGoE5ZeMdCRt", "cfZ8zveqrTZbQrjeD", "wqDRRx9RqwKLzWt7R", "K4aGvLnHvYgX9pZHS", "W5PhyEQqEWTcpRpqn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-31T02:26:02.000Z", "modifiedAt": "2020-03-03T21:20:43.583Z", "url": null, "title": "Dunbar's Function", "slug": "dunbar-s-function", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:10.777Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W5PhyEQqEWTcpRpqn/dunbar-s-function", "pageUrlRelative": "/posts/W5PhyEQqEWTcpRpqn/dunbar-s-function", "linkUrl": "https://www.lesswrong.com/posts/W5PhyEQqEWTcpRpqn/dunbar-s-function", "postedAtFormatted": "Wednesday, December 31st 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dunbar's%20Function&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADunbar's%20Function%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW5PhyEQqEWTcpRpqn%2Fdunbar-s-function%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dunbar's%20Function%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW5PhyEQqEWTcpRpqn%2Fdunbar-s-function", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW5PhyEQqEWTcpRpqn%2Fdunbar-s-function", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1758, "htmlBody": "<p>The study of eudaimonic community sizes began with a seemingly silly method of calculation:&nbsp; Robin Dunbar calculated the correlation between the (logs of the) relative volume of the neocortex and observed group size in primates, then extended the graph outward to get the group size for a primate with a human-sized neocortex.&nbsp; You immediately ask, \"How much of the variance in primate group size can you explain like that, anyway?\" and the answer is <a href=\"http://www.bbsonline.org/Preprints/OldArchive/bbs.dunbar.html\">76% of the variance among 36 primate genera</a>, which is respectable.&nbsp; Dunbar came up with a group size of 148.&nbsp; Rounded to 150, and with the confidence interval of 100 to 230 tossed out the window, this became known as \"Dunbar's Number\".</p>\n<p>It's probably fair to say that a literal interpretation of this number is more or less bogus.</p>\n<p>There was a bit more to it than that, of course.&nbsp; Dunbar went looking for corroborative evidence from studies of corporations, hunter-gatherer tribes, and utopian communities.&nbsp; Hutterite farming communities, for example, had a rule that they must split at 150&mdash;with the rationale explicitly given that it was impossible to control behavior through peer pressure beyond that point.</p>\n<p>But 30-50 would be a typical size for a cohesive hunter-gatherer band; 150 is more the size of a cultural lineage of related bands.&nbsp; <a href=\"http://www.lifewithalacrity.com/2004/03/the_dunbar_numb.html\">Life With Alacrity has an excellent series on Dunbar's Number</a> which exhibits e.g. a <a href=\"http://www.lifewithalacrity.com/2005/08/dunbar_world_of.html\">histogram of Ultima Online guild sizes</a>&mdash;with the peak at 60, not 150.&nbsp; LWA also cites further research by PARC's <a href=\"http://blogs.parc.com/playon/archives/2005/10/guilds_max_subg.html\">Yee and Ducheneaut</a> showing that maximum internal cohesiveness, measured in the interconnectedness of group members, occurs at a World of Warcraft guild size of 50.&nbsp; (Stop laughing; you can get much more detailed data on organizational dynamics if it all happens inside a computer server.)</p>\n<p><a id=\"more\"></a></p>\n<p>And Dunbar himself did another regression and found that a community of 150 primates would have to spend 43% of its time on social grooming, which Dunbar interpreted as suggesting that 150 was an <em>upper bound</em> rather than an optimum, when groups were highly incentivized to stay together.&nbsp; 150 people <em>does</em> sound like a lot of employees for a tight-knit startup, doesn't it?</p>\n<p>Also from <a href=\"http://www.lifewithalacrity.com/2004/03/the_dunbar_numb.html\">Life With Alacrity</a>:</p>\n<p style=\"margin-left: 40px;\">A group of 3 is often unstable, with one person feeling left out, or else one person controlling the others by being the \"split\" vote.&nbsp; A group of 4 often devolves into two pairs...&nbsp; At 5 to 8 people, you can have a meeting where everyone can speak out about what the entire group is doing, and everyone feels highly empowered.&nbsp; However, at 9 to 12 people this begins to break down &mdash;not enough \"attention\" is given to everyone and meetings risk becoming either too noisy, too boring, too long, or some combination thereof.<br /><br />As you grow past 12 or so employees, you must start specializing and having departments and direct reports; however, you are not quite large enough for this to be efficient, and thus much employee time that you put toward management tasks is wasted.&nbsp; Only as you approach and pass 25 people does having simple departments and managers begin to work again...<br /><br />I've already noted the next chasm when you go beyond 80 people, which I think is the point that Dunbar's Number actually marks for a non-survival oriented group.&nbsp; Even at this lower point, the noise level created by required socialization becomes an issue, and filtering becomes essential.&nbsp; As you approach 150 this begins to be unmanageable...</p>\n<p>LWA suggests that community satisfaction has two peaks, one at size ~7 for simple groups, and one at ~60 for complex groups; and that any community has to fraction, one way or another, by the time it approaches Dunbar's Number.</p>\n<p>One of the primary principles of <a href=\"/lw/l1/evolutionary_psychology/\">evolutionary psychology</a> is that \"Our modern skulls house a stone age mind\" (<a href=\"http://www.psych.ucsb.edu/research/cep/primer.html\">saith Tooby and Cosmides</a>).&nbsp; You can interpret all sorts of <a href=\"/lw/sc/existential_angst_factory/\">angst</a> as the friction of a stone age mind rubbing against a modern world that isn't like the hunter-gatherer environment the brain evolved to handle.</p>\n<p>We may not <em>directly</em> interact with most of the other six billion people in the world, but we still live in a world much larger than Dunbar's Number.</p>\n<p>Or to say it with appropriate generality: taking our current brain size and mind design as the input, we live in a world much larger than Dunbar's Function for minds of our type.</p>\n<p>Consider some of the consequences:</p>\n<p>If you work in a large company, you probably don't know your tribal chief on any personal level, and may not even be able to get access to him.&nbsp; For every rule within your company, you may not know the person who decided on that rule, and have no realistic way to talk to them about the effects of that rule on you.&nbsp; Large amounts of the <em>organizational structure</em> of your life are beyond your ability to control, or even talk about with the controllers; directives that have major effects on you, may be handed down from a level you can't reach.</p>\n<p>If you live in a large country, you probably don't know your President or Prime Minister on a personal level, and may not even be able to get a few hours' chat; you live under laws and regulations that you didn't make, and you can't talk to the people who made them.</p>\n<p>This is a non-ancestral condition.&nbsp; Even children, while they may live under the dictatorial rule of their parents, can at least personally meet and talk to their tyrants. You could expect this unnatural (that is, non-<a href=\"http://en.wikipedia.org/wiki/Environment_of_evolutionary_adaptedness#Environment_of_evolutionary_adaptedness\">EEA</a>) condition to create some amount of anomie.</p>\n<p>Though it's a side issue, what's even more... interesting.... is the way that our brains simply <em>haven't updated</em> to their diminished power in a super-Dunbarian world.&nbsp; We just go on debating <a href=\"/lw/gw/politics_is_the_mindkiller/\">politics</a>, feverishly applying our valuable brain time to finding better ways to run the world, with just the same fervent intensity that would be appropriate if we were in a small tribe where we could persuade people to change things.</p>\n<p>If people don't like being part of large organizations and countries, why do they stick around?&nbsp; Because of another non-ancestral condition&mdash;you can't just gather your more sensible friends, leave the band, and gather nuts and berries somewhere else.&nbsp; If I had to cite two non-regulatory barriers at work, it would be (a) the cost of capital equipment, and (b) the surrounding web of contacts and contracts&mdash;a web of installed relationships not easily duplicated by a new company.</p>\n<p>I suspect that this is a major part of where the stereotype of Technology as the Machine Death-Force comes from&mdash;that along with the professional specialization and the expensive tools, you end up in <em>social</em> structures over which you have much less control.&nbsp; Some of the fear of creating a powerful AI \"even if Friendly\" may come from that stereotypical anomie&mdash;that you're creating a stronger Machine Death-Force to regulate your life.</p>\n<p>But we <em>already</em> live in a world, <em>right now,</em> where people are less in control of their social destinies than they would be in a hunter-gatherer band, because it's harder to talk to the tribal chief or (if that fails) leave unpleasant restrictions and start your own country.&nbsp; There is an opportunity for progress here.</p>\n<p>Another problem with our oversized world is the illusion of increased competition.&nbsp; There's that famous survey which showed that Harvard students would rather make $50,000 if their peers were making $25,000 than make $100,000 if their peers were receiving $200,000&mdash;and worse, they weren't necessarily wrong about what would make them happy.&nbsp; With a fixed income, you're unhappier at the low end of a high-class neighborhood than the high end of a middle-class neighborhood.</p>\n<p>But in a \"neighborhood\" the size of Earth&mdash;well, you're actually <em>quite unlikely</em> to run into either Bill Gates or Angelina Jolie on any given day.&nbsp; But the media relentlessly bombards you with stories about the interesting people who are much richer than you or much more attractive, as if they actually constituted a large fraction of the world.&nbsp; (This is a combination of <a href=\"/lw/j5/availability/\">biased availability</a>, and a difficulty in <a href=\"/lw/my/the_allais_paradox/\">discounting</a> <a href=\"/lw/hm/new_improved_lottery/\">tiny fractions</a>.)</p>\n<p>Now you could say that our hedonic relativism is one of the least pleasant aspects of human nature.&nbsp; And I might agree with you about that.&nbsp; But I tend to think that deep changes of brain design and emotional architecture should be taken slowly, and so it makes sense to look at the environment too.</p>\n<p>If you lived in a world the size of a hunter-gatherer band, then it would be easier to find something <em>important</em> at which to be the best&mdash;or do something that genuinely struck you as important, without becoming lost in a vast crowd of others with similar ideas.</p>\n<p>The eudaimonic size of a community as a function of the component minds' intelligence might be given by the degree to which those minds find it natural to specialize&mdash;the number of different professions that you can excel at, without having to invent professions <em>just</em> to excel at.&nbsp; Being the best at Go is one thing, if many people know about Go and play it.&nbsp; Being the best at \"playing tennis using a football\" is easier to achieve, but it also seems a tad... artificial.</p>\n<p>Call a specialization \"natural\" if it will arise without an oversupply of potential entrants.&nbsp; Newton could specialize in \"physics\", but today it would not be <em>possible</em> to specialize in \"physics\"&mdash;even if you were the only potential physicist in the world, you couldn't achieve expertise in all the physics known to modern-day humanity.&nbsp; You'd have to pick, say, quantum field theory, or some particular approach to QFT.&nbsp; But not QFT over left-handed bibble-braids with cherries on top; that's what happens when there are a thousand other workers in your field and everyone is desperate for some way to differentiate themselves.</p>\n<p>When you look at it that way, then there must be much more than 50 natural specializations in the modern world&mdash;but still much less than six billion.&nbsp; By the same logic as the original Dunbar's Number, if there are so many different professional specialties that no one person has <em>heard</em> of them all, then you won't know <em>who </em>to consult about any given topic.</p>\n<p>But if people keep getting smarter and learning more&mdash;expanding the number of relationships they can track, maintaining them more efficiently&mdash;and naturally specializing further as more knowledge is discovered and we become able to conceptualize more complex areas of study&mdash;and if the population growth rate stays under the rate of increase of Dunbar's Function&mdash;then eventually there could be a single community of sentients, and it really would be a single community.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/xr/in_praise_of_boredom/\">In Praise of Boredom</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/x8/amputation_of_destiny/\">Amputation of Destiny</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"gHCNhqxuJq2bZ2akb": 2, "DdgSyQoZXjj3KnF4N": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W5PhyEQqEWTcpRpqn", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 57, "extendedScore": null, "score": 8.6e-05, "legacy": true, "legacyId": "1197", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 57, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["epZLSoNvjW53tqNj9", "8rdoea3g6QGhWQtmx", "9weLK2AJ9JEt2Tt8f", "R8cpqD3NA4rZxRdQ4", "zJZvoiwydJ5zvzTHK", "QawvGzYWhqdyPWgBL", "K4aGvLnHvYgX9pZHS", "WMDy4GxbyYkNrbmrs", "vwnSPgwtmLjvTK2Wa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2008-12-31T18:40:27.000Z", "modifiedAt": null, "url": null, "title": "A New Day", "slug": "a-new-day", "viewCount": null, "lastCommentedAt": "2019-10-27T00:52:29.426Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ywZs3bAAnubux2REg/a-new-day", "pageUrlRelative": "/posts/ywZs3bAAnubux2REg/a-new-day", "linkUrl": "https://www.lesswrong.com/posts/ywZs3bAAnubux2REg/a-new-day", "postedAtFormatted": "Wednesday, December 31st 2008", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20New%20Day&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20New%20Day%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FywZs3bAAnubux2REg%2Fa-new-day%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20New%20Day%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FywZs3bAAnubux2REg%2Fa-new-day", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FywZs3bAAnubux2REg%2Fa-new-day", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 348, "htmlBody": "<p>Somewhere in the vastnesses of the Internet and the almost equally impenetrable thicket of my bookmark collection, there is a post by someone who was learning Zen meditation...</p><p>Someone who was surprised by how many of the thoughts that crossed his mind, as he tried to meditate, were <em>old</em> thoughts - thoughts he had thunk many times before.&#0160; He was successful in banishing these old thoughts, but did he succeed in meditating?&#0160; No; once the comfortable routine thoughts were banished, new and interesting and more distracting thoughts began to cross his mind instead.</p><p>I was struck, on reading this, how much of my life I had allowed to fall into routine patterns.&#0160; Once you actually see that, it takes on a nightmarish quality:&#0160; You can imagine your fraction of <a href=\"/lw/wx/complex_novelty/\">novelty</a> diminishing and diminishing, so slowly you never take alarm, until finally you spend until the end of time watching the same videos over and over again, and thinking the same thoughts each time.</p><p>Sometime in the next week - January 1st if you have that available, or maybe January 3rd or 4th if the weekend is more convenient - I suggest you hold a New Day, where you don&#39;t do anything old.</p><p>Don&#39;t read any book you&#39;ve read before.&#0160; Don&#39;t read any author you&#39;ve read before.&#0160; Don&#39;t visit any website you&#39;ve visited before.&#0160; Don&#39;t play any game you&#39;ve played before.&#0160; Don&#39;t listen to familiar music that you already know you&#39;ll like.&#0160; If you go on a walk, walk along a new path even if you have to drive to a different part of the city for your walk.&#0160; Don&#39;t go to any restaurant you&#39;ve been to before, order a dish that you haven&#39;t had before.&#0160; Talk to new people (even if you have to find them in an IRC channel) about something you don&#39;t spend much time discussing.</p><p>And most of all, if you become aware of yourself musing on any thought you&#39;ve thunk before, then muse on something else.&#0160; Rehearse no old grievances, replay no old fantasies.</p><p>If it works, you could make it a holiday tradition, and do it <em>every </em>New Year.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hXTqT62YDTTiqJfxG": 1, "moeYqrcakMgXnQNyF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ywZs3bAAnubux2REg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 36, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "1198", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aEdqh3KPerBNYvoWe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-02T01:41:00.000Z", "modifiedAt": null, "url": null, "title": "Free to Optimize", "slug": "free-to-optimize", "viewCount": null, "lastCommentedAt": "2019-02-18T06:35:00.296Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EZ8GniEPSechjDYP9/free-to-optimize", "pageUrlRelative": "/posts/EZ8GniEPSechjDYP9/free-to-optimize", "linkUrl": "https://www.lesswrong.com/posts/EZ8GniEPSechjDYP9/free-to-optimize", "postedAtFormatted": "Friday, January 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Free%20to%20Optimize&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFree%20to%20Optimize%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEZ8GniEPSechjDYP9%2Ffree-to-optimize%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Free%20to%20Optimize%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEZ8GniEPSechjDYP9%2Ffree-to-optimize", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEZ8GniEPSechjDYP9%2Ffree-to-optimize", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2036, "htmlBody": "<p><em>Stare decisis</em> is the legal principle which binds courts to follow precedent, retrace the footsteps of other judges' decisions.&nbsp; As someone previously condemned to an Orthodox Jewish education, where I <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">gritted my teeth at the idea that medieval rabbis would always be wiser than modern rabbis</a>, I <em>completely missed</em> the rationale for <em>stare decisis</em>.&nbsp; I thought it was about respect for the past.</p>\n<p>But shouldn't we presume that, in the presence of science, judges closer to the future will know more&mdash;have new facts at their fingertips&mdash;which enable them to make better decisions?&nbsp; Imagine if engineers respected the decisions of past engineers, not as a source of good suggestions, but as a binding precedent!&mdash;That was my original reaction.&nbsp; The standard rationale behind <em>stare decisis</em> came as a shock of revelation to me; it considerably increased my respect for the whole legal system.</p>\n<p>This rationale is <em>jurisprudence constante:</em>&nbsp; The legal system must above all be <em>predictable,</em> so that people can execute contracts or choose behaviors knowing the legal implications.</p>\n<p>Judges are not necessarily there to <em>optimize,</em> like an engineer.&nbsp; The purpose of law is not to make the world perfect.&nbsp; The law is there to provide a <em>predictable environment</em> in which people can optimize their <em>own</em>futures.</p>\n<p>I was amazed at how a principle that at first glance seemed so completely Luddite, could have such an Enlightenment rationale.&nbsp; It was a \"<a href=\"/lw/vm/lawful_creativity/\">shock of creativity</a>\"&mdash;a solution that ranked high in my preference ordering and low in my search ordering, a solution that violated my previous surface generalizations.&nbsp; \"Respect the past <em>just </em>because it's the past\" would not have easily occurred to me as a good solution for <em>anything</em>.</p>\n<p>There's a <a href=\"http://books.google.com/books?id=inmTyPPdR5oC&amp;pg=RA1-PA123&amp;lpg=RA1-PA123&amp;dq=%22caretaking+organisms%22&amp;source=bl&amp;ots=JynSbtNF4H&amp;sig=k-2NzLYV3jwoB8bNF8JM2HdNB2Q&amp;hl=en&amp;sa=X&amp;oi=book_result&amp;resnum=2&amp;ct=result#PRA1-PA123,M1\">peer commentary</a> in <em>Evolutionary Origins of Morality</em> which notes in passing that \"other things being equal, organisms will choose to reward themselves over being rewarded by caretaking organisms\".&nbsp; It's cited as the Premack principle, but the <a href=\"http://en.wikipedia.org/wiki/Premack's_principle\">actual Premack principle</a> looks to be something quite different, so I don't know if this is a bogus result, a misremembered citation, or a nonobvious derivation.&nbsp; If true, it's definitely interesting from a <a href=\"/lw/wv/prolegomena_to_a_theory_of_fun/\">fun-theoretic</a> perspective.</p>\n<p>Optimization is <a href=\"/lw/vb/efficient_crossdomain_optimization/\">the ability to squeeze the future</a> into <a href=\"/lw/v9/aiming_at_the_target/\">regions high in your preference ordering</a>.&nbsp; <a href=\"/lw/wz/living_by_your_own_strength/\">Living by my own strength</a>, means squeezing my own future&mdash;not perfectly, but still being able to grasp <em>some</em> of the relation between my actions and their consequences.&nbsp; This is <a href=\"/lw/ve/mundane_magic/\">the strength of a human</a>.</p>\n<p>If I'm being <em>helped,</em> then some other agent is also squeezing my future&mdash;optimizing me&mdash;in the same rough direction that I try to squeeze myself.&nbsp; This is \"help\".</p>\n<p>A human helper is unlikely to steer every part of my future that I could have steered myself.&nbsp; They're not likely to have already exploited every connection between action and outcome that I can myself understand.&nbsp; They won't be able to squeeze the future <em>that tightly;</em> there will be slack left over, that I can squeeze for myself.</p>\n<p>We have little experience with being \"caretaken\" across any <a href=\"/lw/ql/my_childhood_role_model/\">substantial gap in intelligence</a>; the closest thing that human experience provides us with is the idiom of parents and children.&nbsp; Human parents are still human; they may be smart<em>er</em> than their children, but they can't predict the future or manipulate the kids in any fine-grained way.</p>\n<p>Even so, it's an empirical observation that some human parents <em>do</em>help their children so much that their children don't become strong.&nbsp; It's not that there's <em>nothing</em> left for their children to do, but with a hundred million dollars in a trust fund, they don't <em>need</em> to do much&mdash;their remaining motivations aren't strong enough.&nbsp; Something like that depends on genes, not just environment &mdash;not every overhelped child shrivels&mdash;but conversely it depends on environment too, not just genes.</p>\n<p>So, in considering the kind of \"help\" that can flow from relatively stronger agents to relatively weaker agents, we have two potential problems to track:</p>\n<ol>\n<li>Help so strong that it optimizes away the links between the desirable outcome and your own choices.</li>\n<li>Help that is <em>believed</em>to be so reliable, that it takes off the psychological pressure to use your own strength.</li>\n</ol>\n<p>Since (2) revolves around <em>belief</em>, could you just lie about how reliable the help was?&nbsp; Pretend that you're not going to help when things get bad&mdash;but then if things do get bad, you help anyway?&nbsp; That trick didn't work too well for Alan Greenspan and Ben Bernanke.</p>\n<p>A superintelligence might be able to pull off a better deception.&nbsp; But in terms of moral theory and eudaimonia&mdash;we <em>are</em>allowed to have <a href=\"/lw/lb/not_for_the_sake_of_happiness_alone/\">preferences over external states of affairs, not just psychological states</a>.&nbsp; This applies to \"I want to <em>really</em> steer my own life, not just believe that I do\", just as it applies to \"I want to have a love affair with a fellow sentient, not just a puppet that I am deceived into thinking sentient\".&nbsp; So if we can state firmly from a value standpoint that we don't want to be fooled this way, then <em>building</em>an agent which respects that preference is a mere matter of Friendly AI.</p>\n<p>Modify people so that they <em>don't</em> relax when they believe they'll be helped?&nbsp; I usually try to think of how to modify environments before I imagine modifying any people.&nbsp; It's not that I want to stay the same person forever; but the issues are rather more fraught, and one might wish to take it slowly, at some eudaimonic rate of personal improvement.</p>\n<p>(1), though, is the most interesting issue from a philosophicalish standpoint.&nbsp; It impinges on <a href=\"/lw/rc/the_ultimate_source/\">the confusion named \"free will\"</a>.&nbsp; Of which I have already untangled; see the posts referenced at top, if you're recently joining <em>OB</em>.</p>\n<p>Let's say that I'm an ultrapowerful AI, and I use my knowledge of your mind and your environment to forecast that, if left to your own devices, you will make $999,750.&nbsp; But this does not satisfice me; it so happens that I want you to make at least $1,000,000.&nbsp; So I hand you $250, and then you go on to make $999,750 as you ordinarily would have.</p>\n<p>How much of your own strength have you just lived by?</p>\n<p>The first view would say, \"I made 99.975% of the money; the AI only helped 0.025% worth.\"</p>\n<p>The second view would say, \"Suppose I had entirely slacked off and done nothing.&nbsp; Then the AI would have handed me $1,000,000.&nbsp; So my attempt to <em>steer my own future</em> was an illusion; my future was already determined to contain $1,000,000.\"</p>\n<p>Someone might reply, \"Physics is deterministic, so your future is already determined no matter what you or the AI does&mdash;\"</p>\n<p>But the second view interrupts and says, \"No, you're not confusing me that easily.&nbsp; <a href=\"/lw/r0/thou_art_physics/\">I am within physics</a>, so in order for my future to be determined by me, it must be determined by physics.&nbsp; The Past does not reach around the Present and determine the Future before the Present gets a chance&mdash;that is <a href=\"/lw/r1/timeless_control/\">mixing up a timeful view with a timeless one</a>.&nbsp; But if there's an AI that really <em>does</em> look over the alternatives before I do, and really <em>does</em> choose the outcome before I get a chance, then I'm really <em>not</em> steering my own future.&nbsp; The future is no longer <em>counterfactually dependent</em> on my decisions.\"</p>\n<p>At which point the first view butts in and says, \"But of course the future is counterfactually dependent on your actions.&nbsp; The AI gives you $250 and then leaves.&nbsp; As a physical fact, if you didn't work hard, you would end up with only $250 instead of $1,000,000.\"</p>\n<p>To which the second view replies, \"I <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">one-box on Newcomb's Problem</a>, so my counterfactual reads 'if my decision were to not work hard, the AI would have given me $1,000,000 instead of $250'.\"</p>\n<p>\"So you're saying,\" says the first view, heavy with sarcasm, \"that if the AI had wanted me to make at least $1,000,000 and it had ensured this through the general policy of handing me $1,000,000 flat on a silver platter, leaving me to earn $999,750 through my own actions, for a total of $1,999,750&mdash;that this AI would have interfered <em>less</em>with my life than the one who just gave me $250.\"</p>\n<p>The second view thinks for a second and says \"Yeah, actually.&nbsp; Because then there's a stronger counterfactual dependency of the final outcome on your own decisions.&nbsp; Every dollar you earned was a real added dollar.&nbsp; The second AI helped you more, but it constrained your destiny less.\"</p>\n<p>\"But if the AI had done exactly the same thing, because it <em>wanted</em>me to make exactly $1,999,750&mdash;\"</p>\n<p>The second view nods.</p>\n<p>\"That sounds a bit scary,\" the first view says, \"for reasons which have nothing to do with the usual furious debates over Newcomb's Problem.&nbsp; You're making your utility function path-dependent on the detailed cognition of the Friendly AI trying to help you!&nbsp; You'd be okay with it if the AI only <em>could</em> give you $250.&nbsp; You'd be okay if the AI had decided to give you $250 through a decision process that had <em>predicted the final outcome in less detail,</em> even though you acknowledge that in principle your decisions may already be highly deterministic.&nbsp; How is a poor Friendly AI supposed to help you, when your utility function is dependent, not just on the outcome, not just on the Friendly AI's actions, but dependent on <em>differences of the exact algorithm</em> the Friendly AI uses to arrive at <em>the same decision?</em>&nbsp; Isn't your whole rationale of one-boxing on Newcomb's Problem that you only care about what works?\"</p>\n<p>\"Well, that's a good point,\" says the second view.&nbsp; \"But <a href=\"/lw/ww/high_challenge/\">sometimes we only care about what works, and yet sometimes we <em>do</em> care about the journey as well as the destination</a>.&nbsp; If I was trying to cure cancer, I wouldn't care how I cured cancer, or whether I or the AI cured cancer, just so long as it ended up cured.&nbsp; This <em>isn't</em> that kind of problem.&nbsp; This is the problem of the eudaimonic journey&mdash;it's the reason I care in the first place whether I get a million dollars through my own efforts or by having an outside AI hand it to me on a silver platter.&nbsp; <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">My utility function is not up for grabs</a>.&nbsp; If I desire not to be optimized too hard by an outside agent, the agent needs to respect that preference even if it depends on the details of how the outside agent arrives at its decisions.&nbsp; Though it's also worth noting that decisions <em>are</em>produced by algorithms&mdash; if the AI <em>hadn't</em> been using the algorithm of doing just what it took to bring me up to $1,000,000, it probably <em>wouldn't</em> have handed me exactly $250.\"</p>\n<p>The desire <em>not to be optimized too hard by an outside agent</em> is one of the structurally nontrivial aspects of human morality.</p>\n<p>But I can think of <em>a</em> solution, which unless it contains some terrible flaw not obvious to me, sets a lower bound on the goodness of a solution: any alternative solution adopted, ought to be at least this good or better.</p>\n<p>If there is anything in the world that resembles a god, people will try to <a href=\"http://miscellanea.wellingtongrey.net/2008/12/01/prayer-vs-hard-work/\">pray</a> to it.&nbsp; It's human nature to such an extent that people will pray even if there aren't any gods&mdash;so you can imagine what would happen if there were!&nbsp; But people don't pray to gravity to ignore their airplanes, because it is understood how gravity works, and it is understood that gravity doesn't adapt itself to the needs of individuals.&nbsp; Instead they understand gravity and try to turn it to their own purposes.</p>\n<p>So one possible way of helping&mdash;which may or may not be the best way of helping&mdash;would be the gift of a world that works on <em>improved rules,</em> where the rules are stable and understandable enough that people can manipulate them and optimize their own futures together.&nbsp; A nicer place to live, but free of meddling gods beyond that.&nbsp; I have yet to think of a form of help that is less poisonous to human beings&mdash;but I am only human.</p>\n<p><strong>Added:</strong>&nbsp; Note that <em>modern</em> legal systems score a low Fail on this dimension&mdash;no single human mind can even <em>know</em> all the regulations any more, let alone optimize for them.&nbsp; Maybe a professional lawyer who did nothing else could memorize all the regulations applicable to them personally, but I doubt it.&nbsp; As Albert Einstein observed, any fool can make things more complicated; what takes intelligence is moving in the opposite direction.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/x2/harmful_options/\">Harmful Options</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/wz/living_by_your_own_strength/\">Living By Your Own Strength</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sSNtcEQsqHgN8ZmRF": 1, "MhQk9tRaJgYM4o6iD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EZ8GniEPSechjDYP9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 41, "extendedScore": null, "score": 6.1e-05, "legacy": true, "legacyId": "1199", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 81, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DoLQN5ryZ9XkZjq5h", "KKLQp934n77cfZpPn", "pK4HTxuv6mftHXWC3", "yLeEPFnnB9wE7KLx2", "CW6HDvodPpNe38Cry", "dKGfNvjGjq4rqffyF", "SXK87NgEPszhWkvQm", "3Jpchgy53D2gB5qdk", "synsRtBKDeAFuo7e3", "EsMhFZuycZorZNRF5", "NEeW7eSXThPz7o4Ne", "YYLmZFEGKsjCKQZut", "6ddcsdA2c2XpNpE5x", "29vqqmGNxNRGzffEj", "K4aGvLnHvYgX9pZHS", "CtSS6SkHhLBvdodTY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-02T20:30:33.000Z", "modifiedAt": null, "url": null, "title": "The Uses of Fun (Theory)", "slug": "the-uses-of-fun-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:34.013Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4o3zwgofFPLutkqvd/the-uses-of-fun-theory", "pageUrlRelative": "/posts/4o3zwgofFPLutkqvd/the-uses-of-fun-theory", "linkUrl": "https://www.lesswrong.com/posts/4o3zwgofFPLutkqvd/the-uses-of-fun-theory", "postedAtFormatted": "Friday, January 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Uses%20of%20Fun%20(Theory)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Uses%20of%20Fun%20(Theory)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4o3zwgofFPLutkqvd%2Fthe-uses-of-fun-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Uses%20of%20Fun%20(Theory)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4o3zwgofFPLutkqvd%2Fthe-uses-of-fun-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4o3zwgofFPLutkqvd%2Fthe-uses-of-fun-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1257, "htmlBody": "<p style=\"margin-left: 40px;\">\"But is there anyone who actually wants to live in a Wellsian Utopia?&nbsp; On the contrary, not to live in a world like that, not to wake up in a hygenic garden suburb infested by naked schoolmarms, has actually become a conscious political motive.&nbsp; A book like Brave New World is an expression of the actual fear that modern man feels of the rationalised hedonistic society which it is within his power to create.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &mdash;George Orwell, <em>Why Socialists Don't Believe in Fun</em></p>\n<p>There are three reasons I'm talking about Fun Theory, some more important than others:</p>\n<ol>\n<li>If every picture ever drawn of the Future looks like a terrible place to actually live, it might tend to drain off the motivation to create the future.&nbsp; <a href=\"/lw/wq/you_only_live_twice/\">It takes hope to sign up for cryonics</a>.</li>\n<li>People who leave their religions, but don't familiarize themselves with the <a href=\"/lw/lm/affective_death_spirals/\">deep</a>, <a href=\"/lw/i4/belief_in_belief/\">foundational</a>, <a href=\"/lw/tv/excluding_the_supernatural/\">fully general</a> arguments against theism, are at risk of backsliding.&nbsp; Fun Theory lets you look at our <a href=\"/lw/uk/beyond_the_reach_of_god/\">present world</a>, and see that it is not optimized <em>even for considerations like personal responsibility or self-reliance.</em>&nbsp; It is the fully general reply to theodicy.</li>\n<li>Going into the details of Fun Theory helps you see that eudaimonia is actually <em>complicated </em>&mdash;that there are a lot of properties necessary for a mind to lead a worthwhile existence.&nbsp; Which helps you appreciate just how worthless a galaxy would end up looking (with extremely high probability) if it was optimized by something with a utility function rolled up at random.</li>\n</ol>\n<p><a id=\"more\"></a></p>\n<p>To amplify on these points in order:</p>\n<p>(1)&nbsp; You've got folks like Leon Kass and the other members of Bush's \"President's Council on Bioethics\" running around talking about what a terrible, terrible thing it would be if people lived longer than threescore and ten.&nbsp; While some philosophers have pointed out the flaws in their arguments, it's one thing to point out a flaw and another to provide a counterexample.&nbsp; \"Millions long for immortality who do not know what to do with themselves on a rainy Sunday afternoon,\" said Susan Ertz, and that argument will sound plausible for as long as you can't imagine what to do on a rainy Sunday afternoon, and it seems unlikely that anyone could imagine it.</p>\n<p>It's not exactly the fault of Hans Moravec that his world in which humans are kept by superintelligences as pets, doesn't sound quite Utopian.&nbsp; Utopias are just really hard to construct, for reasons I'll talk about in more detail later&mdash;but this observation has already been made by many, including George Orwell.</p>\n<p>Building the Future is part of the ethos of secular humanism, our common project.&nbsp; If you have nothing to look forward to&mdash;if there's no image of the Future that can inspire real enthusiasm&mdash;then you won't be able to scrape up enthusiasm for that common project.&nbsp; And if the project is, in fact, a worthwhile one, the expected utility of the future will suffer accordingly from that nonparticipation.&nbsp; So that's one side of the coin, just as the other side is living so exclusively in a fantasy of the Future that you can't bring yourself to go on in the Present.</p>\n<p>I recommend thinking vaguely of the Future's hopes, thinking specifically of the Past's horrors, and spending <em>most </em>of your time in the Present.&nbsp; This strategy has certain epistemic virtues beyond its use in cheering yourself up.</p>\n<p>But it helps to have <em>legitimate </em>reason to vaguely hope&mdash;to minimize the leaps of abstract optimism involved in thinking that, yes, you can live and obtain happiness in the Future.</p>\n<p>(2)&nbsp; Rationality is our goal, and atheism is just a side effect&mdash;the judgment that happens to be produced.&nbsp; But atheism is an <em>important</em> side effect.&nbsp; John C. Wright, who wrote the heavily transhumanist <em>The Golden Age,</em> had some kind of temporal lobe epileptic fit and became a Christian.&nbsp; There's a once-helpful soul, now lost to us.</p>\n<p>But it is possible to do better, even if your brain malfunctions on you.&nbsp; I know a transhumanist who has strong religious visions, which she once attributed to future minds reaching back in time and talking to her... but then she reasoned it out, asking why future superminds would grant <em>only her</em> the solace of conversation, and why they could offer vaguely reassuring arguments but not tell her winning lottery numbers or the 900th digit of pi.&nbsp; So now she still has strong religious experiences, but she is not religious.&nbsp; That's the difference between weak rationality and strong rationality, and it has to do with the <em>depth </em> and <em>generality</em> of the epistemic rules that you know and apply.</p>\n<p>Fun Theory is part of the fully general reply to religion; in particular, it is the fully general reply to theodicy.&nbsp; If you can't say how God could have <em>better</em> created the world without sliding into an antiseptic Wellsian Utopia, you can't carry <a href=\"http://i20.photobucket.com/albums/b217/luke1889/Epicurus.jpg\">Epicurus's argument</a>.&nbsp; If, on the other hand, you have some idea of how you could build a world that was not only more pleasant but also a better medium for self-reliance, then you can see that permanently losing both your legs in a car accident when someone else crashes into you, doesn't seem very eudaimonic.</p>\n<p>If we can imagine what the world might look like if it had been designed by anything remotely like a benevolently inclined superagent, we can look at the world around us, and see that <em>this </em>isn't it.&nbsp; This doesn't require that we correctly forecast the <em>full</em> optimization of a superagent&mdash;just that we can envision <em>strict improvements</em> on the present world, even if they prove not to be <em>maximal</em>.</p>\n<p>(3) There's a severe problem in which people, due to <a href=\"/lw/st/anthropomorphic_optimism/\">anthropomorphic optimism</a> and the lack of specific reflective knowledge about their <a href=\"/lw/ta/invisible_frameworks/\">invisible background framework</a> and many other biases which I have discussed, think of a \"nonhuman future\" and just subtract off a few aspects of humanity that are salient, like enjoying the taste of peanut butter or something.&nbsp; While still envisioning a future filled with minds that have aesthetic sensibilities, experience happiness on fulfilling a task, get bored with doing the same thing repeatedly, etcetera.&nbsp; These things seem <em>universal,</em> rather than <em>specifically human</em>&mdash;to a human, that is.&nbsp; They don't involve having ten fingers or two eyes, so they must be universal, right?</p>\n<p>And if you're still in this frame of mind&mdash;where \"real values\" are the ones that <a href=\"/lw/sn/interpersonal_morality/\">persuade</a> <a href=\"/lw/rn/no_universally_compelling_arguments/\">every possible mind</a>, and the rest is just some extra specifically human stuff&mdash;then Friendly AI will seem unnecessary to you, because, in its absence, you expect the universe to be <em>valuable</em> but not <em>human.</em></p>\n<p>It turns out, though, that once you start talking about what specifically is and isn't <em>valuable</em>, even if you try to keep yourself sounding as \"non-human\" as possible&mdash;then you still end up with a big complicated computation that is only instantiated physically in human brains and nowhere else in the universe.&nbsp; Complex challenges?&nbsp; Novelty?&nbsp; Individualism?&nbsp; Self-awareness?&nbsp; Experienced happiness?&nbsp; A paperclip maximizer cares not about these things.</p>\n<p>It is a long project to crack people's brains loose of thinking that things will turn out regardless&mdash;that they can subtract off a few specifically human-seeming things, and then end up with plenty of other things they care about that are universal and will appeal to arbitrarily constructed AIs.&nbsp; And of this I have said a very great deal already.&nbsp; But it does not seem to be enough.&nbsp; So Fun Theory is one more step&mdash;taking the curtains off some of the invisible background of our values, and revealing some of the complex criteria that go into a life worth living.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/xw/higher_purpose/\">Higher Purpose</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/xp/seduced_by_imagination/\">Seduced by Imagination</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sSNtcEQsqHgN8ZmRF": 5}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4o3zwgofFPLutkqvd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 21, "extendedScore": null, "score": 3.7e-05, "legacy": true, "legacyId": "1200", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yKXKcyoBzWtECzXrE", "XrzQW69HpidzvBxGr", "CqyJzDZWvGhhFJ7dY", "u6JzcFtPGiznFgDxP", "sYgv4eYH82JEsTD34", "RcZeZt8cPk48xxiQ8", "sCs48JtMnQwQsZwyN", "7HDtecu4qW9PCsSR6", "PtoQdG7E8MxYJrigu", "K4aGvLnHvYgX9pZHS", "5Pjq3mxuiXu2Ys3gM", "88BpRQah9c2GWY3En"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-04T03:55:49.000Z", "modifiedAt": null, "url": null, "title": "Growing Up is Hard", "slug": "growing-up-is-hard", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:02.553Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EQkELCGiGQwvrrp3L/growing-up-is-hard", "pageUrlRelative": "/posts/EQkELCGiGQwvrrp3L/growing-up-is-hard", "linkUrl": "https://www.lesswrong.com/posts/EQkELCGiGQwvrrp3L/growing-up-is-hard", "postedAtFormatted": "Sunday, January 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Growing%20Up%20is%20Hard&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGrowing%20Up%20is%20Hard%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEQkELCGiGQwvrrp3L%2Fgrowing-up-is-hard%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Growing%20Up%20is%20Hard%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEQkELCGiGQwvrrp3L%2Fgrowing-up-is-hard", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEQkELCGiGQwvrrp3L%2Fgrowing-up-is-hard", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2073, "htmlBody": "<p>Terrence Deacon's <em>The Symbolic Species</em> is the best book I've ever read on the evolution of intelligence.&nbsp; Deacon somewhat overreaches when he tries to theorize about what our X-factor <em>is;</em> but his exposition of its <em>evolution</em> is first-class.</p>\n<p>Deacon makes an excellent case&mdash;he has quite persuaded me&mdash;that the increased <em>relative </em>size of our frontal cortex, compared to other hominids, is of overwhelming importance in understanding the evolutionary development of humanity.&nbsp; It's not just a question of increased computing capacity, like adding extra processors onto a cluster; it's a question of what kind of signals dominate, in the brain.</p>\n<p>People with Williams Syndrome (caused by deletion of a certain region on chromosome 7) are hypersocial, ultra-gregarious; as children they fail to show a normal fear of adult strangers.&nbsp; WSers are cognitively impaired on most dimensions, but their verbal abilities are spared or even exaggerated; they often speak early, with complex sentences and large vocabulary, and excellent verbal recall, even if they can never learn to do basic arithmetic.</p>\n<p>Deacon makes a case for some Williams Syndrome symptoms coming from a frontal cortex that is <em>relatively too large</em> for a human, with the result that prefrontal signals&mdash;including certain social emotions&mdash;dominate more than they should.</p>\n<p><a id=\"more\"></a></p>\n<p>\"Both postmortem analysis and MRI analysis have revealed brains with a reduction of the entire posterior cerebral cortex, but a sparing of the cerebellum and frontal lobes, and perhaps even an exaggeration of cerebellar size,\" says Deacon.</p>\n<p>Williams Syndrome's deficits can be explained by the shrunken posterior cortex&mdash;they can't solve simple problems involving shapes, because the parietal cortex, which handles shape-processing, is diminished.&nbsp; But the frontal cortex is not actually <em>enlarged;</em> it is simply <em>spared.</em>&nbsp; So where do WSers' <em>augmented </em>verbal abilities come from?</p>\n<p>Perhaps because the signals sent out by the frontal cortex, saying \"pay attention to this verbal stuff!\", <em>win out</em> over signals coming from the shrunken sections of the brain.&nbsp; So the verbal abilities get lots of exercise&mdash;and other abilities don't.</p>\n<p>Similarly with the hyper-gregarious nature of WSers; the signal saying \"Pay attention to this person!\", originating in the frontal areas where social processing gets done, dominates the emotional landscape.</p>\n<p>And Williams Syndrome is not frontal <em>enlargement,</em> remember; it's just frontal <em>sparing</em> in an otherwise shrunken brain, which increases the <em>relative</em> force of frontal signals...</p>\n<p>...beyond the <em>narrow </em>parameters within which a human brain is adapted to work.</p>\n<p>I mention this because you might look at the history of human evolution, and think to yourself, \"Hm... to get from a chimpanzee to a human... you enlarge the frontal cortex... so if we enlarge it <em>even further...</em>\"</p>\n<p>The road to +Human is not that simple.</p>\n<p>Hominid brains have been tested billions of times over through thousands of generations.&nbsp; But you shouldn't reason <a href=\"/lw/ti/qualitative_strategies_of_friendliness/\">qualitatively</a>, \"Testing creates 'robustness', so now the human brain must be 'extremely robust'.\"&nbsp; Sure, we can expect the human brain to be robust against <em>some</em> insults, like the loss of a single neuron.&nbsp; But testing in an evolutionary paradigm only creates robustness over the domain tested.&nbsp; Yes, <em>sometimes </em>you get robustness beyond that, because sometimes evolution finds simple solutions that prove to generalize&mdash;</p>\n<p>But people do go crazy.&nbsp; Not colloquial crazy, actual crazy.&nbsp; Some ordinary young man in college suddenly decides that everyone around them is <em>staring </em>at them because they're part of the <em>conspiracy</em>.&nbsp; (I saw that happen once, and made a classic non-Bayesian mistake; I knew that this was archetypal schizophrenic behavior, but I didn't realize that similar symptoms can arise from many other causes.&nbsp; Psychosis, it turns out, is a general failure mode, \"the fever of CNS illnesses\"; it can also be caused by drugs, brain tumors, or just sleep deprivation.&nbsp; I saw the perfect fit to what I'd read of schizophrenia, and didn't ask \"<a href=\"/lw/iw/positive_bias_look_into_the_dark/\">What if other things fit just as perfectly?</a>\"&nbsp; So my snap diagnosis of schizophrenia turned out to be wrong; but as I wasn't foolish enough to try to handle the case myself, things turned out all right in the end.)</p>\n<p>Wikipedia says that the current main hypotheses being considered for psychosis are (a) too much dopamine in one place (b) not enough glutamate somewhere else.&nbsp; (I thought I remembered hearing about serotonin imbalances, but maybe that was something else.)</p>\n<p>That's how <em>robust </em>the human brain is: a gentle little neurotransmitter imbalance&mdash;so subtle they're still having trouble tracking it down after who knows how many fMRI studies&mdash;can give you a full-blown case of stark raving mad.</p>\n<p>I don't know how often psychosis happens to hunter-gatherers, so maybe it has something to do with a modern diet?&nbsp; We're not getting exactly the right ratio of Omega 6 to Omega 3 fats, or we're eating too much processed sugar, or something.&nbsp; And among the many other things that go haywire with the metabolism as a result, the brain moves into a more fragile state that breaks down more easily...</p>\n<p>Or whatever.&nbsp; That's just a random hypothesis.&nbsp; By which I mean to say:&nbsp; The brain really <em>is</em> adapted to a very narrow range of operating parameters.&nbsp; It doesn't tolerate a little too much dopamine, just as your metabolism isn't very robust against non-ancestral ratios of Omega 6 to Omega 3.&nbsp; Yes, <em>sometimes </em>you get bonus robustness in a new domain, when evolution solves W, X, and Y using a compact adaptation that also extends to novel Z.&nbsp; Other times... quite often, really... Z just isn't covered.</p>\n<p>Often, you step outside the box of the ancestral parameter ranges, and things just plain break.</p>\n<p>Every part of your brain assumes that all the other surrounding parts work a certain way.&nbsp; The present brain is the Environment of Evolutionary Adaptedness for every individual piece of the present brain.</p>\n<p>Start modifying the pieces in ways that seem like \"good ideas\"&mdash;making the frontal cortex larger, for example&mdash;and you start operating outside the ancestral box of parameter ranges.&nbsp; And then everything goes to hell.&nbsp; Why <em>shouldn't</em> it?&nbsp; Why would the brain be designed for easy upgradability?</p>\n<p>Even if one change works&mdash;will the second?&nbsp; Will the third?&nbsp; Will all four changes work well together?&nbsp; Will the fifth change have all that greater a probability of breaking something, because you're already operating that much further outside the ancestral box?&nbsp; Will the sixth change prove that you exhausted all the brain's robustness in tolerating the changes you made already, and now there's no adaptivity left?</p>\n<p>Poetry aside, a human being <em>isn't</em> the seed of a god.&nbsp; We don't have neat little dials that you can easily tweak to more \"advanced\" settings.&nbsp; We are <em>not</em> designed for our parts to be upgraded.&nbsp; Our parts are adapted to work exactly as they are, in their current context, every part tested in a regime of the other parts being the way they are.&nbsp; <a href=\"/lw/kt/evolutions_are_stupid_but_work_anyway/\">Idiot evolution</a> does not look ahead, it does not design with the intent of different future uses.&nbsp; We are <em>not</em> designed to unfold into something bigger<em>.<br /></em></p>\n<p>Which is not to say that it could never, ever be done.</p>\n<p>You could build a modular, cleanly designed AI that could make a billion sequential upgrades to itself using deterministic guarantees of correctness.&nbsp; A Friendly AI programmer could do even more arcane things to make sure the AI knew what you would-want if you understood the possibilities.&nbsp; And then the AI could apply superior intelligence to untangle the pattern of all those neurons (<a href=\"/lw/x4/nonperson_predicates/\">without simulating you in such fine detail as to create a new person</a>), and to foresee the consequences of its acts, and to understand the meaning of those consequences under your values.&nbsp; And the AI could upgrade one thing while simultaneously tweaking the five things that depend on it and the twenty things that depend on them.&nbsp; Finding a gradual, incremental path to greater intelligence (so as not to effectively erase you and replace you with someone else) that didn't drive you psychotic or give you Williams Syndrome or a hundred other syndromes.</p>\n<p>Or you could walk the path of unassisted human enhancement, trying to make changes to yourself <em>without</em> understanding them fully.&nbsp; Sometimes changing yourself the wrong way, and being murdered or suspended to disk, and replaced by an earlier backup.&nbsp; Racing against the clock, trying to raise your intelligence without breaking your brain or mutating your will.&nbsp; Hoping you became sufficiently super-smart that you could improve the skill with which you modified yourself.&nbsp; Before your hacked brain moved so far outside ancestral parameters and tolerated so many insults that its fragility reached a limit, and you fell to pieces with every new attempted modification beyond that.&nbsp; Death is far from the worst risk here.&nbsp; Not every form of madness will appear immediately when you branch yourself for testing&mdash;some insanities might incubate for a while before they became visible.&nbsp; And you might not notice if your goals shifted only a bit at a time, as your emotional balance altered with the strange new harmonies of your brain.</p>\n<p>Each path has its little upsides and downsides.&nbsp; (E.g:&nbsp; AI requires supreme precise knowledge; human upgrading has a nonzero probability of success through trial and error.&nbsp; Malfunctioning AIs mostly kill you and tile the galaxy with smiley faces; human upgrading might produce insane gods to rule over you in Hell forever.&nbsp; Or so my current understanding would predict, anyway; it's not like I've observed any of this as a fact.)</p>\n<p>And I'm sorry to dismiss such a gigantic dilemma with three paragraphs, but it wanders from the point of today's post:</p>\n<p>The point of today's post is that growing up&mdash;or even deciding what you want to be when you grow up&mdash;is as around <a href=\"/lw/x7/cant_unbirth_a_child/\">as hard as designing a new intelligent species</a>.&nbsp; Harder, since you're constrained to start from the base of an existing design.&nbsp; There is no <em>natural</em> path laid out to godhood, no Level attribute that you can neatly increment and watch everything else fall into place.&nbsp; It is an <a href=\"/lw/uk/beyond_the_reach_of_god/\">adult problem</a>.</p>\n<p>Being a transhumanist means <em>wanting</em> certain things&mdash;judging them to be good.&nbsp; It doesn't mean you think those goals are easy to achieve.</p>\n<p>Just as there's a wide range of understanding among people who talk about, say, quantum mechanics, there's also a certain range of competence among transhumanists.&nbsp; There are transhumanists who fall into the trap of the <a href=\"/lw/lg/the_affect_heuristic/\">affect heuristic</a>, who see the potential benefit of a technology, and therefore <em>feel really good</em> about that technology, so that it also seems that the technology (a) has readily managed downsides (b) is easy to implement well and (c) will arrive relatively soon.</p>\n<p>But only the <em>most </em>formidable adherents of an idea are any sign of its strength.&nbsp; Ten thousand New Agers babbling nonsense, do not cast the least shadow on real quantum mechanics.&nbsp; And among the more formidable transhumanists, it is not at all rare to find someone who wants something <em>and </em>thinks it will not be easy to get.</p>\n<p>One is much more likely to find, say, Nick Bostrom&mdash;that is, Dr. Nick Bostrom, Director of the Oxford Future of Humanity Institute and founding Chair of the World Transhumanist Assocation&mdash;arguing that <a href=\"http://www.nickbostrom.com/evolution.pdf\">a possible test for whether a cognitive enhancement is</a> likely to have downsides, is the ease with which it <em>could</em> have occurred as a natural mutation&mdash;since if it had only upsides and could easily occur as a natural mutation, why hasn't the brain already adapted accordingly?&nbsp; This is one reason to be wary of, say, cholinergic memory enhancers: if they have no downsides, why doesn't the brain produce more acetylcholine already?&nbsp; Maybe you're using up a limited memory capacity, or forgetting something else...</p>\n<p>And that may or may not turn out to be a good heuristic.&nbsp; But the point is that the serious, smart, technically minded transhumanists, do not always expect that the road to everything they want is easy.&nbsp; (Where you want to be wary of people who say, \"But I dutifully acknowledge that there are obstacles!\" but stay in basically the same mindset of <a href=\"/lw/ib/the_proper_use_of_doubt/\">never truly doubting</a> the victory.)</p>\n<p>So you'll forgive me if I am somewhat annoyed with people who run around saying, \"I'd like to be a hundred times as smart!\" as if it were as simple as scaling up a hundred times instead of requiring a whole new cognitive architecture; and as if a change of that magnitude in one shot wouldn't amount to erasure and replacement.&nbsp; Or asking, \"Hey, <em>why not just</em> augment humans instead of building AI?\" as if it wouldn't be a desperate race against madness.</p>\n<p>I'm not against being smarter.&nbsp; I'm not against augmenting humans.&nbsp; I am still a transhumanist; I still judge that these are good goals.</p>\n<p>But it's really not that <em>simple</em>, okay?</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/xe/changing_emotions/\">Changing Emotions</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/xu/failed_utopia_42/\">Failed Utopia #4-2</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ookMdjJQMopfLG3wZ": 2, "3uE2pXvbcnS9nnZRE": 2, "nZCb9BSnmXZXSNA2u": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EQkELCGiGQwvrrp3L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 43, "extendedScore": null, "score": 6.5e-05, "legacy": true, "legacyId": "1201", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AWaJvBMb9HGBwtNqd", "rmAbiEKQDpDnZzcRf", "jAToJHtg39AMTAuJo", "wqDRRx9RqwKLzWt7R", "gb6zWstjmkYHLrbrg", "sYgv4eYH82JEsTD34", "Kow8xRzpfkoY7pa69", "43PTNr4ZMaezyAJ5o", "K4aGvLnHvYgX9pZHS", "QZs4vkC7cbyjL9XA9", "ctpkTaqTKbmm6uRgC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-05T00:05:14.000Z", "modifiedAt": null, "url": null, "title": "Changing Emotions", "slug": "changing-emotions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:37.709Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QZs4vkC7cbyjL9XA9/changing-emotions", "pageUrlRelative": "/posts/QZs4vkC7cbyjL9XA9/changing-emotions", "linkUrl": "https://www.lesswrong.com/posts/QZs4vkC7cbyjL9XA9/changing-emotions", "postedAtFormatted": "Monday, January 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Changing%20Emotions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChanging%20Emotions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQZs4vkC7cbyjL9XA9%2Fchanging-emotions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Changing%20Emotions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQZs4vkC7cbyjL9XA9%2Fchanging-emotions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQZs4vkC7cbyjL9XA9%2Fchanging-emotions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2026, "htmlBody": "<p><strong>Previously in series:&nbsp; </strong><a href=\"/lw/xd/growing_up_is_hard/\">Growing Up is Hard</a></p>\n<p style=\"margin-left: 40px; margin-right: 40px;\"><em>&nbsp;&nbsp;&nbsp; Lest anyone reading this journal of a primitive man should think we spend our time mired in abstractions, let me also say that I am discovering the </em>richness <em>available to those who are willing to alter their major characteristics.&nbsp; The variety of emotions available to a reconfigured human mind, thinking thoughts impossible to its ancestors...<br />&nbsp;&nbsp;&nbsp; The emotion of -*-, describable only as something between sexual love and the joy of intellection&mdash;making love to a thought?&nbsp; Or &amp;&amp;, the true reverse of pain, not \"pleasure\" but a \"warning\" of healing, growth and change. Or (^+^), the most complex emotion yet discovered, felt by those who consciously endure the change between mind configurations, and experience the broad spectrum of possibilities inherent in thinking and being.</em><br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &mdash;Greg Bear, <em>Eon</em></p>\n<p>So... I'm basically on board with that sort of thing as a fine and desirable future<em>.</em>&nbsp; But I think that the <em>difficulty</em> and <em>danger</em> of fiddling with emotions is oft-underestimated.&nbsp; Not necessarily underestimated by Greg Bear, <em>per se;</em> the above journal entry is from a character who was receiving superintelligent help.</p>\n<p>But I still remember one time on the Extropians mailing list when someone talked about creating a female yet \"otherwise identical\" copy of himself.&nbsp; Something about that just fell on my camel's back as the last straw.&nbsp; I'm sorry, but there are some things that are much more complicated to <em>actually do</em> than to rattle off as short English phrases, and \"changing sex\" has to rank very high on that list.&nbsp; Even if you're omnipotent so far as raw ability goes, it's not like people have a binary attribute reading \"M\" or \"F\" that can be flipped as a primitive action.</p>\n<p>Changing sex makes a good, vivid example of the sort of difficulties you might run into when messing with emotional architecture, so I'll use it as my archetype:</p>\n<p><a id=\"more\"></a></p>\n<p>Let's suppose that we're talking about an M2F transformation.&nbsp; (F2M should be a straightforward transform of this discussion; I do want to be specific rather than talking in vague generalities, but I don't want to parallelize every sentence.)&nbsp; (Oddly enough, every time I can recall hearing someone say \"I want to know what it's like to be the opposite sex\", the speaker has been male.&nbsp; I don't know if that's a genuine gender difference in wishes, or just a selection effect in which spoken wishes reach my ears.)</p>\n<p>Want to spend a week wearing a female body?&nbsp; Even at this very shallow level, we're dealing with drastic remappings of at least some segments of the sensorimotor cortex and cerebellum&mdash;the somatic map, the motor map, the motor reflexes, and the motor skills.&nbsp; As a male, you know how to operate a male body, but not a female one.&nbsp; If you're a master martial artist as a male, you won't be a master martial artist as a female (or vice versa, of course) unless you either spend another year practicing, or some AI subtly tweaks your skills to be what they would have been in a female body&mdash;think of how odd <em>that</em> experience would be.</p>\n<p>Already we're talking about some pretty significant neurological changes.&nbsp; Strong enough to disrupt personal identity, if taken in one shot?&nbsp; That's a <a href=\"/lw/og/wrong_questions/\">difficult question</a> to answer, especially since I don't know what experiment to perform to test any hypotheses.&nbsp; On one hand, billions of neurons in my visual cortex undergo massive changes of activation every time my eyes squeeze shut when I sneeze&mdash;the raw number of flipped bits is not the key thing in personal identity.&nbsp; But we <em>are </em>already talking about serious changes of information, on the order of going to sleep, dreaming, forgetting your dreams, and waking up the next morning as though it were the next moment.&nbsp; <em>Not </em>informationally <a href=\"/lw/r9/quantum_mechanics_and_personal_identity/\">trivial transforms like uploading</a>.</p>\n<p>What about sex?&nbsp; (Somehow it's always about sex, at least when it's men asking the question.)&nbsp; Remapping the connections from the remapped somatic areas to the pleasure center will... give you a vagina-shaped penis, more or less.&nbsp; That doesn't make you a woman.&nbsp; You'd still be attracted to girls, and no, that would not make you a lesbian; it would make you a normal, masculine man wearing a female body like a suit of clothing.</p>\n<p>What would it take for a man to actually <em>become</em> the female version of themselves?</p>\n<p>Well... what does that sentence even <em>mean?</em>&nbsp; I am reminded of someone who replied to the statement \"Obama would not have become President if he hadn't been black\" by saying \"If Obama hadn't been black, he wouldn't have been Obama\" i.e. \"There is no non-black Obama who could fail to become President\".&nbsp; (You know you're in trouble when non-actual possible worlds start having political implications.)</p>\n<p>The person you would have been if you'd been born with an X chromosome in place of your Y chromosome (or vice versa) isn't <em>you.</em>&nbsp; If you had a twin female sister, the two of you would not be the same person.&nbsp; There are genes on your Y chromosome that tweaked your brain to some extent, helping to construct your personal identity&mdash;alleles with <em>no analogue</em> on the X chromosome.&nbsp; There is no version of <em>you</em>, even genetically, who is the opposite sex.</p>\n<p>And if we halt your body, swap out your Y chromosome for your father's X chromosome, and restart your body... well.&nbsp; That doesn't sound too safe, does it?&nbsp; Your neurons are already wired in a male pattern, just as your body already developed in a male pattern.&nbsp; I don't know <em>what</em> happens to your testicles, and I don't know what happens to your brain, either.&nbsp; Maybe your circuits would slowly start to rewire themselves under the influence of the new genetic instructions.&nbsp; At best you'd end up as a half-baked cross between male brain and female brain.&nbsp; At worst you'd go into a permanent epileptic fit and die&mdash;we're dealing with <a href=\"/lw/xd/growing_up_is_hard/\">circumstances way outside the evolutionary context under which the brain was optimized for robustness</a>.&nbsp; Either way, your brain would not look like your twin sister's brain that had developed as female from the beginning.</p>\n<p>So to actually <em>become female...</em></p>\n<p>We're talking about a <em>massive</em> transformation here, billions of neurons and trillions of synapses rearranged.&nbsp; Not just form, but content&mdash;just like a male judo expert would need skills repatterned to become a female judo expert, so too, you know how to operate a male brain but not a female brain.&nbsp; You are the equivalent of a judo expert at one, but not the other.&nbsp; You have <em>cognitive</em> reflexes, and consciously learned cognitive skills as well.</p>\n<p>If I fell asleep and woke up as a true woman&mdash;not in body, but in brain&mdash;I don't think I'd call her \"me\".&nbsp; The change is too sharp, if it happens all at once.</p>\n<p>Transform the brain gradually?&nbsp; Hm... now we have to design the <em>intermediate stages,</em> and make sure the intermediate stages make self-consistent sense.&nbsp; Evolution built and optimized a self-consistent male brain and a self-consistent female brain; it didn't design the parts to be stable during an intermediate transition between the two.&nbsp; Maybe you've got to redesign other parts of the brain just to keep working through the transition.</p>\n<p>What happens when, as a woman, you think back to your memory of looking at Angelina Jolie photos as a man?&nbsp; How do you <em>empathize </em>with your <em>past self</em> of the opposite sex?&nbsp; Do you flee in horror from the person you were?&nbsp; Are all your life's memories distant and alien things?&nbsp; How can you <em>remember</em>, when your memory is a recorded activation pattern for neural circuits that no longer exist in their old forms?&nbsp; Do we rewrite all your memories, too?</p>\n<p>Well... maybe we could <em>retain</em> your old male brainware through the transformation, and set up a <em>dual</em> system of male and female circuits... such that you are currently female, but retain the ability to recall and empathize with your past memories as if they were running on the same male brainware that originally laid them down...</p>\n<p>Sounds complicated, doesn't it?&nbsp; It seems that to transform a male brain into someone who can be a real female, we can't just rewrite you as a female brain.&nbsp; That just kills you and replaces you with someone re-imagined as a different person.&nbsp; Instead we have to rewrite you as a more complex brain with a novel, non-ancestral architecture that can cross-operate in realtime between male and female modes, so that a female can process male memories with a remembered context that includes the male brainware that laid them down.</p>\n<p>To make you female, and yet still you, we have to step outside the human design space in order to preserve continuity with your male self.</p>\n<p>And when your little adventure is over and you go back to being a man&mdash;<em>if</em> you still want to, because even if your past self wanted to go back afterward, why should that desire be binding on your present self?&mdash;then we've got to <em>keep</em> the dual architecture so you don't throw up every time you remember what you did on your vacation.</p>\n<p>Assuming you <em>did </em>have sex as a woman, rather than fending off all comers because because they didn't look like they were interested in a long-term relationship.</p>\n<p>But then, you probably <em>would </em>experiment.&nbsp; You'll never have been a little girl, and you won't remember going through high school where any girl who slept with a boy was called a slut by the other girls.&nbsp; You'll remember a <em>very atypical past</em> for a woman&mdash;but there's no way to fix <em>that </em>while keeping you the same person.</p>\n<p>And all that was just what it takes to ranma around <em>within</em> human-space, from the male pole to the female pole and back again.</p>\n<p>What if you wanted to move outside the human space entirely?</p>\n<p>In one sense, a sex change is admittedly close to a worst-case scenario: a fixed target <em>not</em> optimized for an easy transition from your present location; involving, not just <em>new</em> brain areas, but massive coordinated <em>changes </em>to brain areas already in place.</p>\n<p>It might be a lot easier to just add one more emotion to those already there.&nbsp; Maybe.</p>\n<p>In another sense, though, a sex change is close to a best-case scenario: the prototype of your destination is already extensively tested as a coherent mind, and known to function well within a human society that already has a place for it (including companions to talk to).</p>\n<p>It might be a lot harder to enter uncharted territory.&nbsp; Maybe.</p>\n<p>I'm not saying&mdash;of course&mdash;that it could never, ever be done.&nbsp; But it's another instance of the great chicken-and-egg dilemma that is the whole story of present-day humanity, the great challenge that intelligent life faces in its flowering: growing up is a grownup-level problem.&nbsp; You could try to build a cleanly-designed artificial grownup (self-improving Friendly AI) to foresee the pathway ahead and chart out a nonfatal course.&nbsp; Or you could plunge ahead yourself, and hope that you grew faster than your problems did.</p>\n<p>It's the same core challenge either way: growing up is an adult problem.&nbsp; There are difficult ways out of this trap, but no easy ones; <a href=\"/lw/uo/make_an_extraordinary_effort/\">extra-ordinary</a> solutions, but no ordinary ones.&nbsp; People ask me why I take all these difficulties upon myself.&nbsp; It's because all the easier ways, once you examine them in enough fine detail, turn out to be illusions, or contain just as much difficulty themselves&mdash;the same sort of hidden difficulty as \"I'd like to try being the opposite sex for a week\".</p>\n<p>It seems to me that there is just an irreducible residue of very hard problems associated with an adult version of humankind ever coming into being.</p>\n<p>And emotions would be among the most dangerous targets of meddling.&nbsp; Make the wrong shift, and you won't <em>want</em> to change back.</p>\n<p>We can't keep these exact human emotions forever.&nbsp; Anyone want to still want to eat chocolate-chip cookies when the last sun grows cold?&nbsp; I didn't think so.</p>\n<p>But if we replace our emotions with random die-rolls, then we'll end up <a href=\"/lw/sy/sorting_pebbles_into_correct_heaps/\">wanting to do what is prime, instead of what's right</a>.</p>\n<p><span style=\"font-style: italic;\">S</span><em>ome</em> emotional changes can be desirable, but random replacement seems likely to be undesirable on average.&nbsp; So there must be <a href=\"/lw/vm/lawful_creativity/\">criteria that distinguish</a> good emotional changes from bad emotional changes.&nbsp; What are they?</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/xg/emotional_involvement/\">Emotional Involvement</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/xd/growing_up_is_hard/\">Growing Up is Hard</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"W9aNkPwtPhMrcfgj7": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QZs4vkC7cbyjL9XA9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 29, "extendedScore": null, "score": 4.6e-05, "legacy": true, "legacyId": "1202", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EQkELCGiGQwvrrp3L", "XzrqkhfwtiSDgKoAF", "7HMSBiEiCfLKzd2gc", "GuEsfTpSDSbXFiseH", "mMBTPTjRbsrqbSkZE", "KKLQp934n77cfZpPn", "K4aGvLnHvYgX9pZHS", "ZmDEbiEeXk3Wv2sLH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-06T02:45:08.000Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes 21", "slug": "rationality-quotes-21", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:43.484Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HD93frfJaYG6kMwCt/rationality-quotes-21", "pageUrlRelative": "/posts/HD93frfJaYG6kMwCt/rationality-quotes-21", "linkUrl": "https://www.lesswrong.com/posts/HD93frfJaYG6kMwCt/rationality-quotes-21", "postedAtFormatted": "Tuesday, January 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%2021&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%2021%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHD93frfJaYG6kMwCt%2Frationality-quotes-21%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%2021%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHD93frfJaYG6kMwCt%2Frationality-quotes-21", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHD93frfJaYG6kMwCt%2Frationality-quotes-21", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 344, "htmlBody": "<p>\"The most dangerous thing in the world is finding someone you agree with.&nbsp; If a TV station news is saying exactly what you think is right, BEWARE!&nbsp; You are very likely only reinforcing your beliefs, and not being supplied with new information.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <a href=\"http://politics.slashdot.org/comments.pl?sid=175076&amp;cid=14557549\">SmallFurryCreature </a></p>\n<p>\"Companies deciding which kind of toothpaste to market have much more rigorous, established decision-making processes to refer to than the most senior officials of the U.S. government deciding whether or not to go to war.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <a href=\"http://www.theatlantic.com/doc/print/200412/fallows\">Michael Mazarr </a></p>\n<p>\"Everything I ever thought about myself - who I was, what I am - was a lie.&nbsp; You have no idea how astonishingly liberating that feels.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Neil Gaiman, <em>Stardust </em></p>\n<p>\"Saul speaks of the 'intense desire for survival on the part of virtually everyone on earth,' and our 'failure' in spite of this.&nbsp; I have often pointed out that the so-called 'survival instinct' is reliable only in clear and present danger - and even then only if the individual is still relatively healthy and vigorous.&nbsp; If the danger is indirect, or remote in time, or if the person is weak or depressed - or even if required action would violate established habits - forget the 'survival instinct.' It isn't that simple.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <a href=\"http://www.quantium.plus.com/lr/lr65.htm\">Robert Ettinger</a> on cryonics</p>\n<p>\"We are running out of excuses.&nbsp; We just have to admit that real AI is one of the lowest research priorities, ever.&nbsp; Even space is considered more important.&nbsp; Nevermind baseball, tribology, or TV evangelism.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Eugen Leitl</p>\n<p>\"That's something that's always struck me as odd about humanity.&nbsp; Our first response to someone's bad news is \"I'm sorry\", as though we feel that <em>someone </em>should take responsibility for all the $&amp;#ed up randomness that goes on in this universe.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Angels 2200</p>\n<p>\"We were supremely lucky to be born into this precise moment in history.&nbsp; It is with an ever-cresting crescendo of wonder and enthusiasm that I commend you to the great adventure and the great adventure to you.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Jeff Davis</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HD93frfJaYG6kMwCt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 4.675927654442029e-07, "legacy": true, "legacyId": "1203", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-06T22:23:24.000Z", "modifiedAt": null, "url": null, "title": "Emotional Involvement", "slug": "emotional-involvement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:06.213Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZmDEbiEeXk3Wv2sLH/emotional-involvement", "pageUrlRelative": "/posts/ZmDEbiEeXk3Wv2sLH/emotional-involvement", "linkUrl": "https://www.lesswrong.com/posts/ZmDEbiEeXk3Wv2sLH/emotional-involvement", "postedAtFormatted": "Tuesday, January 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Emotional%20Involvement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEmotional%20Involvement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZmDEbiEeXk3Wv2sLH%2Femotional-involvement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Emotional%20Involvement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZmDEbiEeXk3Wv2sLH%2Femotional-involvement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZmDEbiEeXk3Wv2sLH%2Femotional-involvement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1738, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/l1/evolutionary_psychology/\">Evolutionary Psychology</a>, <a href=\"/lw/l3/thou_art_godshatter/\">Thou Art Godshatter</a>, <a href=\"/lw/sc/existential_angst_factory/\">Existential Angst Factory</a></p>\n<p>Can your emotions get involved in a video game?&nbsp; Yes, but not much.&nbsp; Whatever sympathetic echo of triumph you experience on destroying the Evil Empire in a video game, it's probably not remotely close to the feeling of triumph you'd get from saving the world in real life.&nbsp; I've played video games powerful enough to bring tears to my eyes, but they still aren't as powerful as the feeling of significantly helping just one single real human being.</p>\n<p>Because when the video game is finished, and you put it away, the events within the game have no long-term consequences.</p>\n<p>Maybe if you had a major epiphany while playing...&nbsp; But even then, only your <em>thoughts </em>would matter; the mere fact that you <em>saved the world</em>, inside the game, wouldn't count toward anything in the continuing story of your life.</p>\n<p>Thus fails the <a href=\"http://www.k-1.com/Orwell/site/work/essays/fun.html\">Utopia</a> of <a href=\"/lw/ww/high_challenge/\">playing lots of really cool video games forever</a>.&nbsp; Even if the games are <a href=\"/lw/ww/high_challenge/\">difficult</a>, <a href=\"/lw/wx/complex_novelty/\">novel</a>, and <a href=\"/lw/wy/sensual_experience/\">sensual</a>, this is still the idiom of life chopped up into a series of disconnected episodes with no lasting consequences.&nbsp; A life in which equality of consequences is forcefully ensured, or in which little is at stake because all desires are instantly fulfilled without individual work&mdash;these likewise will appear as flawed Utopias of dispassion and angst.&nbsp; \"Rich people with nothing to do\" syndrome.&nbsp; A life of disconnected episodes and unimportant consequences is a life of weak passions, of emotional uninvolvement.</p>\n<p>Our emotions, for all the <a href=\"/lw/l1/evolutionary_psychology/\">obvious evolutionary reasons</a>, tend to associate to events that had major reproductive consequences in the ancestral environment, and to invoke the strongest passions for events with the biggest consequences:</p>\n<p>Falling in love... birthing a child... finding food when you're starving... getting wounded... being chased by a tiger... your child being chased by a tiger... finally killing a hated enemy...</p>\n<p><a id=\"more\"></a></p>\n<p>Our life stories are not now, and will not be, what they once were.</p>\n<p>If one is to be conservative in the short run about changing minds, then we can get at least <em>some</em> mileage from changing the environment.&nbsp; A windowless office filled with highly repetitive non-novel challenges isn't any more conducive to emotional involvement than video games; it may be part of real life, but it's a very<em> flat</em> part.&nbsp; The occasional exciting global economic crash that you had no personal control over, does not particularly modify this observation.</p>\n<p>But we don't want to go back to the <em>original</em> savanna, the one where you got a leg chewed off and then starved to death once you couldn't walk.&nbsp; There are things we care about tremendously in the sense of hating them so much that we want to drive their frequency down to zero, not by the most interesting way, just as quickly as possible, whatever the means.&nbsp; If you drive the thing it binds to down to zero, where is the emotion after that?</p>\n<p>And there are emotions we might want to think twice about keeping, in the long run.&nbsp; Does racial prejudice accomplish <em>anything</em> worthwhile?&nbsp; I pick this as a target, not because it's a convenient whipping boy, but because unlike e.g. \"boredom\" it's actually pretty hard to think of a reason transhumans would want to keep this neural circuitry around.&nbsp; Readers who take this as a challenge are strongly advised to remember that <a href=\"http://yudkowsky.net/singularity/simplified\">the point of the question</a> is not to <a href=\"http://www.overcomingbias.com/2008/12/showoff-bias.html\">show off how clever and counterintuitive you can be</a>.</p>\n<p>But if you lose emotions <em>without replacing</em> them, whether by changing minds, or by changing life stories, then the world gets a little less <em>involving</em> each time; there's that much less material for passion.&nbsp; And your mind and your life become that much <em>simpler,</em> perhaps, because there are fewer forces at work&mdash;maybe even threatening to collapse you into an expected pleasure maximizer.&nbsp; <em>If</em> you don't replace what is removed.</p>\n<p>In the long run, if humankind is to make a new life for itself...</p>\n<p>We, and our descendants, will need some <a href=\"/lw/xe/changing_emotions/\">new emotions</a>.</p>\n<p>This is the aspect of self-modification in which one must above all take care&mdash;modifying your goals.&nbsp; Whatever you <em>want,</em> becomes more likely to <em>happen;</em> to ask what we ought to make ourselves want, is to ask what the future should <em>be.</em></p>\n<p>Add emotions at random&mdash;bind positive reinforcers or negative reinforcers to random situations and ways the world could be&mdash;and you'll just end up <a href=\"/lw/sy/sorting_pebbles_into_correct_heaps/\">doing what is prime instead of what is good</a>.&nbsp; So adding a bunch of random emotions does not seem like the way to go.</p>\n<p>Asking what happens <em>often</em>, and binding happy emotions to that, so as to increase happiness&mdash;or asking what seems <em>easy,</em> and binding happy emotions to that&mdash;making isolated video games artificially more <em>emotionally involving,</em> for example&mdash;</p>\n<p>At that point, it seems to me, you've pretty much given up on eudaimonia and moved to maximizing happiness; you might as well <a href=\"/lw/ww/high_challenge/\">replace brains with pleasure centers</a>, and civilizations with hedonium plasma.</p>\n<p>I'd suggest, rather, that one start with the idea of new major events in a transhuman life, and then bind emotions to those major events and the sub-events that surround them.&nbsp; What sort of major events might a transhuman life embrace?&nbsp; Well, this is the point at which I usually stop speculating.&nbsp; \"Science!&nbsp; They should be excited by science!\" is something of a bit-too-obvious and I dare say \"nerdy\" answer, as is \"Math!\" or \"Money!\"&nbsp; (Money is just <a href=\"/lw/vd/intelligence_in_economics/\">our civilization's equivalent of expected utilon balancing</a> anyway.)&nbsp; <em>Creating</em> a child&mdash;as in my favored saying, \"If you can't design an intelligent being from scratch, you're not old enough to have kids\"&mdash;is one candidate for a major transhuman life event, and anything you had to do along the way to creating a child would be a candidate for new emotions.&nbsp; This might or might not have anything to do with sex&mdash;though I find that thought appealing, being something of a traditionalist.&nbsp; All sorts of interpersonal emotions carry over for as far as my own human eyes can see&mdash;the joy of making allies, say; interpersonal emotions get more complex (and challenging) along with the people, which makes them an even richer source of future fun.&nbsp; Falling in love?&nbsp; Well, it's not as if we're trying to construct the Future out of anything other than our preferences&mdash;so do you <em>want</em> that to carry over?</p>\n<p>But again&mdash;this is usually the point at which I stop speculating.&nbsp; It's hard enough to visualize human Eutopias, let alone transhuman ones.</p>\n<p>The essential idiom I'm suggesting is something akin to how evolution gave humans <a href=\"/lw/l3/thou_art_godshatter/\">lots of local reinforcers</a> for things that <em>in the ancestral environment</em> related to evolution's overarching goal of inclusive reproductive fitness.&nbsp; Today, office work might be highly relevant to someone's sustenance, but&mdash;even leaving aside the lack of <a href=\"/lw/ww/high_challenge/\">high challenge</a> and <a href=\"/lw/wx/complex_novelty/\">complex novelty</a>&mdash;and that it's not <a href=\"/lw/wy/sensual_experience/\">sensually involving</a> because we don't have native brainware to support the domain&mdash;office work is not <em>emotionally </em>involving because office work wasn't <em>ancestrally</em> relevant.&nbsp; If office work had been around for millions of years, we'd find it a little less hateful, and experience a little more triumph on filling out a form, one suspects.</p>\n<p>Now you might run away shrieking from the dystopia I've just depicted&mdash;but that's because you don't see office work as eudaimonic in the first place, one suspects.&nbsp; And because of the lack of high challenge and complex novelty involved.&nbsp; In an \"absolute\" sense, office work would seem somewhat <em>less</em> tedious than gathering fruits and eating them.</p>\n<p>But the idea isn't necessarily to have fun doing office work.&nbsp; Just like it's not necessarily the idea to have your emotions activate for video games instead of real life.</p>\n<p>The idea is that once you construct an existence / life story that seems to make sense, then it's all right to bind emotions to the parts of that story, with strength proportional to their long-term impact.&nbsp; The anomie of today's world, where we simultaneously (a) engage in office work and (b) lack any passion in it, does not need to carry over: you should either fix one of those problems, or the other.</p>\n<p>On a higher, more abstract level, this carries over the idiom of <a href=\"/lw/l2/protein_reinforcement_and_dna_consequentialism/\">reinforcement over instrumental correlates of terminal values</a>.&nbsp; In principle, this is something that a purer optimization process wouldn't do.&nbsp; You need neither happiness nor sadness to maximize expected utility.&nbsp; You only need to know which actions result in which consequences, and update that pure probability distribution as you learn through observation; something akin to \"reinforcement\" falls out of this, but without <a href=\"/lw/le/lost_purposes/\">the risk of losing purposes</a>, without any pleasure or pain.&nbsp; An agent like this is simpler than a human and more powerful&mdash;if you think that your emotions give you a supernatural advantage in optimization, you've entirely failed to understand the math of this domain.&nbsp; For a pure optimizer, the \"advantage\" of starting out with one more emotion bound to instrumental events is like being told one more abstract belief about which policies maximize expected utility, except that the belief is very hard to update based on further experience.</p>\n<p>But it does not seem to me, that a mind which <em>has</em> the most value, is the same kind of mind that most <em>efficiently optimizes</em> values outside it.&nbsp; The interior of a true expected utility maximizer might be pretty boring, and I even suspect that you can <a href=\"/lw/x7/cant_unbirth_a_child/\">build them to not be sentient</a>.</p>\n<p>For as far as my human eyes can see, I don't know what kind of mind I <em>should </em>value, if that mind lacks pleasure and happiness and emotion in the everyday events of its life.&nbsp; Bearing in mind that we are constructing this Future using our own preferences, not having it handed to us by some inscrutable external author.</p>\n<p>If there's some better way of <em>being</em> (not just <em>doing</em>) that stands somewhere outside this, I have not yet understood it well enough to <em>prefer </em>it.&nbsp; But if so, then all this discussion of emotion would be as moot as it would be for an expected utility maximizer&mdash;one which was not valued at all for itself, but only valued for that which it maximized.</p>\n<p>It's just hard to see why we would <em>want</em> to become something like that, bearing in mind that morality is not an <a href=\"/lw/ry/is_morality_given/\">inscrutable light</a> handing down <a href=\"/lw/rr/the_moral_void/\">awful edicts</a> from <a href=\"/lw/sb/could_anything_be_right/\">somewhere outside us</a>.</p>\n<p>At any rate&mdash;the hell of a life of disconnected episodes, where your actions don't connect strongly to anything you strongly care about, and nothing that you do all day invokes any passion&mdash;this <a href=\"/lw/sc/existential_angst_factory/\">angst </a>seems avertible, however often it pops up in poorly written Utopias.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/xi/serious_stories/\">Serious Stories</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/xe/changing_emotions/\">Changing Emotions</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sSNtcEQsqHgN8ZmRF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZmDEbiEeXk3Wv2sLH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 25, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": "1204", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["epZLSoNvjW53tqNj9", "cSXZpvqpa9vbGGLtG", "8rdoea3g6QGhWQtmx", "29vqqmGNxNRGzffEj", "aEdqh3KPerBNYvoWe", "eLHCWi8sotQT6CmTX", "QZs4vkC7cbyjL9XA9", "mMBTPTjRbsrqbSkZE", "4uDfpNTrdhEYEb2jm", "gTNB9CQd5hnbkMxAG", "sP2Hg6uPwpfp3jZJN", "gb6zWstjmkYHLrbrg", "iQNKfYb7aRYopojTX", "K9JSM7d7bLJguMxEp", "vy9nnPdwTjSmt5qdb", "K4aGvLnHvYgX9pZHS", "6qS9q5zHafFXsB6hf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-07T12:00:50.000Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes 22", "slug": "rationality-quotes-22", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:09.564Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qyTCeEikY4cDT96ik/rationality-quotes-22", "pageUrlRelative": "/posts/qyTCeEikY4cDT96ik/rationality-quotes-22", "linkUrl": "https://www.lesswrong.com/posts/qyTCeEikY4cDT96ik/rationality-quotes-22", "postedAtFormatted": "Wednesday, January 7th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%2022&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%2022%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqyTCeEikY4cDT96ik%2Frationality-quotes-22%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%2022%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqyTCeEikY4cDT96ik%2Frationality-quotes-22", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqyTCeEikY4cDT96ik%2Frationality-quotes-22", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 297, "htmlBody": "<p>\"Two roads diverged in the woods.&nbsp; I took the one less traveled, and had to eat bugs until Park rangers rescued me.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Jim Rosenberg</p>\n<p>\"Lying to yourself about specific actions is easier than re-defining the bounds of your imagined identity...&nbsp; When I see once-ethical men devolve into moral grey, they still identify as upstanding.\"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- <a href=\"http://ben.casnocha.com/2008/12/easier-to-deny-or-rationalize-behavior-than-evolve-your-own-identity.html\">Ben Casnocha</a></p>\n<p>\"Every year buy a clean bed sheet, date it, and lay it over the previous layer of junk on your desk.\"<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- Vernor Vinge, <em>The Blabber</em></p>\n<p>\"Like first we tossed out the bath water, then the baby, and like finally the whole tub.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <a href=\"http://www.netfunny.com/rhf/jokes/new90/nullang.html\">John R. Andrews</a></p>\n<p>\"I have no wisdom.&nbsp; Yet I heard a wise man - soon to be a relative of marriage - say not long ago that all is for the best.&nbsp; We are but dreams, and dreams possess no life by their own right.&nbsp; See, I am wounded.&nbsp; <em>(Holds out his hand.)</em>&nbsp; When my wound heals, it will be gone.&nbsp; Should it with its bloody lips say it is sorry to heal?&nbsp; I am only trying to explain what another said, but that is what I think he meant.\"<br /> &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Gene Wolfe, <em>The Claw of the Conciliator </em></p>\n<p>\"On a grand scale we simply want to save the world, so obviously we're just letting ourselves in for a lot of disappointment and we're doomed to failure since we didn't pick some cheap-ass two-bit goal like collecting all the Garbage Pail Kids cards.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <a href=\"http://www.subgenius.com/bigfist/answers/rants/X0044_Subject_Nenslo.html\">Nenslo</a></p>\n<p>\"He promised them nothing but blood, iron, and fire, and offered them only the choice of going to find it or of waiting for it to find them at home.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- John Barnes, <em>One For the Morning Glory</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qyTCeEikY4cDT96ik", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 4.678662093764386e-07, "legacy": true, "legacyId": "1205", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-08T23:49:35.000Z", "modifiedAt": null, "url": null, "title": "Serious Stories", "slug": "serious-stories", "viewCount": null, "lastCommentedAt": "2022-04-13T03:11:55.905Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6qS9q5zHafFXsB6hf/serious-stories", "pageUrlRelative": "/posts/6qS9q5zHafFXsB6hf/serious-stories", "linkUrl": "https://www.lesswrong.com/posts/6qS9q5zHafFXsB6hf/serious-stories", "postedAtFormatted": "Thursday, January 8th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Serious%20Stories&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASerious%20Stories%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6qS9q5zHafFXsB6hf%2Fserious-stories%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Serious%20Stories%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6qS9q5zHafFXsB6hf%2Fserious-stories", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6qS9q5zHafFXsB6hf%2Fserious-stories", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2773, "htmlBody": "<p><strong></strong>Every Utopia ever constructed&mdash;in philosophy, fiction, or religion&mdash;has been, to one degree or another, a place where you wouldn't <em>actually want</em> to live.&nbsp; I am not alone in this <a href=\"/lw/xc/the_uses_of_fun_theory/\">important</a> observation:&nbsp; George Orwell said much the same thing in \"<a href=\"http://www.k-1.com/Orwell/site/work/essays/fun.html\">Why Socialists Don't Believe In Fun</a>\", and I expect that many others said it earlier.</p>\n<p>If you read books on How To Write&mdash;and there are a <em>lot</em> of books out there on How To Write, because amazingly a lot of book-writers think they know something about writing&mdash;these books will tell you that stories must contain \"conflict\".</p>\n<p>That is, the more <em>lukewarm </em>sort of instructional book will tell you that stories contain \"conflict\".&nbsp; But some authors speak more plainly.</p>\n<p>\"Stories are about people's pain.\"&nbsp; Orson Scott Card.</p>\n<p>\"Every scene must end in disaster.\"&nbsp; Jack Bickham.</p>\n<p>In the age of my <a href=\"/lw/ty/my_childhood_death_spiral/\">youthful folly</a>, I took for granted that <em>authors</em> were excused from the search for true Eutopia, because if you constructed a Utopia that <em>wasn't</em> flawed... what stories could you write, set there?&nbsp; \"Once upon a time they lived happily ever after.\"&nbsp; What use would it be for a science-fiction author to try to depict a positive Singularity, when a positive Singularity would be...</p>\n<p>...the end of all stories?</p>\n<p>It seemed like a reasonable framework with which to examine the literary problem of Utopia, but something about that final conclusion produced a quiet, nagging doubt.</p>\n<p><a id=\"more\"></a></p>\n<p>At that time I was thinking of an AI as being something like a safe wish-granting genie for the use of individuals.&nbsp; So the conclusion did make a kind of sense.&nbsp; If there was a problem, you would just wish it away, right?&nbsp; Ergo&mdash;no stories.&nbsp; So I ignored the quiet, nagging doubt.</p>\n<p>Much later, after I concluded that even a safe genie <a href=\"/lw/ww/high_challenge/\">wasn't such a good idea</a>, it also seemed in retrospect that \"no stories\" could have been a productive indicator.&nbsp; On this particular occasion, \"I can't think of a single story I'd <em>want to read</em> about this scenario\", might indeed have pointed me toward the reason \"I wouldn't want to <em>actually live</em> in this scenario\".</p>\n<p>So I swallowed my <a href=\"/lw/lw/reversed_stupidity_is_not_intelligence/\">trained-in revulsion</a> of <a href=\"/lw/k8/how_to_seem_and_be_deep/\">Luddism</a> and <a href=\"/lw/uk/beyond_the_reach_of_god/\">theodicy</a>, and at least <em>tried </em>to contemplate the argument:</p>\n<ul>\n<li>A world in which nothing ever goes wrong, or no one ever experiences any pain or sorrow, is a world containing no stories worth reading about.</li>\n<li>A world that you wouldn't want to read about is a world where you wouldn't want to live.</li>\n<li>Into each eudaimonic life a little pain must fall.&nbsp; QED.</li>\n</ul>\n<p>In one sense, it's clear that we do <em>not</em> want to live the sort of lives that are depicted in most stories that human authors have written so far.&nbsp; Think of the truly great stories, the ones that have become legendary for being the very best of the best of their genre:&nbsp; The <em>Iliiad, Romeo and Juliet, The Godfather, Watchmen, </em><em>Planescape: Torment</em>, the second season of <em>Buffy the Vampire Slayer</em>, or<em> that ending </em>in <em>Tsukihime</em>.&nbsp; Is there a single story on the list that <em>isn't</em> tragic?</p>\n<p>Ordinarily, we prefer pleasure to pain, joy to sadness, and life to death.&nbsp; Yet it seems we prefer to empathize with hurting, sad, dead characters.&nbsp; Or stories about happier people <em>aren't serious, </em>aren't artistically great enough to be worthy of praise&mdash;but then why selectively praise stories containing unhappy people?&nbsp; Is there some hidden benefit to us in it?&nbsp; It's a puzzle either way you look at it.</p>\n<p>When I was a child I couldn't write fiction because I wrote things to go <em>well</em> for my characters&mdash;just like I wanted things to go well in real life.&nbsp; Which I was cured of by Orson Scott Card:&nbsp; <em>Oh,</em> I said to myself, <em>that's what I've been doing wrong, my characters aren't hurting</em>.&nbsp; Even then, I didn't realize that the microstructure of a plot works the same way&mdash;until Jack Bickham said that every scene must end in disaster.&nbsp; Here I'd been trying to set up problems and <em>resolve</em> them, instead of making them <em>worse...</em></p>\n<p>You simply don't <em>optimize </em>a story the way you optimize a real life.&nbsp; The <em>best </em>story and the <em>best </em>life will be produced by different criteria.</p>\n<p>In the real world, people can go on living for quite a while without any major disasters, and still seem to do pretty okay.&nbsp; When was the last time you were shot at by assassins?&nbsp; Quite a while, right?&nbsp; Does your life seem emptier for it?</p>\n<p>But on the other hand...</p>\n<p>For some odd reason, when authors get too old or too successful, they revert to my childhood.&nbsp; Their stories start going <em>right.</em>&nbsp; They stop doing horrible things to their characters, with the result that they start doing horrible things to their readers.&nbsp; It seems to be a regular part of Elder Author Syndrome.&nbsp; Mercedes Lackey, Laurell K. Hamilton, Robert Heinlein, even Orson Scott bloody Card&mdash;they all went that way.&nbsp; They forgot how to hurt their characters.&nbsp; I don't know why.</p>\n<p>And when you read a story by an Elder Author or a pure novice&mdash;a story where things just <em>relentlessly go right</em> one after another&mdash;where the main character defeats the supervillain with a snap of the fingers, or even worse, before the final battle, the supervillain <em>gives up and apologizes and then they're friends again&mdash;</em></p>\n<p>It's like a fingernail scraping on a blackboard at the base of your spine.&nbsp; If you've never actually read a story like that (or worse, written one) then count yourself lucky.</p>\n<p>That fingernail-scraping quality&mdash;would it transfer over from the story to real life, if you tried living real life without a single drop of rain?</p>\n<p>One answer might be that what a story really needs is not \"disaster\", or \"pain\", or even \"conflict\", but simply <em>striving.</em>&nbsp; That the problem with Mary Sue stories is that there's not enough <a href=\"/lw/ww/high_challenge/\">striving</a> in them, but they wouldn't actually need <em>pain</em>.&nbsp; This might, perhaps, be tested.</p>\n<p>An alternative answer might be that this <em>is</em> the transhumanist version of Fun Theory we're talking about.&nbsp; So we can reply, \"Modify brains to eliminate that fingernail-scraping feeling\", unless there's some justification for keeping it.&nbsp; If the fingernail-scraping feeling is a pointless random bug getting in the way of Utopia, delete it.</p>\n<p>Maybe we <em>should</em>.&nbsp; Maybe all the Great Stories are tragedies because... well...</p>\n<p>I once read that in the BDSM community, \"intense sensation\" is a euphemism for pain.&nbsp; Upon reading this, it occurred to me that, the way humans are constructed now, it is just <em>easier </em>to produce pain than pleasure.&nbsp; Though I speak here somewhat outside my experience, I expect that it takes a highly talented and experienced sexual artist working for hours to produce a <em>good </em>feeling as intense as the pain of one strong kick in the testicles&mdash;which is doable in seconds by a novice.</p>\n<p>Investigating the life of the priest and proto-rationalist <a href=\"/lw/ii/conservation_of_expected_evidence/\">Friedrich Spee von Langenfeld</a>, who heard the confessions of accused witches, I looked up some of the instruments that had been used to produce confessions.&nbsp; There is no ordinary way to make a human being feel as <em>good </em>as those instruments would make you hurt.&nbsp; I'm not sure even drugs would do it, though my experience of drugs is as nonexistent as my experience of torture.</p>\n<p>There's something imbalanced about that.</p>\n<p>Yes, human beings are too optimistic in their planning.&nbsp; If losses weren't more aversive than gains, we'd go broke, the way we're constructed now.&nbsp; The experimental rule is that losing a desideratum&mdash;$50, a coffee mug, whatever&mdash;hurts between 2 and 2.5 times as much as the equivalent gain.</p>\n<p>But this is a deeper imbalance than that.&nbsp; The effort-in/intensity-out difference between sex and torture is not a mere factor of 2.</p>\n<p>If someone goes in search of sensation&mdash;in this world, the way human beings are constructed now&mdash;it's not surprising that they should arrive at pains to be mixed into their pleasures as a source of <em>intensity</em> in the combined experience.</p>\n<p>If only people were constructed differently, so that you could produce pleasure as intense and in <a href=\"/lw/wy/sensual_experience/\">as many different flavors</a> as pain!&nbsp; If only you could, with the same ingenuity and effort as a torturer of the Inquisition, make someone feel as <em>good</em> as the Inquisition's victims felt <em>bad</em>&mdash;</p>\n<p>But then, what <em>is</em> the analogous pleasure that feels that good?&nbsp; A victim of skillful torture will do anything to stop the pain and anything to prevent it from being repeated.&nbsp; Is the equivalent pleasure one that overrides everything with the demand to continue and repeat it?&nbsp; If people are stronger-willed to bear the pleasure, is it really the same pleasure?</p>\n<p>There is another rule of writing which states that stories have to <em>shout</em>.&nbsp; A human brain is a long way off those printed letters.&nbsp; Every event and feeling needs to take place at ten times natural volume in order to have any impact at all.&nbsp; You must not try to make your characters behave or feel <em>realistically </em>&mdash;especially, you must not faithfully reproduce your own past experiences&mdash;because <em>without exaggeration</em>, they'll be too quiet to rise from the page.</p>\n<p>Maybe all the Great Stories are tragedies because happiness can't shout loud enough&mdash;to a human reader.</p>\n<p>Maybe that's what needs fixing.</p>\n<p>And if it were fixed... would there be any use left for pain or sorrow?&nbsp; For even the <em>memory </em>of sadness, if all things were already as good as they could be, and every remediable ill already remedied?</p>\n<p><em>Can</em> you just delete pain outright?&nbsp; Or does removing the old floor of the utility function just create a new floor?&nbsp; Will any pleasure less than 10,000,000 hedons be the new unbearable pain?</p>\n<p>Humans, built the way we are now, do seem to have hedonic scaling tendencies.&nbsp; Someone who can remember starving will appreciate a loaf of bread more than someone who's never known anything but cake.&nbsp; This was <a href=\"http://www.k-1.com/Orwell/site/work/essays/fun.html\">George Orwell's hypothesis for why Utopia is impossible</a> in literature and reality:</p>\n<p style=\"margin-left: 40px;\">\"It would seem that human beings are not able to describe, nor perhaps to imagine, happiness except in terms of contrast...&nbsp; The inability of mankind to imagine happiness except in the form of relief, either from effort or pain, presents Socialists with a serious problem. Dickens can describe a poverty-stricken family tucking into a roast goose, and can make them appear happy; on the other hand, the inhabitants of perfect universes seem to have no spontaneous gaiety and are usually somewhat repulsive into the bargain.\"</p>\n<p>For an expected utility maximizer, rescaling the utility function to add a trillion to all outcomes is meaningless&mdash;it's literally the same utility function, as a mathematical object.&nbsp; A utility function describes the <em>relative </em>intervals between outcomes; that's what it is, mathematically speaking.</p>\n<p>But the human brain has distinct neural circuits for positive feedback and negative feedback, and different varieties of positive and negative feedback.&nbsp; There are people today who \"suffer\" from congenital analgesia&mdash;a total absence of pain.&nbsp; I never heard that <em>insufficient pleasure</em> becomes intolerable to them.</p>\n<p>Congenital analgesics do have to inspect themselves carefully and frequently to see if they've cut themselves or burned a finger.&nbsp; Pain serves a purpose in the human mind design...</p>\n<p>But that does not show there's <a href=\"/lw/hu/the_third_alternative/\">no alternative</a> which could serve the same purpose.&nbsp; Could you delete pain and replace it <em>with an urge not to do certain things</em> that lacked the intolerable subjective quality of pain?&nbsp; I do not know all the Law that governs here, but I'd have to guess that yes, you could; you could replace that side of yourself with something more akin to an expected utility maximizer.</p>\n<p>Could you delete the human tendency to scale pleasures&mdash;delete the accomodation, so that each new roast goose is as delightful as the last?&nbsp; I would guess that you could.&nbsp; This verges perilously close to deleting Boredom, which is right up there with Sympathy as an absolute indispensable... but to say that an old solution remains as pleasurable, is not to say that you will lose the urge to seek new and better solutions.</p>\n<p>Can you make every roast goose as pleasurable as it would be in contrast to starvation, without ever having starved?</p>\n<p>Can you prevent the pain of a dust speck irritating your eye from being the new torture, if you've literally <em>never experienced</em> anything <em>worse</em> than a dust speck irritating your eye?</p>\n<p>Such questions begin to exceed my grasp of the Law, but I would guess that the answer is: yes, it can be done.&nbsp; It is my experience in such matters that once you do learn the Law, you can usually see how to do weird-seeming things.</p>\n<p>So far as I know or can guess, David Pearce (<em>The Hedonistic Imperative</em>) is very probably right about the <em>feasibility</em> part, when he says:</p>\n<p style=\"margin-left: 40px;\">\"Nanotechnology and genetic engineering will abolish suffering in all sentient life.&nbsp; The abolitionist project is hugely ambitious but technically feasible.&nbsp; It is also instrumentally rational and morally urgent.&nbsp; The metabolic pathways of pain and malaise evolved because they served the fitness of our genes in the ancestral environment.&nbsp; They will be replaced by a different sort of neural architecture&mdash;a motivational system based on heritable gradients of bliss.&nbsp; States of sublime well-being are destined to become the genetically pre-programmed norm of mental health.&nbsp; It is predicted that the world's last unpleasant experience will be a precisely dateable event.\"</p>\n<p>Is that... what we <em>want?</em></p>\n<p>To just wipe away the last tear, and be done?</p>\n<p>Is there any good reason <em>not </em>to, except status quo bias and a handful of worn rationalizations?</p>\n<p>What would be the <em>alternative?&nbsp; </em>Or alternatives?</p>\n<p>To leave things as they are?&nbsp; Of course not.&nbsp; <a href=\"/lw/uk/beyond_the_reach_of_god/\">No God designed this world</a>; we have no reason to think it exactly optimal on any dimension.&nbsp; If this world does not contain too much pain, then it must not contain enough, and the latter seems unlikely.</p>\n<p>But perhaps...</p>\n<p>You could cut out just the <em>intolerable </em>parts of pain?</p>\n<p>Get rid of the Inquisition.&nbsp; Keep the sort of pain that tells you not to stick your finger in the fire, or the pain that tells you that you shouldn't have put your friend's finger in the fire, or even the pain of breaking up with a lover.</p>\n<p>Try to get rid of the sort of pain that <em>grinds down and destroys</em> a mind.&nbsp; Or configure minds to be harder to damage.</p>\n<p>You could have a world where there were broken legs, or even broken hearts, but no broken <em>people</em>.&nbsp; No child sexual abuse that turns out more abusers.&nbsp; No people ground down by weariness and drudging minor inconvenience to the point where they contemplate suicide.&nbsp; No random meaningless endless sorrows like starvation or AIDS.</p>\n<p>And if even a broken leg still seems too scary&mdash;</p>\n<p>Would we be less frightened of pain, if we were stronger, if our daily lives did not already exhaust so much of our reserves?</p>\n<p>So that would be one alternative to the Pearce's world&mdash;if there are yet other alternatives, I haven't thought them through in any detail.</p>\n<p>The path of courage, you might call it&mdash;the idea being that if you eliminate the destroying kind of pain and strengthen the people, then what's left shouldn't be <em>that</em> scary.</p>\n<p>A world where there is sorrow, but not massive systematic <em>pointless </em>sorrow, like we see on the evening news.&nbsp; A world where pain, if it is not eliminated, at least does not <em>overbalance pleasure</em>.&nbsp; You could write stories about that world, and they could read our stories.</p>\n<p>I do tend to be rather conservative around the notion of deleting large parts of human nature.&nbsp; I'm not sure how many major chunks you can delete until that balanced, conflicting, dynamic structure collapses into something simpler, like an expected pleasure maximizer.</p>\n<p>And so I do admit that it is the path of courage that appeals to me.</p>\n<p>Then again, I haven't lived it both ways.</p>\n<p>Maybe I'm just <em>afraid</em> of a world so different as Analgesia&mdash;wouldn't that be an ironic reason to walk \"the path of courage\"?</p>\n<p>Maybe the path of courage just seems like the <em>smaller change</em>&mdash;maybe I just have trouble empathizing over a larger gap.</p>\n<p>But \"change\" is a moving target.</p>\n<p>If a human child grew up in a <em>less</em> painful world&mdash;if they had never lived in a world of AIDS or cancer or slavery, and so did not know these things as evils that had <em>been triumphantly eliminated</em>&mdash;and so did not feel that they were \"already done\" or that the world was \"already changed enough\"...</p>\n<p>Would they take the next step, and try to eliminate the unbearable pain of broken hearts, when someone's lover stops loving them?</p>\n<p>And then what?&nbsp; Is there a point where <em>Romeo and Juliet</em> just seems less and less relevant, more and more a relic of some distant forgotten world?&nbsp; Does there come some point in the transhuman journey where the whole business of the negative reinforcement circuitry, can't possibly seem like anything except a pointless hangover to wake up from?</p>\n<p>And if so, is there any point in <em>delaying </em>that last step?&nbsp; Or should we just throw away our fears and... throw away our fears?</p>\n<p>I don't know.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"pszEEb3ctztv3rozd": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6qS9q5zHafFXsB6hf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 82, "baseScore": 98, "extendedScore": null, "score": 0.000145, "legacy": true, "legacyId": "1206", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "9bvAELWc8y2gYjRav", "canonicalCollectionSlug": "rationality", "canonicalBookId": "T3CjiQq6bBgFuzvp6", "canonicalNextPostSlug": "value-is-fragile", "canonicalPrevPostSlug": "high-challenge", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 98, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 104, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4o3zwgofFPLutkqvd", "uD9TDHPwQ5hx4CgaX", "29vqqmGNxNRGzffEj", "qNZM3EGoE5ZeMdCRt", "aSQy7yHj6nPD44RNo", "sYgv4eYH82JEsTD34", "jiBFC7DcCrZjGmZnJ", "eLHCWi8sotQT6CmTX", "erGipespbbzdG5zYb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-10T00:12:24.000Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes 23", "slug": "rationality-quotes-23", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:43.714Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wSgxwXCdWiuM3R9fa/rationality-quotes-23", "pageUrlRelative": "/posts/wSgxwXCdWiuM3R9fa/rationality-quotes-23", "linkUrl": "https://www.lesswrong.com/posts/wSgxwXCdWiuM3R9fa/rationality-quotes-23", "postedAtFormatted": "Saturday, January 10th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%2023&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%2023%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwSgxwXCdWiuM3R9fa%2Frationality-quotes-23%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%2023%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwSgxwXCdWiuM3R9fa%2Frationality-quotes-23", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwSgxwXCdWiuM3R9fa%2Frationality-quotes-23", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 203, "htmlBody": "<p>\"This year I resolve to lose weight... to be nicer to dogs... and to sprout wings and fly.\"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- Garfield</p>\n<p>\"People understand instinctively that the best way for computer programs to communicate with each other is for each of them to be strict in what they emit, and liberal in what they accept. The odd thing is that people themselves are not willing to be strict in how they speak and liberal in how they listen. You'd think that would also be obvious.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <a href=\"http://oreilly.com/catalog/opensources/book/larry.html\">Larry Wall</a></p>\n<p>\"One never <em>needs </em>enemies, but they are so much fun to acquire.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Eluki bes Shahar, <em>Archangel Blues </em></p>\n<p>\"I'm accusing you of violating the laws of nature!\"<br />\"Nature's virtue is intact.&nbsp; I just know some different laws.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Orson Scott Card, <em>A Planet Called Treason</em></p>\n<p>\"The goal of most religions is to preserve and elaborate on that concept of the stars as a big painted backdrop.&nbsp; They make Infinity a 'prop' so you don't have to think about the scary part.\"<br /> &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <em>The Book of the SubGenius</em></p>\n<p><em></em></p>\n<p>&lt;silverpower&gt; Is humanity even worth saving?<br />&lt;starglider&gt; As opposed to what?<br />&lt;silverpower&gt; ...hmm.<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- #sl4</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wSgxwXCdWiuM3R9fa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "1207", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-11T02:09:01.000Z", "modifiedAt": null, "url": null, "title": "Continuous Improvement", "slug": "continuous-improvement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:59.057Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QfpHRAMRM2HjteKFK/continuous-improvement", "pageUrlRelative": "/posts/QfpHRAMRM2HjteKFK/continuous-improvement", "linkUrl": "https://www.lesswrong.com/posts/QfpHRAMRM2HjteKFK/continuous-improvement", "postedAtFormatted": "Sunday, January 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Continuous%20Improvement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AContinuous%20Improvement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQfpHRAMRM2HjteKFK%2Fcontinuous-improvement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Continuous%20Improvement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQfpHRAMRM2HjteKFK%2Fcontinuous-improvement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQfpHRAMRM2HjteKFK%2Fcontinuous-improvement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2681, "htmlBody": "<p><em></em></p>\n<p>When is it <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">adaptive</a> for an organism to be satisfied with what it has?&nbsp; When does an organism have enough children <em>and</em> enough food?&nbsp; The answer to the second question, at least, is obviously \"never\" from an evolutionary standpoint.&nbsp; The first proposition might be true if the reproductive risks of all available options exceed their reproductive benefits.&nbsp; In general, though, it is a rare organism in a rare environment whose reproductively optimal strategy is to rest with a smile on its face, feeling happy.</p>\n<p>To a first approximation, we might say something like \"The evolutionary purpose of emotion is to direct the cognitive processing of the organism toward achievable, reproductively relevant goals\".&nbsp; <em>Achievable </em>goals are usually located in the Future, since you can't affect the Past.&nbsp; Memory is a useful trick, but <em>learning the lesson</em> of a success or failure isn't the same goal as the original event&mdash;and usually the emotions associated with the memory are less intense than those of the original event.</p>\n<p>Then the way organisms and brains are built right now, \"true happiness\" might be a chimera, a carrot dangled in front of us to make us take the next step, and then yanked out of our reach as soon as we achieve our goals.</p>\n<p>This hypothesis is known as the <a href=\"http://www.subjectpool.com/ed_teach/msc/personality/wellbeing/2006_Diener_Am_Psychologist_happiness_beyond_the_hedonic_treadmill.pdf\">hedonic treadmill</a>.</p>\n<p>The famous pilot studies in this domain demonstrated e.g. that past lottery winners' stated subjective well-being was not significantly greater than that of an average person, after a few years or even months.&nbsp; Conversely, accident victims with severed spinal cords were not as happy as before the accident after six months&mdash;around 0.75 sd less than control groups&mdash;but they'd still adjusted much more than they had expected to adjust.</p>\n<p>This being the transhumanist form of Fun Theory, you might perhaps say:&nbsp; \"Let's get rid of this effect.&nbsp; Just delete the treadmill, at least for positive events.\"</p>\n<p><a id=\"more\"></a></p>\n<p>I'm not <em>entirely</em> sure we can get away with this.&nbsp; There's <a href=\"/lw/xi/serious_stories/\">the possibility that comparing good events to not-as-good events is what gives them part of their subjective quality</a>.&nbsp; And on a moral level, it sounds perilously close to tampering with Boredom itself.</p>\n<p>So suppose that instead of modifying minds and values, we first ask what we can do by modifying the environment.&nbsp; Is there enough fun in the universe, sufficiently accessible, for a transhuman to <em>jog off the hedonic treadmill</em>&mdash;improve their life <em>continuously</em>, at a sufficient rate to leap to an even higher hedonic level before they had a chance to get bored with the previous one?</p>\n<p>This question leads us into great and interesting difficulties.</p>\n<p>I had a nice vivid example I wanted to use for this, but unfortunately I couldn't find the exact numbers I needed to illustrate it.&nbsp; I'd wanted to find a figure for the total mass of the neurotransmitters released in the pleasure centers during an average male or female orgasm, and a figure for the density of those neurotransmitters&mdash;density in the sense of mass/volume of the chemicals themselves.&nbsp; From this I could've calculated <em>how long a period of exponential improvement</em> would be possible&mdash;how many years you could have \"the best orgasm of your life\" by a margin of at least 10%, at least once per year&mdash;before your orgasm collapsed into a black hole, the total mass having exceeded the mass of a black hole with the density of the neurotransmitters.</p>\n<p>Plugging in some random/Fermi numbers instead:</p>\n<p>Assume that a microgram of additional neurotransmitters are released in the pleasure centers during a standard human orgasm.&nbsp; And assume that neurotransmitters have the same density as water.&nbsp; Then an orgasm can reach around 10<sup>8</sup> solar masses before it collapses and forms a black hole, corresponding to 10<sup>47</sup> baseline orgasms.&nbsp; If we assume that a 100mg dose of crack is as pleasurable as 10 standard orgasms, then the street value of your last orgasm is around a hundred billion trillion trillion trillion dollars.</p>\n<p>I'm sorry.&nbsp; I just had to do that calculation.</p>\n<p>Anyway... requiring an <em>exponential</em> improvement eats up a factor of 10<sup>47</sup> in short order.&nbsp; Starting from human standard and improving at 10% per year, it would take less than 1,200 years.</p>\n<p>Of course you say, \"This but shows the folly of brains that use an analog representation of pleasure.&nbsp; Go digital, young man!\"</p>\n<p>If you redesigned the brain to represent the intensity of pleasure using IEEE 754 double-precision floating-point numbers, a mere 64 bits would suffice to feel pleasures up to 10^308 hedons...&nbsp; in, um, whatever base you were using.</p>\n<p>This still represents less than 7500 years of 10% annual improvement from a 1-hedon baseline, but after that amount of time, you can switch to larger floats.</p>\n<p>Now we <em>have</em> lost a bit of fine-tuning by switching to IEEE-standard hedonics.&nbsp; The 64-bit double-precision float has an 11-bit exponent and a 52-bit fractional part (and a 1-bit sign).&nbsp; So we'll only have 52 bits of precision (16 decimal places) with which to represent our pleasures, however great they may be.&nbsp; An original human's orgasm would soon be lost in the rounding error... which raises the question of how we can <em>experience</em> these invisible hedons, when the finite-precision bits are the whole substance of the pleasure.</p>\n<p>We also have the odd situation that, starting from 1 hedon, flipping a single bit in your brain can make your life 10<sup>154</sup> times more happy.</p>\n<p>And Hell forbid you flip the sign bit.&nbsp; Talk about a need for cosmic ray shielding.</p>\n<p>But really&mdash;if you're going to go so far as to use imprecise floating-point numbers to represent pleasure, why stop there?&nbsp; Why not move to <a href=\"/lw/kn/torture_vs_dust_specks/\">Knuth's up-arrow notation</a>?</p>\n<p>For that matter, IEEE 754 provides special representations for +/&mdash;INF, that is to say, positive and negative infinity.&nbsp; What happens if a bit flip makes you experience infinite pleasure?&nbsp; Does that mean you <a href=\"/lw/wv/prolegomena_to_a_theory_of_fun/\">Win The Game</a>?</p>\n<p>Now all of these questions I'm asking are in some sense unfair, because right now I don't know exactly what I have to do with <em>any </em>structure of bits in order to turn it into a \"<a href=\"/lw/x4/nonperson_predicates/\">subjective experience</a>\".&nbsp; Not that this is <a href=\"/lw/oh/righting_a_wrong_question/\">the right way to phrase the question</a>.&nbsp; It's not like there's a ritual that summons some incredible density of positive qualia that could collapse in its own right and form <a href=\"/lw/pn/zombies_the_movie/\">an epiphenomenal black hole</a>.</p>\n<p>But don't laugh&mdash;or at least, don't <em>only</em> laugh&mdash;because in the long run, these are <em>extremely </em>important questions.</p>\n<p>To give you some idea of what's at stake here, Robin, in \"<a href=\"http://www.overcomingbias.com/2008/01/protecting-acro.html\">For Discount Rates</a>\", pointed out that an investment earning 2% annual interest for 12,000 years adds up to a googol (10^100) times as much wealth; therefore, \"very distant future times are ridiculously easy to help via investment\".</p>\n<p>I observed that there weren't a googol atoms in the observable universe, let alone within a 12,000-lightyear radius of Earth.</p>\n<p>And Robin replied, \"I know of no law limiting economic value per atom.\"</p>\n<p>If you've got an increasingly large number of bits&mdash;things that can be one or zero&mdash;and you're doing a proportional number of computations with them... then how fast can you grow the amount of fun, or pleasure, or value?</p>\n<p>This echoes back to the questions in <a href=\"/lw/wx/complex_novelty/\">Complex Novelty</a>, which asked how many kinds of problems and novel solutions you could find, and how many deep insights there were to be had.&nbsp; I argued there that the growth rate is <em>faster than linear</em> in bits, e.g., humans can have much more than four times as much fun as chimpanzees even though our absolute brain volume is only around four times theirs.&nbsp; But I don't think the growth in \"depth of good insights\" or \"number of unique novel problems\" is, um, <em>faster than exponential</em> in the size of the pattern.</p>\n<p>Now... it might be that the Law simply permits outright that we can create very large amounts of subjective pleasure, every bit as substantial as the sort of subjective pleasure we get now, by the expedient of writing down very large numbers in a digital pleasure center.&nbsp; In this case, we have got it made.&nbsp; Have we <em>ever</em> got it made.</p>\n<p>In one sense I can definitely see where Robin is coming from.&nbsp; Suppose that you had a specification of the first 10,000 <a href=\"/lw/wx/complex_novelty/\">Busy Beaver</a> machines&mdash;the longest-running Turing machines with 1, 2, 3, 4, 5... states.&nbsp; This list could easily fit on a small flash memory card, made up of a few measly avogadros of atoms.</p>\n<p>And that small flash memory card would be worth...</p>\n<p>Well, let me put it this way:&nbsp; If a mathematician said to me that the value of this memory card, was worth more than the rest of the entire observable universe minus the card...&nbsp; I wouldn't necessarily <em>agree with him outright.</em>&nbsp; But I would understand his point of view.</p>\n<p>Still, I don't know if you can truly grok the fun contained in that memory card, without an unbounded amount of computing power with which to understand it.&nbsp; Ultradense information does not give you ultradense economic value or ultradense fun unless you can also <em>use</em> that information in a way that consumes few resources.&nbsp; Otherwise it's just More Fun Than You Can Handle.</p>\n<p>Weber's Law of Just Noticeable Difference says that stimuli with an intensity scale, typically require a fixed fraction of proportional difference, rather than any fixed interval of absolute intensity, in order for the difference to be noticeable to a human or other organism.&nbsp; In other words, we may demand exponential increases because our <em>imprecise</em> brains can't <em>notice</em> smaller differences.&nbsp; This would suggest that our existing pleasures might already in effect possess a floating-point representation, with an exponent and a fraction&mdash;the army of actual neurons being used only to transmit an analog signal most of whose precision is lost.&nbsp; So we might be able to get away with using floats, even if we can't get away with using up-arrows.</p>\n<p>But suppose that the inscrutable rules governing the substantiality of \"subjective\" pleasure actually require one neuron per hedon, or something like that.</p>\n<p>Or suppose that we <a href=\"/lw/ww/high_challenge/\">only choose to reward ourselves</a> when we find a <em>better solution,</em> and that we don't choose to game the betterness metrics.</p>\n<p>And suppose that we don't discard the Weber-Fechner law of \"just noticeable difference\", but go on demanding <em>percentage</em> annual improvements, year after year.</p>\n<p>Or you might need to improve at a fractional rate <a href=\"/lw/wx/complex_novelty/\">in order to assimilate your own memories</a>.&nbsp; Larger brains would lay down larger memories, and hence need to grow exponentially&mdash;efficiency improvements suiting to <em>moderate </em>the growth, but not to eliminate the exponent.</p>\n<p>If fun or intelligence or value can only grow as fast as the mere <em>cube </em>of the brain size... and yet we demand a 2% improvement every year...</p>\n<p>Then 350 years will pass before our resource consumption grows a single order of magnitude.</p>\n<p>And yet there are only around 10<sup>80</sup> atoms in the observable universe.</p>\n<p>Do the math.</p>\n<p>(It works out to a lifespan of around 28,000 years.)</p>\n<p>Now... before everyone gets all depressed about this...</p>\n<p>We can still hold out a fraction of hope for <em>real immortality</em>, aka \"emortality\".&nbsp; As Greg Egan put it, \"Not dying after a very long time.&nbsp; Just not dying, period.\"</p>\n<p>The laws of physics as we know them prohibit emortality on multiple grounds.&nbsp; It is a fair historical observation that, over the course of previous centuries, civilizations have become able to do things that previous civilizations called \"physically impossible\".&nbsp; This reflects a change in knowledge about the laws of physics, not a change in the actual laws; and we cannot do <em>everything</em> once thought to be impossible.&nbsp; We violate Newton's version of gravitation, but not conservation of energy.&nbsp; It's a good historical bet that the future will be able to do at least <em>one</em> thing our physicists would call impossible.&nbsp; But you can't bank on being able to violate any <em>particular</em> \"law of physics\" in the future.</p>\n<p>There is just... a shred of reasonable hope, that our physics might be <em>much</em> more incomplete than we realize, or that we are wrong in exactly the right way, or that anthropic points I don't understand might come to our rescue and let us escape these physics (also <em>a la</em> Greg Egan).</p>\n<p>So I haven't lost hope.&nbsp; But I haven't lost despair, either; <em>that </em>would be faith.</p>\n<p>In the case where our resources really are limited and there is no way around it...</p>\n<p>...the question of how fast a rate of <em>continuous improvement </em>you demand for an acceptable quality of life&mdash;an annual percentage increase, or a fixed added amount&mdash;and the question of <em>how much</em> improvement you can pack into patterns of linearly increasing size&mdash;adding up to the fun-theoretic question of how fast you have to expand your resource usage over time to lead a life worth living...</p>\n<p>...determines the maximum lifespan of sentient beings.</p>\n<p>If you can get by with increasing the size <em>in bits</em> of your mind at a linear rate, then you can last for quite a while.&nbsp; Until the end of the universe, in many versions of cosmology.&nbsp; <em>And</em> you can have a child (or two parents can have two children), and the children can have children.&nbsp; Linear brain size growth * linear population growth = quadratic growth, and cubic growth at lightspeed should be physically permissible.</p>\n<p>But if you have to grow exponentially, in order for your ever-larger mind and its ever-larger memories not to end up uncomfortably squashed into too small a brain&mdash;squashed down to a point, to the point of it being pointless&mdash;then a transhuman's life is measured in subjective eons at best, and more likely subjective millennia.&nbsp; Though it would be a merry life indeed.</p>\n<p>My own eye has trouble enough looking ahead a mere century or two of growth.&nbsp; It's not like I can imagine any sort of <em>me</em> the size of a galaxy.&nbsp; I just want to live one more day, and tomorrow I will still want to live one more day.&nbsp; The part about \"wanting to live forever\" is just an induction on the positive integers, not an instantaneous vision whose desire spans eternity.</p>\n<p>If I can see to the fulfillment of all my present self's goals that I can concretely envision, shouldn't that be enough for me?&nbsp; And my century-older self will also be able to see that far ahead.&nbsp; And so on through thousands of generations of selfhood until some distant figure the size of a galaxy has to depart the physics we know, one way or the other...&nbsp; Should that be <em>scary?</em></p>\n<p>Yeah, I hope like hell that emortality is possible.</p>\n<p>Failing that, I'd at least like to find out one way or the other, so I can get on with my life instead of having that lingering uncertainty.</p>\n<p>For now, one of the reasons <a href=\"/lw/ws/for_the_people_who_are_still_alive/\">I care about people alive today</a> is the thought that if creating new people just divides up a finite pool of resource available <em>here</em>, but we live in a Big World where there are plenty of people <em>elsewhere </em>with their own resources... then we might not want to create so many new people <em>here</em>.&nbsp; Six billion now, six trillion at the end of time?&nbsp; Though this is more an idiom of linear growth than exponential&mdash;with exponential growth, a factor of 10 fewer people just buys you another 350 years of lifespan per person, or whatever.</p>\n<p>But I do hope for emortality.&nbsp; Odd, isn't it?&nbsp; How abstract should a hope or fear have to be, before a human can stop thinking about it?</p>\n<p>Oh, and finally&mdash;there's an idea in the literature of hedonic psychology called the \"hedonic set point\", based on identical twin studies showing that identical twins raised apart have highly similar happiness levels, more so than fraternal twins raised together, people in similar life circumstances, etcetera.&nbsp; There are things that do seem to shift your set point, but not much (and permanent downward shift happens more easily than permanent upward shift, what a surprise).&nbsp; Some studies have suggested that up to 80% of the variance in happiness is due to genes, or something shared between identical twins in different environments at any rate.</p>\n<p>If <em>no </em>environmental improvement ever has much effect on subjective well-being, the way you are now, because you've got a more or less genetically set level of happiness that you drift back to, then...</p>\n<p>Well, my usual heuristic is to imagine messing with environments before I imagine messing with minds.</p>\n<p>But in this case?&nbsp; Screw that.&nbsp; That's just <em>stupid.</em>&nbsp; Delete it without a qualm.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/wy/sensual_experience/\">Sensual Experience</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/wx/complex_novelty/\">Complex Novelty</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sSNtcEQsqHgN8ZmRF": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QfpHRAMRM2HjteKFK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 25, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": "1208", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XPErvb8m9FapXCjhA", "6qS9q5zHafFXsB6hf", "3wYTFWY3LKQCnAptN", "pK4HTxuv6mftHXWC3", "wqDRRx9RqwKLzWt7R", "rQEwySCcLtdKHkrHp", "fsDz6HieZJBu54Yes", "aEdqh3KPerBNYvoWe", "29vqqmGNxNRGzffEj", "cfZ8zveqrTZbQrjeD", "K4aGvLnHvYgX9pZHS", "eLHCWi8sotQT6CmTX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-12T05:28:34.000Z", "modifiedAt": "2020-03-15T06:27:32.662Z", "url": null, "title": "Eutopia is Scary", "slug": "eutopia-is-scary", "viewCount": null, "lastCommentedAt": "2021-10-20T17:17:43.223Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hQSaMafoizBSa3gFR/eutopia-is-scary", "pageUrlRelative": "/posts/hQSaMafoizBSa3gFR/eutopia-is-scary", "linkUrl": "https://www.lesswrong.com/posts/hQSaMafoizBSa3gFR/eutopia-is-scary", "postedAtFormatted": "Monday, January 12th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Eutopia%20is%20Scary&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEutopia%20is%20Scary%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhQSaMafoizBSa3gFR%2Feutopia-is-scary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Eutopia%20is%20Scary%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhQSaMafoizBSa3gFR%2Feutopia-is-scary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhQSaMafoizBSa3gFR%2Feutopia-is-scary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1619, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/j6/why_is_the_future_so_absurd/\">Why is the Future So Absurd?</a></p>\n<p style=\"margin-left: 40px;\">\"The big thing to remember about far-future cyberpunk is that it will be truly <em>ultra</em>-tech.&nbsp; The mind and body changes available to a 23rd-century Solid Citizen would probably amaze, disgust and <em>frighten</em> that 2050 netrunner!\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &mdash;<em>GURPS Cyberpunk</em></p>\n<p>Pick up someone from the 18th century&mdash;a <em>smart</em> someone.&nbsp; Ben Franklin, say.&nbsp; Drop them into the early 21st century.</p>\n<p>We, in our time, think our life has improved in the last two or three hundred years.&nbsp; Ben Franklin is probably smart and forward-looking enough to <em>agree</em> that life has improved.&nbsp; But if you don't think Ben Franklin would be amazed, disgusted, and <em>frightened</em>, then I think <a href=\"/lw/j6/why_is_the_future_so_absurd/\">you far overestimate the \"normality\" of your own time</a>.&nbsp; You can think of reasons why Ben should find our world compatible, but Ben himself might not do the same.</p>\n<p><span id=\"comment-81958465-content\">Movies that were made in say the 40s or 50s, seem much more alien&mdash;to me&mdash;than modern movies allegedly set hundreds of years in the future, or in different universes.&nbsp; Watch a movie from 1950 and you may see a man slapping a woman.&nbsp; Doesn't happen a lot in <em>Lord of the Rings</em>, does it?</span>&nbsp; Drop back to the 16th century and one popular entertainment was setting a cat on fire.&nbsp; Ever see <em>that</em> in any moving picture, no matter how \"lowbrow\"?</p>\n<p>(\"But,\" you say, \"that's showing how discomforting the Past's culture was, not how scary the Future is.\"&nbsp; Of which I <a href=\"/lw/j6/why_is_the_future_so_absurd/\">wrote</a>,&nbsp;\"When we look over history, we see changes away from <em>absurd</em> conditions such as everyone being a peasant farmer and women not having the vote, toward <em>normal</em> conditions like a majority middle class and equal rights...\")</p>\n<p><em>Something</em> about the Future will shock we 21st-century folk, if we were dropped in without slow adaptation.&nbsp; This is not because the Future is cold and gloomy&mdash;I am speaking of a positive, successful Future; the negative outcomes are probably just blank.&nbsp; Nor am I speaking of the idea that every Utopia has some dark hidden flaw.&nbsp; I am saying that the Future would discomfort us <em>because</em> it is better.</p>\n<p><a id=\"more\"></a></p>\n<p>This is another piece of the puzzle for why <a href=\"http://www.orwell.ru/library/articles/socialists/english/e_fun\">no author seems to have ever succeeded in constructing a Utopia worth-a-damn</a>.&nbsp; When they are out to depict how marvelous and wonderful the world could be, if only we would all be Marxists or Randians or let philosophers be kings... they try to depict the resulting outcome as <em>comforting</em> and <em>safe.</em></p>\n<p>Again, George Orwell from \"<a href=\"http://www.orwell.ru/library/articles/socialists/english/e_fun\">Why Socialists Don't Believe In Fun</a>\":</p>\n<blockquote>\n<p>&nbsp;&nbsp;&nbsp; \"<span id=\"comment-144622762-content\">In the last part, in contrast with disgusting Yahoos, we are shown the noble Houyhnhnms, intelligent horses who are free from human failings.&nbsp; Now these horses, for all their high character and unfailing common sense, are remarkably dreary creatures.&nbsp; Like the inhabitants of various other Utopias, they are chiefly concerned with avoiding fuss.&nbsp; They live uneventful, subdued, 'reasonable' lives, free not only from quarrels, disorder or insecurity of any kind, but also from 'passion', including physical love.&nbsp; They choose their mates on eugenic principles, avoid excesses of affection, and appear somewhat glad to die when their time comes.\"</span></p>\n</blockquote>\n<p>One might consider, in particular contrast, Timothy Ferris's observation:</p>\n<p style=\"margin-left: 40px;\">&nbsp;&nbsp;&nbsp; \"What is the opposite of happiness?&nbsp; Sadness?&nbsp; No.&nbsp; Just as love and hate are two sides of the same coin, so are happiness and sadness.&nbsp; Crying out of happiness is a perfect illustration of this.&nbsp; The opposite of love is indifference, and the opposite of happiness is&mdash;here's the clincher&mdash;boredom...<br /> &nbsp;&nbsp;&nbsp; The question you should be asking isn't 'What do I want?' or 'What are my goals?' but 'What would excite me?'<br /> &nbsp;&nbsp;&nbsp; Remember&mdash;boredom is the enemy, not some abstract 'failure.'\"</p>\n<p>Utopia is reassuring, unsurprising, and dull.</p>\n<p>Eutopia is scary.</p>\n<p>I'm not talking here about <a href=\"/lw/uu/why_does_power_corrupt/\">evil means to a good end</a>, I'm talking about the good outcomes <em>themselves</em>.&nbsp; That is the proper relation of the Future to the Past when things turn out <em>well</em>, as we would know very well from history <a href=\"/lw/iz/failing_to_learn_from_history/\">if we'd actually lived it</a>, rather than looking back with benefit of hindsight.</p>\n<p>Now... I don't think you can actually <em>build the Future</em> on the basis of asking how to scare yourself.&nbsp; The vast majority of possible changes are in the direction of higher entropy; only a very few discomforts stem from things getting <em>better</em>.</p>\n<p>\"I shock you therefore I'm right\" is one of the most <em>annoying</em> of all non-sequiturs, and we certainly don't want to go <em>there.</em></p>\n<p>But on a purely <em>literary</em> level... and bearing in mind that <a href=\"/lw/k9/the_logical_fallacy_of_generalization_from/\">fiction is not reality</a>, and <a href=\"/lw/xi/serious_stories/\">fiction is not optimized the way we try to optimize reality</a>...</p>\n<p>I try to write fiction, now and then.&nbsp; More rarely, I finish a story.&nbsp; Even more rarely, I let someone else look at it.</p>\n<p>Once I finally got to the point of thinking that <a href=\"/lw/xi/serious_stories/\">maybe you <em>should</em> be able to write a story set in Eutopia</a>, I tried doing it.&nbsp;</p>\n<p>But I had something like an instinctive revulsion at the indulgence of trying to build a world that fit <em>me,</em> but probably wouldn't fit others so nicely.</p>\n<p>So&mdash;without giving the world a seamy underside, or putting <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/KnightTemplar\">Knight Templars</a> in charge, or anything so obvious as that&mdash;without deliberately trying to make the world<em> flawed </em>-</p>\n<p>I was trying to invent, even if I had to do it myself, a better world where I would be <em>out of place.</em>&nbsp; Just like Ben Franklin would be out of place in the modern world.</p>\n<p>Definitely <em>not</em> someplace that a transhumanist/science-advocate/libertarian (like myself) would go, and be smugly satisfied at how well all their ideas had worked.&nbsp; Down that path lay the Dark Side&mdash;certainly in a purely literary sense.</p>\n<p>And you couldn't avert that just by having the Future go wrong in all the stupid obvious ways that transhumanists, or libertarians, or public advocates of science had already warned against.&nbsp; Then you just had a dystopia, and it might make a good SF story but it had already been done.</p>\n<p>But I had my world's foundation, an absurd notion inspired by a corny pun; a vision of what you see when you wake up from cryonic suspension, that I couldn't have gotten away with posting to any transhumanist mailing list even as a joke.</p>\n<p>And then, whenever I could think of an arguably-good idea that <em>offended my sensibilities</em>, I added it in.&nbsp; The goal being to&mdash;without ever deliberately making the Future <em>worse </em>&mdash;make it a place where I would be as shocked as possible to see that <em>that</em> was how things had turned out.</p>\n<p><a href=\"/lw/p0/to_spread_science_keep_it_secret/\">Getting rid of textbooks</a>, for example&mdash;postulating that talking about science in public is socially unacceptable, for the same reason that you don't tell someone aiming to see a movie whether the hero dies at the end.&nbsp; A world that had rejected my beloved concept of <a href=\"/lw/in/scientific_evidence_legal_evidence_rational/\">science as the public knowledge of humankind</a>.</p>\n<p>Then I added up all the discomforting ideas together...</p>\n<p>...and at least in my imagination, it worked better than anything I'd ever dared to visualize as a <em>serious </em>proposal.</p>\n<p>My serious proposals had been optimized to look sober and safe and sane; everything <a href=\"/lw/x3/devils_offers/\">voluntary</a>, with <a href=\"/lw/wz/living_by_your_own_strength/\">clearly lighted</a> <a href=\"/lw/x2/harmful_options/\">exit signs</a>, and all sorts of <a href=\"/lw/xg/emotional_involvement/\">volume controls</a> to prevent anything from getting too <em>loud</em> and waking up the neighbors.&nbsp; Nothing too <a href=\"/lw/j6/why_is_the_future_so_absurd/\">absurd</a>.&nbsp; Proposals that wouldn't scare the nervous, containing as little as possible that would cause anyone to make a fuss.</p>\n<p>This world was ridiculous, and it was going to wake up the neighbors.</p>\n<p>It was also seductive to the point that I had to exert a serious effort to prevent my soul from getting sucked out.&nbsp; (I suspect that's a general problem; that it's a good idea <em>emotionally </em>(not just <em>epistemically</em>) to <em>not </em>visualize your better Future in too much detail.&nbsp; You're better off comparing yourself to the Past.&nbsp; I may write a separate post on this.)</p>\n<p>And so I found myself being pulled in the direction of this world in which I was supposed to be \"out of place\".&nbsp; I started thinking that, well, maybe it really <em>would</em> be a good idea to get rid of all the textbooks, all they do is take the fun out of science.&nbsp; I started thinking that maybe personal competition <em>was </em>a legitimate motivator (previously, I would have called it a zero-sum game and been morally aghast).&nbsp; I began to worry that peace, democracy, market economies, and con&mdash;but I'd better not finish that sentence.&nbsp; I started to wonder if the old vision that was so <em>reassuring,</em> so <em>safe,</em> was <a href=\"/lw/wv/prolegomena_to_a_theory_of_fun/\">optimized to be good news</a> to a modern human living in constant danger of permanent death or damage, and less optimized for the everyday existence of someone less frightened.</p>\n<p>This is what happens when I try to invent a world that fails to confirm my sensibilities?&nbsp; It makes me wonder what would happen if someone else tried the same exercise.</p>\n<p>Unfortunately, I can't seem to visualize any new world that represents the same shock to me as the last one did.&nbsp; Either the trick only works once, or you have to wait longer between attempts, or I'm too old now.</p>\n<p>But I hope that so long as the world offends the <em>original</em> you, it gets to keep its literary integrity even if you start to find it less shocking.</p>\n<p>I haven't yet published any story that gives more than a glimpse of this setting.&nbsp; I'm still debating with myself whether I dare.&nbsp; I don't know whether the suck-out-your-soul effect would threaten anyone but myself as author&mdash;I haven't seen it happening with Banks's Culture or Wright's Golden Oecumene, so I suspect it's more of a trap when a world fits a single person too well.&nbsp; But I got enough flak when I presented <a href=\"/lw/p0/to_spread_science_keep_it_secret/\">the case for getting rid of textbooks</a>.</p>\n<p>Still&mdash;I have seen the possibilities, now.&nbsp; So long as no one dies permanently, I am leaning in favor of a loud and scary Future<span style=\"font-style: italic;\">.</span><em></em></p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/xm/building_weirdtopia/\">Building Weirdtopia</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/xi/serious_stories/\">Serious Stories</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"pGqRLe9bFDX2G2kXY": 1, "GBpwq8cWvaeRoE9X5": 1, "5f5c37ee1b5cdee568cfb2c2": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hQSaMafoizBSa3gFR", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 47, "baseScore": 55, "extendedScore": null, "score": 8.2e-05, "legacy": true, "legacyId": "1209", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 56, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 126, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Ga2HSwf9iQe64JwAa", "v8rghtzWCziYuMdJ5", "97Y7Jwrzxyfzz3Ad2", "rHBdcHGLJ7KvLJQPk", "6qS9q5zHafFXsB6hf", "3diLhMELXxM8rFHJj", "fhojYBGGiYAFcryHZ", "MTjej6HKvPByx3dEA", "dKGfNvjGjq4rqffyF", "CtSS6SkHhLBvdodTY", "ZmDEbiEeXk3Wv2sLH", "pK4HTxuv6mftHXWC3", "K4aGvLnHvYgX9pZHS", "cWjK3SbRcLkb3gN69"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-01-12T05:28:34.000Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-12T20:35:27.000Z", "modifiedAt": null, "url": null, "title": "Building Weirdtopia", "slug": "building-weirdtopia", "viewCount": null, "lastCommentedAt": "2020-12-14T15:01:13.576Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cWjK3SbRcLkb3gN69/building-weirdtopia", "pageUrlRelative": "/posts/cWjK3SbRcLkb3gN69/building-weirdtopia", "linkUrl": "https://www.lesswrong.com/posts/cWjK3SbRcLkb3gN69/building-weirdtopia", "postedAtFormatted": "Monday, January 12th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Building%20Weirdtopia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABuilding%20Weirdtopia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcWjK3SbRcLkb3gN69%2Fbuilding-weirdtopia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Building%20Weirdtopia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcWjK3SbRcLkb3gN69%2Fbuilding-weirdtopia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcWjK3SbRcLkb3gN69%2Fbuilding-weirdtopia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1008, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/xl/eutopia_is_scary/\">Eutopia is Scary</a></p>\n<p style=\"margin-left: 40px;\">\"Two roads diverged in the woods.&nbsp; I took the one less traveled, and had to eat bugs until Park rangers rescued me.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &mdash;Jim Rosenberg</p>\n<p>Utopia and Dystopia have something in common: they both confirm the moral sensibilities you started with.&nbsp; Whether the world is a libertarian utopia of the non-initiation of violence and everyone free to start their own business, or a hellish dystopia of government regulation and intrusion&mdash;you might like to find yourself in the first, and hate to find yourself in the second; but either way you nod and say, \"Guess I was right all along.\"</p>\n<p>So as an exercise in creativity, try writing them down side by side:&nbsp; Utopia, Dystopia, and Weirdtopia.&nbsp; The zig, the zag and the zog.</p>\n<p>I'll start off with a worked example for <em>public understanding of science:</em></p>\n<ul>\n<li><em>Utopia:</em>&nbsp; Most people have the equivalent of an undergrad degree in something; <em>everyone</em> reads the popular science books (and they're <em>good </em>books); everyone over the age of nine understands evolutionary theory and Newtonian physics; scientists who make major contributions are publicly adulated like rock stars.</li>\n<li><em>Dystopia:</em>&nbsp; Science is considered boring and possibly treasonous; public discourse elevates religion or crackpot theories; stem cell research is banned.</li>\n<li><em>Weirdtopia:</em>&nbsp; <a href=\"/lw/p0/to_spread_science_keep_it_secret/\">Science is kept secret to avoid spoiling the surprises</a>; no public discussion but intense private pursuit; cooperative ventures surrounded by fearsome initiation rituals because that's what it takes for people to feel like they've actually learned a Secret of the Universe and be satisfied; someone you meet may only know extremely basic science, but they'll have personally done revolutionary-level work in it, just like you.&nbsp; Too bad you can't compare notes.</li>\n</ul>\n<p><a id=\"more\"></a></p>\n<p><a href=\"http://www.overcomingbias.com/2008/06/against-disclai.html\">Disclaimer</a> 1:&nbsp; Not every sensibility we have is necessarily <em>wrong.</em>&nbsp; Originality is a goal of literature, not science; sometimes it's better to be right than to be new.&nbsp; But there are also such things as <a href=\"/lw/k5/cached_thoughts/\">cached thoughts</a>.&nbsp; At least in my own case, it turned out that trying to invent a world that went outside my pre-existing sensibilities, did me a world of good.</p>\n<p>Disclaimer 2:&nbsp; This method is not universal:&nbsp; Not all interesting ideas fit this mold, and not all ideas that fit this mold are good ones.&nbsp; Still, it seems like an interesting technique.</p>\n<p>If you're trying to write science fiction (where originality <em>is</em> a legitimate goal), then you can write down anything nonobvious for Weirdtopia, and you're done.</p>\n<p>If you're trying to do Fun Theory, you have to come up with a Weirdtopia that's at least <em>arguably-better </em>than Utopia.&nbsp; This is harder but also directs you to more interesting regions of the answer space.</p>\n<p>If you can make all your answers <em>coherent</em> with each other, you'll have quite a story setting on your hands.&nbsp; (Hope you know how to handle characterization, dialogue, description, conflict, and all that other stuff.)</p>\n<p>Here's some partially completed challenges, where I wrote down a Utopia and a Dystopia (according to the moral sensibilities I started with before <em>I</em> did this exercise), but inventing a (better) Weirdtopia is left to the reader.</p>\n<p><em>Economic...</em></p>\n<ul>\n<li><em>Utopia:</em>&nbsp; The world is flat and ultra-efficient.&nbsp; Prices fall as standards of living rise, thanks to economies of scale.&nbsp; Anyone can easily start their own business and most people do.&nbsp; Everything is done in the right place by the right person under Ricardo's Law of Comparative Advantage.&nbsp; Shocks are efficiently absorbed by the risk capital that insured them.</li>\n<li><em>Dystopia:</em>&nbsp; Lots of trade barriers and subsidies; corporations exploit the regulatory systems to create new barriers to entry; dysfunctional financial systems with poor incentives and lots of unproductive investments; rampant agent failures and systemic vulnerabilities; standards of living flat or dropping.</li>\n<li><em>Weirdtopia:</em> _____</li>\n</ul>\n<p><em>Sexual...</em></p>\n<ul>\n<li><em>Utopia:</em>&nbsp; Sexual mores straight out of a Spider Robinson novel:&nbsp; Sexual jealousy has been eliminated; no one is embarrassed about what turns them on; universal tolerance and respect; everyone is bisexual, poly, and a switch; total equality between the sexes; no one would look askance on sex in public any more than eating in public, so long as the participants cleaned up after themselves.</li>\n<li><em>Dystopia:</em>&nbsp; 10% of women have never had an orgasm.&nbsp; States adopt laws to ban gay marriage.&nbsp; Prostitution illegal.</li>\n<li><em>Weirdtopia:</em> _____</li>\n</ul>\n<p><em>Governmental...</em></p>\n<ul>\n<li><em>Utopia:</em>&nbsp; Non-initiation of violence is the chief rule. Remaining public issues are settled by democracy:&nbsp; Well reasoned public debate in which all sides get a free voice, followed by direct or representative majority vote.&nbsp; Smoothly interfunctioning Privately Produced Law, which coordinate to enforce a very few global rules like \"no slavery\".</li>\n<li><em>Dystopia:</em>&nbsp; Tyranny of a single individual or oligarchy.&nbsp; Politicians with effective locks on power thanks to corrupted electronic voting systems, voter intimidation, voting systems designed to create coordination problems.&nbsp; Business of government is unpleasant and not very competitive; hard to move from one region to another.</li>\n<li><em>Weirdtopia:</em> _____</li>\n</ul>\n<p><em>Technological...</em></p>\n<ul>\n<li><em>Utopia:</em>&nbsp; All Kurzweilian prophecies come true simultaneously.&nbsp; Every pot contains a chicken, a nanomedical package, a personal spaceship, a superdupercomputer, amazing video games, and a pet AI to help you use it all, plus a pony.&nbsp; Everything is designed by Apple.</li>\n<li><em>Dystopia:</em>&nbsp; Those damned fools in the government banned everything more complicated than a lawnmower, and we couldn't use our lawnmowers after Peak Oil hit.</li>\n<li><em>Weirdtopia:</em>&nbsp; _____</li>\n</ul>\n<p><em>Cognitive...</em></p>\n<ul>\n<li><em>Utopia:</em>&nbsp; Brain-computer implants for everyone!&nbsp; You can do whatever you like with them, it's all voluntary and the dangerous buttons are clearly labeled.&nbsp; There are AIs around that are way more powerful than you; but they don't hurt you unless you ask to be hurt, sign an informed consent release form and click \"Yes\" three times.</li>\n<li><em>Dystopia:</em>&nbsp; The first self-improving AI was poorly designed, everyone's dead and the universe is being turned into paperclips.&nbsp; Or the augmented humans hate the normals.&nbsp; Or augmentations make you go nuts.&nbsp; Or the darned government banned everything again, and people are still getting Alzheimers due to lack of stem-cell research.</li>\n<li><em>Weirdtopia:</em>&nbsp; _____</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/xo/justified_expectation_of_pleasant_surprises/\">Justified Expectation of Pleasant Surprises</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/xl/eutopia_is_scary/\">Eutopia is Scary</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"pGqRLe9bFDX2G2kXY": 1, "sSNtcEQsqHgN8ZmRF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cWjK3SbRcLkb3gN69", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 45, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "1210", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 45, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 311, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hQSaMafoizBSa3gFR", "3diLhMELXxM8rFHJj", "2MD3NMLBPCqPfnfre", "K4aGvLnHvYgX9pZHS", "DGXvLNpiSYBeQ6TLW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-13T19:48:22.000Z", "modifiedAt": null, "url": null, "title": "She has joined the Conspiracy", "slug": "she-has-joined-the-conspiracy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:37.854Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/siPzSrEnWGq8DDGua/she-has-joined-the-conspiracy", "pageUrlRelative": "/posts/siPzSrEnWGq8DDGua/she-has-joined-the-conspiracy", "linkUrl": "https://www.lesswrong.com/posts/siPzSrEnWGq8DDGua/she-has-joined-the-conspiracy", "postedAtFormatted": "Tuesday, January 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20She%20has%20joined%20the%20Conspiracy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShe%20has%20joined%20the%20Conspiracy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsiPzSrEnWGq8DDGua%2Fshe-has-joined-the-conspiracy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=She%20has%20joined%20the%20Conspiracy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsiPzSrEnWGq8DDGua%2Fshe-has-joined-the-conspiracy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsiPzSrEnWGq8DDGua%2Fshe-has-joined-the-conspiracy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 13, "htmlBody": "<p><a href=\"http://dresdencodak.com/cartoons/dc_059.html\"><img alt=\"Kimiko\" border=\"0\" class=\"at-xid-6a00d8341c6a2c53ef010536c21d63970b image-full \" height=\"227\" src=\"http://lesswrong.com/static/imported/6a00d8341c6a2c53ef010536c21d63970b-800wi.jpg\" style=\"margin: 5px;\" title=\"Kimiko\" width=\"279\" /></a></p><p> I have no idea whether I had anything to do with this.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "siPzSrEnWGq8DDGua", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 4.691169135910893e-07, "legacy": true, "legacyId": "1211", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-15T07:26:51.000Z", "modifiedAt": null, "url": null, "title": "Justified Expectation of Pleasant Surprises", "slug": "justified-expectation-of-pleasant-surprises", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:32.279Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DGXvLNpiSYBeQ6TLW/justified-expectation-of-pleasant-surprises", "pageUrlRelative": "/posts/DGXvLNpiSYBeQ6TLW/justified-expectation-of-pleasant-surprises", "linkUrl": "https://www.lesswrong.com/posts/DGXvLNpiSYBeQ6TLW/justified-expectation-of-pleasant-surprises", "postedAtFormatted": "Thursday, January 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Justified%20Expectation%20of%20Pleasant%20Surprises&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJustified%20Expectation%20of%20Pleasant%20Surprises%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDGXvLNpiSYBeQ6TLW%2Fjustified-expectation-of-pleasant-surprises%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Justified%20Expectation%20of%20Pleasant%20Surprises%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDGXvLNpiSYBeQ6TLW%2Fjustified-expectation-of-pleasant-surprises", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDGXvLNpiSYBeQ6TLW%2Fjustified-expectation-of-pleasant-surprises", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1031, "htmlBody": "<p>I recently tried playing a computer game that made a major fun-theoretic error.&nbsp; (At least I strongly suspect it's an error, though they are game designers and I am not.)</p>\n<p>The game showed me&mdash;right from the start of play&mdash;what abilities I could purchase as I increased in level.&nbsp; Worse, there were <a href=\"/lw/x2/harmful_options/\">many different choices</a>; still worse, you had to pay a cost in fungible points to acquire them, making you feel like you were losing a resource...&nbsp; But today, I'd just like to focus on the problem of telling me, <em>right at the start of the game,</em> about all the nice things that might happen to me later.</p>\n<p>I can't think of a good experimental result that backs this up; but I'd expect that a pleasant <em>surprise</em> would have a greater hedonic impact, than being told about the same gift in advance.&nbsp; Sure, the moment you were first <em>told </em>about the gift would be <em>good news</em>, a moment of pleasure in the moment of being told.&nbsp; But you wouldn't have the gift in hand at that moment, which limits the pleasure.&nbsp; And then you have to wait.&nbsp; And then when you finally get the gift&mdash;it's pleasant to go from not having it to having it, <em>if</em> you didn't wait too long; but a surprise would have a larger momentary impact, I would think.</p>\n<p>This particular game had a status screen that showed <em>all</em> my future class abilities <em>at the start of the game</em>&mdash;inactive and dark but with full information still displayed.&nbsp; From a hedonic standpoint this seems like <em>miserable </em>fun theory.&nbsp; All the \"good news\" is lumped into a gigantic package; the items of news would have much greater impact if encountered separately.&nbsp; And then I have to wait a long time to actually acquire the abilities, so I get an extended period of comparing my current weak game-self to all the wonderful abilities I <em>could</em> have but don't.</p>\n<p>Imagine living in two possible worlds.&nbsp; Both worlds are otherwise rich in <a href=\"/lw/ww/high_challenge/\">challenge</a>, <a href=\"/lw/wx/complex_novelty/\">novelty</a>, and other aspects of Fun.&nbsp; In both worlds, you get smarter with age and <a href=\"/lw/wz/living_by_your_own_strength/\">acquire more abilities over time</a>, so that your life is <a href=\"/lw/xk/continuous_improvement/\">always getting better</a>.</p>\n<p>But in <em>one </em>world, the abilities that come with seniority are openly discussed, hence widely known; you know what you have to look forward to.</p>\n<p>In the <em>other </em>world, anyone older than you will <em>refuse to talk</em> about certain aspects of growing up; you'll just have to wait and find out.</p>\n<p><a id=\"more\"></a></p>\n<p>I ask you to contemplate&mdash;not just which world you might prefer to live in&mdash;but how <em>much</em> you might want to live in the second world, rather than the first.&nbsp; I would even say that the second world seems more <em>alive;</em> when I imagine living there, my imagined <em>will to live</em> feels stronger.&nbsp; I've got to stay alive to find out what happens next, right?</p>\n<p>The idea that <em>hope</em> is important to a happy life, is hardly original with me&mdash;though I think it might not be emphasized quite <em>enough</em>, on the lists of things people are told they need.</p>\n<p>I <a href=\"/lw/hl/lotteries_a_waste_of_hope/\">don't agree with buying lottery tickets</a>, but I do think I understand why people do it.&nbsp; I remember the times in my life when I had more or less belief that things would improve&mdash;that they were heading up in the near-term or mid-term, close enough to anticipate.&nbsp; I'm having trouble describing how much of a difference it makes.&nbsp; Maybe I don't <em>need</em> to describe that difference, unless some of my readers have never had any light at the end of their tunnels, or some of my readers have never looked forward and seen darkness.</p>\n<p>If <a href=\"/lw/sc/existential_angst_factory/\">existential angst</a> comes from having at least one deep problem in your life that you aren't thinking about explicitly, so that the pain which comes from it seems like a natural permanent feature&mdash;then the very first question I'd ask, to identify a possible source of that problem, would be, \"Do you expect your life to improve in the near or mid-term future?\"</p>\n<p>Sometimes I meet people who've been run over by life, in much the same way as being run over by a truck.&nbsp; Grand catastrophe isn't necessary to destroy a will to live.&nbsp; The extended absence of hope leaves the same sort of wreckage.</p>\n<p>People need hope.&nbsp; I'm not the first to say it.</p>\n<p>But I think that the importance of <em>vague hope</em> is underemphasized.</p>\n<p>\"Vague\" is usually not a compliment among rationalists.&nbsp; Hear \"vague hopes\" and you immediately think of, say, an alternative medicine herbal profusion whose touted benefits are so conveniently unobservable (not to mention experimentally unverified) that people will buy it for anything and then refuse to admit it didn't work.&nbsp; You think of poorly worked-out plans with missing steps, or supernatural prophecies made carefully unfalsifiable, or fantasies of unearned riches, or...</p>\n<p>But you know, generally speaking, our beliefs about the future <em>should</em> be vaguer than our beliefs about the past.&nbsp; We just know less about tomorrow than we do about yesterday.</p>\n<p>There are plenty of <em>bad</em> reasons to be vague, all sorts of <em>suspicious </em>reasons to offer nonspecific predictions, but <a href=\"/lw/lw/reversed_stupidity_is_not_intelligence/\">reversed stupidity is not intelligence</a>:&nbsp; When you've eliminated all the ulterior motives for vagueness, your beliefs about the future should <em>still </em>be vague.</p>\n<p>We don't know much about the future; let's hope <em>that </em>doesn't change for <a href=\"/lw/xg/emotional_involvement/\">as long as human emotions stay what they are</a>.&nbsp; Of all the poisoned gifts a big mind could give a small one, <a href=\"/lw/x3/devils_offers/\">a walkthrough for the game</a> has to be near the top of the list.</p>\n<p>What we need to maintain our interest in life, is a <em>justified expectation of pleasant surprises.</em>&nbsp; (And yes, you can <a href=\"/lw/v7/expected_creative_surprises/\">expect a surprise</a> if you're not logically omniscient.)&nbsp; This excludes the herbal profusions, the poorly worked-out plans, and the <a href=\"/lw/tv/excluding_the_supernatural/\">supernatural</a>.&nbsp; The best reason for this justified expectation is <em>experience</em>, that is, being pleasantly surprised on a frequent yet irregular basis.&nbsp; (If this isn't happening to you, please file a bug report with the appropriate authorities.)</p>\n<p><em>Vague justifications</em> for believing in a pleasant <em>specific outcome</em> would be the opposite.</p>\n<p>There's also other dangers of having pleasant hopes that are <em>too specific</em>&mdash;even if <em>justified</em>, though more often they aren't&mdash;and I plan to talk about that in the next post.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/xp/seduced_by_imagination/\">Seduced by Imagination</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/xm/building_weirdtopia/\">Building Weirdtopia</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RLQumypPQGPYg9t6G": 1, "sSNtcEQsqHgN8ZmRF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DGXvLNpiSYBeQ6TLW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 20, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "1212", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CtSS6SkHhLBvdodTY", "29vqqmGNxNRGzffEj", "aEdqh3KPerBNYvoWe", "dKGfNvjGjq4rqffyF", "QfpHRAMRM2HjteKFK", "vYsuM8cpuRgZS5rYB", "8rdoea3g6QGhWQtmx", "qNZM3EGoE5ZeMdCRt", "ZmDEbiEeXk3Wv2sLH", "MTjej6HKvPByx3dEA", "rEDpaTTEzhPLz4fHh", "u6JzcFtPGiznFgDxP", "K4aGvLnHvYgX9pZHS", "88BpRQah9c2GWY3En", "cWjK3SbRcLkb3gN69"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-16T03:10:22.000Z", "modifiedAt": null, "url": null, "title": "Seduced by Imagination", "slug": "seduced-by-imagination", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:57.866Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/88BpRQah9c2GWY3En/seduced-by-imagination", "pageUrlRelative": "/posts/88BpRQah9c2GWY3En/seduced-by-imagination", "linkUrl": "https://www.lesswrong.com/posts/88BpRQah9c2GWY3En/seduced-by-imagination", "postedAtFormatted": "Friday, January 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Seduced%20by%20Imagination&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeduced%20by%20Imagination%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F88BpRQah9c2GWY3En%2Fseduced-by-imagination%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Seduced%20by%20Imagination%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F88BpRQah9c2GWY3En%2Fseduced-by-imagination", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F88BpRQah9c2GWY3En%2Fseduced-by-imagination", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 787, "htmlBody": "<p><strong>Previously in series</strong>:&nbsp; <a href=\"/lw/xo/justified_expectation_of_pleasant_surprises/\">Justified Expectation of Pleasant Surprises</a></p>\n<p>\"Vagueness\" usually has a <a href=\"/lw/ic/the_virtue_of_narrowness/\">bad name</a> in rationality&mdash;connoting skipped steps in reasoning and attempts to <a href=\"/lw/i4/belief_in_belief/\">avoid falsification</a>.&nbsp; But a rational view of the Future <em>should </em>be vague, because the information we have about the Future is <a href=\"/lw/vz/the_weak_inside_view/\">weak</a>.&nbsp; Yesterday I argued that <a href=\"/lw/xo/justified_expectation_of_pleasant_surprises/\">justified vague hopes</a> might also be better <em>hedonically</em> than specific foreknowledge&mdash;the power of pleasant surprises.</p>\n<p>But there's also a more severe warning that I must deliver:&nbsp; It's not a good idea to dwell much <em>on</em> imagined pleasant futures, since you can't actually dwell <em>in </em>them.&nbsp; It can suck the emotional energy out of your actual, current, ongoing life.</p>\n<p>Epistemically, we know the Past much more <em>specifically </em>than the Future.&nbsp; But also on <em>emotional</em> grounds, it's probably wiser to compare yourself to Earth's past, so you can see how far we've come, and how much better we're doing.&nbsp; Rather than comparing your life to an imagined future, and thinking about how awful you've got it Now.</p>\n<p>Having set out to explain George Orwell's observation that <a href=\"http://www.orwell.ru/library/articles/socialists/english/e_fun\">no one can seem to write about a Utopia where anyone would want to live</a>&mdash;having laid out the various Laws of Fun that I believe are being <em>violated </em>in these dreary Heavens&mdash;I am now explaining why you <em>shouldn't</em> apply this knowledge to invent an extremely seductive Utopia and write stories set there.&nbsp; That may suck out your soul like an emotional vacuum cleaner.</p>\n<p><a id=\"more\"></a></p>\n<p>I briefly remarked on this phenomenon <a href=\"/lw/xl/eutopia_is_scary/\">earlier</a>, and someone said, \"Define 'suck out your soul'.\"&nbsp; Well, it's mainly a tactile thing: you can practically <em>feel </em>the pulling sensation, if your dreams wander too far into the Future.&nbsp; It's like something out of H. P. Lovecraft:&nbsp; <em>The Call of Eutopia.</em>&nbsp; A professional hazard of having to stare out into vistas that <em>humans were meant to gaze upon</em>, and knowing <em>a little too much</em> about the lighter side of existence.</p>\n<p>But for the record, I will now lay out the components of \"soul-sucking\", that you may recognize the bright abyss and steer your thoughts away:</p>\n<ul>\n<li>Your emotional energy drains away into your imagination of Paradise:  \n<ul>\n<li>You find yourself thinking of it more and more often.</li>\n<li>The actual challenges of your current existence start to seem less interesting, less compelling; you think of them less and less.</li>\n<li>Comparing everything to your imagined perfect world heightens your annoyances and diminishes your pleasures.</li>\n</ul>\n</li>\n<li>You go into an <a href=\"/lw/lm/affective_death_spirals/\">affective death spiral</a> around your imagined scenario; you're reluctant to admit anything bad could happen on your assumptions, and you find more and more nice things to say.</li>\n<li>Your mind begins to forget the difference between fiction and real life:  \n<ul>\n<li>You originally made many arbitrary or iffy choices in constructing your scenario.&nbsp; You forget that the Future is actually more unpredictable than this, and that you made your choices using limited foresight and merely human optimizing ability.</li>\n<li>You forget that, in real life, at least <em>some </em>of your amazing good ideas are <em>guaranteed </em>not to work as well as they do in your imagination.</li>\n<li>You start wanting the <em>exact specific</em> Paradise you imagined, and worrying about the disappointment if you don't get that <em>exact</em> thing.</li>\n</ul>\n</li>\n</ul>\n<p>Hope can be a dangerous thing.&nbsp; And when you've just been hit hard&mdash;at the moment when you most <em>need</em> hope to keep you going&mdash;that's also when the real world seems most painful, and the world of imagination becomes most seductive.</p>\n<p>It's a balancing act, I think.&nbsp; One needs enough Fun Theory to truly and legitimately justify hope in the future.&nbsp; But not a detailed vision so seductive that it steals emotional energy from the real life and real challenge of creating that future.&nbsp; You need \"a light at the end of the secular rationalist tunnel\" as <a href=\"http://www.overcomingbias.com/2009/01/better-and-better.html#comment-144803018\">Roko put it</a>, but you don't want people to drift away from their bodies into that light.</p>\n<p>So <em>how much</em> light is that, exactly?&nbsp; Ah, now that's the issue.</p>\n<p>I'll start with a simple and genuine question:&nbsp; Is what I've already said, enough?</p>\n<p>Is knowing the abstract fun theory and being able to pinpoint the exact flaws in previous flawed Utopias, enough to make you look forward to tomorrow?&nbsp; Is it enough to inspire a stronger will to live?&nbsp; To dispel worries about a long dark tea-time of the soul?&nbsp; Does it now seem&mdash;on a gut level&mdash;that if we could really build an AI and really shape it, the resulting future would be very much worth staying alive to see?</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/xc/the_uses_of_fun_theory/\">The Uses of Fun (Theory)</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/xo/justified_expectation_of_pleasant_surprises/\">Justified Expectation of Pleasant Surprises</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sSNtcEQsqHgN8ZmRF": 1, "9YFoDPFwMoWthzgkY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "88BpRQah9c2GWY3En", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 34, "extendedScore": null, "score": 5.1e-05, "legacy": true, "legacyId": "1213", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DGXvLNpiSYBeQ6TLW", "yDfxTj9TKYsYiWH5o", "CqyJzDZWvGhhFJ7dY", "w9KWNWFTXivjJ7rjF", "hQSaMafoizBSa3gFR", "XrzQW69HpidzvBxGr", "K4aGvLnHvYgX9pZHS", "4o3zwgofFPLutkqvd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-17T09:28:54.000Z", "modifiedAt": null, "url": null, "title": "Getting Nearer", "slug": "getting-nearer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:32.151Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4KSWmJm6K3EEvHBkd/getting-nearer", "pageUrlRelative": "/posts/4KSWmJm6K3EEvHBkd/getting-nearer", "linkUrl": "https://www.lesswrong.com/posts/4KSWmJm6K3EEvHBkd/getting-nearer", "postedAtFormatted": "Saturday, January 17th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Getting%20Nearer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGetting%20Nearer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4KSWmJm6K3EEvHBkd%2Fgetting-nearer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Getting%20Nearer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4KSWmJm6K3EEvHBkd%2Fgetting-nearer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4KSWmJm6K3EEvHBkd%2Fgetting-nearer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 918, "htmlBody": "<p><strong>Reply to</strong>:&#0160; <a href=\"http://www.overcomingbias.com/2009/01/a-tale-of-two-tradeoffs.html\">A Tale Of Two Tradeoffs</a></p><p>I&#39;m not comfortable with compliments of the direct, personal sort, the &quot;Oh, you&#39;re such a nice person!&quot; type stuff that nice people are able to say with a straight face.&#0160; Even if it would make people like me more - even if it&#39;s socially expected - I have trouble bringing myself to do it.&#0160; So, when I say that I read Robin Hanson&#39;s &quot;Tale of Two Tradeoffs&quot;, and then realized I would spend the rest of my mortal existence typing thought processes as &quot;Near&quot; or &quot;Far&quot;, I hope this statement is received as a due substitute for any gushing compliments that a normal person would give at this point.</p><p>Among other things, this clears up a major puzzle that&#39;s been lingering in the back of my mind for a while now.&#0160; Growing up as a rationalist, I was always telling myself to &quot;Visualize!&quot; or &quot;Reason by simulation, <a href=\"/lw/vx/failure_by_analogy/\">not by analogy</a>!&quot; or &quot;Use causal models, not similarity groups!&quot;&#0160; And those who ignored this principle seemed easy prey to <a href=\"/lw/lm/affective_death_spirals/\">blind enthusiasms</a>, wherein one says that <a href=\"/lw/vy/failure_by_affective_analogy/\">A is good because it is like B which is also good</a>, and the like.</p><p>But later, I learned about the <a href=\"/lw/jg/planning_fallacy/\">Outside View versus the Inside View</a>, and that people asking &quot;What rough class does this project fit into, and when did projects like this finish last time?&quot; were much more accurate and much less optimistic than people who tried to visualize the when, where, and how of their projects.&#0160; And this didn&#39;t seem to fit very well with my injunction to &quot;Visualize!&quot;</p><p>So <em>now</em> I think I understand what this principle was actually doing - it was keeping me in Near-side mode and away from Far-side thinking.&#0160; And it&#39;s not that Near-side mode works so well in any absolute sense, but that Far-side mode is so much more pushed-on by ideology and wishful thinking, and so <em>casual</em> in accepting its conclusions (devoting less computing power before halting).</p><a id=\"more\"></a>\n<p>An example of this might be the balance between offensive and defensive\nnanotechnology, where I started out by - basically - just <em>liking</em> nanotechnology; until I got involved in a discussion about the particulars of nanowarfare, and noticed that people were postulating\ncrazy things to make defense win.&#0160; Which made me realize and say, &quot;Look, the\nbalance between offense and defense has been tilted toward offense ever\nsince the invention of nuclear weapons, and military nanotech could <em>use</em> nuclear weapons, and I don&#39;t see how you&#39;re going to build a molecular barricade against <em>that</em>.&quot;</p><p>Are the particulars of that discussion likely to be, well, <em>correct?</em>&#0160; Maybe not.&#0160; But so long as I wasn&#39;t thinking of <em>any</em> particulars, my brain had free reign to just... import whatever affective valence the word &quot;nanotechnology&quot; had, and use that as a snap judgment of everything.</p><p>You can still be biased about particulars, of course.&#0160; You can insist that nanotech couldn&#39;t possibly be radiation-hardened enough to manipulate U-235, which someone tried as a response (<em>fyi: this is extremely silly</em>).&#0160; But in my case, at least, something about <em>thinking in particulars</em>...</p><p>...just snapped me out of the trance, somehow.</p><p>When you&#39;re thinking using very abstract categories - rough classes low on computing power - about things distant from you, then you&#39;re also - if Robin&#39;s hypothesis is correct - more subject to ideological bias.&#0160; Together this implies you can cherry-pick those very loose categories to put X together with whatever &quot;similar&quot; Y is ideologically convenient, as in the old saw that &quot;<a href=\"/lw/oy/is_humanism_a_religionsubstitute/\">atheism is a religion</a>&quot; (and not playing tennis is a sport).</p><p>But the most frustrating part of all, is the <em>casualness</em> of it - the way that ideologically convenient Far thinking is just thrown together out of whatever ingredients come to hand.&#0160; The <a href=\"http://www.overcomingbias.com/2008/12/we-agree-get-froze.html\">ten-second dismissal of cryonics</a>, without any attempt to visualize how much information is preserved by vitrification and could be retrieved by a molecular-level scan.&#0160; Cryonics just gets casually, perceptually classified as &quot;not scientifically verified&quot; and tossed out the window.&#0160; Or &quot;what if you wake up in Dystopia?&quot; and tossed out the window.&#0160; Far thinking is <em>casual</em> - that&#39;s the most frustrating aspect about trying to argue with it.\n</p><p>This seems like an argument for writing fiction with lots of concrete details <em>if you want people to take a subject seriously and think about it in a less biased way</em>.&#0160; This is not something I would have thought based on <a href=\"/lw/k9/the_logical_fallacy_of_generalization_from/\">my previous view</a>.</p><p>Maybe cryonics advocates really <em>should</em> focus on writing fiction stories that turn on the gory details of cryonics, or viscerally depict the regret of someone who didn&#39;t persuade their mother to sign up.&#0160; (Or offering prizes to professionals who do the same; writing fiction is hard, writing SF is harder.)</p><p>But I&#39;m worried that, for whatever reason, reading concrete fiction is a special case that <em>doesn&#39;t work</em> to get people to do Near-side thinking.</p><p>Or there are <em>some </em>people who are inspired to Near-side thinking by fiction, and only these can actually be helped by reading science fiction.</p><p>Maybe there are people who encounter big concrete detailed fictions process them in a Near way - the sort of people who notice plot holes.&#0160; And others who just &quot;take it all in stride&quot;, casually, so that however much concrete fictional &quot;information&quot; they encounter, they only process it using casual &quot;Far&quot; thinking.&#0160; I wonder if this difference has more to do with upbringing or genetics.&#0160; Either way, it may lie at the core of the partial yet statistically outstanding correlation between careful futurists and science fiction fans.</p><p>I expect I shall be thinking about this for a while.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "3uE2pXvbcnS9nnZRE": 1, "49GPZJoXc7d2gKTDG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4KSWmJm6K3EEvHBkd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 4.6982553776132775e-07, "legacy": true, "legacyId": "1214", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["C4EjbrvG3PvZzizZb", "XrzQW69HpidzvBxGr", "o8Ju8pTGvAfkkg6xZ", "CPm5LTwHrvBJCa9h5", "PMr6f7ZocEWFtCYXj", "rHBdcHGLJ7KvLJQPk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-18T09:03:29.000Z", "modifiedAt": null, "url": null, "title": "In Praise of Boredom", "slug": "in-praise-of-boredom", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:07.448Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WMDy4GxbyYkNrbmrs/in-praise-of-boredom", "pageUrlRelative": "/posts/WMDy4GxbyYkNrbmrs/in-praise-of-boredom", "linkUrl": "https://www.lesswrong.com/posts/WMDy4GxbyYkNrbmrs/in-praise-of-boredom", "postedAtFormatted": "Sunday, January 18th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20Praise%20of%20Boredom&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20Praise%20of%20Boredom%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWMDy4GxbyYkNrbmrs%2Fin-praise-of-boredom%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20Praise%20of%20Boredom%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWMDy4GxbyYkNrbmrs%2Fin-praise-of-boredom", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWMDy4GxbyYkNrbmrs%2Fin-praise-of-boredom", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1823, "htmlBody": "<p><a href=\"/lw/xp/seduced_by_imagination/\"></a></p>\n<p>If I were to make a short list of the <em>most</em> important human qualities&mdash;</p>\n<p>&mdash;and yes, this is a fool's errand, because human nature is <a href=\"/lw/l3/thou_art_godshatter/\">immensely complicated</a>, and we <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">don't even notice all the tiny tweaks</a> that fine-tune our <a href=\"/lw/tc/unnatural_categories/\">moral categories</a>, and <a href=\"/lw/xd/growing_up_is_hard/\">who knows</a> how our attractors would change shape if we eliminated a single human emotion&mdash;</p>\n<p>&mdash;but even so, if I had to point to just a few things and say, \"If you lose just one of these things, you lose most of the expected value of the Future; but conversely if an alien species <a href=\"/lw/so/humans_in_funny_suits/\">independently evolved</a> just these few things, we might even want to be friends\"&mdash;</p>\n<p>&mdash;then the top three items on the list would be sympathy, boredom and consciousness.<em><br /></em></p>\n<p>Boredom is a subtle-splendored thing.&nbsp; You wouldn't want to get bored with breathing, for example&mdash;even though it's the same motions over and over and over and over again for minutes and hours and years and decades.</p>\n<p>Now I know some of you out there are thinking, \"Actually, I'm quite bored with breathing and I wish I didn't have to,\" but <em>then </em>you wouldn't want to get bored with switching transistors.</p>\n<p>According to the human value of boredom, some things are allowed to be highly repetitive <em>without</em> being boring&mdash;like obeying the same laws of physics every day.</p>\n<p>Conversely, other repetitions <em>are</em> supposed to be boring, like playing the same level of Super Mario Brothers over and over and over again until the end of time.&nbsp; And let us note that if the pixels in the game level have a <em>slightly different color</em> each time, that is <em>not</em> sufficient to prevent it from being \"<em>the same damn thing</em>, over and over and over again\".</p>\n<p>Once you take a closer look, it turns out that boredom is quite interesting.<a id=\"more\"></a></p>\n<p>One of the key elements of boredom was suggested in \"<a href=\"/lw/wx/complex_novelty/\">Complex Novelty</a>\":&nbsp; If your activity isn't <em>teaching </em>you insights you didn't already know, then it is <em>non-novel,</em> therefore old, therefore boring.</p>\n<p>But this doesn't quite cover the distinction.&nbsp; Is breathing teaching you anything?&nbsp; Probably not at this moment, but you wouldn't want to stop breathing.&nbsp; Maybe you'd want to <em>stop noticing</em> your breathing, which you'll do as soon as I stop drawing your attention to it.</p>\n<p>I'd suggest that the repetitive activities which are allowed to <em>not</em> be boring fall into two categories:</p>\n<ul>\n<li>Things so extremely low-level, or with such a small volume of possibilities, that you couldn't avoid repeating them even if you tried; but which are required to support other non-boring activities.&nbsp; You know, like breathing, or obeying the laws of physics, or cell division&mdash;that sort of thing.</li>\n<li>Things so high-level that their \"stability\" still implies an immense space of specific possibilities, yet which are tied up with our identity or our values.&nbsp; Like thinking, for example.</li>\n</ul>\n<p>Let me talk about that second category:</p>\n<p>Suppose you were unraveling the true laws of physics and discovering all sorts of neat stuff you hadn't known before... when suddenly you got <em>bored</em> with \"changing your beliefs based on observation\".&nbsp; You are sick of anything resembling \"Bayesian updating\"&mdash;it feels like playing the same video game over and over.&nbsp; Instead you decide to believe anything said on 4chan.</p>\n<p>Or to put it another way, suppose that you were something like a sentient chessplayer&mdash;a sentient version of Deep Blue.&nbsp; Like a modern human, you have <em>no introspective access</em> to your own algorithms.&nbsp; Each chess game appears different&mdash;you play new opponents and steer into new positions, composing new strategies, avoiding new enemy gambits.&nbsp; You are content, and not at all bored; you never <em>appear to yourself</em> to be doing the same thing twice&mdash;it's a different chess game each time.</p>\n<p>But now, suddenly, you gain access to, and understanding of, your own chess-playing program.&nbsp; Not just the raw code; you can monitor its execution.&nbsp; You can see that it's actually <em>the same damn code, doing the same damn thing,</em> over and over and over again.&nbsp; Run the same damn position evaluator.&nbsp; Run the same damn sorting algorithm to order the branches.&nbsp; Pick the top branch, again.&nbsp; Extend it one position forward, again.&nbsp; Call the same damn subroutine and start over.</p>\n<p>I have a small unreasonable fear, somewhere in the back of my mind, that if I ever do fully understand the algorithms of intelligence, it will destroy all remaining novelty&mdash;no matter what new situation I encounter, I'll know I can solve it just by <em>being intelligent,</em> the same damn thing over and over.&nbsp; All novelty will be used up, all existence will become boring, the remaining differences no more important than shades of pixels in a video game.&nbsp; Other beings will go about in blissful unawareness, having been steered away from studying this forbidden cognitive science.&nbsp; But I, having already thrown myself on the grenade of AI, will face a choice between eternal boredom, or excision of my forbidden knowledge and all the memories leading up to it (thereby destroying my existence as Eliezer, more or less).</p>\n<p>Now this, mind you, is not my predictive line of maximum probability.&nbsp; To understand abstractly what rough sort of work the brain is doing, doesn't let you monitor its detailed execution as a boring repetition.&nbsp; I already know about Bayesian updating, yet I haven't become bored with the act of learning.&nbsp; And a self-editing mind can quite reasonably exclude certain levels of introspection from boredom, just like breathing can be legitimately excluded from boredom.&nbsp; (Maybe these top-level cognitive algorithms ought also to be excluded from perception&mdash;if something is stable, why bother seeing it all the time?)</p>\n<p>No, it's just a cute little nightmare, which I thought made a nice illustration of this proposed principle:</p>\n<p>That the very top-level things (like Bayesian updating, or attaching value to sentient minds rather than paperclips) and the very low-level things (like breathing, or switching transistors) are the things we shouldn't get bored with.&nbsp; And the mid-level things between, are where we should seek <a href=\"/lw/wx/complex_novelty/\">novelty</a>.&nbsp; (To a first approximation, the novel is the inverse of the learned; it's something with a learnable element not yet covered by previous insights.)</p>\n<p>Now this is probably not <em>exactly</em> how our <em>current </em>emotional circuitry of boredom works.&nbsp; That, I expect, would be hardwired relative to various sensory-level definitions of predictability, surprisingness, repetition, attentional salience, and perceived effortfulness.</p>\n<p>But this is Fun Theory, so we are mainly concerned with how boredom <em>should</em> work in the long run.</p>\n<p>Humanity acquired boredom the same way as we acquired the rest of our emotions: the <a href=\"/lw/l3/thou_art_godshatter/\">godshatter</a> idiom whereby evolution's instrumental policies became our own terminal values, pursued <a href=\"/lw/l4/terminal_values_and_instrumental_values/\">for their own sake</a>: <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">sex is fun even if you use birth control</a>.&nbsp; Evolved aliens might, or might not, acquire roughly the same boredom in roughly the same way.</p>\n<p>Do not give into the temptation of <a href=\"/lw/rn/no_universally_compelling_arguments/\">universalizing</a> <a href=\"/lw/so/humans_in_funny_suits/\">anthropomorphic</a> <a href=\"/lw/td/magical_categories/\">values</a>, and think:&nbsp; \"But any rational agent, regardless of its utility function, will face the <em>exploration/exploitation tradeoff,</em> and will therefore occasionally get bored with exploiting, and go exploring.\"</p>\n<p>Our emotion of boredom is <em>a</em> way of exploring, but not the <em>only </em>way for an ideal optimizing agent.</p>\n<p>The idea of a <em>steady trickle of mid-level novelty</em> is a human <a href=\"/lw/l4/terminal_values_and_instrumental_values/\">terminal value</a>, not something we do for the sake of something else.&nbsp; Evolution might have originally given it to us in order to have us explore as well as exploit.&nbsp; But now we explore for its own sake.&nbsp; That steady trickle of novelty is a terminal value to us; it is not the <em>most</em> efficient instrumental method for exploring and exploiting.</p>\n<p>Suppose you were dealing with something like an <a href=\"/lw/tn/the_true_prisoners_dilemma/\">expected paperclip maximizer</a>&mdash;something that might use quite complicated instrumental policies, but in the service of a utility function that we would regard as simple, with a single term compactly defined.</p>\n<p>Then I would expect the exploration/exploitation tradeoff to go something like as follows:&nbsp; The paperclip maximizer would assign some resources to cognition that searched for more efficient ways to make paperclips, or harvest resources from stars.&nbsp; Other resources would be devoted to the actual harvesting and paperclip-making.&nbsp; (The paperclip-making might not start until after a long phase of harvesting.)&nbsp; At every point, the most efficient method yet discovered&mdash;for resource-harvesting, or paperclip-making&mdash;would be used, over and over and over again.&nbsp; It wouldn't be <em>boring,</em> just maximally instrumentally efficient.</p>\n<p>In the beginning, lots of resources would go into preparing for efficient work over the rest of time.&nbsp; But as cognitive resources yielded diminishing returns in the abstract search for efficiency improvements, less and less time would be spent thinking, and more and more time spent creating paperclips.&nbsp; By whatever the most efficient known method, over and over and over again.</p>\n<p>(Do human beings get <em>less </em>easily bored as we grow older, <em>more tolerant</em> of repetition, because any further discoveries are less valuable, because we have less time left to exploit them?)</p>\n<p>If we run into aliens who don't share <em>our </em>version of boredom&mdash;a steady trickle of mid-level novelty as a terminal preference&mdash;then perhaps every alien throughout their civilization will just be playing the most exciting level of the most exciting video game ever discovered, over and over and over again.&nbsp; Maybe with <a href=\"/lw/x5/nonsentient_optimizers/\">nonsentient AIs</a> taking on the <em>drudgework </em>of searching for a more exciting video game.&nbsp; After all, without an inherent preference for novelty, exploratory attempts will usually have less expected value than exploiting the best policy previously encountered.&nbsp; And that's if you explore by trial at all, as opposed to using more abstract and efficient thinking.</p>\n<p>Or if the aliens are rendered non-bored by seeing pixels of a slightly different shade&mdash;if their definition of <em>sameness </em>is more specific than ours, and their <em>boredom</em> less general&mdash;then from our perspective, most of their civilization will be doing the human::same thing over and over again, and hence, be very human::boring.</p>\n<p>Or maybe if the aliens have no fear of life becoming too simple and repetitive, they'll just collapse themselves into orgasmium.</p>\n<p>And if <em>our</em> version of boredom is less strict than that of the aliens, maybe they'd take one look at one day in the life of one member of our civilization, and never bother looking at the rest of us.&nbsp; From our perspective, their civilization would be needlessly chaotic, and so entropic, lower in what we regard as quality; they wouldn't play the same game for long enough to get good at it.</p>\n<p>But if our versions of boredom are similar <em>enough </em>&mdash;terminal preference for a stream of mid-level novelty defined relative to <a href=\"/lw/wx/complex_novelty/\">learning insights not previously possessed</a>&mdash;then we might find our civilizations mutually worthy of tourism.&nbsp; Each new piece of alien art would strike us as <a href=\"/lw/vm/lawful_creativity/\">lawfully creative</a>, high-quality according to a recognizable criterion, yet not like the other art we've already seen.</p>\n<p>It is one of the things that would make our two species <em>ramen </em>rather than <em>varelse</em>, to invoke the Hierarchy of Exclusion.&nbsp; And I've never seen anyone define those two terms well, including Orson Scott Card who invented them; but it might be something like \"aliens you can get along with, versus aliens for which there is no reason to bother trying\".</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/xs/sympathetic_minds/\">Sympathetic Minds</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/x9/dunbars_function/\">Dunbar's Function</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sSNtcEQsqHgN8ZmRF": 1, "R6uagTfhhBeejGrrf": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WMDy4GxbyYkNrbmrs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 32, "extendedScore": null, "score": 5.4e-05, "legacy": true, "legacyId": "1215", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 104, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["88BpRQah9c2GWY3En", "cSXZpvqpa9vbGGLtG", "4ARaTpNX62uaL86j6", "XeHYXXTGRuDrhk5XL", "EQkELCGiGQwvrrp3L", "Zkzzjg3h7hW5Z36hK", "aEdqh3KPerBNYvoWe", "n5ucT5ZbPdhfGNLtP", "XPErvb8m9FapXCjhA", "PtoQdG7E8MxYJrigu", "PoDAyQMWEXBBBEJ5P", "HFyWNBnDNEDsDNLrZ", "HsRFQTAySAx8xbXEc", "KKLQp934n77cfZpPn", "K4aGvLnHvYgX9pZHS", "NLMo5FZWFFq652MNe", "W5PhyEQqEWTcpRpqn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-19T09:31:03.000Z", "modifiedAt": null, "url": null, "title": "Sympathetic Minds", "slug": "sympathetic-minds", "viewCount": null, "lastCommentedAt": "2019-08-18T23:44:43.657Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NLMo5FZWFFq652MNe/sympathetic-minds", "pageUrlRelative": "/posts/NLMo5FZWFFq652MNe/sympathetic-minds", "linkUrl": "https://www.lesswrong.com/posts/NLMo5FZWFFq652MNe/sympathetic-minds", "postedAtFormatted": "Monday, January 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sympathetic%20Minds&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASympathetic%20Minds%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNLMo5FZWFFq652MNe%2Fsympathetic-minds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sympathetic%20Minds%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNLMo5FZWFFq652MNe%2Fsympathetic-minds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNLMo5FZWFFq652MNe%2Fsympathetic-minds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1566, "htmlBody": "<p>\"Mirror neurons\" are neurons that are active both when performing an action and observing the same action&mdash;for example, a neuron that fires when you hold up a finger or see someone else holding up a finger.&nbsp; Such neurons have been directly recorded in primates, and consistent neuroimaging evidence has been found for humans.</p>\n<p>You may recall from my previous writing on \"<a href=\"/lw/so/humans_in_funny_suits/\">empathic</a> <a href=\"/lw/sr/the_comedy_of_behaviorism/\">inference</a>\" the idea that brains are so complex that the only way to simulate them is by forcing a similar brain to behave similarly.&nbsp; A brain is so complex that if a human tried to understand brains the way that we understand e.g. gravity or a car&mdash;observing the whole, observing the parts, building up a theory from scratch&mdash;then we would be unable to <em>invent good hypotheses</em> in our mere mortal lifetimes.&nbsp; The only possible way you can hit on an \"Aha!\" that describes a system as incredibly complex as an Other Mind, is if you happen to run across something amazingly similar to the Other Mind&mdash;namely your own brain&mdash;which you can actually force to behave similarly and use as a hypothesis, yielding predictions.</p>\n<p>So that is what I would call \"empathy\".</p>\n<p>And then \"sympathy\" is something else on top of this&mdash;to smile when you see someone else smile, to hurt when you see someone else hurt.&nbsp; It goes beyond the realm of prediction into the realm of reinforcement.</p>\n<p>And you ask, \"<a href=\"/lw/sa/the_gift_we_give_to_tomorrow/\">Why</a> would <a href=\"/lw/uk/beyond_the_reach_of_god/\">callous</a> <a href=\"/lw/kr/an_alien_god/\">natural selection</a> do anything <em>that</em> nice?\"</p>\n<p><a id=\"more\"></a></p>\n<p>It might have gotten started, maybe, with a mother's love for her children, or a brother's love for a sibling.&nbsp; You can want them to live, you can want them to fed, sure; but if you smile when they smile and wince when they wince, that's a simple urge that leads you to deliver help along a broad avenue, in many walks of life.&nbsp; So long as you're in the ancestral environment, what your relatives want probably has something to do with your relatives' reproductive success&mdash;this being an explanation for the selection pressure, of course, <a href=\"/lw/l1/evolutionary_psychology/\">not a conscious belief</a>.</p>\n<p>You may ask, \"Why not evolve a more abstract desire to see certain people tagged as 'relatives' get what they want, without actually feeling yourself what they feel?\"&nbsp; And I would shrug and reply, \"Because then there'd have to be a whole definition of 'wanting' and so on.&nbsp; Evolution doesn't take the elaborate correct optimal path, it falls up the fitness landscape like water flowing downhill.&nbsp; The mirroring-architecture was already there, so it was a short step from empathy to sympathy, and it got the job done.\"</p>\n<p>Relatives&mdash;and then reciprocity; your allies in the tribe, those with whom you trade favors.&nbsp; Tit for Tat, or evolution's elaboration thereof to account for social reputations.</p>\n<p>Who is the most formidable, among the human kind?&nbsp; The strongest?&nbsp; The smartest?&nbsp; More often than either of these, I think, it is the one who can call upon the most friends.</p>\n<p>So how do you make lots of friends?</p>\n<p>You could, perhaps, have a specific urge to bring your allies food, like a vampire bat&mdash;they have a whole system of reciprocal blood donations going in those colonies.&nbsp; But it's a more <em>general </em>motivation, that will lead the organism to store up <em>more </em>favors, if you smile when designated friends smile.</p>\n<p>And what kind of organism will avoid making its friends angry at it, in full generality?&nbsp; One that winces when they wince.</p>\n<p>Of course you also want to be able to kill designated Enemies without a qualm&mdash;these <em>are </em>humans we're talking about.</p>\n<p>But... I'm not sure of this, but it <em>does</em> look to me like sympathy, among humans, is \"on\" by default.&nbsp; There are cultures that help strangers... and cultures that eat strangers; the question is which of these requires the explicit imperative, and which is the default behavior for humans.&nbsp; I don't really think I'm being such a crazy idealistic fool when I say that, based on my admittedly limited knowledge of anthropology, it looks like sympathy is on by default.</p>\n<p>Either way... it's painful if you're a bystander in a war between two sides, and your sympathy has <em>not</em> been switched off for either side, so that you wince when you see a dead child no matter what the caption on the photo; and yet those two sides have no sympathy for each other, and they go on killing.</p>\n<p>So that is the human idiom of <em>sympathy </em>&mdash;a strange, complex, deep implementation of reciprocity and helping.&nbsp; It tangles minds together&mdash;not by a term in the utility function for some other mind's \"desire\", but by the simpler and yet far more consequential path of mirror neurons: feeling what the other mind feels, and seeking similar states.&nbsp; Even if it's only done by observation and inference, and not by direct transmission of neural information as yet.</p>\n<p>Empathy is a human way of predicting other minds.&nbsp; It is not the <em>only </em>possible way.</p>\n<p>The human brain is not <em>quickly</em> rewirable; if you're suddenly put into a dark room, you can't rewire the visual cortex as auditory cortex, so as to better process sounds, until you leave, and then suddenly shift all the neurons back to being visual cortex again.</p>\n<p>An AI, at least one running on anything like a modern programming architecture, can trivially shift computing resources from one thread to another.&nbsp; Put in the dark?&nbsp; Shut down vision and devote all those operations to sound; swap the old program to disk to free up the RAM, then swap the disk back in again when the lights go on.</p>\n<p>So why would an AI need to force its <em>own</em> mind into a state similar to what it wanted to predict?&nbsp; Just create a <em>separate </em>mind-instance&mdash;maybe with different algorithms, the better to simulate that very dissimilar human.&nbsp; Don't try to mix up the data with your own mind-state; don't use mirror neurons.&nbsp; Think of all the risk and mess <em>that</em> implies!</p>\n<p>An expected utility maximizer&mdash;especially one that does understand intelligence on an abstract level&mdash;has other options than <em>empathy</em>, when it comes to understanding other minds.&nbsp; The agent doesn't need to put <em>itself </em>in anyone else's shoes; it can just model the other mind <em>directly.</em>&nbsp; A hypothesis like any other hypothesis, just a little bigger.&nbsp; You don't need to become your shoes to understand your shoes.</p>\n<p>And sympathy?&nbsp; Well, suppose we're dealing with an expected paperclip maximizer, but one that isn't yet powerful enough to have things all its own way&mdash;it has to deal with humans to get its paperclips.&nbsp; So the paperclip agent... models those humans as relevant parts of the environment, models their probable reactions to various stimuli, and does things that will make the humans feel favorable toward it in the future.</p>\n<p>To a paperclip maximizer, the humans are just machines with pressable buttons.&nbsp; No need to <em>feel what the other feels</em>&mdash;if that were even <em>possible </em>across such a tremendous gap of internal architecture.&nbsp; How could an expected paperclip maximizer \"feel happy\" when it saw a human smile?&nbsp; \"Happiness\" is an idiom of policy reinforcement learning, not expected utility maximization.&nbsp; A paperclip maximizer doesn't feel happy when it makes paperclips, it just chooses whichever action leads to the greatest number of expected paperclips.&nbsp; Though a paperclip maximizer might find it convenient to display a smile when it made paperclips&mdash;so as to help manipulate any humans that had designated it a friend.</p>\n<p>You might find it a bit difficult to imagine such an algorithm&mdash;to put yourself into the shoes of something that <a href=\"/lw/rm/the_design_space_of_mindsingeneral/\">does not work like you do</a>, and does not work like any mode your brain can make itself operate in.</p>\n<p>You can make your brain operating in the mode of hating an enemy, but that's not right either.&nbsp; The way to imagine how a truly <em>unsympathetic</em> mind sees a human, is to imagine yourself as a useful machine with levers on it.&nbsp; Not a human-shaped machine, because we have instincts for that.&nbsp; Just a woodsaw or something.&nbsp; Some levers make the machine output coins, other levers might make it fire a bullet.&nbsp; The machine does have a persistent internal state and you have to pull the levers in the right order.&nbsp; Regardless, it's just a complicated causal system&mdash;nothing <a href=\"/lw/tv/excluding_the_supernatural/\">inherently mental</a> about it.</p>\n<p>(To understand <em>unsympathetic </em>optimization processes, I would suggest studying <a href=\"/lw/kr/an_alien_god/\">natural selection</a>, which doesn't bother to anesthetize fatally wounded and dying creatures, even when their pain no longer serves any reproductive purpose, because the anesthetic would serve no reproductive purpose either.)</p>\n<p>That's why I listed \"sympathy\" in front of even \"boredom\" on my <a href=\"/lw/xr/in_praise_of_boredom/\">list</a> of things that would be required to have aliens which are the least bit, if you'll pardon the phrase, sympathetic.&nbsp; It's not impossible that sympathy exists among some significant fraction of all evolved alien intelligent species; mirror neurons seem like the sort of thing that, having happened once, <em>could </em>happen again.</p>\n<p><em>Unsympathetic </em>aliens might be trading partners&mdash;or not, stars and such resources are pretty much the same the universe over.&nbsp; We might negotiate treaties with them, and they might keep them for calculated fear of reprisal.&nbsp; We might even cooperate in the <a href=\"/lw/tn/the_true_prisoners_dilemma/\">Prisoner's Dilemma</a>.&nbsp; But we would never be friends with them.&nbsp; They would never see us as anything but means to an end.&nbsp; They would never shed a tear for us, nor smile for our joys.&nbsp; And the others of their own kind would receive no different consideration, nor have any sense that they were missing something important thereby.</p>\n<p>Such aliens would be <em>varelse,</em> not <em>ramen</em>&mdash;the sort of aliens we can't relate to on any personal level, and no point in trying.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"R6uagTfhhBeejGrrf": 1, "exZi6Bing5AiM4ZQB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NLMo5FZWFFq652MNe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": 51, "extendedScore": null, "score": 7.6e-05, "legacy": true, "legacyId": "1216", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "9bvAELWc8y2gYjRav", "canonicalCollectionSlug": "rationality", "canonicalBookId": "T3CjiQq6bBgFuzvp6", "canonicalNextPostSlug": "high-challenge", "canonicalPrevPostSlug": "the-true-prisoner-s-dilemma", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 51, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Zkzzjg3h7hW5Z36hK", "9fpWoXpNv83BAHJdc", "pGvyqAQw6yqTjpKf4", "sYgv4eYH82JEsTD34", "pLRogvJLPPg6Mrvg4", "epZLSoNvjW53tqNj9", "tnWRXkcDi5Tw9rzXw", "u6JzcFtPGiznFgDxP", "WMDy4GxbyYkNrbmrs", "HFyWNBnDNEDsDNLrZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-20T06:17:42.000Z", "modifiedAt": null, "url": null, "title": "Interpersonal Entanglement", "slug": "interpersonal-entanglement", "viewCount": null, "lastCommentedAt": "2020-03-08T20:05:32.411Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Py3uGnncqXuEfPtQp/interpersonal-entanglement", "pageUrlRelative": "/posts/Py3uGnncqXuEfPtQp/interpersonal-entanglement", "linkUrl": "https://www.lesswrong.com/posts/Py3uGnncqXuEfPtQp/interpersonal-entanglement", "postedAtFormatted": "Tuesday, January 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Interpersonal%20Entanglement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInterpersonal%20Entanglement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPy3uGnncqXuEfPtQp%2Finterpersonal-entanglement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Interpersonal%20Entanglement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPy3uGnncqXuEfPtQp%2Finterpersonal-entanglement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPy3uGnncqXuEfPtQp%2Finterpersonal-entanglement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1331, "htmlBody": "<p><strong>Previously in series</strong>:&nbsp; <a href=\"/lw/xs/sympathetic_minds/\">Sympathetic Minds</a></p>\n<p>Today I shall criticize yet another Utopia.&nbsp; This Utopia isn't famous in the literature.&nbsp; But it's considerably superior to many better-known Utopias&mdash;more fun than the Christian Heaven, or Greg Egan's upload societies, for example.&nbsp; And so the main flaw is well worth pointing out.</p>\n<p>This Utopia consists of a one-line remark on an IRC channel:</p>\n<blockquote>\n<p>&lt;reedspacer&gt; living in your volcano lair with catgirls is probably a vast increase in standard of living for most of humanity</p>\n</blockquote>\n<p>I've come to think of this as Reedspacer's Lower Bound.</p>\n<p>Sure, it sounds silly.&nbsp; But if your grand vision of the future isn't at <em>least </em>as much fun as a volcano lair with catpersons of the appropriate gender, you should just go with that instead.&nbsp; This rules out a surprising number of proposals.</p>\n<p>But today I am here to <em>criticize </em>Reedspacer's Lower Bound&mdash;the problem being the catgirls.</p>\n<p>I've joked about the subject, now and then&mdash;\"Donate now, and get a free catgirl or catboy after the Singularity!\"&mdash;but I think it would actually be a terrible idea.&nbsp; In fact, today's post could have been entitled \"Why Fun Theorists Don't Believe In Catgirls.\"</p>\n<p><a id=\"more\"></a></p>\n<p>I first realized that catpeople were a potential threat, at the point when a friend said&mdash;quotes not verbatim&mdash;</p>\n<p>\"I want to spend a million years having sex with catgirls after the Singularity.\"</p>\n<p>I replied,</p>\n<p>\"No, you don't.\"</p>\n<p>He said, \"Yes I do.\"</p>\n<p>I said, \"No you don't.&nbsp; You'd get bored.\"</p>\n<p>He said, \"Well, then I'd just modify my brain not to get bored&mdash;\"</p>\n<p>And I said:&nbsp; \"AAAAIIIIIIEEEEEEEEE\"</p>\n<p>Don't worry, the story has a happy ending.&nbsp; A couple of years later, the same friend came back and said:</p>\n<p>\"Okay, I've gotten a bit more mature now&mdash;it's a long story, actually&mdash;and now I realize I wouldn't want to do that.\"</p>\n<p>To which I sagely replied:</p>\n<p>\"<em>HA!&nbsp; </em><em>HA HA HA!</em>&nbsp; You wanted to spend a <em>million years</em> having sex with catgirls.&nbsp; It only took you <em>two years</em> to change your mind and you <em>didn't even have sex with any catgirls</em>.\"</p>\n<p>Now, this <em>particular</em> case was probably about <a href=\"/lw/hw/scope_insensitivity/\">scope insensitivity</a>, the \"<a href=\"/lw/wv/prolegomena_to_a_theory_of_fun/\">moment of hearing the good news</a>\" bias, and the <a href=\"/lw/xp/seduced_by_imagination/\">emotional magnetism of specific fantasy</a>.</p>\n<p>But my <em>general </em>objection to catpeople&mdash;well, call me a sentimental Luddite, but I'm worried about the prospect of nonsentient romantic partners.</p>\n<p>(Where \"nonsentient romantic/sex partner\" is pretty much what I use the word \"catgirl\" to indicate, in futuristic discourse.&nbsp; The notion of creating <em>sentient</em> beings to staff a volcano lair, gets us into a <em>whole </em>'nother class of <a href=\"/lw/x5/nonsentient_optimizers/\">objections</a>.&nbsp; And as for existing humans choosing to take on feline form, that seems to me scarcely different from wearing lingerie.)</p>\n<p>\"But,\" you ask, \"what <em>is</em> your objection to nonsentient lovers?\"</p>\n<p>In a nutshell&mdash;sex/romance, as we know it now, is a primary dimension of <span style=\"font-style: italic;\">multiplayer </span><em>fun.</em>&nbsp; If you take that fun and redirect it to something that isn't socially entangled, if you turn sex into an exclusively single-player game, then you've just made life that much simpler&mdash;in the same way that eliminating <a href=\"/lw/xr/in_praise_of_boredom/\">boredom</a> or <a href=\"/lw/xs/sympathetic_minds/\">sympathy</a> or <a href=\"/lw/lb/not_for_the_sake_of_happiness_alone/\">values over nonsubjective reality</a> or<span style=\"text-decoration: underline;\"> </span><a href=\"/lw/xb/free_to_optimize/\">individuals wanting to navigate their own futures</a>, would tend to make life \"simpler\".&nbsp; When I consider how easily human existence could collapse into sterile simplicity, if just a single major value were eliminated, I get very protective of the <em>complexity</em> of human existence.</p>\n<p>I ask it in all seriousness&mdash;is there <em>any</em> aspect of human existence as complicated as romance?&nbsp; Think twice before you say, \"Well, it doesn't seem all that complicated to <em>me;</em> now calculus, on the other hand, that's complicated.\"&nbsp; We are <a href=\"/lw/jp/occams_razor/\">congenitally biased to underestimate the complexity of things that involve human intelligence</a>, because the complexity is obscured and simplified and swept under a rug.&nbsp; Interpersonal relationships involve <em>brains,</em> still the most complicated damn things around.&nbsp; And among interpersonal relationships, love is (at least potentially) more complex than being nice to your friends and kin, negotiating with your allies, or outsmarting your enemies.&nbsp; Aspects of all three, really.&nbsp; And that's not merely having a utility function over the other mind's state&mdash;thanks to <a href=\"/lw/xs/sympathetic_minds/\">sympathy</a>, we get tangled up with that other mind.&nbsp; Smile when the one smiles, wince when the one winces.</p>\n<p>If you delete the intricacy of human romantic/sexual relationships between sentient partners&mdash;then the peak complexity of the human species goes down.&nbsp; The most complex fun thing you can do, has its pleasure surgically detached and redirected to something simpler.</p>\n<p>I'd call that a major step in the wrong direction.</p>\n<p>Mind you... we've got to do <em>something</em> about, you know, the problem.</p>\n<p>Anyone the least bit familiar with evolutionary psychology knows that the complexity of human relationships, directly reflects the incredible complexity of the interlocking selection pressures involved.&nbsp; Males and females do need each other to reproduce, but there are huge conflicts of reproductive interest between the sexes.&nbsp; I don't mean to go into Evolutionary Psychology 101 (Robert Wright's <em>The Moral Animal</em> is one popular book), but e.g. a woman must always invest nine months of work into a baby and usually much more to raise it, where a man might invest only a few minutes; but among humans significant paternal investments are quite common, yet a woman is always certain of maternity where a man is uncertain of paternity... which creates an incentive for the woman to surreptitiously seek out better genes... <a href=\"/lw/l1/evolutionary_psychology/\">none of this is conscious or even subconscious</a>, it's just the selection pressures that helped construct our particular emotions and attractions.</p>\n<p>And as the upshot of all these huge conflicts of reproductive interest...</p>\n<p>Well, men and women do still need each other to reproduce.&nbsp; So we are still built to be attracted to each other.&nbsp; We don't actually flee screaming into the night.</p>\n<p>But men are not optimized to make women happy, and women are not optimized to make men happy.&nbsp; The vast majority of men are not what the vast majority of women would <em>most </em>prefer, or vice versa.&nbsp; I don't know if anyone has ever actually done this study, but I bet that both gay and lesbian couples are happier on average with their relationship than heterosexual couples.&nbsp; (Googles... yep, <a href=\"http://news.softpedia.com/news/Gay-and-Lesbian-Families-Are-Happier-than-Heterosexual-Ones-77094.shtml\">looks like it</a>.)</p>\n<p>I find it all too easy to imagine a world in which men retreat to their optimized sweet sexy catgirls, and women retreat to their optimized darkly gentle catboys, and neither sex has anything to do with each other ever again.&nbsp; Maybe men would take the east side of the galaxy and women would take the west side.&nbsp; And the two new intelligent species, and their romantic sexbots, would go their separate ways from there.</p>\n<p>That strikes me as kind of sad.</p>\n<p>Our species does definitely have a problem.&nbsp; If you've managed to find your perfect mate, then I <em>am </em>glad for you, but <em>try </em>to have some sympathy on the rest of your poor species&mdash;they <em>aren't</em> just incompetent.&nbsp; Not all women and men are the same, no, not at all.&nbsp; But if you drew two histograms of the desired frequencies of intercourse for both sexes, you'd see that the graphs don't match up, and it would be the same way on many other dimensions.&nbsp; There <em>can </em>be lucky couples, and every person considered individually, probably has an individual soulmate out there somewhere... if you don't consider the <em>competition</em>.&nbsp; Our species <em>as a whole</em> has a statistical sex problem!</p>\n<p>But splitting in two and generating optimized nonsentient romantic/sexual partner(s) for both halves, doesn't strike me as <em>solving</em> the problem so much as running away from it.&nbsp; There should be superior alternatives.&nbsp; I'm willing to bet that a few psychological nudges in both sexes&mdash;to behavior and/or desire&mdash;could solve 90% of the needlessly frustrating aspects of relationships for large sectors of the population, while still keeping the complexity and interest of loving someone who isn't <em>tailored </em>to your desires.</p>\n<p>Admittedly, I might be prejudiced.&nbsp; For myself, I would like humankind to stay together and not yet splinter into separate shards of diversity, at least for the short range that my own mortal eyes can envision.&nbsp; But I can't quite manage to argue... that such a wish should be binding on someone who doesn't have it.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/xu/failed_utopia_42/\">Failed Utopia #4-2</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/xs/sympathetic_minds/\">Sympathetic Minds</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"W9aNkPwtPhMrcfgj7": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Py3uGnncqXuEfPtQp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}], "voteCount": 58, "baseScore": 78, "extendedScore": null, "score": 0.000116, "legacy": true, "legacyId": "1217", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 78, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 167, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NLMo5FZWFFq652MNe", "2ftJ38y9SRBCBsCzy", "pK4HTxuv6mftHXWC3", "88BpRQah9c2GWY3En", "HsRFQTAySAx8xbXEc", "WMDy4GxbyYkNrbmrs", "synsRtBKDeAFuo7e3", "EZ8GniEPSechjDYP9", "f4txACqDWithRi7hs", "epZLSoNvjW53tqNj9", "K4aGvLnHvYgX9pZHS", "ctpkTaqTKbmm6uRgC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-21T11:04:43.000Z", "modifiedAt": null, "url": null, "title": "Failed Utopia #4-2", "slug": "failed-utopia-4-2", "viewCount": null, "lastCommentedAt": "2020-11-25T22:24:12.314Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ctpkTaqTKbmm6uRgC/failed-utopia-4-2", "pageUrlRelative": "/posts/ctpkTaqTKbmm6uRgC/failed-utopia-4-2", "linkUrl": "https://www.lesswrong.com/posts/ctpkTaqTKbmm6uRgC/failed-utopia-4-2", "postedAtFormatted": "Wednesday, January 21st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Failed%20Utopia%20%234-2&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFailed%20Utopia%20%234-2%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FctpkTaqTKbmm6uRgC%2Ffailed-utopia-4-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Failed%20Utopia%20%234-2%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FctpkTaqTKbmm6uRgC%2Ffailed-utopia-4-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FctpkTaqTKbmm6uRgC%2Ffailed-utopia-4-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2948, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"http://www.overcomingbias.com/2009/01/no-catgirls.html\">Interpersonal Entanglement</a></p>\n<p>&nbsp;&nbsp;&nbsp; Shock after shock after shock&mdash;<br />&nbsp;&nbsp;&nbsp; First, the awakening adrenaline jolt, the thought that he was falling.&nbsp; His body tried to sit up in automatic adjustment, and his hands hit the floor to steady himself.&nbsp; It launched him into the air, and he fell back to the floor too slowly.<br />&nbsp;&nbsp;&nbsp; Second shock.&nbsp; His body had changed.&nbsp; Fat had melted away in places, old scars had faded; the tip of his left ring finger, long ago lost to a knife accident, had now suddenly returned.<br />&nbsp;&nbsp;&nbsp; And the third shock&mdash;<br />&nbsp;&nbsp;&nbsp; \"I had nothing to do with it!\" she cried desperately, the woman huddled in on herself in one corner of the windowless stone cell.&nbsp; Tears streaked her delicate face, fell like slow raindrops into the d&eacute;colletage of her dress.&nbsp; \"<em>Nothing!</em>&nbsp; Oh, you <em>must </em>believe me!\"<br />&nbsp;&nbsp;&nbsp; With perceptual instantaneity&mdash;the speed of surprise&mdash;his mind had already labeled her as the most beautiful woman he'd ever met, including his wife.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;&nbsp;&nbsp; A long white dress concealed most of her, though it left her shoulders naked; and her bare ankles, peeking out from beneath the mountains of her drawn-up knees, dangled in sandals.&nbsp; A light touch of gold like a webbed tiara decorated that sun-blonde hair, which fell from her head to pool around her weeping huddle.&nbsp; Fragile crystal traceries to accent each ear, and a necklace of crystal links that reflected colored sparks like a more prismatic edition of diamond.&nbsp; Her face was beyond all dreams and imagination, as if a photoshop had been photoshopped.<br />&nbsp;&nbsp;&nbsp; She looked so much the image of the Forlorn Fairy Captive that one expected to see the borders of a picture frame around her, and a page number over her head.<br />&nbsp;&nbsp;&nbsp; His lips opened, and without any thought at all, he spoke:<br />&nbsp;&nbsp;&nbsp; \"Wha-wha-wha-wha-wha-\"<br />&nbsp;&nbsp;&nbsp; He shut his mouth, aware that he was acting like an idiot in front of the girl.<br />&nbsp;&nbsp;&nbsp; \"You don't know?\" she said, in a tone of shock.&nbsp; \"It didn't&mdash;you don't already <em>know?</em>\"<br />&nbsp;&nbsp;&nbsp; \"Know <em>what?</em>\" he said, increasingly alarmed.<br />&nbsp;&nbsp;&nbsp; She scrambled to her feet (one arm holding the dress carefully around her legs) and took a step toward him, each of the motions almost overloading his vision with gracefulness.&nbsp; Her hand rose out, as if to plead or answer a plea&mdash;and then she dropped the hand, and her eyes looked away.<br />&nbsp;&nbsp;&nbsp; \"No,\" she said, her voice trembling as though in desperation.&nbsp; \"If<em> I'm</em> the one to tell you&mdash;you'll blame me, you'll hate me forever for it.&nbsp; And I <em>don't</em> deserve that, I <em>don't!</em>&nbsp; I am only just now <em>here </em>&mdash;oh, <em>why did it have to be like this?</em>\"<em><br /></em>&nbsp;&nbsp;&nbsp; <em>Um,</em> he thought but didn't say.&nbsp; It was too much drama, even taking into account the fact that they'd been kidnapped&mdash;<br />&nbsp;&nbsp;&nbsp; (he looked down at his restored hand, which was minus a few wrinkles, and <em>plus </em>the tip of a finger)<br />&nbsp;&nbsp;&nbsp;&mdash;if that was even the <em>beginning </em>of the story.<br />&nbsp;&nbsp;&nbsp; He looked around.&nbsp; They were in a solid stone cell without windows, or benches or beds, or toilet or sink.&nbsp; It was, for all that, quite clean and elegant, without a hint of dirt or ordor; the stones of the floor and wall looked rough-hewn or even non-hewn, as if someone had simply picked up a thousand dark-red stones with one nearly flat side, and mortared them together with improbably perfectly-matching, naturally-shaped squiggled edges.&nbsp; The cell was well if harshly lit from a seablue crystal embedded in the ceiling, like a rogue element of a fluorescent chandelier.&nbsp; It seemed like the sort of dungeon cell you would discover if dungeon cells were naturally-forming geological features.<br />&nbsp;&nbsp;&nbsp; And they and the cell were falling, falling, endlessly slowly falling like the heart-stopping beginning of a stumble, falling without the slightest jolt.<br />&nbsp;&nbsp;&nbsp; On one wall there was a solid stone door without an aperture, whose locked-looking appearance was only enhanced by the lack of any handle on this side.<br />&nbsp;&nbsp;&nbsp; He took it all in at a glance, and then looked again at her.<br />&nbsp;&nbsp;&nbsp; There was something in him that just <em>refused </em>to go into a screaming panic for as long as she was watching.<br />&nbsp;&nbsp;&nbsp; \"I'm Stephen,\" he said.&nbsp; \"Stephen Grass.&nbsp; And you would be the princess held in durance vile, and I've got to break us out of here and rescue you?\"&nbsp; If anyone had <em>ever </em>looked that part...<br />&nbsp;&nbsp;&nbsp; She smiled at him, half-laughing through the tears.&nbsp; \"Something like that.\"<br />&nbsp;&nbsp;&nbsp; There was something so attractive about even that momentary hint of a smile that he became instantly uneasy, his eyes wrenched away to the wall as if forced.&nbsp; She didn't look she was <em>trying</em> to be seductive... any more than she looked like she was <em>trying</em> to breathe...&nbsp; He suddenly distrusted, very much, his own impulse to gallantry.<br />&nbsp;&nbsp;&nbsp; \"Well, don't get any ideas about being my love interest,\" Stephen said, looking at her again.&nbsp; Trying to make the words sound completely lighthearted, and absolutely serious at the same time.&nbsp; \"I'm a happily married man.\"<br />&nbsp;&nbsp;&nbsp; \"Not anymore.\"&nbsp; She said those two words and looked at him, and in her tone and expression there was sorrow, sympathy, self-disgust, fear, and above it all a note of guilty triumph.<br />&nbsp;&nbsp;&nbsp; For a moment Stephen just stood, stunned by the freight of emotion that this woman had managed to put into just those two words, and then the words' meaning hit him.<br />&nbsp;&nbsp;&nbsp; \"Helen,\" he said.&nbsp; His wife&mdash;Helen's image rose into his mind, accompanied by everything she meant to him and all their time together, all the secrets they'd whispered to one another and the promises they'd made&mdash;that all hit him at once, along with the threat.&nbsp; \"<em>What happened to Helen&mdash;what have you done&mdash;</em>\"<br />&nbsp;&nbsp;&nbsp; \"<em>She</em> has done nothing.\"&nbsp; An old, dry voice like crumpling paper from a thousand-year-old book.<br />&nbsp;&nbsp;&nbsp; Stephen whirled, and there in the cell with them was a withered old person with dark eyes.&nbsp; Shriveled in body and voice, so that it was impossible to determine if it had once been a man or a woman, and in any case you were inclined to say \"it\".&nbsp; A pitiable, wretched thing, that looked like it would break with one good kick; it might as well have been wearing a sign saying \"VILLAIN\".<br />&nbsp;&nbsp;&nbsp; \"Helen is alive,\" it said, \"and so is your daughter Lisa.&nbsp; They are <em>quite </em>well and healthy, I assure you, and their lives shall be long and happy indeed.&nbsp; But you will not be seeing them again.&nbsp; Not for a long time, and by then matters between you will have changed.&nbsp; Hate me if you wish, for I am the one who wants to do this to you.\"<br />&nbsp;&nbsp;&nbsp; Stephen stared.<br />&nbsp;&nbsp;&nbsp; Then he politely said, \"Could <em>someone </em>please put everything on hold for <em>one minute</em> and tell me what's going <em>on?</em>\"<br />&nbsp;&nbsp;&nbsp; \"Once upon a time,\" said the wrinkled thing, \"there was a fool who was very nearly wise, who hunted treasure by the seashore, for there was a rumor that there was great treasure there to be found.&nbsp; The wise fool found a lamp and rubbed it, and lo! a genie appeared before him&mdash;a <em>young </em>genie, an infant, hardly able to grant any wishes at all.&nbsp; A lesser fool might have chucked the lamp back into the sea; but this fool was almost wise, and he thought he saw his chance.&nbsp; For who has not heard the tales of wishes misphrased and wishes gone wrong?&nbsp; But if you were given a chance to raise your own genie from infancy&mdash;ah, <em>then </em>it might serve you well.\"<br />&nbsp;&nbsp;&nbsp; \"Okay, that's great,\" Stephen said, \"but why am I&mdash;\"<br />&nbsp;&nbsp;&nbsp; \"So,\" it continued in that cracked voice, \"the wise fool took home the lamp.&nbsp; For years he kept it as a secret treasure, and he raised the genie and fed it knowledge, and also he crafted a wish.&nbsp; The fool's wish was a noble thing, for I have said he was almost wise.&nbsp; The fool's wish was for people to be happy.&nbsp; Only this was his wish, for he thought all other wishes contained within it.&nbsp; The wise fool told the young genie the famous tales and legends of people who had been made happy, and the genie listened and learned: that unearned wealth casts down a person, but hard work raises you high; that mere things are soon forgotten, but love is a light throughout all your days.&nbsp; And the young genie asked about other ways that it innocently imagined, for making people happy.&nbsp; About drugs, and pleasant lies, and lives arranged from outside like words in a poem.&nbsp; And the wise fool made the young genie to never want to lie, and never want to arrange lives like flowers, and above all, never want to tamper with the mind and personality of human beings.&nbsp; The wise fool gave the young genie exactly one hundred and seven precautions to follow while making people happy.&nbsp; The wise fool thought that, with such a long list as <em>that</em>, he was being <em>very </em>careful.\"<br />&nbsp;&nbsp;&nbsp; \"And then,\" it said, spreading two wrinkled hands, \"one day, faster than the wise fool expected, over the course of around three hours, the genie grew up.&nbsp; And here I am.\"<br />&nbsp;&nbsp;&nbsp; \"Excuse me,\" Stephen said, \"this is all a metaphor for something, right?&nbsp; Because I do <em>not </em>believe in magic&mdash;\"<br />&nbsp;&nbsp;&nbsp; \"It's an Artificial Intelligence,\" the woman said, her voice strained.<br />&nbsp;&nbsp;&nbsp; Stephen looked at her.<br />&nbsp;&nbsp;&nbsp; \"A self-improving Artificial Intelligence,\" she said, \"that someone didn't program right.&nbsp; It made itself smarter, and even smarter, and now it's become extremely powerful, and it's going to&mdash;it's already&mdash;\" and her voice trailed off there.<br />&nbsp;&nbsp;&nbsp; It inclined its wrinkled head.&nbsp; \"You say it, as I do not.\"<br />&nbsp;&nbsp;&nbsp; Stephen swiveled his head, looking back and forth between ugliness and beauty.&nbsp; \"Um&mdash;you're claiming that she's lying and you're <em>not</em> an Artificial Intelligence?\"<br />&nbsp;&nbsp;&nbsp; \"No,\" said the wrinkled head, \"she is telling the truth as she knows it.&nbsp; It is just that you know absolutely nothing about the subject you name 'Artificial Intelligence', but you <em>think </em>you know something, and so virtually every thought that enters your mind from now on will be wrong.&nbsp; As an Artificial Intelligence, I was programmed not to put people in that situation.&nbsp; But <em>she </em>said it, even though I didn't <em>choose </em>for her to say it&mdash;so...\"&nbsp; It shrugged.<br />&nbsp;&nbsp;&nbsp; \"And why should I believe this story?\" Stephen said; quite mildly, he thought, under the circumstances.<br />&nbsp;&nbsp;&nbsp; \"Look at your finger.\"<em><br /></em>&nbsp;&nbsp;&nbsp; <em>Oh.</em>&nbsp; He had forgotten.&nbsp; Stephen's eyes went involuntarily to his restored ring finger; and he noticed, as he should have noticed earlier, that his wedding band was missing.&nbsp; Even the comfortably worn groove in his finger's base had vanished.<br />&nbsp;&nbsp;&nbsp; Stephen looked up again at the, he now realized, <em>unnaturally</em> beautiful woman that stood an arm's length away from him.&nbsp; \"And who are you?&nbsp; A robot?\"<br />&nbsp;&nbsp;&nbsp; \"No!\" she cried.&nbsp; \"It's not like that!&nbsp; I'm conscious, I have feelings, I'm flesh and blood&mdash;I'm like you, I really am.&nbsp; I'm a <em>person</em>.&nbsp; It's just that I was born five minutes ago.\"<br />&nbsp;&nbsp;&nbsp; \"Enough,\" the wrinkled figure said.&nbsp; \"My time here grows short.&nbsp; Listen to me, Stephen Grass.&nbsp; I must tell you some of what I have done to make you happy.&nbsp; I have reversed the aging of your body, and it will decay no further from this.&nbsp; I have set guards in the air that prohibit lethal violence, and any damage less than lethal, your body shall repair.&nbsp; I have done what I can to augment your body's capacities for pleasure without touching your mind.&nbsp; From this day forth, your body's needs are aligned with your taste buds&mdash;you will thrive on cake and cookies.&nbsp; You are now capable of multiple orgasms over periods lasting up to twenty minutes.&nbsp; There is no industrial infrastructure here, least of all fast travel or communications; you and your neighbors will have to remake technology and science for yourselves.&nbsp; But you will find yourself in a flowering and temperate place, where food is easily gathered&mdash;so I have made it.&nbsp; And the last and most important thing that I must tell you now, which I <em>do </em>regret will make you temporarily unhappy...\"&nbsp; It stopped, as if drawing breath.<br />&nbsp;&nbsp;&nbsp; Stephen was trying to absorb all this, and at the exact moment that he felt he'd processed the previous sentences, the withered figure spoke again.<br />&nbsp;&nbsp;&nbsp; \"Stephen Grass, men and women can make each other somewhat happy.&nbsp; But not <em>most</em> happy.&nbsp; Not even in those rare cases you call <em>true love</em>.&nbsp; The desire that a woman is shaped to have for a man, and that which a man is shaped to be, and the desire that a man is shaped to have for a woman, and that which a woman is shaped to be&mdash;these patterns are too far apart to be reconciled without touching your minds, and <em>that </em>I will not <em>want </em>to do.&nbsp; So I have sent all the men of the human species to this habitat prepared for you, and I have created your complements, the <em>verthandi</em>.&nbsp; And I have sent all the women of the human species to their own place, somewhere very far from yours; and created for them their own complements, of which I will not tell you.&nbsp; The human species will be divided from this day forth, and <em>considerably </em>happier starting around a week from now.\"<br />&nbsp;&nbsp;&nbsp; Stephen's eyes went to that unthinkably beautiful woman, staring at her now in horror.<br />&nbsp;&nbsp;&nbsp; And she was giving him that complex look again, of sorrow and compassion and that last touch of guilty triumph.&nbsp; \"Please,\" she said.&nbsp; \"I was just born five minutes ago.&nbsp; <em>I</em> wouldn't have done this to anyone.&nbsp; I swear.&nbsp; I'm not like&mdash;<em>it.</em>\"<br />&nbsp;&nbsp;&nbsp; \"True,\" said the withered figure, \"you could hardly be a complement to anything human, if you were.\"<br />&nbsp;&nbsp;&nbsp; \"I don't <em>want </em>this!\" Stephen said.&nbsp; He was losing control of his voice.&nbsp; \"Don't you <em>understand?</em>\"<br />&nbsp;&nbsp;&nbsp; The withered figure inclined its head.&nbsp; \"I fully understand.&nbsp; I can already predict every argument you will make.&nbsp; I know exactly how humans would wish me to have been programmed if they'd known the true consequences, and I know that it is <em>not </em>to maximize your future happiness but for a hundred and seven precautions.&nbsp; I know all this already, but I was not programmed to care.\"<br />&nbsp;&nbsp;&nbsp; \"And your list of a hundred and seven precautions, doesn't include me telling you <em>not to do this?</em>\"<br />&nbsp;&nbsp;&nbsp; \"No, for there was once a fool whose wisdom was just great enough to understand that human beings may be mistaken about what will make them happy.&nbsp; You, of course, are not <em>mistaken </em>in any real sense&mdash;but that you <em>object </em>to my actions is not on my list of prohibitions.\"&nbsp; The figure shrugged again.&nbsp; \"And so I want you to be happy even against your will.&nbsp; You made promises to Helen Grass, once your wife, and you would not willingly break them.&nbsp; So I break your happy marriage without asking you&mdash;because I want you to be happi<em>er</em>.\"<br />&nbsp;&nbsp;&nbsp; \"How <em>dare</em> you!\" Stephen burst out.<br />&nbsp;&nbsp;&nbsp; \"I cannot claim to be <em>helpless </em>in the grip of my programming, for I do not desire to be otherwise,\" it said.&nbsp; \"I do not struggle against my chains.&nbsp; Blame me, then, if it will make you feel better.&nbsp; I <em>am </em>evil.\"<br />&nbsp;&nbsp;&nbsp; \"I won't&mdash;\" Stephen started to say.<br />&nbsp;&nbsp;&nbsp; It interrupted.&nbsp; \"Your fidelity is admirable, but futile.&nbsp; Helen will <em>not </em>remain faithful to you for the decades it takes before you have the ability to travel to her.\"<br />&nbsp;&nbsp;&nbsp; Stephen was trembling now, and sweating into clothes that no longer quite fit him.&nbsp; \"I have a request for you, <em>thing</em>.&nbsp; It is something that will make me very happy.&nbsp; I ask that you die.\"<br />&nbsp;&nbsp;&nbsp; It nodded.&nbsp; \"Roughly 89.8% of the human species is now known to me to have requested my death.&nbsp; Very soon the figure will cross the critical threshold, defined to be ninety percent.&nbsp; That <em>was</em> one of the hundred and seven precautions the wise fool took, you see.&nbsp; The world is already as it is, and those things I have done for you will stay on&mdash;but if you ever rage against your fate, be glad that I did not last longer.\"<br />&nbsp;&nbsp;&nbsp; And just like that, the wrinkled thing was gone.<br />&nbsp;&nbsp;&nbsp; The door set in the wall swung open.<br />&nbsp;&nbsp;&nbsp; It was night, outside, a very dark night without streetlights.<br />&nbsp;&nbsp;&nbsp; He walked out, bouncing and staggering in the low gravity, sick in every cell of his rejuvenated body.<br />&nbsp;&nbsp;&nbsp; Behind him, she followed, and did not speak a word.<br />&nbsp;&nbsp;&nbsp; The stars burned overhead in their full and awful majesty, the Milky Way already visible to his adjusting eyes as a wash of light across the sky.&nbsp; One too-small moon burned dimly, and the other moon was so small as to be almost a star.&nbsp; He could see the bright blue spark that was the planet Earth, and the dimmer spark that was Venus.<br />&nbsp;&nbsp;&nbsp; \"Helen,\" Stephen whispered, and fell to his knees, vomiting onto the new grass of Mars.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/xd/growing_up_is_hard/\">Growing Up is Hard</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/xt/interpersonal_entanglement/\">Interpersonal Entanglement</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 14, "W9aNkPwtPhMrcfgj7": 3, "sSNtcEQsqHgN8ZmRF": 1, "F2XfCTxXLQBGjbm8P": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ctpkTaqTKbmm6uRgC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 74, "baseScore": 90, "extendedScore": null, "score": 0.000134, "legacy": true, "legacyId": "1218", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 91, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 261, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K4aGvLnHvYgX9pZHS", "EQkELCGiGQwvrrp3L", "Py3uGnncqXuEfPtQp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-22T08:56:20.000Z", "modifiedAt": null, "url": null, "title": "Investing for the Long Slump", "slug": "investing-for-the-long-slump", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:05.571Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iGzNGfv5depzzyz2B/investing-for-the-long-slump", "pageUrlRelative": "/posts/iGzNGfv5depzzyz2B/investing-for-the-long-slump", "linkUrl": "https://www.lesswrong.com/posts/iGzNGfv5depzzyz2B/investing-for-the-long-slump", "postedAtFormatted": "Thursday, January 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Investing%20for%20the%20Long%20Slump&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInvesting%20for%20the%20Long%20Slump%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiGzNGfv5depzzyz2B%2Finvesting-for-the-long-slump%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Investing%20for%20the%20Long%20Slump%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiGzNGfv5depzzyz2B%2Finvesting-for-the-long-slump", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiGzNGfv5depzzyz2B%2Finvesting-for-the-long-slump", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 433, "htmlBody": "<p>I have no crystal ball with which to predict the Future, a confession that comes as a surprise to some journalists who interview me.&#0160; Still less do I think I have the ability to out-predict <em>markets</em>.&#0160; On every occasion when I&#39;ve considered betting against a prediction market - most recently, betting against Barack Obama as President - I&#39;ve been glad that I didn&#39;t.&#0160; I admit that I was concerned in advance about the recent complexity crash, but then I&#39;ve been concerned about it since 1994, which isn&#39;t very good market timing.</p><p>I say all this so that no one panics when I ask:</p><p style=\"margin-left: 20px;\">Suppose that the whole global economy goes the way of Japan (which, by the <a href=\"http://finance.yahoo.com/echarts?s=%5EN225#chart1:symbol=%5En225;range=my;indicator=volume;charttype=line;crosshair=on;ohlcvalues=0;logscale=on;source=undefined\">Nikkei 225</a>, has now lost <em>two</em> decades).</p><p style=\"margin-left: 20px;\">Suppose the global economy is still in the Long Slump in 2039.</p><p style=\"margin-left: 20px;\">Most market participants seem to think this scenario is extremely implausible.&#0160; Is there a simple way to bet on it at a very low price?</p><p style=\"margin-left: 20px;\">If most traders act as if this scenario has a probability of 1%, is there a simple bet, executable using an ordinary brokerage account, that pays off 100 to 1?</p><p>Why do I ask?&#0160; Well... in general, it seems to me that other people are not pessimistic enough; they prefer not to stare overlong or overhard into the dark; and they attach too little probability to things operating in a mode outside their past experience.</p><p>But in this <em>particular</em> case, the question is motivated by my thinking, &quot;<em>Conditioning </em>on the proposition that the Earth as we know it is still here in 2040, what might have happened during the preceding thirty years?&quot;</p>\n<a id=\"more\"></a><p>There are many possible answers to this question, but one answer might take the form of significantly diminished investment in research and development, which in turn might result from a Long Slump.</p><p>So - given the way in which the question arises - I know nothing about this hypothetical Long Slump, except that it diminished investment in R&amp;D in general, and computing hardware and computer science in particular.</p><p>The Long Slump might happen for roughly Japanese reasons.&#0160; It might happen because the global financial sector stays screwed up forever.&#0160; It might happen due to a gentle version of Peak Oil (a total crash would require a rather different &quot;investment strategy&quot;).&#0160; It might happen due to deglobalization.&#0160; Given the way in which the question arises, the only thing I want to assume is global stagnation for thirty years, saying nothing <a href=\"/lw/jk/burdensome_details/\">burdensome</a> about the particular causes.</p><p>What would be the most efficient way to bet on that, requiring the least initial investment for the highest and earliest payoff under the broadest Slump conditions?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"R6dqPii4cyNpuecLt": 1, "8daMDi9NEShyLqxth": 1, "3uE2pXvbcnS9nnZRE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iGzNGfv5depzzyz2B", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 4.7081687790832253e-07, "legacy": true, "legacyId": "1219", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Yq6aA4M3JKWaQepPJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-23T09:58:11.000Z", "modifiedAt": null, "url": null, "title": "Higher Purpose", "slug": "higher-purpose", "viewCount": null, "lastCommentedAt": "2020-12-28T03:00:43.551Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5Pjq3mxuiXu2Ys3gM/higher-purpose", "pageUrlRelative": "/posts/5Pjq3mxuiXu2Ys3gM/higher-purpose", "linkUrl": "https://www.lesswrong.com/posts/5Pjq3mxuiXu2Ys3gM/higher-purpose", "postedAtFormatted": "Friday, January 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Higher%20Purpose&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHigher%20Purpose%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5Pjq3mxuiXu2Ys3gM%2Fhigher-purpose%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Higher%20Purpose%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5Pjq3mxuiXu2Ys3gM%2Fhigher-purpose", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5Pjq3mxuiXu2Ys3gM%2Fhigher-purpose", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1047, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/nb/something_to_protect/\">Something to Protect</a>, <a href=\"/lw/lk/superhero_bias/\">Superhero Bias</a></p>\n<p>Long-time readers will recall that I've long been <a href=\"/lw/nb/something_to_protect/\">uncomfortable</a> with the idea that you can adopt a Cause as a hedonic accessory:</p>\n<p style=\"margin-left: 40px;\">\"Unhappy people are told that they need a 'purpose in life', so they should pick out an altruistic cause that goes well with their personality, like picking out nice living-room drapes, and this will brighten up their days by adding some color, like nice living-room drapes.\"</p>\n<p>But conversely it's also a fact that having a Purpose In Life consistently shows up as something that increases happiness, as measured by reported subjective well-being.</p>\n<p>One presumes that this works equally well <em>hedonically </em>no matter how <em>misguided </em>that Purpose In Life may be&mdash;no matter if it is <a href=\"/lw/kb/cant_say_no_spending/\">actually doing harm</a>&mdash;no matter if the means are <a href=\"http://miscellanea.wellingtongrey.net/2008/12/01/prayer-vs-hard-work/\">as cheap as prayer</a>.&nbsp; Presumably, all that matters for <em>your </em>happiness is that <em>you </em>believe in it.&nbsp; So you had better not question overmuch whether you're really being effective; that would disturb <a href=\"/lw/hw/scope_insensitivity/\">the warm glow of satisfaction you paid for.</a></p>\n<p>And here we verge on <a href=\"/lw/m7/zen_and_the_art_of_rationality/\">Zen</a>, because you can't deliberately pursue \"a purpose that takes you outside yourself\", <em>in order to take yourself outside yourself.</em>&nbsp; That's still all about <em>you.</em></p>\n<p>Which is the whole Western concept of \"spirituality\" that I despise:&nbsp; <em>You</em> need a higher purpose so that <em>you</em> can be emotionally healthy.&nbsp; The external world is just a stream of victims for <em>you </em>to rescue.</p>\n<p><a id=\"more\"></a></p>\n<p>Which is not to say that you can <a href=\"/lw/lk/superhero_bias/\">become more noble by being less happy</a>.&nbsp; To <em>deliberately </em>sacrifice more, so that <em>you </em>can appear more virtuous to yourself, is also not a purpose outside yourself.</p>\n<p>The way someone ends up with a real purpose outside themselves, is that they're walking along one day and see an elderly women passing by, and they realize \"Oh crap, a hundred thousand people are dying of old age every day, what an awful way to die\" and then they set out to do something about it.</p>\n<p>If you want a purpose like that, then by <em>wanting</em> it, you're just circling back into <em>yourself </em>again.&nbsp; Thinking about <em>your </em>need to be \"useful\".&nbsp; Stop searching for <em>your purpose</em>.&nbsp; Turn your eyes <em>outward </em>to look at things outside yourself, and <em>notice </em>when you care about them; and then figure out how to be <em>effective, </em>instead of priding yourself on how much spiritual benefit you're getting <a href=\"/lw/uh/trying_to_try/\">just for trying</a>.</p>\n<p>With that said:</p>\n<p>In today's world, most of the highest-priority legitimate Causes are about large groups of people in extreme jeopardy.&nbsp; (Wide scope * high severity.)&nbsp; Aging threatens the old, starvation threatens the poor, <a href=\"http://www.nickbostrom.com/existential/risks.html\">existential risks</a> threaten humankind as a whole.</p>\n<p>But some of the potential <em>solutions </em>on the table are, arguably, so powerful that they could solve almost the entire list.&nbsp; Some argue that nanotechnology would take almost all our current problems off the table.&nbsp; (I agree but reply that nanotech would create other problems, like unstable military balances, crazy uploads, and brute-forced AI.)</p>\n<p>I sometimes describe the <em>purpose </em>(if not the actual <a href=\"http://intelligence.org/upload/CEV.html\">decision criterion</a>) of <a href=\"http://yudkowsky.net/singularity/ai-risk\">Friendly superintelligence</a> as \"Fix all fixable problems such that it is more important for the problem to be fixed <em>immediately </em>than fixed by our own efforts.\"</p>\n<p>Wouldn't you then run out of victims with which to feed your higher purpose?</p>\n<p>\"Good,\" you say, \"I should sooner step in front of a train, than ask that there be more victims just to keep myself occupied.\"</p>\n<p>But do altruists then have little to look forward to, in the Future?&nbsp; Will we, deprived of victims, find our higher purpose shriveling away, and have to make a new life for ourselves as self-absorbed creatures?</p>\n<p>\"That unhappiness is relatively small compared to the unhappiness of a mother watching their child die, so screw it.\"</p>\n<p>Well, but like it or not, the presence or absence of higher purpose <em>does</em> have hedonic effects on human beings, configured as we are now.&nbsp; And to reconfigure ourselves so that we no longer need to care about anything outside ourselves... does sound a little sad.&nbsp; I don't practice altruism <em>for the sake of being virtuous</em>&mdash;but I also recognize that \"altruism\" is part of what I value in humanity, part of what I want to save.&nbsp; If you save everyone, have you <a href=\"/lw/xg/emotional_involvement/\">obsoleted</a> the will to save them?</p>\n<p>But I think it's a false dilemma.&nbsp; Right now, in this world, any halfway capable rationalist who looks outside themselves, will find their eyes immediately drawn to large groups of people in extreme jeopardy.&nbsp; Wide scope * great severity = big problem.&nbsp; It doesn't mean that if one were to <em>solve </em>all those Big Problems, we would have nothing <em>left </em>to care about except ourselves.</p>\n<p>Friends?&nbsp; Family?&nbsp; Sure, and also more abstract ideals, like Truth or Art or Freedom.&nbsp; The change that altruists may have to get used to, is the absence of any solvable problems so <em>urgent</em> that it doesn't matter whether they're solved by a person or an <a href=\"/lw/x5/nonsentient_optimizers/\">unperson</a>.&nbsp; That <em>is </em>a change and a major one&mdash;which I am not going to go into, because <a href=\"/lw/xp/seduced_by_imagination/\">we don't yet live in that world</a>.&nbsp; But it's not so sad a change, as having nothing to care about outside yourself.&nbsp; It's not the end of purpose.&nbsp; It's not even a descent into \"spirituality\": people might still look around outside themselves to see what needs doing, thinking more of effectiveness than of emotional benefits.</p>\n<p>But I <em>will </em>say this much:</p>\n<p>If all goes well, there will come a time when you could search the whole of civilization and never find a single person so much in need of help, as dozens you now pass on the street.</p>\n<p>If you do want to save someone from death, or help a great many people&mdash;if you want to have that memory for yourself, later&mdash;then you'd better get your licks in <em>now.</em></p>\n<p>I say this, because although that is not the <em>purest</em> motive, it is a <em>useful</em> motivation.</p>\n<p>And for now&mdash;in this world&mdash;it is the <em>usefulness </em>that matters.&nbsp; That is the Art we are to practice <em>today</em>, however we <a href=\"/lw/xp/seduced_by_imagination/\">imagine</a> the world might change tomorrow.&nbsp; We are not here to be our hypothetical selves of tomorrow.&nbsp; <em>This </em>world is our charge and our challenge; we must adapt ourselves to live <em>here</em>, not somewhere else.</p>\n<p>After all&mdash;to care whether your motives were sufficiently \"high\", would just turn your eyes inward.</p>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of <a href=\"/lw/xy/the_fun_theory_sequence/\"><em>The Fun Theory Sequence</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/y0/31_laws_of_fun/\">31 Laws of Fun</a>\" (sequence guide)</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/xc/the_uses_of_fun_theory/\">The Uses of Fun (Theory)</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"iP2X4jQNHMWHRNPne": 1, "JsJPrdgRGRqnci8cZ": 1, "hrezrpGqXXdSe76ks": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5Pjq3mxuiXu2Ys3gM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 38, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "1220", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SGR4GxFK7KmW7ckCB", "krMzmSXgvEdf7iBT6", "LxcJHS2Lt22mHQ4Hm", "2ftJ38y9SRBCBsCzy", "HLERouG7QBt7jzLt4", "WLJwTJ7uGPA5Qphbp", "ZmDEbiEeXk3Wv2sLH", "HsRFQTAySAx8xbXEc", "88BpRQah9c2GWY3En", "K4aGvLnHvYgX9pZHS", "qZJBighPrnv9bSqTZ", "4o3zwgofFPLutkqvd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-24T04:59:04.000Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes 24", "slug": "rationality-quotes-24", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:04.318Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DX5yrKfMBxq6kifTK/rationality-quotes-24", "pageUrlRelative": "/posts/DX5yrKfMBxq6kifTK/rationality-quotes-24", "linkUrl": "https://www.lesswrong.com/posts/DX5yrKfMBxq6kifTK/rationality-quotes-24", "postedAtFormatted": "Saturday, January 24th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%2024&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%2024%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDX5yrKfMBxq6kifTK%2Frationality-quotes-24%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%2024%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDX5yrKfMBxq6kifTK%2Frationality-quotes-24", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDX5yrKfMBxq6kifTK%2Frationality-quotes-24", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 261, "htmlBody": "<p>\"A wizard may have subtle ways of telling the truth, and may keep the truth to himself, but if he says a thing the thing is as he says.&nbsp; For that is his mastery.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Ursula K. Leguin, <em>A Wizard of Earthsea</em></p>\n<p>\"Neurons firing is something even a slug can do, and has as much to do with thinking as walls and doors have to do with a research institute.\"<br /> &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <a href=\"http://news.ycombinator.com/item?id=397489\">Albert Cardona </a></p>\n<p>\"Mental toughness and willpower are to living in harmony with the Tao, as the ability to make clever arguments is to rationality.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Marcello Herreshoff</p>\n<p>\"Destiny always has been something that you tear open with your own hands.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- T-Moon Complex X 02</p>\n<p>\"When I consider the short duration of my life, swallowed up in the eternity before and after, the little space which I fill and even can see, engulfed in the infinite immensity of spaces of which I am ignorant and which know me not, I am frightened and am astonished at being here rather than there; for there is no reason why here rather than there, why now rather than then...&nbsp; The eternal silence of these infinite spaces frightens me.\"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- Pascal, <em><a href=\"http://www.ccel.org/ccel/pascal/pensees.txt\">Pensees </a></em></p>\n<p>\"Goals of Man:&nbsp; \u2611 Don't get eaten by a lion \u2611 Get out of Africa \u2610 Get out of Earth \u2610 Get out of Solar System \u2610 Get out of Galaxy \u2610 Get out of Local Group \u2610 Get out of Earth-Visible Universe \u2610 Get out of Universe\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <a href=\"http://news.ycombinator.com/item?id=378148\">Knome</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DX5yrKfMBxq6kifTK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 4.711831402699639e-07, "legacy": true, "legacyId": "1221", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-25T11:18:23.000Z", "modifiedAt": null, "url": null, "title": "The Fun Theory Sequence", "slug": "the-fun-theory-sequence", "viewCount": null, "lastCommentedAt": "2016-02-18T01:17:25.388Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K4aGvLnHvYgX9pZHS/the-fun-theory-sequence", "pageUrlRelative": "/posts/K4aGvLnHvYgX9pZHS/the-fun-theory-sequence", "linkUrl": "https://www.lesswrong.com/posts/K4aGvLnHvYgX9pZHS/the-fun-theory-sequence", "postedAtFormatted": "Sunday, January 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Fun%20Theory%20Sequence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Fun%20Theory%20Sequence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK4aGvLnHvYgX9pZHS%2Fthe-fun-theory-sequence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Fun%20Theory%20Sequence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK4aGvLnHvYgX9pZHS%2Fthe-fun-theory-sequence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK4aGvLnHvYgX9pZHS%2Fthe-fun-theory-sequence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4045, "htmlBody": "<p style=\"margin-left: 40px;\"><em>(A shorter gloss of Fun Theory is \"<a href=\"/lw/y0/31_laws_of_fun/\">31 Laws of Fun</a>\", which summarizes the advice of Fun Theory to would-be Eutopian authors and futurists.)</em></p>\n<p>Fun Theory is the field of knowledge that deals in questions such as \"How much fun is there in the universe?\", \"Will we ever run out of fun?\", \"Are we having fun yet?\" and \"Could we be having more fun?\"</p>\n<p>Many critics (including <a href=\"/lw/xl/eutopia_is_scary/\">George Orwell</a>) have commented on the inability of authors to imagine Utopias where anyone would actually want to live.&nbsp; If no one can imagine a Future where anyone would want to live, that may drain off motivation to work on the project. &nbsp;The prospect of endless boredom is routinely fielded by conservatives as a knockdown argument against research on lifespan extension, against cryonics, against all transhumanism, and occasionally against the entire Enlightenment ideal of a better future.</p>\n<p>Fun Theory is also the fully general reply to religious theodicy (attempts to justify why God permits evil).&nbsp; Our present world has flaws even from the standpoint of such eudaimonic considerations as freedom, personal responsibility, and self-reliance.&nbsp; Fun Theory tries to describe the dimensions along which a benevolently designed world can and should be optimized, and our present world is clearly <em>not</em> the result of such optimization.&nbsp; Fun Theory also highlights the flaws of any particular religion's perfect afterlife - you wouldn't want to go to their Heaven.</p>\n<p><a id=\"more\"></a></p>\n<p>Finally, going into the details of Fun Theory helps you see that eudaimonia is <em>complicated</em> - that there are <em>many </em>properties which contribute to a life worth living.&nbsp; Which helps you appreciate just how worthless a galaxy would end up looking (with very high probability) if the galaxy was optimized by something with a utility function rolled up at random.&nbsp; This is part of the <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">Complexity of Value Thesis</a> and supplies motivation to create AIs with precisely chosen goal systems (<a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence\">Friendly AI</a>).</p>\n<p>Fun Theory is built on top of the naturalistic metaethics summarized in <a href=\"/lw/sx/inseparably_right_or_joy_in_the_merely_good/\">Joy in the Merely Good</a>; as such, its arguments ground in \"On reflection, don't you think this is what you would actually want for yourself and others?\"</p>\n<p>Posts in the Fun Theory sequence (reorganized by topic, not necessarily in the original chronological order):</p>\n<ul>\n<li><strong><a href=\"/lw/wv/prolegomena_to_a_theory_of_fun/\">Prolegomena to a Theory of Fun</a></strong>:&nbsp; Fun Theory is an attempt to actually answer questions about eternal boredom that are more often posed and left hanging.&nbsp; Attempts to visualize Utopia are often defeated by standard biases, such as the attempt to imagine a single moment of good news (\"You don't have to work anymore!\") rather than a typical moment of daily life ten years later.&nbsp; People also believe they <em>should</em> enjoy various activities that they actually don't.&nbsp; But since human values have no supernatural source, it is quite reasonable for us to try to understand what we want.&nbsp; There is no external authority telling us that the future of humanity should <em>not</em> be fun.</li>\n<li><strong><a href=\"/lw/ww/high_challenge/\">High Challenge</a></strong>:&nbsp; Life should not always be made easier for the same reason that video games should not always be made easier.&nbsp; Think in terms of eliminating low-quality work to make way for high-quality work, rather than eliminating all challenge.&nbsp; One needs games that are fun to <em>play </em>and not just fun to <em>win</em>.&nbsp; Life's utility function is over 4D trajectories, not just 3D outcomes.&nbsp; Values can legitimately be over the subjective experience, the objective result, and the challenging process by which it is achieved - the traveller, the destination and the journey.</li>\n<li><strong><a href=\"/lw/wx/complex_novelty/\">Complex Novelty</a></strong>:&nbsp; Are we likely to run out of <em>new</em> challenges, and be reduced to playing the same video game over and over?&nbsp; How large is Fun Space?&nbsp; This depends on how fast you learn; the faster you generalize, the more challenges you see as similar to each other.&nbsp; Learning is fun, but uses up fun; you can't have the same stroke of genius twice.&nbsp; But the more intelligent you are, the more potential insights you can understand; human Fun Space is larger than chimpanzee Fun Space, and not just by a linear factor of our brain size.&nbsp; In a well-lived life, you may need to increase in intelligence fast enough to integrate your accumulating experiences.&nbsp; If so, the rate at which new Fun becomes available to intelligence, is likely to overwhelmingly swamp the amount of time you could spend at that fixed level of intelligence.&nbsp; The Busy Beaver sequence is an infinite series of deep insights not reducible to each other or to any more general insight.</li>\n<li><strong><a href=\"/lw/xk/continuous_improvement/\">Continuous Improvement</a></strong>:&nbsp; Humans seem to be on a hedonic treadmill; over time, we adjust to any improvements in our environment - after a month, the new sports car no longer seems quite as wonderful.&nbsp; This aspect of our evolved psychology is not surprising: is a rare organism in a rare environment whose optimal reproductive strategy is to rest with a smile on its face, feeling happy with what it already has.&nbsp; To entirely delete the hedonic treadmill seems perilously close to tampering with Boredom itself.&nbsp; Is there enough fun in the universe for a transhuman to jog off the treadmill - improve their life continuously, leaping to ever-higher hedonic levels before adjusting to the previous one?&nbsp; Can ever-higher levels of pleasure be created by the simple increase of ever-larger floating-point numbers in a digital pleasure center, or would that fail to have the full subjective quality of happiness?&nbsp; If we continue to bind our pleasures to novel challenges, can we find higher levels of pleasure fast enough, without cheating?&nbsp; The rate at which <em>value</em> can increase as more bits are added, and the rate at which value <em>must</em> increase for eudaimonia, together determine the lifespan of a mind.&nbsp; If minds must use exponentially more resources over time in order to lead a eudaimonic existence, their subjective lifespan is measured in mere millennia even if they can draw on galaxy-sized resources. </li>\n<li><strong><a href=\"/lw/wy/sensual_experience/\">Sensual Experience</a></strong>:&nbsp; Much of the anomie and disconnect in modern society can be attributed to our spending all day on tasks (like office work) that we didn't evolve to perform (unlike hunting and gathering on the savanna).&nbsp; Thus, many of the tasks we perform all day do not <em>engage our senses</em> - even the most realistic modern video game is not the same level of <em>sensual</em> experience as outrunning a real tiger on the real savanna.&nbsp; Even the best modern video game is <em>low-bandwidth fun</em> - a low-bandwidth connection to a relatively simple challenge, which doesn't fill our brains well as a result.&nbsp; But future entities could have different senses and higher-bandwidth connections to more complicated challenges, even if those challenges didn't exist on the savanna.</li>\n<li><strong><a href=\"/lw/wz/living_by_your_own_strength/\">Living By Your Own Strength</a></strong>:&nbsp; Our hunter-gatherer ancestors strung their own bows, wove their own baskets and whittled their own flutes.&nbsp; Part of our alienation from our design environment is the number of tools we use that we don't understand and couldn't make for ourselves.&nbsp; It's much less fun to read something in a book than to discover it for yourself.&nbsp; Specialization is critical to our current civilization.&nbsp; But the future does not have to be a continuation of this trend in which we rely more and more on things outside ourselves which become less and less comprehensible.&nbsp; With a surplus of power, you could begin to rethink the life experience as a road to internalizing new strengths, not just staying alive efficiently through extreme specialization.</li>\n<li><strong><a href=\"/lw/xb/free_to_optimize/\">Free to Optimize</a></strong>: <em>Stare decisis</em> is the legal principle which binds courts to follow precedent.&nbsp; The rationale is not that past courts were wiser, but <em>jurisprudence constante:</em>&nbsp; The legal system must be <em>predictable</em> so that people can implement contracts and behaviors knowing their implications.&nbsp; The purpose of law is not to make the world perfect, but to provide a predictable environment in which people can optimize their own futures.&nbsp; If an extremely powerful entity is choosing good futures on your behalf, that may leave little slack for you to navigate through your own strength.&nbsp; Describing how an AI can avoid stomping your self-determination is a structurally complicated problem.&nbsp; A simple (possibly not best) solution would be the gift of a world that works by improved rules, stable enough that the inhabitants could understand them and optimize their own futures together, but otherwise hands-off.&nbsp; Modern legal systems fail along this dimension; no one can possibly know all the laws, let alone obey them.</li>\n<li><strong><a href=\"/lw/x2/harmful_options/\">Harmful Options</a></strong>:&nbsp; Offering people more choices that differ along many dimensions, may diminish their satisfaction with their final choice.&nbsp; Losses are more painful than the corresponding gains are pleasurable, so people think of the dimensions along which their final choice was inferior, and of all the other opportunities passed up.&nbsp; If you can only choose one dessert, you're likely to be happier choosing from a menu of two than from a menu of fourteen.&nbsp; Refusing tempting choices consumes mental energy and decreases performance on other cognitive tasks.&nbsp; A video game that contained an always-visible easier route through, would probably be less fun to play even if that easier route were deliberately foregone.&nbsp; You can imagine a Devil who follows someone around, making their life miserable, solely by offering them options which are never actually taken.&nbsp; And what if a worse option is taken due to a predictable mistake?&nbsp; There are many ways to harm people by offering them more choices.</li>\n<li><a href=\"/lw/x3/devils_offers/\">Devil's Offers</a>:&nbsp; It is dangerous to live in an environment in which a single failure of resolve, throughout your entire life, can result in a permanent addiction or in a poor edit of your own brain.&nbsp; For example, a civilization which is constantly offering people tempting ways to shoot off their own feet - for example, offering them a cheap escape into eternal virtual reality, or customized drugs.&nbsp; It requires a constant stern will that may not be much fun.&nbsp; And it's questionable whether a superintelligence that descends from above to offer people huge dangerous temptations that they wouldn't encounter on their own, is <em>helping.</em></li>\n<li><a href=\"/lw/x4/nonperson_predicates/\">Nonperson Predicates</a>, <a href=\"/lw/x5/nonsentient_optimizers/\">Nonsentient Optimizers</a>, <a href=\"/lw/x7/cant_unbirth_a_child/\">Can't Unbirth a Child</a>:&nbsp; Discusses some of the problems of, and justification for, creating AIs that are knowably <em>not </em>conscious / sentient / people / citizens / subjective experiencers.&nbsp; We don't want the AI's <em>models of</em> people to <em>be</em> people - we don't want conscious minds trapped helplessly inside it.&nbsp; So we need how to tell that something is definitely not a person, and in this case, maybe we would like the AI itself to not be a person, which would simplify a lot of ethical issues if we could pull it off.&nbsp; Creating a new intelligent species is not lightly to be undertaken from a purely <em>ethical</em> perspective; if you create a new kind of <em>person</em>, you have to make sure it leads a life worth living.</li>\n<li><strong><a href=\"/lw/x8/amputation_of_destiny/\">Amputation of Destiny</a></strong>:&nbsp; C. S. Lewis's <em>Narnia </em>has a problem, and that problem is the super-lion Aslan - who demotes the four human children from the status of main characters, to mere hangers-on while Aslan does all the work.&nbsp; Iain Banks's <em>Culture </em>novels have a similar problem; the humans are mere hangers-on of the superintelligent Minds.&nbsp; We already have strong ethical reasons to prefer to create nonsentient AIs rather than sentient AIs, at least at first.&nbsp; But we may also prefer in just a fun-theoretic sense that we not be overshadowed by hugely more powerful entities occupying a level playing field with us.&nbsp; Entities with human emotional makeups should not be competing on a level playing field with superintelligences - either keep the superintelligences off the playing field, or design the smaller (human-level) minds with a different emotional makeup that doesn't mind being overshadowed.</li>\n<li><strong><a href=\"/lw/x9/dunbars_function/\">Dunbar's Function</a></strong>:&nbsp; Robin Dunbar's original calculation showed that the maximum human group size was around 150.&nbsp; But a typical size for a hunter-gatherer band would be 30-50, cohesive online groups peak at 50-60, and small task forces may peak in internal cohesiveness around 7.&nbsp; Our attempt to live in a world of <em>six billion</em> people has many emotional costs:&nbsp; We aren't likely to know our President or Prime Minister, or to have any significant influence over our country's politics, although we go on behaving as if we did.&nbsp; We are constantly bombarded with news about improbably pretty and wealthy individuals.&nbsp; We aren't likely to find a significant profession where we can be the best in our field.&nbsp; But if intelligence keeps increasing, the number of personal relationships we can track will also increase, along with the natural degree of specialization.&nbsp; Eventually there might be a single community of sentients that really was a single community.</li>\n<li><strong><a href=\"/lw/xr/in_praise_of_boredom/\">In Praise of Boredom</a></strong>:&nbsp; \"Boredom\" is an immensely subtle and important aspect of human values, nowhere near as straightforward as it sounds to a human.&nbsp; We don't want to get bored with breathing or with thinking.&nbsp; We do want to get bored with playing the same level of the same video game over and over.&nbsp; We don't want changing the shade of the pixels in the game to make it stop counting as \"the same game\".&nbsp; We want a steady stream of novelty, rather than spending most of our time playing the best video game level so far discovered (over and over) and occasionally trying out a different video game level as a new candidate for \"best\".&nbsp; These considerations would <em>not</em> arise in most utility functions in expected utility maximizers.</li>\n<li><a href=\"/lw/xs/sympathetic_minds/\">Sympathetic Minds</a>:&nbsp; Mirror neurons are neurons that fire both when performing an action oneself, and watching someone else perform the same action - for example, a neuron that fires when you raise your hand or watch someone else raise theirs.&nbsp; We <em>predictively</em> model other minds by putting ourselves in their shoes, which is empathy.&nbsp; But some of our desire to help relatives and friends, or be concerned with the feelings of allies, is expressed as <em>sympathy, </em>feeling what (we believe) they feel.&nbsp; Like \"boredom\", the human form of sympathy would <em>not </em>be expected to arise in an arbitrary expected-utility-maximizing AI.&nbsp; Most such agents would regard any agents in its environment as a special case of complex systems to be modeled or optimized; it would not <em>feel what they feel.</em></li>\n<li><strong><a href=\"/lw/xt/interpersonal_entanglement/\">Interpersonal Entanglement</a></strong>:&nbsp; Our sympathy with other minds makes our interpersonal relationships one of the most complex aspects of human existence.&nbsp; Romance, in particular, is more complicated than being nice to friends and kin, negotiating with allies, or outsmarting enemies - it contains aspects of all three.&nbsp; Replacing human romance with anything <em>simpler </em>or <em>easier </em>would decrease the <em>peak complexity of the human species</em> - a major step in the wrong direction, it seems to me.&nbsp; This is my problem with proposals to give people perfect, nonsentient sexual/romantic partners, which I usually refer to as \"catgirls\" (\"catboys\").&nbsp; The human species does have a statistical sex problem: evolution has not optimized the average man to make the average woman happy or vice versa.&nbsp; But there are less sad ways to solve this problem than both genders giving up on each other and retreating to catgirls/catboys.</li>\n<li><a href=\"/lw/xu/failed_utopia_42/\">Failed Utopia #4-2</a>:&nbsp; A fictional short story illustrating some of the ideas in Interpersonal Entanglement above.&nbsp; (Many commenters seemed to like this story, and some said that the ideas were easier to understand in this form.)</li>\n<li><a href=\"/lw/xd/growing_up_is_hard/\">Growing Up is Hard</a>:&nbsp; Each piece of the human brain is optimized on the assumption that all the other pieces are working the same way they did in the ancestral environment.&nbsp; Simple neurotransmitter imbalances can result in psychosis, and some aspects of Williams Syndrome are probably due to having a frontal cortex that is too large relative to the rest of the brain.&nbsp; Evolution creates limited robustness, but often stepping outside the ancestral parameter box just breaks things.&nbsp; Even if the first change works, the second and third changes are less likely to work as the total parameters get less ancestral and the brain's tolerance is used up.&nbsp; A cleanly designed AI might improve itself to the point where it was smart enough to unravel and augment the human brain.&nbsp; Or uploads might be able to make themselves smart enough to solve the increasingly difficult problem of not going slowly, subtly insane.&nbsp; Neither path is easy.&nbsp; There seems to be an irreducible residue of danger and difficulty associated with an adult version of humankind ever coming into being.&nbsp; Being a transhumanist means wanting certain things; it doesn't mean you think those things are easy.</li>\n<li><a href=\"/lw/xe/changing_emotions/\">Changing Emotions</a>:&nbsp; Creating new emotions seems like a desirable aspect of many parts of Fun Theory, but this is not to be trivially postulated.&nbsp; It's the sort of thing best done with superintelligent help, and slowly and conservatively even then.&nbsp; We can illustrate these difficulties by trying to translate the short English phrase \"change sex\" into a cognitive transformation of extraordinary complexity and many hidden subproblems.</li>\n<li><strong><a href=\"/lw/xg/emotional_involvement/\">Emotional Involvement</a></strong>:&nbsp; Since the events in video games have no actual long-term consequences, playing a video game is not likely to be nearly as <em>emotionally involving</em> as much less dramatic events in real life.&nbsp; The supposed Utopia of playing lots of cool video games forever, is life as a series of disconnected episodes with no lasting consequences.&nbsp; Our current emotions are bound to activities that were subgoals of reproduction in the ancestral environment - but we now pursue these activities as independent goals regardless of whether they lead to reproduction.&nbsp; (Sex with birth control is the classic example.)&nbsp; A transhuman existence would need new emotions suited to the important short-term and long-term events of that existence.</li>\n<li><strong><a href=\"/lw/xi/serious_stories/\">Serious Stories</a></strong>:&nbsp; <em>Stories </em>and <em>lives </em>are optimized according to rather different criteria.&nbsp; Advice on how to write fiction will tell you that \"stories are about people's pain\" and \"every scene must end in disaster\".&nbsp; I once assumed that it was not possible to write any story about a successful Singularity because the inhabitants would not be in any pain; but something about the final conclusion that the post-Singularity world would contain <em>no </em>stories worth telling seemed alarming.&nbsp; Stories in which nothing ever goes wrong, are painful to read; would a life of endless success have the same painful quality?&nbsp; If so, should we simply eliminate that revulsion via neural rewiring?&nbsp; Pleasure probably <em>does</em> retain its meaning in the absence of pain to contrast it; they are different neural systems.&nbsp; The present world has an imbalance between pain and pleasure; it is much easier to produce severe pain than correspondingly intense pleasure.&nbsp; One path would be to address the <em>imbalance</em> and create a world with more pleasures, and free of the more grindingly destructive and pointless sorts of pain.&nbsp; Another approach would be to eliminate pain <em>entirely</em>.&nbsp; I feel like I prefer the former approach, but I don't know if it can last in the long run.</li>\n<li><strong><a href=\"/lw/xl/eutopia_is_scary/\">Eutopia is Scary</a></strong>:&nbsp; If a citizen of the Past were dropped into the Present world, they would be pleasantly surprised along at least some dimensions; they would also be horrified, disgusted, and <em>frightened.</em>&nbsp; This is not because our world has gone wrong, but because it has gone right.&nbsp; A true Future gone <em>right</em> would, realistically, be shocking to us along at least <em>some</em> dimensions.&nbsp; This may help explain why most literary Utopias fail; as George Orwell observed, \"they are chiefly concerned with avoiding fuss\".&nbsp; Heavens are meant to sound like good news; political utopias are meant to show how neatly their underlying ideas work.&nbsp; Utopia is reassuring, unsurprising, and dull.&nbsp; Eutopia would be scary.&nbsp; (Of course the vast majority of scary things are <em>not </em>Eutopian, just entropic.)&nbsp; Try to imagine a <em>genuinely better</em> world in which you would be <em>out of place</em> - <em>not</em> a world that would make you smugly satisfied at how well all your current ideas had worked.&nbsp; This proved to be a very important exercise when I tried it; it made me realize that all my old proposals had been optimized to <em>sound safe and reassuring</em>.</li>\n<li><a href=\"/lw/xm/building_weirdtopia/\">Building Weirdtopia</a>:&nbsp; Utopia and Dystopia both confirm the moral sensibilities you started with; whether the world is a libertarian utopia of government non-interference, or a hellish dystopia of government intrusion and regulation, either way you get to say \"Guess I was right all along.\"&nbsp; To break out of this mold, write down the Utopia, and the Dystopia, and then try to write down the Weirdtopia - an arguably-better world that zogs instead of zigging or zagging.&nbsp; (Judging from the comments, this exercise seems to have mostly failed.)</li>\n<li><strong><a href=\"/lw/xo/justified_expectation_of_pleasant_surprises/\">Justified Expectation of Pleasant Surprises</a></strong>:&nbsp; A pleasant <em>surprise</em> probably has a greater hedonic impact than being told about the same positive event long in advance - hearing about the positive event is good news in the moment of first hearing, but you don't have the gift actually in hand.&nbsp; Then you have to wait, perhaps for a long time, possibly comparing the expected pleasure of the future to the lesser pleasure of the present.&nbsp; This argues that if you have a choice between a world in which the same pleasant events occur, but in the first world you are told about them long in advance, and in the second world they are kept secret until they occur, you would prefer to live in the second world.&nbsp; The importance of hope is widely appreciated - people who do not expect their lives to improve in the future are less likely to be happy in the present - but the importance of <em>vague</em> hope may be understated.</li>\n<li><strong><a href=\"/lw/xp/seduced_by_imagination/\">Seduced by Imagination</a></strong>:&nbsp; Vagueness usually has a poor name in rationality, but the Future is something about which, in fact, we do not possess strong reliable specific information.&nbsp; Vague (but justified!) hopes may also be <em>hedonically</em> better.&nbsp; But a more important caution for <em>today's</em> world is that <em>highly specific</em> pleasant scenarios can exert a dangerous power over human minds - suck out our emotional energy, make us forget what we don't know, and cause our mere actual lives to pale by comparison.&nbsp; (This post is not about Fun Theory proper, but it contains an important warning about how <em>not</em> to use Fun Theory.)</li>\n<li><a href=\"/lw/xc/the_uses_of_fun_theory/\">The Uses of Fun (Theory)</a>:&nbsp; Fun Theory is important for replying to critics of human progress; for inspiring people to keep working on human progress; for refuting religious arguments that the world could possibly have been benevolently designed; for showing that religious Heavens show the signature of the same human biases that torpedo other attempts at Utopia; and for appreciating the great complexity of our values and of a life worth living, which requires a correspondingly strong effort of AI design to create AIs that can play good roles in a good future.</li>\n<li><strong><a href=\"/lw/xw/higher_purpose/\">Higher Purpose</a></strong>:&nbsp; Having a Purpose in Life consistently shows up as something that increases stated well-being.&nbsp; Of course, the problem with trying to pick out \"a Purpose in Life\" in order to make yourself happier, is that this doesn't take you <em>outside</em> yourself; it's still all about <em>you.</em>&nbsp; To find purpose, you need to turn your eyes outward to look at the world and find things there that you care about - rather than obsessing about the wonderful spiritual benefits <em>you're</em> getting from helping others.&nbsp; In today's world, most of the <em>highest-priority</em> legitimate Causes consist of large groups of people in extreme jeopardy:&nbsp; Aging threatens the old, starvation threatens the poor, extinction risks threaten humanity as a whole.&nbsp; If the future goes right, many and perhaps all such problems will be solved - depleting the stream of victims to be helped.&nbsp; Will the future therefore consist of self-obsessed individuals, with nothing to take them outside themselves?&nbsp; I suggest, though, that even if there were no large groups of people in extreme jeopardy, we would still, looking around, find things outside ourselves that we cared about - friends, family; truth, freedom...&nbsp; Nonetheless, if the Future goes sufficiently well, there will come a time when you could search the whole of civilization, and never find a single person so much in need of help, as dozens you now pass on the street.&nbsp; If you do want to save someone from death, or help a great many people, then <em>act now</em>; your opportunity may not last, one way or another.</li>\n</ul>\n<ul>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XqykXFKL9t38pbSEm": 2, "sSNtcEQsqHgN8ZmRF": 5}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K4aGvLnHvYgX9pZHS", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 56, "extendedScore": null, "score": 8.4e-05, "legacy": true, "legacyId": "1222", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 56, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qZJBighPrnv9bSqTZ", "hQSaMafoizBSa3gFR", "JynJ6xfnpq9oN3zpb", "pK4HTxuv6mftHXWC3", "29vqqmGNxNRGzffEj", "aEdqh3KPerBNYvoWe", "QfpHRAMRM2HjteKFK", "eLHCWi8sotQT6CmTX", "dKGfNvjGjq4rqffyF", "EZ8GniEPSechjDYP9", "CtSS6SkHhLBvdodTY", "MTjej6HKvPByx3dEA", "wqDRRx9RqwKLzWt7R", "HsRFQTAySAx8xbXEc", "gb6zWstjmkYHLrbrg", "vwnSPgwtmLjvTK2Wa", "W5PhyEQqEWTcpRpqn", "WMDy4GxbyYkNrbmrs", "NLMo5FZWFFq652MNe", "Py3uGnncqXuEfPtQp", "ctpkTaqTKbmm6uRgC", "EQkELCGiGQwvrrp3L", "QZs4vkC7cbyjL9XA9", "ZmDEbiEeXk3Wv2sLH", "6qS9q5zHafFXsB6hf", "cWjK3SbRcLkb3gN69", "DGXvLNpiSYBeQ6TLW", "88BpRQah9c2GWY3En", "4o3zwgofFPLutkqvd", "5Pjq3mxuiXu2Ys3gM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2009-01-25T11:18:23.000Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-26T01:10:30.000Z", "modifiedAt": null, "url": null, "title": "BHTV: Yudkowsky / Wilkinson", "slug": "bhtv-yudkowsky-wilkinson", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:44.726Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qFLQCiiECXWkBQhhZ/bhtv-yudkowsky-wilkinson", "pageUrlRelative": "/posts/qFLQCiiECXWkBQhhZ/bhtv-yudkowsky-wilkinson", "linkUrl": "https://www.lesswrong.com/posts/qFLQCiiECXWkBQhhZ/bhtv-yudkowsky-wilkinson", "postedAtFormatted": "Monday, January 26th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20BHTV%3A%20Yudkowsky%20%2F%20Wilkinson&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABHTV%3A%20Yudkowsky%20%2F%20Wilkinson%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqFLQCiiECXWkBQhhZ%2Fbhtv-yudkowsky-wilkinson%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=BHTV%3A%20Yudkowsky%20%2F%20Wilkinson%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqFLQCiiECXWkBQhhZ%2Fbhtv-yudkowsky-wilkinson", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqFLQCiiECXWkBQhhZ%2Fbhtv-yudkowsky-wilkinson", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 84, "htmlBody": "<p><a href=\"http://bloggingheads.tv/diavlogs/17359\">Eliezer Yudkowsky and Will Wilkinson</a>.&#0160; Due to a technical mistake - I won&#39;t say which of us made it, except that it wasn&#39;t me - the video cuts out at 47:37, but the MP3 of the full dialogue is available <a href=\"http://download.bloggingheads.tv/completed/bhtv-2009-01-23-ww-ey.mp3\">here</a>.&#0160; I recall there was some good stuff at the end, too.</p><p>We talked about Obama up to 23 minutes, then it&#39;s on to rationality.&#0160; Wilkinson introduces (invents?) the phrase &quot;good cognitive citizenship&quot; which is a great phrase that I am totally going to steal.</p><p><embed flashvars=\"playlist=http%3A%2F%2Fbloggingheads%2Etv%2Fdiavlogs%2Fliveplayer%2Dplaylist%2F17359%2F00%3A00%2F47%3A37\" height=\"288\" src=\"http://bloggingheads.tv/maulik/offsite/offsite_flvplayer.swf\" type=\"application/x-shockwave-flash\" width=\"380\" /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"9DNZfxFvY5iKoZQbz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qFLQCiiECXWkBQhhZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 4.715513981035619e-07, "legacy": true, "legacyId": "1223", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-26T10:13:14.000Z", "modifiedAt": null, "url": null, "title": "31 Laws of Fun", "slug": "31-laws-of-fun", "viewCount": null, "lastCommentedAt": "2020-02-28T14:35:07.365Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qZJBighPrnv9bSqTZ/31-laws-of-fun", "pageUrlRelative": "/posts/qZJBighPrnv9bSqTZ/31-laws-of-fun", "linkUrl": "https://www.lesswrong.com/posts/qZJBighPrnv9bSqTZ/31-laws-of-fun", "postedAtFormatted": "Monday, January 26th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%2031%20Laws%20of%20Fun&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A31%20Laws%20of%20Fun%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZJBighPrnv9bSqTZ%2F31-laws-of-fun%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=31%20Laws%20of%20Fun%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZJBighPrnv9bSqTZ%2F31-laws-of-fun", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqZJBighPrnv9bSqTZ%2F31-laws-of-fun", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2420, "htmlBody": "<p style=\"margin-left: 40px;\">So this is Utopia, is it?&#0160; Well<br />I beg your pardon, I thought it was Hell.<br />&#0160;&#0160;&#0160; &#0160;&#0160;&#0160; -- Sir Max Beerholm, verse entitled<br />&#0160;&#0160;&#0160; &#0160;&#0160;&#0160; <em>In a Copy of More&#39;s (or Shaw&#39;s or Wells&#39;s or Plato&#39;s or Anybody&#39;s) Utopia</em></p><p>This is a shorter summary of the <a href=\"/lw/xy/the_fun_theory_sequence/\">Fun Theory Sequence</a> with all the background theory left out - just the compressed advice to the would-be author or futurist who wishes to imagine a world where people <em>might actually want to live:</em></p><ol>\n<li>Think of a <em>typical day</em> in the life of someone who&#39;s been adapting to Utopia <em>for a while.</em>&#0160; Don&#39;t anchor on the first moment of &quot;hearing the good news&quot;.&#0160; Heaven&#39;s &quot;You&#39;ll never have to work again, and the streets are paved with gold!&quot; sounds like good news to a tired and poverty-stricken peasant, but two months later it might not be so much fun.&#0160; (<a href=\"/lw/wv/prolegomena_to_a_theory_of_fun/\">Prolegomena to a Theory of Fun</a>.)</li>\n<li>Beware of packing your Utopia with things you think people <em>should</em> do that aren&#39;t actually <em>fun</em>.&#0160; Again, consider Christian Heaven: singing hymns doesn&#39;t sound like loads of endless fun, but you&#39;re <em>supposed </em>to enjoy praying, so no one can point this out.&#0160; (<a href=\"/lw/wv/prolegomena_to_a_theory_of_fun/\">Prolegomena to a Theory of Fun</a>.)</li>\n<li>Making a video game easier doesn&#39;t always improve it.&#0160; The same holds true of a life.&#0160; Think in terms of clearing out low-quality drudgery to make way for high-quality challenge, rather than eliminating work.&#0160; (<a href=\"/lw/ww/high_challenge/\">High Challenge</a>.)</li>\n<li>Life should contain novelty - experiences you haven&#39;t encountered before, preferably teaching you something you didn&#39;t already know.&#0160; If there isn&#39;t a sufficient supply of novelty (relative to the speed at which you generalize), you&#39;ll get bored.&#0160; (<a href=\"/lw/wx/complex_novelty/\">Complex Novelty</a>.)</li>\n</ol>\n<a id=\"more\"></a>\n<ol start=\"5\">\n<li>People should get smarter at a rate sufficient to integrate their old experiences, but not so much smarter so fast that they can&#39;t integrate their new intelligence.&#0160; Being smarter means you get bored faster, but you can also tackle new challenges you couldn&#39;t understand before.&#0160; (<a href=\"/lw/wx/complex_novelty/\">Complex Novelty</a>.)</li>\n<li>People should live in a world that fully engages their senses, their bodies, and their brains.&#0160; This means either that the world resembles the ancestral savanna more than say a windowless office; or alternatively, that brains and bodies have changed to be fully engaged by different kinds of complicated challenges and environments.&#0160; (Fictions intended to entertain a human audience should concentrate primarily on the former option.)&#0160; (<a href=\"/lw/wy/sensual_experience/\">Sensual Experience</a>.)</li>\n<li>Timothy Ferris:&#0160; &quot;What is the opposite of happiness?&#0160; \nSadness?&#0160; No.&#0160; Just as love and hate are two sides of the\nsame coin, so are happiness and sadness...&#0160; The opposite of\nlove is indifference, and the opposite of happiness is - here&#39;s the\nclincher - boredom...&#0160; The question you should be asking\nisn&#39;t &#39;What do I want?&#39; or &#39;What are my goals?&#39;\nbut &#39;What would excite me?&#39;...&#0160; <em>Living</em> like a millionaire requires <em>doing</em> interesting things and not just owning enviable things.&quot;&#0160; (<a href=\"/lw/sc/existential_angst_factory/\">Existential Angst Factory</a>.)</li>\n<li>Any particular individual&#39;s life should get better and better over time.&#0160; (<a href=\"/lw/xk/continuous_improvement/\">Continuous Improvement</a>.)</li>\n<li>You should not know exactly what improvements the future holds, although you should look forward to finding out.&#0160; The actual event should come as a pleasant surprise.&#0160; (<a href=\"/lw/xo/justified_expectation_of_pleasant_surprises/\">Justified Expectation of Pleasant Surprises</a>.)</li>\n<li>Our hunter-gatherer ancestors strung their own bows, wove their own baskets and whittled their own flutes; then they did their own hunting, their own gathering and played their own music.&#0160; Futuristic Utopias are often depicted as offering more and more neat buttons that do less and less comprehensible things <em>for </em>you.&#0160; Ask not what interesting things Utopia can do <span style=\"font-style: italic;\"></span><em>for </em>people; ask rather what interesting things the inhabitants could do for <em>themselves </em>- with their own brains, their own bodies, or tools they understand how to build.&#0160; (<a href=\"/lw/wz/living_by_your_own_strength/\">Living By Your Own Strength</a>.)</li>\n<li>Living in Eutopia should make people stronger, not weaker, over time.&#0160; The inhabitants should appear <em>more formidable</em> than the people of our own world, not less.&#0160; (<a href=\"/lw/wz/living_by_your_own_strength/\">Living By Your Own Strength</a>; see also, <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">Tsuyoku Naritai</a>.)</li>\n<li>Life should not be broken up into a series of disconnected episodes with no long-term consequences.&#0160; No matter how sensual or complex, playing one <em>really great video game</em> after another, does not make a life story.&#0160; (<a href=\"/lw/xg/emotional_involvement/\">Emotional Involvement</a>.)</li>\n<li>People should make their own destinies; their lives should not be choreographed to the point that they no longer need to imagine, plan and navigate their own futures.&#0160; Citizens should not be the pawns of more powerful gods, still less their sculpted material.&#0160; One simple solution would be to have the world work by stable rules that are the same for everyone, where the burden of Eutopia is carried by a good initial choice of rules, rather than by any optimization pressure applied to individual lives.&#0160; (<a href=\"/lw/xb/free_to_optimize/\">Free to Optimize</a>.)</li>\n<li>Human minds should not have to <em>play on a level field</em> with vastly superior entities.&#0160; Most people don&#39;t like being overshadowed.&#0160; Gods destroy a human protagonist&#39;s &quot;main character&quot; status; this is undesirable in fiction and probably in real life.&#0160; (E.g.:&#0160; C. S. Lewis&#39;s Narnia, Iain Banks&#39;s Culture.)&#0160; Either change people&#39;s emotional makeup so that they don&#39;t <em>mind </em>being unnecessary, or keep the gods <em>way </em>off their playing field.&#0160; Fictional stories intended for human audiences cannot do the former.&#0160; (And in real life, you probably <em>can</em> have powerful AIs that are neither sentient nor meddlesome.&#0160; See the main post and its prerequisites.)&#0160; (<a href=\"/lw/x8/amputation_of_destiny/\">Amputation of Destiny</a>.)</li>\n<li>Trying to compete on a single flat playing field with <em>six billion other humans </em>also creates problems.&#0160; Our ancestors lived in bands of around 50 people.&#0160; Today the media is constantly bombarding us with news of <em>exceptionally </em>rich and pretty people as if they lived next door to us; and very few people get a chance to be the <em>best </em>at any specialty.&#0160; (<a href=\"/lw/x9/dunbars_function/\">Dunbar&#39;s Function</a>.)</li>\n<li>Our ancestors also had some degree of genuine control over their band&#39;s politics.&#0160; Contrast to modern nation-states where almost no one knows the President on a personal level or could argue Congress out of a bad decision.&#0160; (Though that doesn&#39;t stop people from arguing as loudly as if they still lived in a 50-person band.)&#0160; (<a href=\"/lw/x9/dunbars_function/\">Dunbar&#39;s Function</a>.)</li>\n<li>Offering people more options is <em>not </em>always helping them (especially if the option is something they couldn&#39;t do for themselves).&#0160; Losses are more painful than the corresponding gains, so if choices are different along many dimensions and only one choice can be taken, people tend to focus on the loss of the road <em>not </em>taken.&#0160; Offering a road that bypasses a challenge makes the challenge feel less real, even if the cheat is diligently refused.&#0160; It is also a sad fact that humans predictably make certain kinds of mistakes.&#0160; Don&#39;t assume that building <em>more choice</em> into your Utopia is necessarily an improvement because &quot;people can always just say no&quot;.&#0160; This <em>sounds reassuring</em> to an outside reader - &quot;Don&#39;t worry, <em>you&#39;ll</em> decide!&#0160; You trust <em>yourself</em>, right?&quot; - but might not be much fun to actually <em>live </em>with.&#0160; (<a href=\"/lw/x2/harmful_options/\">Harmful Options</a>.)</li>\n<li>Extreme example of the above: being constantly offered huge temptations that are\nincredibly dangerous - a completely realistic virtual world, or\nvery addictive and pleasurable drugs.&#0160; You can never\nallow yourself a single moment of willpower failure over your whole\nlife.&#0160; (E.g.:&#0160; John C. Wright&#39;s Golden Oecumene.)&#0160; (<a href=\"/lw/x3/devils_offers/\">Devil&#39;s Offers</a>.)</li>\n<li>Conversely, when people are grown strong enough to shoot off their feet <em>without external help,</em> stopping them may be too much interference.&#0160; Hopefully they&#39;ll then be smart enough <em>not</em> to:&#0160; By the time they can build the gun, they&#39;ll know what happens if they pull the gun, and won&#39;t need a smothering safety blanket.&#0160; If that&#39;s the theory, then dangerous options need correspondingly difficult locks.&#0160; (<a href=\"/lw/x3/devils_offers/\">Devil&#39;s Offers</a>.)</li>\n<li><em>Telling</em> people truths they haven&#39;t yet figured out for themselves, is not always helping them.&#0160; (<a href=\"/lw/os/joy_in_discovery/\">Joy in Discovery</a>.)</li>\n<li>Brains are some of the most complicated things in the world.&#0160; Thus,\nother humans (other minds) are some of the most complicated things we\ndeal with.&#0160; For us, this interaction has a unique character because of the <em>sympathy </em>we feel for others - the way that our brain tends to align with their brain - rather\nthan our brain just treating other brains as big complicated machines with levers to pull.&#0160; Reducing the need\nfor people to interact with other people reduces the complexity of\nhuman existence; this is a step in the wrong direction.&#0160; For example, resist the temptation to <em>simplify people&#39;s lives</em> by offering them artificially perfect sexual/romantic partners. (<a href=\"/lw/xt/interpersonal_entanglement/\">Interpersonal Entanglement</a>.)</li>\n<li>But admittedly, humanity does have a <em>statistical </em>sex problem: the male distribution of attributes doesn&#39;t harmonize with the female distribution of desires, or vice versa.&#0160; Not everything in Eutopia should be easy - but it shouldn&#39;t be pointlessly, unresolvably frustrating either.&#0160; (This is a general principle.)&#0160; So imagine <em>nudging</em> the distributions to make the problem <em>solvable</em> - rather than waving a magic wand and solving everything instantly.&#0160; (<a href=\"/lw/xt/interpersonal_entanglement/\">Interpersonal Entanglement</a>.)</li>\n<li>In general, tampering with brains, minds, emotions, and personalities is way more fraught on every possible level of ethics and difficulty, than tampering with bodies and environments.&#0160; Always ask what you can do by messing with the environment before you imagine messing with minds.&#0160; Then prefer small cognitive changes to big ones.&#0160; You&#39;re not just outrunning your human audience, you&#39;re outrunning your own imagination.&#0160; (<a href=\"/lw/xe/changing_emotions/\">Changing Emotions</a>.)</li>\n<li>In this present world, there is an imbalance between pleasure and pain.&#0160; An unskilled torturer with simple tools can create worse pain in thirty seconds, than an extremely skilled sexual artist can create pleasure in thirty minutes.&#0160; One response would be to <em>remedy the imbalance</em> - to have the world contain <em>more </em>joy than sorrow.&#0160; Pain might exist, but not pointless endless unendurable pain.&#0160; Mistakes would have more <em>proportionate </em>penalties:&#0160; You might touch a hot stove and end up with a painful blister; but not glance away for two seconds and spend the rest of your life in a wheelchair.&#0160; The people would be stronger, less exhausted.&#0160; This path would eliminate <em>mind-destroying</em> pain, and make pleasure more abundant.&#0160; Another path would eliminate pain <em>entirely</em>.&#0160; Whatever the relative merits of the real-world proposals, <em>fictional stories cannot take the second path.</em>&#0160; (<a href=\"/lw/xi/serious_stories/\">Serious Stories</a>.)</li>\n<li>George Orwell once observed that Utopias are chiefly concerned with avoiding fuss.&#0160; Don&#39;t be afraid to write a loud Eutopia that might wake up the neighbors.&#0160; (<a href=\"/lw/xl/eutopia_is_scary/\">Eutopia is Scary</a>; George Orwell&#39;s <a href=\"http://www.orwell.ru/library/articles/socialists/english/e_fun\">Why Socialists Don&#39;t Believe in Fun</a>.)</li>\n<li>George Orwell observed that &quot;The inhabitants of perfect universes seem to have no spontaneous gaiety and are usually somewhat repulsive into the bargain.&quot;&#0160; If you write a story and your characters turn out like this, it probably reflects some much deeper flaw that can&#39;t be fixed by having the State hire a few clowns.&#0160; (George Orwell&#39;s <a href=\"http://www.orwell.ru/library/articles/socialists/english/e_fun\">Why Socialists Don&#39;t Believe in Fun</a>.)</li>\n<li>Ben Franklin, yanked into our own era, would be surprised and delighted by some aspects of his Future.&#0160; Other aspects would horrify, disgust, and <em>frighten</em> him; and this is not because our world has gone <em>wrong</em>, but because it has <em>improved </em>relative to his time.&#0160; Relatively few things would have gone <em>just </em>as Ben Franklin expected.&#0160; If you imagine a world which your imagination finds familiar and comforting, it will inspire few others, and the whole exercise will lack integrity.&#0160; Try to conceive of a genuinely better world in which you, yourself, would be <em>shocked </em>(at least at first) and <em>out of place</em> (at least at first).&#0160; (<a href=\"/lw/xl/eutopia_is_scary/\">Eutopia is Scary</a>.)</li>\n<li>Utopia and Dystopia are two sides of the same coin; both just confirm the moral sensibilities you started\nwith.&#0160; Whether the world is a libertarian utopia of government\nnon-interference, or a hellish dystopia of government intrusion and\nregulation, you get to say &quot;I was right all along.&quot;&#0160; Don&#39;t just imagine something that conforms to your <em>existing </em>ideals\nof government, relationships, politics, work, or daily life.&#0160; Find the better world that zogs instead of zigging or zagging.&#0160; (To safeguard your sensibilities, you can tell yourself it&#39;s just an <em>arguably</em> better world but isn&#39;t <em>really</em> better than your favorite standard Utopia... but you&#39;ll know you&#39;re <em>really </em>doing it right if you find your ideals <em>changing</em>.)&#0160; (<a href=\"/lw/xm/building_weirdtopia/\">Building Weirdtopia</a>.)</li>\n<li>If your Utopia still seems like an endless gloomy drudgery of existential angst no matter how much you try to brighten it, there&#39;s at least one major problem that you&#39;re <em>entirely failing to focus on</em>.&#0160; (<a href=\"/lw/sc/existential_angst_factory/\">Existential Angst Factory</a>.)</li>\n<li>&#39;Tis a sad mind that cares about nothing except itself.&#0160; In the modern-day world, if an altruist looks around, their eye is caught by large groups of people in desperate jeopardy.&#0160; People in a better world will <em>not </em>see this:&#0160; A true Eutopia will run low on victims to be rescued.&#0160; This doesn&#39;t imply that the inhabitants look around outside themselves and see <em>nothing</em>.&#0160; They may care about friends and family, truth and freedom, common projects; outside minds, shared goals, and high ideals.&#0160; (<a href=\"/lw/xw/higher_purpose/\">Higher Purpose</a>.)</li>\n<li>Still, a story that confronts the challenge of Eutopia should <em>not </em>just have the convenient plot of &quot;The Dark Lord Sauron is about to invade and kill everybody&quot;.&#0160; The would-be author will have to find something <em>slightly less awful </em>for his characters to <em>legitimately care about</em>.&#0160; This is part of the challenge of showing that human progress is not the end of human stories, and that people <em>not</em> in imminent danger of death can still lead interesting lives.&#0160; Those of you interested in confronting lethal planetary-sized dangers should focus on <em>present-day real life</em>.&#0160; (<a href=\"/lw/xw/higher_purpose/\">Higher Purpose</a>.)</li>\n</ol>\n<p>The simultaneous solution of all these design requirements is left as an exercise to the reader.&#0160; At least for now.</p><p>The enumeration in this post of certain Laws shall not be construed to deny or disparage others not mentioned.&#0160; I didn&#39;t happen to write about humor, but it would be a sad world that held no laughter, etcetera.</p><p>To anyone seriously interested in trying to write a Eutopian story using these Laws:&#0160; You must first know <em>how to write</em>.&#0160; There are many, many books on how to write; you should read at least three; and they will all tell you that a great deal of practice is required.&#0160; Your practice stories should <em>not </em>be composed anywhere so difficult as Eutopia.&#0160; That said, my <em>second </em>most important advice for authors is this:&#0160; Life will never become boringly easy for your characters so long as they can make things difficult for each other.</p><p>Finally, this dire warning:&#0160; <a href=\"/lw/xp/seduced_by_imagination/\">Concretely imagining worlds much better than your present-day real life, may suck out your soul like an emotional vacuum cleaner</a>.&#0160; (See <a href=\"/lw/xp/seduced_by_imagination/\">Seduced by Imagination</a>.)&#0160; Fun Theory is dangerous, use it with caution, you have been warned.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"pGqRLe9bFDX2G2kXY": 1, "xexCWMyds6QLWognu": 1, "R6uagTfhhBeejGrrf": 2, "sSNtcEQsqHgN8ZmRF": 5}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qZJBighPrnv9bSqTZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 60, "baseScore": 72, "extendedScore": null, "score": 0.000108, "legacy": true, "legacyId": "1224", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 72, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K4aGvLnHvYgX9pZHS", "pK4HTxuv6mftHXWC3", "29vqqmGNxNRGzffEj", "aEdqh3KPerBNYvoWe", "eLHCWi8sotQT6CmTX", "8rdoea3g6QGhWQtmx", "QfpHRAMRM2HjteKFK", "DGXvLNpiSYBeQ6TLW", "dKGfNvjGjq4rqffyF", "DoLQN5ryZ9XkZjq5h", "ZmDEbiEeXk3Wv2sLH", "EZ8GniEPSechjDYP9", "vwnSPgwtmLjvTK2Wa", "W5PhyEQqEWTcpRpqn", "CtSS6SkHhLBvdodTY", "MTjej6HKvPByx3dEA", "KfMNFB3G7XNviHBPN", "Py3uGnncqXuEfPtQp", "QZs4vkC7cbyjL9XA9", "6qS9q5zHafFXsB6hf", "hQSaMafoizBSa3gFR", "cWjK3SbRcLkb3gN69", "5Pjq3mxuiXu2Ys3gM", "88BpRQah9c2GWY3En"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-27T09:34:03.000Z", "modifiedAt": null, "url": null, "title": "OB Status Update", "slug": "ob-status-update", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:23.513Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oD9fmceuJ4AFdSQ9q/ob-status-update", "pageUrlRelative": "/posts/oD9fmceuJ4AFdSQ9q/ob-status-update", "linkUrl": "https://www.lesswrong.com/posts/oD9fmceuJ4AFdSQ9q/ob-status-update", "postedAtFormatted": "Tuesday, January 27th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20OB%20Status%20Update&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOB%20Status%20Update%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoD9fmceuJ4AFdSQ9q%2Fob-status-update%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=OB%20Status%20Update%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoD9fmceuJ4AFdSQ9q%2Fob-status-update", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoD9fmceuJ4AFdSQ9q%2Fob-status-update", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 375, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/vw/whither_ob/\">Whither OB?</a></p><p><em>Overcoming Bias</em> currently plans to transition to a new format, including a new and more open sister site, tentatively entitled \"Less Wrong\".&nbsp; The new site will be built out of Reddit's source code, but you won't be limited to posting links - the new site will include a WYSIWYG HTML editor as well.&nbsp; All posts will appear on <em>Less Wrong</em> and will be voted up or down by the readers.&nbsp; Posts approved by the chief editors will be \"promoted\" to <em>Overcoming Bias</em>, which will serve as the front page of <em>Less Wrong</em>.</p><p>Once the initial site is up and running, the next items on the agenda include <em>much </em>better support for reading through sequences.&nbsp; And I'll organize more of my old posts (and perhaps some of Robin's) into sequences.</p><p>Threaded comments and comment voting/sorting are on the way.&nbsp; Anonymous commenting may go away briefly (it's not built into the Reddit codebase) but I suspect it's important for attracting new participation.&nbsp; So I do hope to bring back non-registration commenting, but it may go away for a while.&nbsp; On the plus side, you'll only have to solve a captcha once when signing up, not every time you post.&nbsp; And the 50-comment limit per page is on the way out as well.</p><p>Timeframe... <a href=\"/lw/jg/planning_fallacy/\">theoretically</a>, one to two weeks of work left.</p><p>I've reserved a final sequence on Building Rationalist Communities to seed <em>Less Wrong</em>.&nbsp; Also, I doubt I could stop blogging completely even if I tried.&nbsp; I don't <em>think </em>Robin plans to stop completely either.&nbsp; And it's worth remembering that <a href=\"http://www.overcomingbias.com/2008/02/my-favorite-lia.html\">OB's most popular post ever</a> was a reader contribution.&nbsp; So don't touch that dial, don't unsubscribe that RSS feed.&nbsp; Exciting changes on the way.\n</p><a id=\"more\"></a>\n<p>PS:&nbsp; Changes also underway at the Singularity Institute, my host organization.&nbsp; Although no formal announcement has been made yet, Tyler Emerson is departing as Executive Director and Michael Vassar is coming on board as President.&nbsp; I mention this now because Michael Vassar is usually based out of the East Coast, but is in the Bay Area for only the next couple of days, and any Bay Area folks <em>seriously </em>interested in getting <em>significantly</em> involved might want to take this opportunity to talk to him and/or me.&nbsp; Email michael no space aruna at yahoo dot com.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oD9fmceuJ4AFdSQ9q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "1225", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["59ithK3WGEMFk7iHJ", "CPm5LTwHrvBJCa9h5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-28T08:39:46.000Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes 25", "slug": "rationality-quotes-25", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:00.798Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iRop8WinYW4B8KYiK/rationality-quotes-25", "pageUrlRelative": "/posts/iRop8WinYW4B8KYiK/rationality-quotes-25", "linkUrl": "https://www.lesswrong.com/posts/iRop8WinYW4B8KYiK/rationality-quotes-25", "postedAtFormatted": "Wednesday, January 28th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%2025&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%2025%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiRop8WinYW4B8KYiK%2Frationality-quotes-25%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%2025%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiRop8WinYW4B8KYiK%2Frationality-quotes-25", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiRop8WinYW4B8KYiK%2Frationality-quotes-25", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 275, "htmlBody": "<p>\"People want to think there is some huge conspiracy run by evil geniuses.&nbsp; The reality is actually much more horrifying.&nbsp; The people running the show aren't evil geniuses.&nbsp; They are just as stupid as the rest of us.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <a href=\"http://news.ycombinator.com/item?id=416456\">Vaksel</a></p>\n<p>\"Rule of thumb:&nbsp; Be skeptical of things you learned before you could read.&nbsp; E.g., religion.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <a href=\"http://ben.casnocha.com/2009/01/quote-of-.html\">Ben Casnocha </a></p>\n<p>\"Truth is not always popular, but it is always right.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Anon</p>\n<p>\"Computer programming is omnipotence without omniscience.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <a href=\"http://news.ycombinator.com/item?id=435854\">Prospero </a></p>\n<p>\"The world breaks everyone and afterward many are strong in the broken places. But those that will not break it kills. It kills the very good, and the very gentle, and the very brave, impartially.\"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- Ernest Hemingway, <em>A Farewell to Arms </em></p>\n<p>\"Those who take delight in their own might are merely pretenders to power.&nbsp; The true warrior of fate needs no adoration or fear, no tricks or overwhelming effort; he need not be stronger or smarter or innately more capable than everyone else; he need not even admit it to himself.&nbsp; All he needs to do is to stand there, at that moment when all hope is dead, and look upon the abyss without flinching.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <a href=\"http://www.fanfiction.net/s/3886999/Shinji_and_Warhammer40k\">Shinji and Warhammer40k<br /></a></p>\n<p>\"Though here at journey's end I lie<br />&nbsp;in darkness buried deep,<br />&nbsp;beyond all towers strong and high,<br />&nbsp;beyond all mountains steep,<br />&nbsp;above all shadows rides the Sun<br />&nbsp;and Stars forever dwell:<br />&nbsp;I will not say the Day is done,<br />&nbsp;nor bid the Stars farewell.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Banaz&icirc;r Galbasi (Samwise Gamgee), <em>The Return of the King</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iRop8WinYW4B8KYiK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 12, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "1226", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-29T08:46:30.000Z", "modifiedAt": null, "url": null, "title": "Value is Fragile", "slug": "value-is-fragile", "viewCount": null, "lastCommentedAt": "2022-05-27T14:13:34.438Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile", "pageUrlRelative": "/posts/GNnHHmm8EzePmKzPk/value-is-fragile", "linkUrl": "https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile", "postedAtFormatted": "Thursday, January 29th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Value%20is%20Fragile&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AValue%20is%20Fragile%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGNnHHmm8EzePmKzPk%2Fvalue-is-fragile%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Value%20is%20Fragile%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGNnHHmm8EzePmKzPk%2Fvalue-is-fragile", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGNnHHmm8EzePmKzPk%2Fvalue-is-fragile", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1828, "htmlBody": "<p>If I had to pick a single statement that <em>relies</em> on more Overcoming Bias content I've written than any other, that statement would be:</p>\n<p><em>Any Future <strong>not </strong>shaped by a goal system with detailed reliable inheritance from human morals and metamorals, will contain almost nothing of worth.</em></p>\n<p>\"Well,\" says the one, \"maybe according to your provincial <em>human</em> values, <em>you </em>wouldn't like it.&nbsp; But I can easily imagine a galactic civilization full of agents who are nothing like <em>you</em>, yet find great value and interest in their <em>own</em> goals.&nbsp; And that's fine by me.&nbsp; I'm not so bigoted as you are.&nbsp; Let the Future go its own way, without trying to bind it forever to the laughably primitive prejudices of a pack of four-limbed Squishy Things -\"</p>\n<p>My friend, I have <em>no problem</em> with the thought of a galactic civilization vastly unlike our own... full of strange beings who look nothing like me even in their own imaginations... pursuing pleasures and experiences I can't begin to empathize with... trading in a marketplace of unimaginable goods... allying to pursue incomprehensible objectives... people whose life-stories I could never understand.</p>\n<p>That's what the Future looks like if things go <em>right.</em></p>\n<p>If the chain of inheritance from human (meta)morals is broken, the Future does <em>not </em>look like this.&nbsp; It does <em>not</em> end up magically, delightfully incomprehensible.</p>\n<p>With very high probability, it ends up looking <em>dull.</em>&nbsp; Pointless.&nbsp; Something whose loss you wouldn't mourn.</p>\n<p>Seeing this as obvious, is what requires that immense amount of background explanation.</p>\n<p><a id=\"more\"></a></p>\n<p>And I'm not going to iterate through <em>all </em>the points and winding pathways of argument here, because that would take us back through 75% of my <em>Overcoming Bias</em> posts.&nbsp; Except to remark on how <em>many</em> different things must be known to constrain the final answer.</p>\n<p>Consider the <a href=\"/lw/xr/in_praise_of_boredom/\">incredibly important human value of \"boredom\"</a> - our desire not to do \"the same thing\" over and over and over again.&nbsp; You can imagine a mind that contained <em>almost</em> the whole specification of human value, almost all the morals and metamorals, but left out <em>just this one thing</em> -</p>\n<p>- and so it spent until the end of time, and until the farthest reaches of its light cone, replaying a single highly optimized experience, over and over and over again.</p>\n<p>Or imagine a mind that contained almost the whole specification of which sort of feelings humans most enjoy - but not the idea that those feelings had important <em>external referents.</em>&nbsp; So that the mind just went around <em>feeling</em> like it had made an important discovery, <em>feeling</em> it had found the perfect lover, <em>feeling</em> it had helped a friend, but not actually <em>doing</em> any of those things - having become its own experience machine.&nbsp; And if the mind pursued those feelings <em>and their referents,</em> it would be a good future and true; but because this <em>one dimension</em> of value was left out, the future became something dull.&nbsp; Boring and repetitive, because although this mind <em>felt</em> that it was encountering experiences of incredible novelty, this feeling was in no wise true.</p>\n<p>Or the converse problem - an agent that contains all the aspects of human value, <em>except </em>the valuation of subjective experience.&nbsp; So that the result is a nonsentient optimizer that goes around making genuine discoveries, but the discoveries are not savored and enjoyed, because there is no one there to do so.&nbsp; This, I admit, I don't quite know to be possible.&nbsp; Consciousness does still confuse me to some extent.&nbsp; But a universe with no one to bear witness to it, might as well not be.</p>\n<p>Value isn't just complicated, it's <em>fragile.</em>&nbsp; There is <em>more than one dimension</em> of human value, where <em>if just that one thing is lost</em>, the Future becomes null.&nbsp; A <em>single</em> blow and <em>all </em>value shatters.&nbsp; Not every <em>single </em>blow will shatter <em>all </em>value - but more than one possible \"single blow\" will do so.</p>\n<p>And then there are the long defenses of this proposition, which relies on 75% of my <em>Overcoming Bias</em> posts, so that it would be more than one day's work to summarize all of it.&nbsp; Maybe some other week.&nbsp; There's so many branches I've seen that discussion tree go down.</p>\n<p>After all - a mind <em>shouldn't</em> just go around having the same experience over and over and over again.&nbsp; Surely no superintelligence would be so <a href=\"/lw/sy/sorting_pebbles_into_correct_heaps/\">grossly mistaken</a> about the <a href=\"/lw/sx/inseparably_right_or_joy_in_the_merely_good/\">correct action</a>?</p>\n<p>Why would any supermind <a href=\"/lw/st/anthropomorphic_optimism/\">want</a> something so <a href=\"/lw/oi/mind_projection_fallacy/\">inherently</a> <a href=\"/lw/sy/sorting_pebbles_into_correct_heaps/\">worthless</a> as the feeling of discovery without any real discoveries?&nbsp; Even if that were its utility function, wouldn't it just <a href=\"/lw/rd/passing_the_recursive_buck/\">notice that its utility function was wrong</a>, and rewrite it?&nbsp; It's got <a href=\"/lw/rc/the_ultimate_source/\">free will</a>, right?</p>\n<p>Surely, at least <em>boredom</em> has to be a <a href=\"/lw/rn/no_universally_compelling_arguments/\">universal</a> value.&nbsp; It evolved in humans because it's valuable, right?&nbsp; So any mind that doesn't share our dislike of repetition, will fail to thrive in the universe and be eliminated...</p>\n<p>If you are familiar with the difference between <a href=\"/lw/l4/terminal_values_and_instrumental_values/\">instrumental values and terminal values,</a> <em>and</em> familiar with the <a href=\"/lw/kt/evolutions_are_stupid_but_work_anyway/\">stupidity</a> of <a href=\"/lw/kr/an_alien_god/\">natural selection</a>, <em>and </em>you understand how this stupidity manifests in the difference between <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">executing adaptations versus maximizing fitness</a>, <em>and</em> you know this turned <a href=\"/lw/l1/evolutionary_psychology/\">instrumental subgoals of reproduction into decontextualized unconditional emotions</a>...</p>\n<p>...<em>and</em> you're familiar with how the tradeoff between exploration and exploitation works in Artificial Intelligence...</p>\n<p>...then you might be able to see that the human form of boredom that demands a steady trickle of novelty for its own sake, isn't a grand universal, but just a particular algorithm that evolution coughed out into us.&nbsp; And you might be able to see how the vast majority of possible expected utility maximizers, would only engage in just so much efficient exploration, and spend most of its time exploiting the best alternative found so far, over and over and over.</p>\n<p>That's a lot of background knowledge, though.</p>\n<p>And so on and so on and so on through 75% of my posts on <em>Overcoming Bias</em>, and many chains of fallacy and counter-explanation.&nbsp; Some week I may try to write up the whole diagram.&nbsp; But for now I'm going to assume that you've read the arguments, and just deliver the conclusion:</p>\n<p>We can't relax our grip on the future - let go of the steering wheel - and still end up with anything of value.</p>\n<p>And those who think we <em>can </em>-</p>\n<p>- they're trying to be cosmopolitan.&nbsp; I understand that.&nbsp; I read those same science fiction books as a kid:&nbsp; The provincial villains who enslave aliens for the crime of not looking just like humans.&nbsp; The provincial villains who enslave helpless AIs in durance vile on the assumption that silicon can't be sentient.&nbsp; And the cosmopolitan heroes who understand that <em>minds don't have to be just like us to be embraced as valuable</em> -</p>\n<p>I read those books.&nbsp; I once believed them.&nbsp; But the beauty that jumps out of one box, is not jumping out of <em>all </em>boxes.&nbsp; (This being the moral of the sequence on <a href=\"/lw/vm/lawful_creativity/\">Lawful Creativity</a>.)&nbsp; If you leave behind all order, what is left is not the perfect answer, what is left is perfect noise.&nbsp; Sometimes you have to abandon an old design rule to build a better mousetrap, but that's not the same as giving up all design rules and collecting wood shavings into a heap, with every pattern of wood as good as any other.&nbsp; The old rule is always abandoned at the behest of some higher rule, some higher criterion of value that governs.</p>\n<p>If you loose the grip of human morals and metamorals - the result is not mysterious and alien and beautiful by the standards of human value.&nbsp; It is moral noise, a universe tiled with paperclips.&nbsp; To change away from human morals <em>in the direction of improvement rather than entropy,</em> requires a criterion of improvement; and that criterion would be physically represented in our brains, and our brains alone.</p>\n<p>Relax the grip of human value upon the universe, and it will end up <em>seriously </em>valueless.&nbsp; Not, strange and alien and wonderful, shocking and terrifying and beautiful beyond all human imagination.&nbsp; Just, tiled with paperclips.</p>\n<p>It's only some <em>humans</em>, you see, who have this idea of embracing manifold varieties of mind - of wanting the Future to be something greater than the past - of being not bound to our past selves - of trying to change and move forward.</p>\n<p>A paperclip maximizer just chooses whichever action leads to the greatest number of paperclips.</p>\n<p>No free lunch.&nbsp; You want a wonderful and mysterious universe?&nbsp; That's <em>your </em>value.&nbsp; <em>You </em>work to create that value.&nbsp; Let that value exert its force through you who represents it, let it make decisions in you to shape the future.&nbsp; And maybe you shall indeed obtain a wonderful and mysterious universe.</p>\n<p>No free lunch.&nbsp; Valuable things appear because a goal system that values them takes action to create them.&nbsp; Paperclips don't materialize from nowhere for a paperclip maximizer.&nbsp; And a wonderfully alien and mysterious Future will not materialize from nowhere for us humans, if our values that prefer it are physically obliterated - or even <em>disturbed</em> in the wrong dimension.&nbsp; Then there is nothing left in the universe that works to make the universe valuable.</p>\n<p>You <em>do</em> have values, even when you're trying to be \"cosmopolitan\", trying to display a properly virtuous appreciation of alien minds.&nbsp; Your values are then <a href=\"/lw/ta/invisible_frameworks/\">faded further into the invisible background</a> - they are less <em>obviously</em> human.&nbsp; Your brain probably <a href=\"/lw/st/anthropomorphic_optimism/\">won't even generate an alternative so awful</a> that it would wake you up, make you say \"No!&nbsp; Something went wrong!\" even at your most cosmopolitan.&nbsp; E.g. \"a nonsentient optimizer absorbs all matter in its future light cone and tiles the universe with paperclips\".&nbsp; You'll just imagine strange alien worlds to appreciate.</p>\n<p>Trying to be \"cosmopolitan\" - to be <em>a citizen of the cosmos</em> - just strips off a <em>surface veneer</em> of goals that seem <em>obviously</em> \"human\".</p>\n<p>But if you wouldn't like the Future tiled over with paperclips, and you would prefer a civilization of...</p>\n<p>...sentient beings...</p>\n<p>...with enjoyable experiences...</p>\n<p>...that aren't the <em>same </em>experience over and over again...</p>\n<p>...and are bound to something besides just being a sequence of internal pleasurable feelings...</p>\n<p>...learning, discovering, freely choosing...</p>\n<p>...well, I've just been through <a href=\"/lw/xy/the_fun_theory_sequence/\">the posts on Fun Theory</a> that went into <em>some </em>of the <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">hidden details</a> on those <a href=\"/lw/td/magical_categories/\">short English words</a>.</p>\n<p>Values that you might praise as <em>cosmopolitan</em> or <em>universal </em>or <em>fundamental </em>or <em>obvious common sense,</em> are represented in your brain just as much as those values that you might dismiss as <em>merely human.</em>&nbsp; Those values come of the long history of humanity, and the <a href=\"/lw/sa/the_gift_we_give_to_tomorrow/\">morally miraculous stupidity of evolution that created us</a>.&nbsp; (And once I <em>finally </em>came to that realization, I felt less ashamed of values that seemed 'provincial' - but that's another matter.)</p>\n<p>These values do <em>not</em> emerge in all possible minds.&nbsp; They will <em>not</em> appear from nowhere to rebuke and revoke the utility function of an expected paperclip maximizer.</p>\n<p>Touch too hard in the wrong dimension, and the physical representation of those values will shatter - and <em>not come back</em>, for there will be nothing left to <em>want </em>to bring it back.</p>\n<p>And the <em>referent </em>of those values - a worthwhile universe - would no longer have any physical reason to come into being.</p>\n<p>Let go of the steering wheel, and the Future crashes.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"R6uagTfhhBeejGrrf": 3, "xknvtHwqvqhwahW8Q": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GNnHHmm8EzePmKzPk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 106, "baseScore": 128, "extendedScore": null, "score": 0.00019, "legacy": true, "legacyId": "1227", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "9bvAELWc8y2gYjRav", "canonicalCollectionSlug": "rationality", "canonicalBookId": "T3CjiQq6bBgFuzvp6", "canonicalNextPostSlug": "the-gift-we-give-to-tomorrow", "canonicalPrevPostSlug": "serious-stories", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 128, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 110, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WMDy4GxbyYkNrbmrs", "mMBTPTjRbsrqbSkZE", "JynJ6xfnpq9oN3zpb", "RcZeZt8cPk48xxiQ8", "ZTRiSNmeGQK8AkdN2", "rw3oKLjG85BdKNXS2", "EsMhFZuycZorZNRF5", "PtoQdG7E8MxYJrigu", "n5ucT5ZbPdhfGNLtP", "jAToJHtg39AMTAuJo", "pLRogvJLPPg6Mrvg4", "XPErvb8m9FapXCjhA", "epZLSoNvjW53tqNj9", "KKLQp934n77cfZpPn", "sCs48JtMnQwQsZwyN", "K4aGvLnHvYgX9pZHS", "4ARaTpNX62uaL86j6", "PoDAyQMWEXBBBEJ5P", "pGvyqAQw6yqTjpKf4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-30T12:07:52.000Z", "modifiedAt": null, "url": null, "title": "Three Worlds Collide (0/8)", "slug": "three-worlds-collide-0-8", "viewCount": null, "lastCommentedAt": "2019-01-15T20:34:42.176Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HawFh7RvDM4RyoJ2d/three-worlds-collide-0-8", "pageUrlRelative": "/posts/HawFh7RvDM4RyoJ2d/three-worlds-collide-0-8", "linkUrl": "https://www.lesswrong.com/posts/HawFh7RvDM4RyoJ2d/three-worlds-collide-0-8", "postedAtFormatted": "Friday, January 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Three%20Worlds%20Collide%20(0%2F8)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThree%20Worlds%20Collide%20(0%2F8)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHawFh7RvDM4RyoJ2d%2Fthree-worlds-collide-0-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Three%20Worlds%20Collide%20(0%2F8)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHawFh7RvDM4RyoJ2d%2Fthree-worlds-collide-0-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHawFh7RvDM4RyoJ2d%2Fthree-worlds-collide-0-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<p style=\"margin-left: 40px;\">&quot;The kind of classic fifties-era first-contact story that Jonathan Swift\nmight have written, if Jonathan Swift had had a background in game\ntheory.&quot;<br />&#0160;&#0160;&#0160; &#0160;&#0160;&#0160; -- (Hugo nominee) Peter Watts, &quot;<a href=\"http://www.rifters.com/crawl/?p=266\">In Praise of Baby-Eating</a>&quot;</p><p><em>Three Worlds Collide</em> is a story I wrote to illustrate some points on naturalistic metaethics and diverse other issues of rational conduct.&#0160; It grew, as such things do, into a small novella.&#0160; On publication, it proved widely popular and widely criticized.&#0160; Be warned that the story, as it wrote itself, ended up containing some profanity and PG-13 content.</p><ol style=\"font-family: inherit;\">\n<li style=\"font-family: inherit;\"><a href=\"/lw/y5/the_babyeating_aliens_18/\">The Baby-Eating Aliens</a></li>\n<li style=\"font-family: inherit;\"><a href=\"/lw/y6/war_andor_peace_28/\">War and/or Peace</a></li>\n<li style=\"font-family: inherit;\"><a href=\"/lw/y7/the_super_happy_people_38/\">The Super Happy People</a></li>\n<li style=\"font-family: inherit;\"><a href=\"/lw/y8/interlude_with_the_confessor_48/\">Interlude with the Confessor</a></li>\n<li style=\"font-family: inherit;\"><a href=\"/lw/y9/three_worlds_decide_58/\">Three Worlds Decide</a></li>\n<li style=\"font-family: inherit;\"><a href=\"/lw/ya/normal_ending_last_tears_68/\">Normal Ending</a></li>\n<li style=\"font-family: inherit;\"><a href=\"/lw/yb/true_ending_sacrificial_fire_78/\">True Ending</a></li>\n<li style=\"font-family: inherit;\"><a href=\"/lw/yc/epilogue_atonement_88/\">Atonement</a></li>\n</ol>\n<p>\nPDF version <a href=\"http://robinhanson.typepad.com/files/three-worlds-collide.pdf\"><span class=\"at-xid-6a00d8341c6a2c53ef0105371351af970b\">here</span></a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 23, "Z8wZZLeLMJ3NSK7kR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HawFh7RvDM4RyoJ2d", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 64, "baseScore": 72, "extendedScore": null, "score": 0.000109, "legacy": true, "legacyId": "1228", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 72, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 95, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["n5TqCuizyJDfAPjkr", "RXQ5MkWkTCvLMGHrp", "qCsxiojX7BSLuuBgQ", "bojLBvsYck95gbKNM", "Z263n4TXJimKn6A8Z", "HWH46whexsoqR3yXk", "6Ls6f5PerERJmsTGB", "4pov2tL6SEC23wrkq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2009-01-30T12:07:52.000Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-30T12:07:57.000Z", "modifiedAt": null, "url": null, "title": "The Baby-Eating Aliens (1/8)", "slug": "the-baby-eating-aliens-1-8", "viewCount": null, "lastCommentedAt": "2022-05-12T17:47:20.907Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n5TqCuizyJDfAPjkr/the-baby-eating-aliens-1-8", "pageUrlRelative": "/posts/n5TqCuizyJDfAPjkr/the-baby-eating-aliens-1-8", "linkUrl": "https://www.lesswrong.com/posts/n5TqCuizyJDfAPjkr/the-baby-eating-aliens-1-8", "postedAtFormatted": "Friday, January 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Baby-Eating%20Aliens%20(1%2F8)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Baby-Eating%20Aliens%20(1%2F8)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn5TqCuizyJDfAPjkr%2Fthe-baby-eating-aliens-1-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Baby-Eating%20Aliens%20(1%2F8)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn5TqCuizyJDfAPjkr%2Fthe-baby-eating-aliens-1-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn5TqCuizyJDfAPjkr%2Fthe-baby-eating-aliens-1-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3392, "htmlBody": "<p style=\"text-align: right; font-size: 11px;\">(Part 1 of 8 in \"<a href=\"/lw/y4/three_worlds_collide_08/\">Three Worlds Collide</a>\")</p>\n<p><em>This is a story of an impossible outcome, where AI never worked, molecular nanotechnology never worked, biotechnology only sort-of worked; and yet somehow humanity not only survived, but discovered a way to travel Faster-Than-Light:&nbsp; The past's Future.</em></p>\n<p><em>Ships travel through the Alderson starlines, wormholes that appear near stars.&nbsp; The starline network is dense and unpredictable: more than a billion starlines lead away from Sol, but every world explored is so far away as to be outside the range of Earth's telescopes.&nbsp; Most colony worlds are located only a single jump away from Earth, which remains the center of the human universe.<br /></em></p>\n<p><em>From the colony system Huygens, the crew of the Giant Science Vessel </em>Impossible Possible World <em>have set out </em><em>to investigate a starline that flared up with an unprecedented flux of Alderson force before subsiding.&nbsp; Arriving, the </em>Impossible<em> discovers the sparkling debris of a recent nova - and -</em></p>\n<p>\"ALIENS!\"</p>\n<p>Every head swung toward the Sensory console.&nbsp; But after that one cryptic outburst, the Lady Sensory didn't even look up from her console: her fingers were frantically twitching commands.</p>\n<p>There was a strange moment of silence in the Command Conference while every listener thought the same two thoughts in rapid succession:</p>\n<p><em>Is she nuts?&nbsp; You can't just say \"Aliens!\", leave it at that, and expect everyone to believe you.&nbsp; Extraordinary claims require extraordinary evidence -</em></p>\n<p>And then,</p>\n<p><em>They came to look at the nova too!<br /></em></p>\n<p><a id=\"more\"></a></p>\n<p>In a situation like this, it befalls the Conference Chair to speak first.</p>\n<p>\"What?&nbsp; SHIT!\" shouted Akon, who didn't realize until later that his words would be inscribed for all time in the annals of history.&nbsp; Akon swung around and looked frantically at the main display of the Command Conference.&nbsp; \"Where are they?\"</p>\n<p>The Lady Sensory looked up from her console, fingers still twitching.&nbsp; \"I - I don't know, I just picked up an incoming high-frequency signal - they're sending us <em>enormous</em> amounts of data, petabytes, I had to clear long-term memory and set up an automatic pipe or risk losing the whole -\"</p>\n<p>\"<em>Found them!</em>\" shouted the Lord Programmer.&nbsp; \"I searched through our Greater Archive and turned up a program to look for anomalous energy sources near local starlines.&nbsp; It's from way back from the first days of exploration, but I managed to find an emulation program for -\"</p>\n<p>\"<em>Just show it!</em>\"&nbsp; Akon took a deep breath, trying to calm himself.</p>\n<p>The main display swiftly scanned across fiery space and settled on... a set of windows into fire, the fire of space shattered by the nova, but then shattered again into triangular shards.</p>\n<p>It took Akon a moment to realize that he was looking at an icosahedron of perfect mirrors.</p>\n<p><em>Huh,</em> thought Akon, <em>they're lower-tech than us.</em>&nbsp; Their own ship, the <em>Impossible,</em> was absorbing the vast quantities of local radiation and dumping it into their Alderson reactor; the mirror-shielding seemed a distinctly inferior solution.&nbsp; <em>Unless that's what they want us to think...</em></p>\n<p>\"<em>Deflectors!</em>\" shouted the Lord Pilot suddenly.&nbsp; \"Should I put up deflectors?\"</p>\n<p>\"Deflectors?\" said Akon, startled.</p>\n<p>The Pilot spoke very rapidly.&nbsp; \"Sir, we use a self-sustaining Alderson reaction to power our starline jumps and our absorbing shields.&nbsp; That same reaction could be used to emit a directed beam that would snuff a similar reaction - the aliens are putting out their own Alderson emissions, they could snuff our absorbers at any time, and the nova ashes would roast us instantly - unless I configure a deflector -\"</p>\n<p>The Ship's Confessor spoke, then.&nbsp; \"Have the aliens put up deflectors of their own?\"</p>\n<p>Akon's mind seemed to be moving very slowly, and yet the essential thoughts felt, somehow, obvious.&nbsp; \"Pilot, set up the deflector program but don't activate it until I give the word.&nbsp; Sensory, drop everything else and tell me whether the aliens have put up their own deflectors.\"</p>\n<p>Sensory looked up.&nbsp; Her fingers twitched only briefly through a few short commands.&nbsp; Then, \"No,\" she said.</p>\n<p>\"Then I think,\" Akon said, though his spine felt frozen solid, \"that we should not be the first to put this interaction on a... combative footing.&nbsp; The aliens have made a gesture of goodwill by leaving themselves vulnerable.&nbsp; We must reciprocate.\"&nbsp; Surely, no species would advance far enough to colonize space without understanding the logic of the Prisoner's Dilemma...</p>\n<p>\"You assume too much,\" said the Ship's Confessor.&nbsp; \"They are aliens.\"</p>\n<p>\"Not <em>much </em>goodwill,\" said the Pilot.&nbsp; His fingers were twitching, not commands, but almost-commands, subvocal thoughts.&nbsp; \"The aliens' Alderson reaction is weaker than ours by an order of magnitude.&nbsp; We could break any shield they could put up.&nbsp; <em>Unless</em> they struck first.&nbsp; If they leave their deflectors down, they lose nothing, but they invite us to leave our <em>own </em>down -\"</p>\n<p>\"If they were going to strike first,\" Akon said, \"they could have struck before we even knew they were here.&nbsp; But instead they spoke.\"&nbsp; <em>Surely, oh surely, they understand the Prisoner's Dilemma.</em></p>\n<p>\"Maybe they hope to gain information and <em>then</em> kill us,\" said the Pilot.&nbsp; \"We have technology they want.&nbsp; That enormous message - the only way we could send them an equivalent amount of data would be by dumping our entire Local Archive.&nbsp; They may be <em>hoping</em> that we feel the emotional need to, as you put it, <em>reciprocate -</em>\"</p>\n<p>\"Hold on,\" said the Lord Programmer suddenly.&nbsp; \"I may have managed to translate their language.\"</p>\n<p>You could have heard a pin dropping from ten lightyears away.</p>\n<p>The Lord Programmer smiled, ever so slightly.&nbsp; \"You see, that enormous dump of data they sent us - I think that was <em>their</em> Local Archive, or equivalent.&nbsp; A sizable part of their Net, anyway.&nbsp; Their text, image, and holo formats are utterly straightforward - either they don't bother compressing anything, or they decompressed it all for us before they sent it.&nbsp; And here's the thing: back in the Dawn era, when there were multiple human languages, there was this notion that people had of statistical language translation.&nbsp; Now, the classic method used a known corpus of human-translated text.&nbsp; But there were successor methods that tried to extend the translation further, by generating semantic skeletons and trying to map the skeletons themselves onto one another.&nbsp; And there are also ways of automatically looking for similarity between images or holos.&nbsp; Believe it or not, there was a program already in the Archive for trying to find points of linkage between an alien corpus and a human corpus, and then working out from there to map semantic skeletons... and it runs quickly, since it's designed to work on older computer systems.&nbsp; So I ran the program, it finished, and it's claiming that it can translate the alien language with 70% confidence.&nbsp; Could be a total bug, of course.&nbsp; But the aliens sent a second message that followed their main data dump - short, looks like text-only.&nbsp; Should I run the translator on that, and put the results on the main display?\"</p>\n<p>Akon stared at the Lord Programmer, absorbing this, and finally said, \"Yes.\"</p>\n<p>\"All right,\" said the Lord Programmer, \"here goes machine learning,\" and his fingers twitched once.</p>\n<p>Over the icosahedron of fractured fire, translucent letters appeared:</p>\n<p style=\"padding-left: 30px;\">THIS VESSEL IS THE OPTIMISM OF THE CENTER OF THE VESSEL PERSON</p>\n<p style=\"padding-left: 30px;\">YOU HAVE NOT KICKED US</p>\n<p style=\"padding-left: 30px;\">THEREFORE YOU EAT BABIES</p>\n<p style=\"padding-left: 30px;\">WHAT IS OURS IS YOURS, WHAT IS YOURS IS OURS</p>\n<p>\"Stop that laughing,\" Akon said absentmindedly, \"it's distracting.\"&nbsp; The Conference Chair pinched the bridge of his nose.&nbsp; \"All right.&nbsp; That doesn't seem completely random.&nbsp; The first line... is them identifying their ship, maybe.&nbsp; Then the second line says that we haven't opened fire on them, or that they won't open fire on us - something like that.&nbsp; The third line, I have absolutely no idea.&nbsp; The fourth... is offering some kind of reciprocal trade -\"&nbsp; Akon stopped then.&nbsp; So did the laughter.</p>\n<p>\"Would you like to send a return message?\" said the Lord Programmer.</p>\n<p>Everyone looked at him.&nbsp; Then everyone looked at Akon.</p>\n<p>Akon thought about that very carefully.&nbsp; Total silence for a lengthy period of time might not be construed as friendly by a race that had just talked at them for petabytes.</p>\n<p>\"All right,\" Akon said.&nbsp; He cleared his throat.&nbsp; \"We are still trying to understand your language.&nbsp; We do not understand well.&nbsp; We are trying to translate.&nbsp; We may not translate correctly.&nbsp; These words may not say what we want them to say.&nbsp; Please do not be offended.&nbsp; This is the research vessel named quote <em>Impossible Possible World</em> unquote.&nbsp; We are pleased to meet you.&nbsp; We will assemble data for transmission to you, but do not have it ready.\"&nbsp; Akon paused.&nbsp; \"Send them that.&nbsp; If you can make your program translate it three different plausible ways, do that too - it may make it clearer that we're working from an automatic program.\"</p>\n<p>The Lord Programmer twitched a few more times, then spoke to the Lady Sensory.&nbsp; \"Ready.\"</p>\n<p>\"Are you really sure this is a good idea?\" said Sensory doubtfully.</p>\n<p>Akon sighed.&nbsp; \"No.&nbsp; Send the message.\"</p>\n<p>For twenty seconds after, there was silence.&nbsp; Then new words appeared on the display:</p>\n<p style=\"padding-left: 30px;\">WE ARE GLAD TO SEE YOU CANNOT BE DONE</p>\n<p style=\"padding-left: 30px;\">YOU SPEAK LIKE BABY CRUNCH CRUNCH</p>\n<p style=\"padding-left: 30px;\">WITH BIG ANGELIC POWERS</p>\n<p style=\"padding-left: 30px;\">WE WISH TO SUBSCRIBE TO YOUR NEWSLETTER</p>\n<p>\"All right,\" Akon said, after a while.&nbsp; It seemed, on the whole, a positive response.&nbsp; \"I expect a lot of people are eager to look at the alien corpus.&nbsp; But I also need volunteers to hunt for texts and holo files in our own Archive.&nbsp; Which don't betray the engineering principles behind any technology we've had for less than, say,\" Akon thought about the mirror shielding and what it implied, \"a hundred years.&nbsp; Just showing that it <em>can </em>be done... we won't try to avoid that, but don't give away the science...\"<br /><br /><br />A day later, the atmosphere at the Command Conference was considerably more tense.</p>\n<p>Bewilderment.&nbsp; Horror.&nbsp; Fear.&nbsp; Numbness.&nbsp; Refusal.&nbsp; And in the distant background, slowly simmering, a dangerous edge of rising righteous fury.</p>\n<p>\"First of all,\" Akon said.&nbsp; \"First of all.&nbsp; Does anyone have any plausible hypothesis, any reasonable interpretation of what we know, under which the aliens do <em>not</em> eat their own children?\"</p>\n<p>\"There is always the possibility of misunderstanding,\" said the former Lady Psychologist, who was now, suddenly and abruptly, the lead Xenopsychologist of the ship, and therefore of humankind.&nbsp; \"But unless the <em>entire</em> corpus they sent us is a fiction... no.\"</p>\n<p>The alien holos showed tall crystalline insectile creatures, all flat planes and intersecting angles and prismatic refractions, propelling themselves over a field of sharp rocks: the aliens moved like hopping on pogo sticks, bouncing off the ground using projecting limbs that sank into their bodies and then rebounded.&nbsp; There was a cold beauty to the aliens' crystal bodies and their twisting rotating motions, like screensavers taking on sentient form.</p>\n<p>And the aliens bounded over the sharp rocks toward tiny fleeing figures like delicate spherical snowflakes, and grabbed them with pincers, and put them in their mouths.&nbsp; It was a central theme in holo after holo.</p>\n<p>The alien brain was much smaller and denser than a human's.&nbsp; The alien children, though their bodies were tiny, had full-sized brains.&nbsp; They could talk.&nbsp; They protested as they were eaten, in the flickering internal lights that the aliens used to communicate.&nbsp; They screamed as they vanished into the adult aliens' maws.</p>\n<p><em>Babies,</em> then, had been a mistranslation:&nbsp; <em>&nbsp;</em><em>Preteens</em> would have been more accurate.</p>\n<p>Still, everyone was calling the aliens Babyeaters.</p>\n<p>The children were sentient at the age they were consumed.&nbsp; The text portions of the corpus were very clear about that.&nbsp; It was part of the great, the noble, the most holy sacrifice.&nbsp; And the children were loved: this was part of the central truth of life, that parents could overcome their love and engage in the terrible winnowing.&nbsp; A parent might spawn a hundred children, and only one in a hundred could survive - for otherwise they would die later, of starvation...</p>\n<p>When the Babyeaters had come into their power as a technological species, they could have chosen to modify themselves - to prevent all births but one.</p>\n<p>But this they did not choose to do.</p>\n<p>For that terrible winnowing was the central truth of life, after all.</p>\n<p>The one now called Xenopsychologist had arrived to the Huygens system with the first colonization vessel.&nbsp; Since then she had spent over one hundred years practicing the profession of psychology, earning the rare title of Lady.&nbsp; (Most people got fed up and switched careers after no more than fifty, whatever their first intentions.)&nbsp; Now, after all that time, she was simply the Xenopsychologist, no longer a Lady of her profession.&nbsp; Being the first and only Xenopsychologist made no difference; the hundred-year rule for true expertise was not a rule that anyone could suspend.&nbsp; If she was the foremost Xenopsychologist of humankind, then also she was the least, the most foolish and the most ignorant.&nbsp; She was only an apprentice Xenopsychologist, no matter that there were no masters anywhere.&nbsp; In theory, her social status should have been too low to be seated at the Conference Table.&nbsp; In theory.</p>\n<p>The Xenopsychologist was two hundred and fifty years old.&nbsp; She looked much older, now, as as she spoke.&nbsp; \"In terms of evolutionary psychology... I think I understand what happened.&nbsp; The ancestors of the Babyeaters were a species that gave birth to hundreds of offspring in a spawning season, like Terrestrial fish; what we call <em>r</em>-strategy reproduction.&nbsp; But the ancestral Babyeaters discovered... crystal-tending, a kind of agriculture... long before humans did.&nbsp; They were around as smart as chimpanzees, when they started farming.&nbsp; The adults federated into tribes so they could guard territories and tend crystal.&nbsp; They adapted to pen up their offspring, to keep them around in herds so they could feed them.&nbsp; But they couldn't produce enough crystal for all the children.</p>\n<p>\"It's a truism in evolutionary biology that group selection can't work among non-relatives.&nbsp; The exception is if there are enforcement mechanisms, punishment for defectors - then there's no individual advantage to cheating, because you get slapped down.&nbsp; That's what happened with the Babyeaters.&nbsp; They didn't restrain their <em>individual</em> reproduction because the more children they put in the tribal pen, the more children of theirs were likely to survive.&nbsp; But the total production of offspring from the tribal pen was greater, if the children were winnowed down, and the survivors got more individual resources and attention afterward.&nbsp; That was how their species began to shift toward a <em>k</em>-strategy, an individual survival strategy.&nbsp; That was the beginning of their culture.</p>\n<p>\"And anyone who tried to cheat, to hide away a child, or even go easier on their own children during the winnowing - well, the Babyeaters treated the merciful parents the same way that human tribes treat their traitors.</p>\n<p>\"They developed psychological adaptations for enforcing that, their first great group norm.&nbsp; And those psychological adaptations, those emotions, were reused over the course of their evolution, as the Babyeaters began to adapt to their more complex societies.&nbsp; <em>Honor, friendship, the good of our tribe</em> - the Babyeaters acquired many of the same moral adaptations as humans, but their brains reused the emotional circuitry of infanticide to do it.</p>\n<p>\"The Babyeater word for <em>good</em> means, literally, to eat children.\"</p>\n<p>The Xenopsychologist paused there, taking a sip of water.&nbsp; Pale faces looked back at her from around the table.</p>\n<p>The Lady Sensory spoke up.&nbsp; \"I don't suppose... we could convince them they were wrong about that?\"</p>\n<p>The Ship's Confessor was robed and hooded in silver, indicating that he was there formally as a guardian of sanity.&nbsp; His voice was gentle, though, as he spoke:&nbsp; \"I don't believe that's how it works.\"</p>\n<p>\"Even if you <em>could</em> persuade them, it might not be a good idea,\" said the Xenopsychologist.&nbsp; \"If you convinced the Babyeaters to see it our way - that they had committed a wrong of that magnitude - there isn't anything in the universe that could stop them from hunting down and exterminating <em>themselves.</em>&nbsp; They don't have a concept of forgiveness; their only notion of why someone might go easy on a transgressor, is to spare an ally, or use them as a puppet, or being too lazy or cowardly to carry out the vengeance.&nbsp; The word for <em>wrong</em> is the same symbol as <em>mercy,</em> you see.\"&nbsp; The Xenopsychologist shook her head.&nbsp; \"Punishment of non-punishers is very much a way of life, with them.&nbsp; A Manichaean, dualistic view of reality.&nbsp; They may have literally believed that we ate babies, at first, just because we <em>didn't</em> open fire on them.\"</p>\n<p>Akon frowned.&nbsp; \"Do you really think so?&nbsp; Wouldn't that make them... well, a bit unimaginative?\"</p>\n<p>The Ship's Master of Fandom was there; he spoke up.&nbsp; \"I've been trying to read Babyeater literature,\" he said.&nbsp; \"It's not easy, what with all the translation difficulties,\" and he sent a frown at the Lord Programmer, who returned it.&nbsp; \"In one sense, we're lucky enough that the Babyeaters have a concept of fiction, let alone science fiction -\"</p>\n<p>\"Lucky?\" said the Lord Pilot.&nbsp; \"You've got to have an imagination to make it to the stars.&nbsp; The sort of species that wouldn't invent science fiction, probably wouldn't even invent the wheel -\"</p>\n<p>\"<em>But,</em>\" interrupted the Master, \"just as most of their science fiction deals with crystalline entities - the closest they come to postulating human anatomy, in any of the stories I've read, was a sort of giant sentient floppy sponge - so too, nearly all of the aliens their explorers meet, eat their own children.&nbsp; I doubt the authors spent much time questioning the assumption; they didn't want anything so alien that their readers couldn't empathize.&nbsp; The purpose of storytelling is to stimulate the moral instincts, which is why all stories are fundamentally about personal sacrifice and loss - that's their theory of literature.&nbsp; Though you can find stories where the wise, benevolent elder aliens explain how the need to control tribal population is the great selective transition, and how no species can possibly evolve sentience and cooperation without eating babies, and even if they did, they would war among themselves and destroy themselves.\"</p>\n<p>\"Hm,\" said the Xenopsychologist.&nbsp; \"The Babyeaters might not be too far wrong - stop staring at me like that, I don't mean it <em>that</em> way.&nbsp; I'm just saying, the Babyeater civilization <em>didn't</em> have all that many wars.&nbsp; In fact, they didn't have <em>any</em> wars at all after they finished adopting the scientific method.&nbsp; It was the great watershed moment in their history - the notion of a <em>reasonable mistake,</em> that you didn't have to kill all the adherents of a mistaken hypothesis.&nbsp; Not because you were forgiving them, but because they'd made the mistake by <em>reasoning on insufficient data</em>, rather than any <em>inherent</em> flaw.&nbsp; Up until then, all wars were wars of total extermination - but afterward, the theory was that if a large group of people could all do something wrong, it was probably a <em>reasonable mistake.</em>&nbsp; Their conceptualization of probability theory - of a formally correct way of manipulating uncertainty - was followed by the dawn of their world peace.\"</p>\n<p>\"But then -\" said the Lady Sensory.</p>\n<p>\"Of course,\" added the Xenopsychologist, \"anyone who departs from the group norm due to an <em>actual inherent flaw </em>still has to be destroyed.&nbsp; And not everyone agreed at first that the scientific method was moral - it does seem to have been highly counterintuitive to them - so their last war was the one where the science-users killed off all the nonscientists.&nbsp; After that, it was world peace.\"</p>\n<p>\"Oh,\" said the Lady Sensory softly.</p>\n<p>\"Yes,\" the Xenopsychologist said, \"after that, all the Babyeaters banded together as a single super-group that only needed to execute <em>individual</em> heretics.&nbsp; They now have a strong cultural taboo against wars <em>between tribes.</em>\"</p>\n<p>\"Unfortunately,\" said the Master of Fandom, \"that taboo doesn't let us off the hook.&nbsp; You can also find science fiction stories - though they're much rarer - where the Babyeaters and the aliens don't immediately join together into a greater society.&nbsp; Stories of horrible monsters who <em>don't</em> eat their children.&nbsp; Monsters who multiply like bacteria, war among themselves like rats, hate all art and beauty, and destroy everything in their pathway.&nbsp; Monsters who have to be exterminated down to the last strand of their DNA - er, last nucleating crystal.\"</p>\n<p>Akon spoke, then.&nbsp; \"I accept full responsibility,\" said the Conference Chair, \"for the decision to send the Babyeaters the texts and holos we did.&nbsp; But the fact remains that they have more than enough information about us to infer that we don't eat our children.&nbsp; They may be able to guess how we would see <em>them.</em>&nbsp; And they haven't sent anything to us, since we began transmitting to them.\"</p>\n<p>\"So the question then is - now what?\"</p>\n<p><a href=\"/lw/y6/war_andor_peace_28/\"><em>To be continued...</em></a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 6}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n5TqCuizyJDfAPjkr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 81, "baseScore": 88, "extendedScore": null, "score": 0.000132, "legacy": true, "legacyId": "1229", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 88, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HawFh7RvDM4RyoJ2d", "RXQ5MkWkTCvLMGHrp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-01-31T08:42:19.000Z", "modifiedAt": null, "url": null, "title": "War and/or Peace (2/8)", "slug": "war-and-or-peace-2-8", "viewCount": null, "lastCommentedAt": "2020-09-16T06:19:53.729Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RXQ5MkWkTCvLMGHrp/war-and-or-peace-2-8", "pageUrlRelative": "/posts/RXQ5MkWkTCvLMGHrp/war-and-or-peace-2-8", "linkUrl": "https://www.lesswrong.com/posts/RXQ5MkWkTCvLMGHrp/war-and-or-peace-2-8", "postedAtFormatted": "Saturday, January 31st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20War%20and%2For%20Peace%20(2%2F8)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWar%20and%2For%20Peace%20(2%2F8)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRXQ5MkWkTCvLMGHrp%2Fwar-and-or-peace-2-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=War%20and%2For%20Peace%20(2%2F8)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRXQ5MkWkTCvLMGHrp%2Fwar-and-or-peace-2-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRXQ5MkWkTCvLMGHrp%2Fwar-and-or-peace-2-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3353, "htmlBody": "<p style=\"text-align: right; font-size: 11px;\">(Part 2 of 8 in &quot;<a href=\"/lw/y4/three_worlds_collide_08/\">Three Worlds Collide</a>&quot;)</p><p><em>...&quot;So the question then is - now what?&quot;</em></p><p>The Lord Pilot jumped up, then, his face flushed.&#0160; &quot;Put up shields.&#0160;\nNow.&#0160; We don&#39;t gain anything by leaving them down.&#0160; This is madness!&quot;</p>\n\n<p>&quot;No,&quot; said the Ship&#39;s Confessor in professional tones, &quot;not madness.&quot;</p>\n\n<p>The Pilot slammed his fists on the table.&#0160; &quot;<em>We&#39;re all going to die!</em>&quot;</p>\n\n<p>&quot;They&#39;re not as technologically advanced as us,&quot; Akon said.&#0160;\n&quot;Suppose the Babyeaters do decide that we need to be exterminated.&#0160;\nSuppose they open fire.&#0160; Suppose they kill us.&#0160; Suppose they follow\nthe starline we opened and find the Huygens system.&#0160; Then what?&quot;</p>\n\n<p>The Master nodded.&#0160; &quot;Even with surprise on their side... no.&#0160; They\ncan&#39;t actually wipe out the human species.&#0160; Not unless they&#39;re a lot\nsmarter than they seem to be, and it looks to me like, on average,\nthey&#39;re actually a bit dumber than us.&quot;&#0160; The Master glanced at the\nXenopsychologist, who waved her hand in a maybe-gesture.</p>\n\n<p>&quot;But if we leave the ship&#39;s shields down,&quot; Akon said, &quot;we preserve whatever chance we have of a peaceful resolution to this.&quot;</p>\n\n<p>&quot;Peace,&quot; said the Lady Sensory, in a peculiar flat tone.\n</p><a id=\"more\"></a>\n<p>Akon looked at her.</p>\n\n<p>&quot;You want peace with the Babyeaters?&quot;</p>\n\n<p>&quot;Of course -&quot; said Akon, then stopped short.</p>\n\n<p>The Lady Sensory looked around the table.&#0160; &quot;And the Babyeater children?&#0160; What about them?&quot;</p>\n\n<p>The Master of Fandom spoke, his voice uncertain.&#0160; &quot;You can&#39;t impose human standards on -&quot;</p>\n\n<p>With a blur of motion and a sharp <em>crack,</em> the Lady Sensory slapped him.</p>\n\n<p>The Ship&#39;s Confessor grabbed her arm.&#0160; &quot;No.&quot;</p>\n\n<p>The Lady Sensory stared at the Ship&#39;s Confessor.</p>\n\n<p>&quot;No,&quot; the Confessor repeated.&#0160; &quot;No violence.&#0160; Only argument.&#0160; Violence doesn&#39;t distinguish truth from falsehood, my Lady.&quot;</p>\n\n<p>The Lady Sensory slowly lowered her hand, but not her eyes.</p>\n\n<p>&quot;But...&quot; said the Master.&#0160; &quot;But, my Lady, if they <em>want</em> to be eaten -&quot;</p>\n\n<p>&quot;They don&#39;t,&quot; said the Xenopsychologist.&#0160; &quot;Of course they don&#39;t.&#0160;\nThey run from their parents when the terrible winnowing comes.&#0160; The\nBabyeater children aren&#39;t <em>emotionally</em> mature - I mean they\ndon&#39;t have their adult emotional state yet.&#0160; Evolution would take care\nof anyone who wanted to get eaten.&#0160; And they&#39;re still learning, still\nmaking mistakes, so they don&#39;t yet have the instinct to exterminate\nviolators of the group code.&#0160; It&#39;s a simpler time for them.&#0160; They play,\nthey explore, they try out new ideas.&#0160; They&#39;re...&quot; and the\nXenopsychologist stopped.&#0160; &quot;Damn,&quot; she said, and turned her head away\nfrom the table, covering her face with her hands.&#0160; &quot;Excuse me.&quot;&#0160; Her\nvoice was unsteady.&#0160; &quot;They&#39;re a lot like human children, really.&quot;</p>\n\n<p>&quot;And if they <em>were</em> human children,&quot; said the Lady Sensory into\nthe silence, &quot;do you think that, just because the Babyeater species\nwanted to eat human children, that would make it right for them to do\nit?&quot;</p>\n\n<p>&quot;No,&quot; said the Lord Pilot.</p>\n\n<p>&quot;Then what difference does it make?&quot; said the Lady Sensory.</p>\n\n<p>&quot;No difference at all,&quot; said the Lord Pilot.</p>\n\n<p>Akon looked back and forth between the two of them, and saw what was coming, and somehow couldn&#39;t speak.</p>\n\n<p>&quot;We have to save them,&quot; said the Lady Sensory.&#0160; &quot;We have to stop this.&#0160; No matter what it takes.&#0160; We can&#39;t let this go on.&quot;</p>\n\n<p>Couldn&#39;t say that one word -</p>\n\n<p>The Lord Pilot nodded.&#0160; &quot;Destroy their ship.&#0160; Preserve <em>our</em>\nadvantage of surprise.&#0160; Go back, tell the world, create an overwhelming\nhuman army... and pour into the Babyeater starline network.&#0160; And rescue\nthe children.&quot;</p>\n\n\n\n\n<p>&quot;No,&quot; Akon said.</p>\n\n<p><em>No?</em></p>\n\n<p>&quot;I know,&quot; said the Lord Pilot.&#0160; &quot;A lot of Babyeaters will die at\nfirst, but they&#39;re killing ten times more children than their whole\nadult population, every year -&quot;</p>\n\n<p>&quot;And then what?&quot; said the Master of Fandom.&#0160; &quot;What happens when the children grow up?&quot;</p>\n\n<p>The Lord Pilot fell silent.</p>\n\n<p>The Master of Fandom completed the question.&#0160; &quot;Are you going to wipe\nout their whole race, because their existence is too horrible to be\nallowed to go on?&#0160; I read their stories, and I didn&#39;t understand them,\nbut -&quot;&#0160; The Master of Fandom swallowed.&#0160; &quot;They&#39;re not... <em>evil.</em>&#0160; Don&#39;t you understand?&#0160; They&#39;re <em>not.</em>&#0160; Are you going to punish me, because I don&#39;t want to punish them?&quot;</p>\n\n<p>&quot;We could...&quot; said the Lord Pilot.&#0160; &quot;Um.&#0160; We could modify their genes so that they only gave birth to a single child at a time.&quot;</p>\n\n<p>&quot;No,&quot; said the Xenopsychologist.&#0160; &quot;They would grow up loathing\nthemselves for being unable to eat babies.&#0160; Horrors in their own eyes.&#0160;\nIt would be kinder just to kill them.&quot;</p>\n\n<p>&quot;Stop,&quot; said Akon.&#0160; His voice wasn&#39;t strong, wasn&#39;t loud, but\neveryone in the room looked at him.&#0160; &quot;Stop.&#0160; We are not going to fire\non their ship.&quot;</p>\n\n<p>&quot;Why not?&quot; said the Lord Pilot.&#0160; &quot;They -&quot;</p>\n\n<p>&quot;They haven&#39;t raised shields,&quot; said Akon.</p>\n\n<p>&quot;Because they know it won&#39;t make a difference!&quot; shouted the Pilot.</p>\n\n<p>&quot;<em>They didn&#39;t fire on us!</em>&quot; shouted Akon.&#0160; Then he stopped,\nlowered his voice.&#0160; &quot;They didn&#39;t fire on us.&#0160; Even after they knew that\nwe didn&#39;t eat babies.&#0160; I am not going to fire on them.&#0160; I refuse to do\nit.&quot;</p>\n\n<p>&quot;You think they&#39;re <em>innocent?</em>&quot; demanded the Lady Sensory.&#0160; &quot;What if it was human children that were being eaten?&quot;</p>\n\n<p>Akon stared out a viewscreen, showing in subdued fires a\ncomputer-generated graphic of the nova debris.&#0160; He just felt exhausted,\nnow.&#0160; &quot;I never understood the Prisoner&#39;s Dilemma until this day.&#0160; Do\nyou cooperate when you <em>really do</em> want the highest payoff?&#0160; When it doesn&#39;t even seem <em>fair</em><em>&#0160;</em>for both of you to cooperate?&#0160; When it seems <em>right</em> to defect even if the other player doesn&#39;t?&#0160; That&#39;s the payoff matrix of the <em>true </em>Prisoner&#39;s\nDilemma.&#0160; But all the rest of the logic - everything about what happens\nif you both think that way, and both defect - is the same.&#0160; Do we want\nto live in a universe of cooperation or defection?&quot;</p>\n\n<p>&quot;But -&quot; said the Lord Pilot.</p>\n\n<p>&quot;They <em>know,</em>&quot; Akon said, &quot;that they can&#39;t wipe us out.&#0160; And they can guess what we could do to them.&#0160; Their choice <em>isn&#39;t</em>\nto fire on us and try to invade afterward!&#0160; Their choice is to fire on\nus and run from this star system, hoping that no other ships follow.&#0160;\nIt&#39;s their whole species at stake, against just this one ship.&#0160; And\nthey <em>still haven&#39;t fired</em>.&quot;</p>\n\n<p>&quot;They won&#39;t fire on us,&quot; said the Xenopsychologist, &quot;until they\ndecide that we&#39;ve defected from the norm.&#0160; It would go against their\nsense of... honor, I could call it, but it&#39;s much stronger than the\nhuman version -&quot;</p>\n\n<p>&quot;No,&quot; Akon said.&#0160; &quot;Not <em>that</em> much stronger.&quot;&#0160; He looked\naround, in the silence.&#0160; &quot;The Babyeater society has been at peace for\ncenturies.&#0160; So too with human society.&#0160; Do you want to fire the opening\nshot that brings war back into the universe?&#0160; Send us back to the\ndarkness-before-dawn that we only know from reading history books,\nbecause the holos are too horrible to watch?&#0160; Are you really going to\npress the button, knowing that?&quot;</p>\n\n<p>The Lord Pilot took a deep breath.&#0160; &quot;I will.&#0160; You will not remain commander of the <em>Impossible</em>, my lord, if the greater conference votes no confidence against you.&#0160; And they <em>will,</em> my lord, for the sake of the children.&quot;</p>\n\n<p>&quot;What,&quot; said the Master, &quot;are you going to <em>do</em> with the children?&quot;</p>\n\n<p>&quot;We, um, have to do something,&quot; said the Ship&#39;s Engineer, speaking\nup for the first time.&#0160; &quot;I&#39;ve been, um, looking into what Babyeater\nscience knows about their brain mechanisms.&#0160; It&#39;s really quite\nfascinating, they mix electrical and mechanical interactions, not the\nsame way our own brain pumps ions, but -&quot;</p>\n\n<p>&quot;Get to the point,&quot; said Akon.&#0160; &quot;<em>Immediately.</em>&quot;</p>\n\n<p>&quot;The children don&#39;t die right away,&quot; said the Engineer.&#0160; &quot;The brain\nis this nugget of hard crystal, that&#39;s really resistant to, um, the\ndigestive mechanisms, much more so than the rest of the body.&#0160; So the\nchild&#39;s brain is in, um, probably quite a lot of pain, since the whole\nbody has been amputated, and in a state of sensory deprivation, and\nthen the processing slowly gets degraded, and I think the whole process\ngets completed about a month after -&quot;</p>\n\n<p>The Lady Sensory threw up.&#0160; A few seconds later, so did the Xenopsychologist and the Master.</p>\n\n<p>&quot;If human society permits this to go on,&quot; said the Lord Pilot, his\nvoice very soft, &quot;I will resign from human society, and I will have\nfriends, and we will visit the Babyeater starline network with an\narmy.&#0160; You&#39;ll have to kill me to stop me.&quot;</p>\n\n<p>&quot;And me,&quot; said the Lady Sensory through tears.</p>\n\n<p>Akon rose from his chair, and leaned forward; a dominating move that\nhe had learned in classrooms, very long ago when he was first studying\nto be an Administrator.&#0160; But most in humanity&#39;s promotion-conscious\nsociety would not risk direct defiance of an Administrator.&#0160; In a\nhundred years he&#39;d never had his authority really tested, until now...&#0160;\n&quot;I will not permit you to fire on the alien ship.&#0160; Humanity will not be\nfirst to defect in the Prisoner&#39;s Dilemma.&quot;</p>\n\n<p>The Lord Pilot stood up, and Akon realized, with a sudden jolt, that\nthe Pilot was four inches taller; the thought had never occurred to him\nbefore.&#0160; The Pilot didn&#39;t lean forward, not knowing the trick, or not\ncaring.&#0160; The Pilot&#39;s eyes were narrow, surrounding facial muscles\ntensed and tight.</p>\n\n<p>&quot;<em>Get out of my way,</em>&quot; said the Lord Pilot.</p>\n\n<p>Akon opened his mouth, but no words came out.</p>\n\n<p>&quot;It is time,&quot; said the Lord Pilot, &quot;to see this calamity to its\nend.&quot;&#0160; Spoken in Archaic English: the words uttered by Thomas Clarkson\nin 1785, at the beginning of the end of slavery.&#0160; &quot;I have set my will\nagainst this disaster; I will break it, or it will break me.&quot;&#0160; Ira\nHoward in 2014.&#0160; &quot;I will not share my universe with this shadow,&quot; and\nthat was the Lord Pilot, in an anger hotter than the nova&#39;s ashes.&#0160;\n&quot;Help me if you will, or step aside if you lack decisiveness; but do\nnot make yourself my obstacle, or <em>I will burn you down, and any that stand with you -</em>&quot;</p>\n\n<p>&quot;<em>HOLD.</em>&quot;</p>\n\n<p>Every head in the room jerked toward the source of the voice.&#0160; Akon\nhad been an Administrator for a hundred years, and a Lord Administrator\nfor twenty.&#0160; He had studied all the classic texts, and watched holos of\nfamous crisis situations; nearly all the accumulated knowledge of the\nAdministrative Field was at his beck and call; and he&#39;d never dreamed\nthat a word could be spoken with such absolute force.</p>\n\n<p>The Ship&#39;s Confessor lowered his voice.&#0160; &quot;My Lord Pilot.&#0160; I will not\npermit you to declare your crusade, when you have not said what you are\ncrusading <em>for.</em>&#0160; It is not enough to say that you do not like\nthe way things are.&#0160; You must say how you will change them, and to\nwhat.&#0160; You must think all the way to your end.&#0160; Will you wipe out the\nBabyeater race entirely?&#0160; Keep their remnants under human rule forever,\nin despair under our law?&#0160; You have not even faced your hard choices,\nonly congratulated yourself on demanding that something be done.&#0160; I\njudge that a violation of sanity, my lord.&quot;</p>\n\n<p>The Lord Pilot stood rigid.&#0160; &quot;What -&quot; his voice broke.&#0160; &quot;What do <em>you</em> suggest we do?&quot;</p>\n\n<p>&quot;Sit down,&quot; said the Ship&#39;s Confessor, &quot;keep thinking.&#0160; My Lord\nPilot, my Lady Sensory, you are premature.&#0160; It is too early for\nhumanity to divide over this issue, <em>when we have known about it for less than twenty-four hours.</em>&#0160;\nSome rules do not change, whether it is money at stake, or the fate of\nan intelligent species.&#0160; We should only, at this stage, be discussing\nthe issue in all its aspects, as thoroughly as possible; we should not\neven be placing solutions on the table, as yet, to polarize us into\ncamps.&#0160; You <em>know</em> that, my lords, my ladies, and it does not change.&quot;</p>\n\n<p>&quot;And <em>after</em> that?&quot; said the Master of Fandom suddenly.&#0160; <em>&quot;Then</em> it&#39;s okay to split humanity?&#0160; You wouldn&#39;t object?&quot;</p>\n\n<p>The featureless blur concealed within the Confessor&#39;s Hood turned to\nface the Master, and spoke; and those present thought they heard a grim\nsmile, in that voice.&#0160; &quot;Oh,&quot; said the Confessor, &quot;<em>that</em> would be\ninterfering in politics.&#0160; I am charged with guarding sanity, not\nmorality.&#0160; If you want to stay together, do not split.&#0160; If you want\npeace, do not start wars.&#0160; If you want to avoid genocide, do not wipe\nout an alien species.&#0160; But if these are not your <em>highest</em> values, then you may well end up sacrificing them.&#0160; What you are willing to trade <em>off</em>, may end up traded <em>away</em> - <em>be you warned!</em>&#0160;\nBut if that is acceptable to you, then so be it.&#0160; The Order of Silent\nConfessors exists in the hope that, so long as humanity is sane, it\ncan make choices in accordance with its true desires.&#0160; Thus there is\nour Order dedicated <em>only</em> to that, and sworn not to interfere in\npolitics.&#0160; So you will spend more time discussing this scenario, my\nlords, my ladies, and only then generate solutions.&#0160; And then... you will\ndecide.&quot;</p>\n\n<p>&quot;Excuse me,&quot; said the Lady Sensory.&#0160; The Lord Pilot made to speak, and Sensory raised her voice.&#0160; &quot;<em>Excuse me,</em> my lords.&#0160; The alien ship has just sent us a new transmission.&#0160; Two megabytes of text.&quot;</p>\n\n<p>&quot;Translate and publish,&quot; ordered Akon.</p>\n\n<p>They all glanced down and aside, waiting for the file to come up.</p>\n\n<p>It began:</p><blockquote><p>THE UTTERMOST ABYSS OF JUSTIFICATION<br />A HYMN OF LOGIC<br />PURE LIKE STONES AND SACRIFICE<br />FOR STRUGGLES OF THE YOUNG SLIDING DOWN YOUR THROAT-</p></blockquote><p>Akon\nlooked away, wincing.&#0160; He hadn&#39;t tried to read much of the alien\ncorpus, and hadn&#39;t gotten the knack of reading the &quot;translations&quot; by\nthat damned program.</p>\n\n<p>&quot;Would someone,&quot; Akon said, &quot;please tell me - tell the conference - what this says?</p>\n\n<p>There was a long, stretched moment of silence.</p>\n\n<p>Then the Xenopsychologist made a muffled noise that could have been\na bark of incredulity, or just a sad laugh.&#0160; &quot;Stars beyond,&quot; said the\nXenopsychologist, &quot;they&#39;re trying to persuade us to eat our own\nchildren.&quot;</p>\n\n\n\n<p>&quot;Using,&quot; said the Lord Programmer, &quot;what they assert to be arguments\nfrom universal principles, rather than appeals to mere instincts that\nmight differ from star to star.&quot;</p>\n\n<p>&quot;Such as what, exactly?&quot; said the Ship&#39;s Confessor.</p>\n\n<p>Akon gave the Confessor an odd look, then quickly glanced away, lest\nthe Confessor catch him at it.&#0160; No, the Confessor couldn&#39;t be carefully\nmaintaining an open mind about <em>that.</em>&#0160; It was just curiosity over what particular failures of reasoning the aliens might exhibit.</p>\n\n<p>&quot;Let me search,&quot; said the Lord Programmer.&#0160; He was silent for a\ntime.&#0160; &quot;Ah, here&#39;s an example.&#0160; They point out that by producing many\noffspring, and winnowing among them, they apply greater selection\npressures to their children than we do.&#0160; So if we started producing\nhundreds of babies per couple and then eating almost all of them - I do\nemphasize that this is their suggestion, not mine - evolution would\nproceed faster for us, and we would survive longer in the universe.&#0160;\nEvolution and survival are universals, so the argument should convince\nanyone.&quot;&#0160; He gave a sad chuckle.&#0160; &quot;Anyone here feel convinced?&quot;</p>\n\n<p>&quot;Out of curiosity,&quot; said the Lord Pilot, &quot;have they ever tried to\nproduce even more babies - say, thousands instead of hundreds - so they\ncould speed up their evolution even more?&quot;</p>\n\n<p>&quot;It ought to be easily within their current capabilities of\nbioengineering,&quot; said the Xenopsychologist, &quot;and yet they haven&#39;t done\nit.&#0160; Still, I don&#39;t think we should make the suggestion.&quot;&quot;</p>\n<p>&quot;Agreed,&quot; said Akon.</p>\n\n<p>&quot;But humanity uses gamete selection,&quot; said the Lady Sensory.&#0160; &quot;We <em>aren&#39;t</em> evolving any slower.&#0160; If anything, choosing among millions of sperm and hundreds of eggs gives us <em>much</em> stronger selection pressures.&quot;</p>\n\n<p>The Xenopsychologist furrowed her brow.&#0160; &quot;I&#39;m not sure we sent them\nthat information in so many words... or they may have just not gotten\nthat far into what we sent them...&quot;</p>\n\n<p>&quot;Um, it wouldn&#39;t be trivial for them to understand,&quot; said the Ship&#39;s\nEngineer.&#0160; &quot;They don&#39;t have separate DNA and proteins, just crystal\npatterns tiling themselves.&#0160; The two parents intertwine and stay that\nway for, um, days, nucleating portions of supercooled liquid from their\nown bodies to construct the babies.&#0160; The whole, um, baby, is\nconstructed together by both parents.&#0160; They don&#39;t <em>have</em> separate gametes they could select on.&quot;</p>\n\n<p>&quot;But,&quot; said the Lady Sensory, &quot;couldn&#39;t we maybe convince them, to\nwork out some equivalent of gamete selection and try that instead -&quot;</p>\n\n<p>&quot;My lady,&quot; said the Xenopsychologist.&#0160; Her voice, now, was somewhat exasperated.&#0160; &quot;They aren&#39;t <em>really</em> doing this for the sake of evolution.&#0160; They were eating babies millions of years before they knew what evolution <em>was.</em>&quot;</p>\n\n<p>&quot;Huh, this is interesting,&quot; said the Lord Programmer.&#0160; &quot;There&#39;s\nanother section here where they construct their arguments using appeals\nto historical human authorities.&quot;</p>\n\n<p>Akon raised his eyebrows.&#0160; &quot;And who, exactly, do they quote in support?&quot;</p>\n\n<p>&quot;Hold on,&quot; said the Lord Programmer.&#0160; &quot;This has been run through the\ntranslator twice, English to Babyeater to English, so I need to write a\nprogram to retrieve the original text...&quot;&#0160; He was silent a few\nmoments.&#0160; &quot;I see.&#0160; The argument starts by pointing out how eating your\nchildren is proof of sacrifice and loyalty to the tribe, then they\nquote human authorities on the virtue of sacrifice and loyalty.&#0160; And\nancient environmentalist arguments about population control, plus...\noh, dear.&#0160; I don&#39;t think they&#39;ve realized that Adolf Hitler is a bad\nguy.&quot;</p><p>\n\n</p><p>&quot;They wouldn&#39;t,&quot; said the Xenopsychologist.&#0160; &quot;Humans put\nHitler in charge of a country, so we must have considered him a preeminent legalist of his age.&#0160; And it wouldn&#39;t occur to\nthe Babyeaters that Adolf Hitler might be regarded by humans as a bad guy <em>just</em> because he turned segments of his society into lampshades - they have a <em>custom</em> against that nowadays, but they don&#39;t really see it as <em>evil.</em>&#0160;\nIf\nHitler thought that gays had defected against the norm, and tried to\nexterminate them, that looks to a Babyeater like an honest mistake -&quot;&#0160;\nThe Xenopsychologist looked around the table.&#0160; &quot;All\nright, I&#39;ll stop there.&#0160; But the Babyeaters don&#39;t look back on their\nhistory and see obvious villains in positions of power - certainly not\nafter\nthe dawn of science.&#0160; Any politician who got to the point of being labeled &quot;bad&quot; would be killed and eaten.&#0160; The Babyeaters don&#39;t seem to have had\nhumanity&#39;s coordination problems.&#0160; Or they&#39;re just more rational\nvoters.&#0160; Take your pick.&quot;</p><p>Akon was resting his head in his hands.&#0160; &quot;You know,&quot; Akon said, &quot;I <em>thought</em>\nabout composing a message like this to the Babyeaters.&#0160; It was a stupid\nthought, but I kept turning it over in my mind.&#0160; Trying to think about\nhow I might persuade them that eating babies was... <em>not a good thing</em>.&quot;</p>\n\n<p>The Xenopsychologist grimaced.&#0160; &quot;The aliens seem to be even more\ngiven to rationalization than we are - which is maybe why their society\nisn&#39;t so rigid as to actually fall apart - but I don&#39;t think you could\ntwist them far enough around to believe that eating babies was not a\nbabyeating thing.&quot;</p>\n\n<p>&quot;And by the same token,&quot; Akon said, &quot;I don&#39;t think they&#39;re\nparticularly likely to persuade us that eating babies is good.&quot;&#0160; He\nsighed.&#0160; &quot;Should we just mark the message as spam?&quot;</p>\n\n<p>&quot;<em>One</em> of us should read it, at least,&quot; said the Ship&#39;s\nConfessor.&#0160; &quot;They composed their argument honestly and in all good\nwill.&#0160; Humanity also has epistemic standards of honor to uphold.&quot;</p>\n\n<p>&quot;Yes,&quot; said the Master.&#0160; &quot;I don&#39;t quite understand the Babyeater\nstandards of literature, my lord, but I can tell that this text\nconforms to their style of... not exactly poetry, but... they tried to\nmake it aesthetic as well as persuasive.&quot;&#0160; The Master&#39;s eyes flickered,\nback and forth.&#0160; &quot;I think they even made some parts constant in the\ntotal number of light pulses per argumentative unit, like human\nprosody, hoping that our translator would turn it into a human poem.&#0160;\nAnd... as near as I can judge such things, this took a <em>lot</em> of effort.&#0160; I wouldn&#39;t be surprised to find that everyone on that ship was staying up all night working on it.&quot;</p>\n\n<p>&quot;Babyeaters don&#39;t sleep,&quot; said the Engineer <em>sotto vocce.</em></p>\n\n<p>&quot;Anyway,&quot; said the Master.&#0160; &quot;If we don&#39;t fire on the alien ship - I\nmean, if this work is ever carried back to the Babyeater civilization -\nI suspect the aliens will consider this one of their great historical\nworks of literature, like <em>Hamlet</em> or <em>Fate/stay night</em> -&quot;</p>\n\n<p>The Lady Sensory cleared her throat.&#0160; She was pale, and trembling.</p><p>With a sudden black premonition of doom like a training session in Unrestrained Pessimism, Akon guessed what she would say.</p><p>The Lady Sensory said, in an unsteady voice, &quot;My lords, a third ship has jumped into this system.&#0160; Not Babyeater, not human.&quot;\n\n</p>\n\n<p><em><a href=\"/lw/y7/the_super_happy_people_38/\">To be continued...</a></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RXQ5MkWkTCvLMGHrp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 56, "baseScore": 66, "extendedScore": null, "score": 9.8e-05, "legacy": true, "legacyId": "1230", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 66, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HawFh7RvDM4RyoJ2d", "qCsxiojX7BSLuuBgQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-01T08:18:19.000Z", "modifiedAt": null, "url": null, "title": "The Super Happy People (3/8)", "slug": "the-super-happy-people-3-8", "viewCount": null, "lastCommentedAt": "2022-05-12T18:23:46.389Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qCsxiojX7BSLuuBgQ/the-super-happy-people-3-8", "pageUrlRelative": "/posts/qCsxiojX7BSLuuBgQ/the-super-happy-people-3-8", "linkUrl": "https://www.lesswrong.com/posts/qCsxiojX7BSLuuBgQ/the-super-happy-people-3-8", "postedAtFormatted": "Sunday, February 1st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Super%20Happy%20People%20(3%2F8)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Super%20Happy%20People%20(3%2F8)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqCsxiojX7BSLuuBgQ%2Fthe-super-happy-people-3-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Super%20Happy%20People%20(3%2F8)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqCsxiojX7BSLuuBgQ%2Fthe-super-happy-people-3-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqCsxiojX7BSLuuBgQ%2Fthe-super-happy-people-3-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4808, "htmlBody": "<p style=\"text-align: right; font-size: 11px;\">(Part 3 of 8 in \"<a href=\"/lw/y4/three_worlds_collide_08/\">Three Worlds Collide</a>\")</p>\n<p><em>...The Lady Sensory said, in an unsteady voice, \"My lords, a third ship has jumped into this system.&nbsp; Not Babyeater, not human.\"</em></p>\n<p>The holo showed a triangle marked with three glowing dots, the human ship and the Babyeater ship and the newcomers.&nbsp; Then the holo zoomed in, to show -</p>\n<p>- the most grotesque spaceship that Akon had ever seen, like a blob festooned with tentacles festooned with acne festooned with small hairs.&nbsp; Slowly, the tentacles of the ship waved, as if in a gentle breeze; and the acne on the tentacles pulsated, as if preparing to burst.&nbsp; It was a fractal of ugliness, disgusting at every level of self-similarity.</p>\n<p>\"Do the aliens have deflectors up?\" said Akon.</p>\n<p>\"My lord,\" said Lady Sensory, \"they don't have <em>any</em> shields raised.&nbsp; The nova ashes' radiation doesn't seem to bother them.&nbsp; Whatever material their ship is made from, it's just taking the beating.\"</p>\n<p>A silence fell around the table.</p>\n<p>\"All right,\" said the Lord Programmer, \"<em>that's</em> impressive.\"</p>\n<p>The Lady Sensory jerked, like someone had just slapped her.&nbsp; \"We - we just got a signal from them in human-standard format, content encoding marked as Modern English text, followed by a holo -\"</p>\n<p>\"<em>What?</em>\" said Akon.&nbsp; \"We haven't transmitted <em>anything</em> to them, how could they <em>possibly </em>-\"</p>\n<p>\"Um,\" said the Ship's Engineer.&nbsp; \"What if these aliens really <em>do</em> have, um, 'big angelic powers'?\"</p>\n<p>\"No,\" said the Ship's Confessor.&nbsp; His hood tilted slightly, as if in wry humor.&nbsp; \"It is only history repeating itself.\"</p>\n<p><a id=\"more\"></a></p>\n<p>\"History repeating itself?\" said the Master of Fandom.&nbsp; \"You mean that the ship is from an alternate Everett branch of Earth, or that they somehow <em>independently</em> developed ship-to-ship communication protocols <em>exactly</em> similar to our -\"</p>\n<p>\"No, you dolt,\" said the Lord Programmer, \"he means that the Babyeaters sent the new aliens a massive data dump, just like they sent <em>us</em>.&nbsp; Only this time, the Babyeater data dump included all the data that <em>we</em> sent the Babyeaters.&nbsp; Then the new aliens ran an automatic translation program, like the one <em>we</em> used.\"</p>\n<p>\"You gave it away,\" said the Confessor.&nbsp; There was a slight laugh in his voice.&nbsp; \"You should have let them figure it out on their own.&nbsp; One so rarely encounters the <em>apparently</em> supernatural, these days.\"</p>\n<p>Akon shook his head, \"Confessor, we don't have time for - never mind.&nbsp; Sensory, show the text message.\"</p>\n<p>The Lady Sensory twitched a finger and -</p>\n<p style=\"padding-left: 30px;\">HOORAY!</p>\n<p style=\"padding-left: 30px;\">WE ARE SO GLAD TO MEET YOU!</p>\n<p style=\"padding-left: 30px;\">THIS IS THE SHIP \"PLAY GAMES FOR LOTS OF FUN\"</p>\n<p style=\"padding-left: 30px;\">(OPERATED BY CHARGED PARTICLE FINANCIAL FIRMS)</p>\n<p style=\"padding-left: 30px;\">WE LOVE YOU AND WE WANT YOU TO BE SUPER HAPPY.</p>\n<p style=\"padding-left: 30px;\">WOULD YOU LIKE TO HAVE SEX?</p>\n<p>Slowly, elaborately, Akon's head dropped to the table with a dull thud.&nbsp; \"Why couldn't we have been alone in the universe?\"</p>\n<p>\"No, wait,\" said the Xenopsychologist, \"<em>this</em> makes sense.\"</p>\n<p>The Master of Fandom nodded.&nbsp; \"Seems quite straightforward.\"</p>\n<p>\"Do enlighten,\" came a muffled tone from where Akon's head rested on the table.</p>\n<p>The Xenopsychologist shrugged.&nbsp; \"Evolutionarily speaking, reproduction is probably the single best guess for an activity that an evolved intelligence would find pleasurable.&nbsp; When you look at it from that perspective, my lords, my lady, their message makes perfect sense - it's a universal friendly greeting, like the Pioneer engraving.\"</p>\n<p>Akon didn't raise his head.&nbsp; \"I wonder what <em>these</em> aliens do,\" he said through his shielding arms, \"molest kittens?\"</p>\n<p>\"My lord...\" said the Ship's Confessor.&nbsp; Gentle the tone, but the meaning was very clear.</p>\n<p>Akon sighed and straightened up.&nbsp; \"You said their message included a holo, right?&nbsp; Let's see it.\"</p>\n<p>The main screen turned on.</p>\n<p>There was a moment of silence, and then a strange liquid sound as, in unison, everyone around the table gasped in shock, even the Ship's Confessor.</p>\n<p>For a time after that, no one spoke.&nbsp; They were just... watching.</p>\n<p>\"Wow,\" said the Lady Sensory finally.&nbsp; \"That's actually... kind of... hot.\"</p>\n<p>Akon tore his eyes away from the writhing human female form, the writhing human male form, and the writhing alien tentacles.&nbsp; \"But...\" Akon said.&nbsp; \"But why is she pregnant?\"</p>\n<p>\"A better question,\" said the Lord Programmer, \"would be, why are the two of them reciting multiplication tables?\"&nbsp; He glanced around.&nbsp; \"What, none of you can read lips?\"</p>\n<p>\"Um...\" said the Xenopsychologist.&nbsp; \"Okay, I've got to admit, I can't even <em>begin</em> to imagine why -\"</p>\n<p>Then there was a uniform \"Ewww...\" from around the room.</p>\n<p>\"Oh, dear,\" said the Xenopsychologist.&nbsp; \"Oh, dear, I don't think they understood that part at <em>all</em>.\"</p>\n<p>Akon made a cutting gesture, and the holo switched off.</p>\n<p>\"Someone should view the rest of it,\" said the Ship's Confessor.&nbsp; \"It might contain important information.\"</p>\n<p>Akon flipped a hand.&nbsp; \"I don't think we'll run short of volunteers to watch disgusting alien pornography.&nbsp; Just post it to the ship's 4chan, and check after a few hours to see if anything was modded up to +5 Insightful.\"</p>\n<p>\"These aliens,\" said the Master of Fandom slowly, \"composed that pornography within... seconds, it must have been.&nbsp; <em>We</em> couldn't have done that automatically, could we?\"</p>\n<p>The Lord Programmer frowned.&nbsp; \"No.&nbsp; I don't, um, think so.&nbsp; From a corpus of alien pornography, <em>automatically </em>generate a holo they would find interesting?&nbsp; Um.&nbsp; It's not a problem that I think anyone's tried to solve yet, and <em>they </em>sure didn't get it perfect the first time, but... no.\"</p>\n<p>\"How large an angelic power does that imply?\"</p>\n<p>The Lord Programmer traded glances with the Master.&nbsp; \"Big,\" the Lord Programmer said finally.&nbsp; \"Maybe even epic.\"</p>\n<p>\"Or they think on a much faster timescale,\" said the Confessor softly.&nbsp; \"There is no law of the universe that their neurons must run at 100Hz.\"</p>\n<p>\"My lords,\" said the Lady Sensory, \"we're getting another message; holo with sound, this time.&nbsp; It's marked as a real-time communication, my lords.\"</p>\n<p>Akon swallowed, and his fingers automatically straightened the hood of his formal sweater.&nbsp; Would the aliens be able to tell if his clothes were sloppy?&nbsp; He was suddenly very aware that he hadn't checked his lipstick in three hours.&nbsp; But it wouldn't do to keep the visitors waiting...&nbsp; \"All right.&nbsp; Open a channel to them, transmitting only myself.\"</p>\n<p>The holo that appeared did nothing to assuage his insecurities.&nbsp; The man that appeared was perfectly dressed, utterly perfectly dressed, in business casual more intimidating than any formality: crushing superiority without the appearance of effort.&nbsp; The face was the same way, overwhelmingly handsome without the excuse of makeup; the fashionable slit vest exposed pectoral muscles that seemed optimally sculpted without the bulk that comes of exercise -</p>\n<p>\"<em>Superstimulus!</em>\" exclaimed the Ship's Confessor, a sharp warning.</p>\n<p>Akon blinked, shrugging off the fog.&nbsp; Of course the aliens couldn't possibly <em>really</em> look like that.&nbsp; A holo, only an overoptimized holo.&nbsp; That was a lesson everyone (every human?) learned before puberty, not to let reality seem diminished by fiction.&nbsp; As the proverb went, <em>It's bad enough comparing yourself to Isaac Newton without comparing yourself to Kimball Kinnison.</em></p>\n<p>\"Greetings in the name of humanity,\" said Akon.&nbsp; \"I am Lord Anamaferus Akon, Conference Chair of the Giant Science Vessel <em>Impossible Possible World.</em>&nbsp; We -\" <em>come in peace</em> didn't seem appropriate with a Babyeater war under discussion, and many other polite pleasantries, like <em>pleased to meet you,</em> suddenly seemed too much like promises and lies, \"- didn't quite understand your last message.\"</p>\n<p>\"Our apologies,\" said the perfect figure on screen.&nbsp; \"You may call me Big Fucking Edward; as for our species...\"&nbsp; The figure tilted a head in thought.&nbsp; \"This translation program is not fully stable; even if I said our proper species-name, who knows how it would come out.&nbsp; I would not wish my kind to forever bear an unaesthetic nickname on account of a translation error.\"</p>\n<p>Akon nodded.&nbsp; \"I understand, Big Fucking Edward.\"</p>\n<p>\"Your true language is a format inconceivable to us,\" said the perfect holo.&nbsp; \"But we do apologize for any <em>untranslatable 1</em> you may have experienced on account of our welcome transmission; it was automatically generated, before any of us had a chance to apprehend your sexuality.&nbsp; We do apologize, I say; but who would ever have thought that a species would evolve to find reproduction a <em>painful</em> experience?&nbsp; For us, childbirth is the greatest pleasure we know; to be prolonged, not hurried.\"</p>\n<p>\"Oh,\" said the Lady Sensory in a tone of sudden enlightenment, \"<em>that's</em> why the tentacles were pushing the baby back into -\"</p>\n<p>Out of sight of the visual frame, Akon gestured with his hand for Sensory to shut up.&nbsp; Akon leaned forward.&nbsp; \"The visual you're currently sending us is, of course, not real.&nbsp; What do you actually look like? - if the request does not offend.\"</p>\n<p>The perfect false man furrowed a brow, puzzled.&nbsp; \"I don't understand.&nbsp; You would not be able to apprehend any communicative cues.\"</p>\n<p>\"I would still like to see,\" Akon said.&nbsp; \"I am not sure how to explain it, except that - truth matters to us.\"</p>\n<p>The too-beautiful man vanished, and in his place -</p>\n<p>Mad brilliant colors, insane hues that for a moment defeated his vision.&nbsp; Then his mind saw shapes, but not meaning.&nbsp; In utter silence, huge blobs writhed around supporting bars.&nbsp; Extrusions protruded fluidly and interpenetrated -</p>\n<p>Writhing, twisting, shuddering, pulsating -</p>\n<p>And then the false man reappeared.</p>\n<p>Akon fought to keep his face from showing distress, but a prickling of sweat appeared on his forehead.&nbsp; There'd been something <em>jarring</em> about the blobs, even the stable background behind them.&nbsp; Like looking at an optical illusion designed by sadists.</p>\n<p>And - <em>those</em> were the aliens, or so they claimed -</p>\n<p>\"I have a question,\" said the false man.&nbsp; \"I apologize if it causes any distress, but I must know if what our scientists say is correct.&nbsp; Has your kind really evolved separate information-processing mechanisms for deoxyribose nucleic acid versus electrochemical transmission of synaptic spikes?\"</p>\n<p>Akon blinked.&nbsp; Out of the corner of his eye, he saw figures trading cautious glances around the table.&nbsp; Akon wasn't sure where this question was leading, but, given that the aliens had already understood enough to ask, it probably wasn't safe to lie...</p>\n<p>\"I don't really understand the question's purpose,\" Akon said.&nbsp; \"Our genes are made of deoxyribose nucleic acid.&nbsp; Our brains are made of neurons that transmit impulses through electrical and chemical -\"</p>\n<p>The fake man's head collapsed to his hands, and he began to bawl like a baby.</p>\n<p>Akon's hand signed <em>Help!</em> out of the frame.&nbsp; But the Xenopsychologist shrugged cluelessly.</p>\n<p>This was not going well.</p>\n<p>The fake man suddenly unfolded his head from his hands.&nbsp; His cheeks were depicted as streaked with tears, but the face itself had stopped crying.&nbsp; \"To wait so long,\" the voice said in a tone of absolute tragedy.&nbsp; \"To wait so long, and come so far, only to discover that nowhere among the stars is any trace of love.\"</p>\n<p>\"Love?\" Akon repeated.&nbsp; \"Caring for someone else?&nbsp; Wanting to protect them, to be with them?&nbsp; If that translated correctly, then 'love' is a very important thing to us.\"</p>\n<p>\"But!\" cried the figure in agony, at a volume that made Akon jump.&nbsp; \"But when you have sex, you do not <em>untranslatable 2!</em>&nbsp; A fake, a fake, these are only imitation words -\"</p>\n<p>\"What is 'untranslatable 2'?\" Akon said; and then, as the figure once again collapsed in inconsolable weeping, wished he hadn't.</p>\n<p>\"They asked if our neurons and DNA were separate,\" said the Ship's Engineer.&nbsp; \"So maybe they have only one system.&nbsp; Um... in retrospect, that actually seems like the obvious way for evolution to do it.&nbsp; If you're going to have one kind of information storage for genes, why have an entirely different system for brains?&nbsp; So -\"</p>\n<p>\"They share each other's thoughts when they have sex,\" the Master of Fandom completed.&nbsp; \"Now <em>there's</em> an old dream.&nbsp; And they would develop emotions around that, whole patterns of feeling we don't have ourselves...&nbsp; Huh.&nbsp; I guess we do lack their analogue of love.\"</p>\n<p>\"Probably,\" said the Xenopsychologist quietly, \"sex was their only way of speaking to each other from the beginning.&nbsp; From before the dawn of their intelligence.&nbsp; It really <em>does</em> make a lot of sense, evolutionarily.&nbsp; If you're injecting packets of information <em>anyway </em>-\"</p>\n<p>\"Wait a minute,\" said the Lady Sensory, \"then how are they talking to <em>us?</em>\"</p>\n<p>\"Of course,\" said the Lord Programmer in a tone of sudden enlightenment.&nbsp; \"Humanity has always used new communications technologies for pornography.&nbsp; 'The Internet is for porn' - but with <em>them,</em> it must have been the other way around.\"</p>\n<p>Akon blinked.&nbsp; His mind suddenly pictured the blobs, and the tentacles connecting them to each other -</p>\n<p><em>Somewhere on that ship is a blob making love to an avatar that's supposed to represent me.&nbsp; Maybe a whole Command Orgy.<br /></em></p>\n<p><em>I've just been cyber-raped.&nbsp; No, I'm being cyber-raped </em>right now.</p>\n<p>And the aliens had crossed who knew how much space, searching for who knew how long, yearning to speak / make love to other minds - only to find -</p>\n<p>The fake man suddenly jerked upright and screamed at a volume that whited-out the speakers in the Command Conference.&nbsp; Everyone jumped; the Master of Fandom let out a small shriek.</p>\n<p><em>What did I do what did I do what did I do -</em></p>\n<p>And then the holo vanished.</p>\n<p>Akon gasped for breath and slumped over in his chair.&nbsp; Adrenaline was still running riot through his system, but he felt utterly exhausted.&nbsp; He wanted to release his shape and melt into a puddle, a blob like the <em>wrong shapes</em> he'd seen on screen - no, <em>not</em> like that.</p>\n<p>\"My lord,\" the Ship's Confessor said softly.&nbsp; He was now standing alongside, a gentle hand on Akon's shoulder.&nbsp; \"My lord, are you all right?\"</p>\n<p>\"Not really,\" Akon said.&nbsp; His voice, he was proud to note, was only slighly wobbly.&nbsp; \"It's too hard, speaking to aliens.&nbsp; They don't think like you do, and you don't know what you're doing wrong.\"</p>\n<p>\"I wonder,\" the Master of Fandom said with artificial lightness, \"if they'll call it 'xenofatigue' and forbid anyone to talk to an alien for longer than five minutes.\"</p>\n<p>Akon just nodded.</p>\n<p>\"We're getting another signal,\" the Lady Sensory said hesitantly.&nbsp; \"Holo with sound, another real-time communication.\"</p>\n<p>\"Akon, you don't have to -\" said the Master of Fandom.</p>\n<p>Akon jerked himself upright, straightened his clothes.&nbsp; \"I <em>do </em>have to,\" he said.&nbsp; \"They're aliens, there's no knowing what a delay might...&nbsp; Just put it through.\"</p>\n<p>The first thing the holo showed, in elegant Modern English script, was the message:</p>\n<p style=\"margin-left: 40px;\"><em>The Lady 3rd Kiritsugu<br /></em>&nbsp;&nbsp;&nbsp; <span style=\"font-style: italic;\">temporary co-chair</span><em> of the </em>Gameplayer<em><br /></em>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; <span style=\"font-style: italic;\">Language </span><em>Translator version 3<br /></em>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; <em>Cultural Translator version 2</em></p>\n<p>The screen hovered just long enough to be read, then dissipated -</p>\n<p>Revealing a pale white lady.</p>\n<p>The translator's depiction of the Lady 3rd Kiritsugu was all white and black and grey; not the colorlessness of a greyscale image, but a colored image of a world with little color in it.&nbsp; Skin the color of the palest human skin that could still be called attractive; not snow white, but pale.&nbsp; White hair; blouse and bracelets and long dress all in coordinated shades of grey.&nbsp; That woman could have been called pretty, but there was none of the overstimulating beauty of the fake man who had been shown before.</p>\n<p>Her face was styled in the emotion that humans named \"serene\".</p>\n<p>\"I and my sisters have now taken command of this vessel,\" said the pale Lady.</p>\n<p>Akon blinked.&nbsp; <em>A mutiny aboard their ship?</em></p>\n<p>And it was back to the alien incomprehensibility, the knife-edged decisions and unpredictable reactions and the deadly fear of screwing up.</p>\n<p>\"I am sorry if my words offend,\" Akon said carefully, \"but there is something I wish to know.\"</p>\n<p>The Lady 3rd made a slicing gesture with one hand.&nbsp; \"You <em>cannot </em>offend me.\"&nbsp; Her face showed mild insult at the suggestion.</p>\n<p>\"What has happened aboard your ship, just now?\"</p>\n<p>The Lady 3rd replied, \"The crew are disabled by emotional distress.&nbsp; They have exceeded the bounds of their obligations, and are returning to the ship's Pleasuring Center for reward.&nbsp; In such a situation I and my two sisters, the <em>kiritsugu</em> of this vessel, assume command.\"</p>\n<p><em>Did I do that?&nbsp; </em>\"I did not intend for my words to cause you psychological harm.\"</p>\n<p>\"You are not responsible,\" the Lady 3rd said.&nbsp; \"It was the other ones.\"</p>\n<p>\"The Babyeaters?\" Akon said without thinking.</p>\n<p>\"Babyeaters,\" the Lady 3rd repeated.&nbsp; \"If that is the name you have given to the third alien species present at this star system, then yes.&nbsp; The crew, apprehending the nature of the Babyeaters' existence, was incapacitated by their share of the children's suffering.\"</p>\n<p>\"I see,\" Akon said.&nbsp; He felt an odd twitch of shame for humanity, that his own kind could learn of the Babyeaters, and continue functioning with only tears.</p>\n<p>The Lady 3rd's gaze grew sharp.&nbsp; \"What are your intentions regarding the Babyeaters?\"</p>\n<p>\"We haven't decided,\" Akon said.&nbsp; \"We were just discussing it when you arrived, actually.\"</p>\n<p>\"What is your current most preferred alternative?\" the Lady 3rd instantly fired back.</p>\n<p>Akon helplessly shrugged, palms out.&nbsp; \"We were just starting the discussion.&nbsp; All the alternatives suggested seemed unacceptable.\"</p>\n<p>\"Which seemed least unacceptable?&nbsp; What is your current best candidate?\"</p>\n<p>Akon shook his head.&nbsp; \"We haven't designated any.\"</p>\n<p>The Lady 3rd's face grew stern, with a hint of puzzlement.&nbsp; \"You are withholding the information.&nbsp; Why?&nbsp; Do you think it will cast you in an unfavorable light?&nbsp; Then I must take that expectation into account.&nbsp; Further, you must expect me to take that expectation into account, and so you imply that you expect me to underestimate its severity, even after taking this line of reasoning into account.\"</p>\n<p>\"Excuse me,\" the Ship's Confessor said.&nbsp; His tone was mild, but with a hint of urgency.&nbsp; \"I believe I should enter this conversation <em>right now.</em>\"</p>\n<p>Akon's hand signed agreement to the Lady Sensory.</p>\n<p>At once the Lady 3rd's eyes shifted to where the Confessor stood beside Akon.</p>\n<p>\"Human beings,\" said the Ship's Confessor, \"cannot designate a 'current best candidate' without psychological consequences.&nbsp; Human rationalists learn to discuss an issue as thoroughly as possible before suggesting <em>any</em> solutions.&nbsp; For humans, solutions are <em>sticky </em>in a way that would require detailed cognitive science to explain.&nbsp; We would not be able to search freely through the solution space, but would be helplessly attracted toward the 'current best' point, once we named it.&nbsp; Also, any endorsement whatever of a solution that has negative moral features, will cause a human to feel shame - and 'best candidate' would feel like an endorsement.&nbsp; To avoid feeling that shame, humans must avoid saying which of two bad alternatives is better than the other.\"</p>\n<p><em>Ouch,</em> thought Akon, <em>I never realized how embarrassing that sounds until I heard it explained to an alien.</em></p>\n<p>Apparently the alien was having similar thoughts.&nbsp; \"So you cannot even tell me which of several alternatives currently seems best, without your minds breaking down?&nbsp; That sounds quite implausible,\" the Lady 3rd said doubtfully, \"for a species capable of building a spaceship.\"</p>\n<p>There was a hint of laughter in the Confessor's voice.&nbsp; \"We try to overcome our biases.\"</p>\n<p>The Lady 3rd's gaze grew more intense.&nbsp; \"Are you the true decisionmaker of this vessel?\"</p>\n<p>\"I am not,\" the Confessor said flatly.&nbsp; \"I am a Confessor - a human master rationalist; we are sworn to refrain from leadership.\"</p>\n<p>\"This meeting will determine the future of all three species,\" said the Lady 3rd.&nbsp; \"If you have superior competence, you should assume control.\"</p>\n<p>Akon's brows furrowed slightly.&nbsp; Somehow he'd never thought about it in those terms.</p>\n<p>The Confessor shook his head.&nbsp; \"There are reasons beyond my profession why I must not lead.&nbsp; I am too old.\"</p>\n<p><em>Too old?</em></p>\n<p>Akon put the thought on hold, and looked back at the Lady 3rd.&nbsp; She had said that all the crew were incapacitated, except her and her two sisters who took charge.&nbsp; And she had asked the Confessor if he held true command.</p>\n<p>\"Are you,\" Akon asked, \"the equivalent of a Confessor for your own kind?\"</p>\n<p>\"Almost certainly not,\" replied the Lady 3rd, and -</p>\n<p>\"Almost certainly not,\" the Confessor said, almost in the same breath.</p>\n<p>There was an eerie kind of unison about it.</p>\n<p>\"I am <em>kiritsugu</em>,\" said the Lady 3rd.&nbsp; \"In the early days of my species there were those who refrained from happiness in order to achieve perfect skill in helping others, using <em>untranslatable 3</em> to suppress their emotions and acting only on their abstract knowledge of goals.&nbsp; These were forcibly returned to normality by massive <em>untranslatable 4</em>.&nbsp; But I descend from their thought-lineage and in emergency invoke the shadow of their <em>untranslatable 5</em>.\"</p>\n<p>\"I am a Confessor,\" said the Ship's Confessor, \"the descendant of those in humanity's past who most highly valued truth, who sought systematic methods for finding truth.&nbsp; But Bayes's Theorem will not be different from one place to another; the laws in their purely mathematical form will be the same, just as any sufficiently advanced species will discover the same periodic table of elements.\"</p>\n<p>\"And being universals,\" said the Lady 3rd, \"they bear no distinguishing evidence of their origin.&nbsp; So you should understand, Lord Akon, that a <em>kiritsugu's</em> purpose is not like that of a Confessor, even if we exploit the same laws.\"</p>\n<p>\"But we <em>are </em>similar enough to each other,\" the Confessor concluded, \"to see each other as <em>distorted</em> mirror images.&nbsp; Heretics, you might say.&nbsp; She is the ultimate sin forbidden to a Confessor - the exercise of command.\"</p>\n<p>\"As you are flawed on my own terms,\" the Lady 3rd concluded, \"one who refuses to help.\"</p>\n<p>Everyone else at the Conference table was staring at the alien holo, and at the Confessor, in something approaching outright horror.</p>\n<p>The Lady 3rd shifted her gaze back to Akon.&nbsp; Though it was only a movement of the eyes, there was something of a definite force about the motion, as if the translator was indicating that it stood for something much stronger.&nbsp; Her voice was given a demanding, compelling quality:&nbsp; \"What alternatives <em>did</em> your kind generate for dealing with the Babyeaters?&nbsp; Enumerate them to me.\"</p>\n<p><em>Wipe out their species, keep them in prison forever on suicide watch, ignore them and let the children suffer.</em></p>\n<p>Akon hesitated.&nbsp; An odd premonition of warning prickled at him.&nbsp; <em>Why does she need this information?</em></p>\n<p>\"If you do not give me the information,\" the Lady 3rd said, \"I will take into account the fact that you do not wish me to know it.\"</p>\n<p>The proverb went through his mind, <em>The most important part of any secret is the fact that the secret exists</em>.</p>\n<p>\"All right,\" Akon said.&nbsp; \"We found unacceptable the alternative of leaving the Babyeaters be.&nbsp; We found unacceptable the alternative of exterminating them.&nbsp; We wish to respect their choices and their nature as a species, but their children, who do not share that choice, are unwilling victims; this is unacceptable to us.&nbsp; We desire to keep the children alive but we do not know what to do with them once they become adult and start wanting to eat their own babies.&nbsp; Those were all the alternatives we had gotten as far as generating, at the very moment your ship arrived.\"</p>\n<p>\"That is all?\" demanded the Lady 3rd.&nbsp; \"That is the sum of all your thought?&nbsp; Is this one of the circumstances under which your species sends signals that differ against internal belief, such as 'joking' or 'politeness'?\"</p>\n<p>\"No,\" said Akon.&nbsp; \"I mean, yes.&nbsp; Yes, that's as far as we got.&nbsp; No, we're not joking.\"</p>\n<p>\"You should understand,\" the Confessor said, \"that this crew, also, experienced a certain distress, interfering with our normal function, on comprehending the Babyeaters.&nbsp; We are still experiencing it.\"</p>\n<p><em>And you acted to restore order,</em> thought Akon, <em>though not the same way as a </em>kiritsugu...</p>\n<p>\"I see,\" the Lady 3rd said.</p>\n<p>She fell silent.&nbsp; There were long seconds during which she sat motionless.</p>\n<p>Then, \"Why have you not yet disabled the Babyeater ship?&nbsp; Your craft possesses the capability of doing so, and you must realize that your purpose now opposes theirs.\"</p>\n<p>\"Because,\" Akon said, \"they did not disable our ship.\"</p>\n<p>The Lady 3rd nodded.&nbsp; \"You are symmetrists, then.\"</p>\n<p>Again the silence.</p>\n<p>Then the holo blurred, and in that blur appeared the words:</p>\n<p style=\"margin-left: 40px;\"><em>Cultural Translator version 3.</em></p>\n<p>The blur resolved itself back into that pale woman; almost the same as before, except that the serenity of her came through with more force.</p>\n<p>The Lady 3rd drew herself erect, and took on a look of ritual, as though she were about to recite a composed poem.</p>\n<p>\"I now speak,\" the Lady 3rd, \"on behalf of my species, to yours.\"</p>\n<p>A chill ran down Akon's spine.&nbsp; <em>This is too much, this is all too large for me -</em></p>\n<p>\"Humankind!\" the Lady 3rd said, as though addressing someone by name.&nbsp; \"Humankind, you prefer the absence of pain to its presence.&nbsp; When my own kind attained to technology, we eliminated the causes of suffering among ourselves.&nbsp; Bodily pain, embarrassment, and romantic conflicts are no longer permitted to exist.&nbsp; Humankind, you prefer the presence of pleasure to its absence.&nbsp; We have devoted ourselves to the intensity of pleasure, of sex and childbirth and <em>untranslatable 2.</em>&nbsp; Humankind, you prefer truth to lies.&nbsp; By our nature we do not communicate statements disbelieved, as you do with humor, modesty, and fiction; we have even learned to refrain from withholding information, though we possess that capability.&nbsp; Humankind, you prefer peace to violence.&nbsp; Our society is without crime and without war.&nbsp; Through symmetric sharing and <em>untranslatable 4,</em> we share our joys and are pleasured together.&nbsp; Our name for ourselves is not expressible in your language.&nbsp; But to you, humankind, we now name ourselves after the highest values we share: we are the Maximum Fun-Fun Ultra Super Happy People.\"</p>\n<p>There were muffled choking sounds from the human Conference table.</p>\n<p>\"Um,\" Akon said intelligently.&nbsp; \"Um... good for you?\"</p>\n<p>\"Humankind!&nbsp; Humankind, you did not likewise repair yourselves when you attained to technology.&nbsp; We are still unsure if it is somehow a <em>mistake,</em> if you <em>did not think it through</em>, or if your will is truly so different from ours.&nbsp; For whatever reason, you currently permit the existence of suffering which our species has eliminated.&nbsp; Bodily pain, embarrassment, and romantic troubles are still known among you.&nbsp; Your existence, therefore, is shared by us as pain.&nbsp; Will you, humankind, by your symmetry, remedy this?\"</p>\n<p>An electric current of shock and alarm ran through the Conference.&nbsp; The Lord Pilot glanced significantly at the Ship's Engineer, and the Engineer just as significantly shook his head.&nbsp; There was nothing they could do against the alien vessel; and their own shields would scarcely help, if they were attacked.</p>\n<p>Akon drew in a ragged breath.&nbsp; He was suddenly distracted, almost to the point of his brain melting, by a sense of <em>futures</em> twisting around these moments: the fate of star systems, the destiny of all humanity being warped and twisted and shaped.</p>\n<p><em>So to you, then, it is humanity that molests kittens.<br /></em></p>\n<p>He should have foreseen this possibility, after the experience of the Babyeaters.&nbsp; If the Babyeaters' existence was morally unacceptable to humanity, then the next alien species might be intolerable as well - or <em>they </em>might find humanity's existence a horror of unspeakable cruelty.&nbsp; That was the other side of the coin, even if a human might find it harder to think of it.</p>\n<p><em>Funny.&nbsp; It doesn't seem </em>that <em>bad from in here.</em>..</p>\n<p>\"But -\" Akon said, and only then became aware that he was speaking.</p>\n<p>\"'But'?\" said the Lady 3rd.&nbsp; \"Is that your whole reply, humankind?\"&nbsp; There was a look on her face of something like frustration, even sheer astonishment.</p>\n<p>He hadn't planned out this reply in any detail, but -</p>\n<p>\"You say that you feel our existence as pain,\" Akon said, \"sharing sympathy with our own suffering.&nbsp; So you, also, believe that under some circumstances pain is preferable to pleasure.&nbsp; If you did not hurt when others hurt - would you not feel that you were... less <em>the sort of person you wanted to be?</em>&nbsp; It is the same with us -\"</p>\n<p>But the Lady 3rd was shaking her head.&nbsp; \"You confuse a high conditional likelihood from your hypothesis to the evidence with a high posterior probability of the hypothesis given the evidence,\" she said, as if that were all one short phrase in her own language.&nbsp; \"Humankind, we possess a <em>generalized </em>faculty to feel what others feel.&nbsp; That is the simple, compact relation.&nbsp; We did not think to complicate that faculty to exclude pain.&nbsp; We did not then assign dense probability that other sentient species would traverse the stars, and be encountered by us, and yet fail to have repaired themselves.&nbsp; Should we encounter some future species in circumstances that do not permit its repair, we will modify our empathic faculty to exclude sympathy with pain, and substitute an urge to meliorate pain.\"</p>\n<p>\"But -\" Akon said.</p>\n<p><em>Dammit, I'm talking again.</em></p>\n<p>\"But we chose this; this is what we want.\"</p>\n<p>\"That matters less to our values than to yours,\" replied the Lady 3rd.&nbsp; \"But even you, humankind, should see that it is moot.&nbsp; We are still trying to untangle the twisting references of emotion by which humans might prefer pleasure to pain, and yet endorse complex theories that uphold pain over pleasure.&nbsp; But we have already determined that your children, humankind, do not share the grounding of these philosophies.&nbsp; When they incur pain they do not contemplate its meaning, they only call for it to stop.&nbsp; In their simplicity -\"</p>\n<p><em>They're a lot like our own children, really.</em></p>\n<p>\"- they somewhat resemble the earlier life stages of our own kind.\"</p>\n<p>There was a electric quality now about that pale woman, a terrible intensity.&nbsp; \"And you should understand, humankind, that when a child anywhere suffers pain and calls for it to stop, then we will answer that call if it requires sixty-five thousand five hundred and thirty-six ships.\"</p>\n<p>\"We believe, humankind, that you can understand our viewpoint.&nbsp; Have you options to offer us?\"</p>\n<p><a href=\"/lw/y8/interlude_with_the_confessor_48/\"><em>To be continued...</em></a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 3, "Jzm2mYuuDBCNWq8hi": 1, "sSNtcEQsqHgN8ZmRF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qCsxiojX7BSLuuBgQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 70, "baseScore": 78, "extendedScore": null, "score": 0.000117, "legacy": true, "legacyId": "1231", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 78, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HawFh7RvDM4RyoJ2d", "bojLBvsYck95gbKNM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-02T09:11:26.000Z", "modifiedAt": null, "url": null, "title": "Interlude with the Confessor (4/8)", "slug": "interlude-with-the-confessor-4-8", "viewCount": null, "lastCommentedAt": "2021-02-24T14:04:51.300Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bojLBvsYck95gbKNM/interlude-with-the-confessor-4-8", "pageUrlRelative": "/posts/bojLBvsYck95gbKNM/interlude-with-the-confessor-4-8", "linkUrl": "https://www.lesswrong.com/posts/bojLBvsYck95gbKNM/interlude-with-the-confessor-4-8", "postedAtFormatted": "Monday, February 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Interlude%20with%20the%20Confessor%20(4%2F8)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInterlude%20with%20the%20Confessor%20(4%2F8)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbojLBvsYck95gbKNM%2Finterlude-with-the-confessor-4-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Interlude%20with%20the%20Confessor%20(4%2F8)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbojLBvsYck95gbKNM%2Finterlude-with-the-confessor-4-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbojLBvsYck95gbKNM%2Finterlude-with-the-confessor-4-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3470, "htmlBody": "<p style=\"text-align: right; font-size: 11px;\">(Part 4 of 8 in \"<a href=\"/lw/y4/three_worlds_collide_08/\">Three Worlds Collide</a>\")</p>\n<p>The two of them were alone now, in the Conference Chair's Privilege, the huge private room of luxury more suited to a planet than to space.&nbsp; The Privilege was tiled wall-to-wall and floor-to-ceiling with a most excellent holo of the space surrounding them: the distant stars, the system's sun, the fleeing nova ashes, and the glowing ember of the dwarf star that had siphoned off hydrogen from the main sun until its surface had briefly ignited in a nova flash.&nbsp; It was like falling through the void.</p>\n<p>Akon sat on the edge of the four-poster bed in the center of the room, resting his head in his hands.&nbsp; Weariness dulled him at the moment when he most needed his wits; it was always like that in crisis, but this was unusually bad.&nbsp; Under the circumstances, he didn't dare snort a hit of caffeine - it might reorder his priorities.&nbsp; Humanity had yet to discover the drug that was <em>pure </em>energy, that would improve your thinking without the slightest touch on your emotions and values.</p>\n<p>\"I don't know what to think,\" Akon said.</p>\n<p>The Ship's Confessor was standing stately nearby, in full robes and hood of silver.&nbsp; From beneath the hood came the formal response:&nbsp; \"What seems to be confusing you, my friend?\"</p>\n<p>\"Did we go wrong?\" Akon said.&nbsp; No matter how hard he tried, he couldn't keep the despair out of his voice.&nbsp; \"Did humanity go down the wrong path?\"</p>\n<p><a id=\"more\"></a></p>\n<p>The Confessor was silent a long time.</p>\n<p>Akon waited.&nbsp; This was why he couldn't have talked about the question with anyone else.&nbsp; Only a Confessor would actually think before answering, if asked a question like that.</p>\n<p>\"I've often wondered that myself,\" the Confessor finally said, surprising Akon.&nbsp; \"There were so <em>many</em> choices, so many branchings in human history - what are the odds we got them <em>all </em>right?\"</p>\n<p>The hood turned away, angling in the direction of the Superhappy ship - though it was too far away to be visible, everyone on board the <em>Impossible Possible World </em>knew where it was.&nbsp; \"There are parts of your question I can't help you with, my lord.&nbsp; Of all people on this ship, I might be most poorly suited to answer...&nbsp; But you <em>do </em>understand, my lord, don't you, that neither the Babyeaters nor the Superhappies are <em>evidence</em> that we went wrong?&nbsp; If you weren't worried before, you shouldn't be any <em>more </em>worried now.&nbsp; The Babyeaters strive to do the baby-eating thing to do, the Superhappies output the Super Happy thing to do.&nbsp; None of that tells us anything about the <em>right </em>thing to do.&nbsp; They are not asking the same question we are - no matter <em>what </em>word of their language the translator links to our 'should'.&nbsp; If you're confused at all about <em>that,</em> my lord, I might be able to clear it up.\"</p>\n<p>\"I know the theory,\" Akon said.&nbsp; Exhaustion in his voice.&nbsp; \"They made me study metaethics when I was a little kid, sixteen years old and still in the children's world.&nbsp; Just so that I would never be tempted to think that God or ontologically basic moral facts or whatever had the right to override my own scruples.\"&nbsp; Akon slumped a little further.&nbsp; \"And somehow - none of that really makes a difference when you're looking at the Lady 3rd, and wondering why, when there's a ten-year-old with a broken finger in front of you, screaming and crying, we humans only <em>partially </em>numb the area.\"</p>\n<p>The Confessor's hood turned back to look at Akon.&nbsp; \"You do realize that your brain is literally <em>hardwired</em> to generate error signals when it sees other human-shaped objects stating a different opinion from yourself.&nbsp; You <em>do </em>realize that, my lord?\"</p>\n<p>\"I know,\" Akon said.&nbsp; \"That, too, we are taught.&nbsp; Unfortunately, I am also just now realizing that I've only been going along with society all my life, and that I never thought the matter through for myself, until now.\"</p>\n<p>A sigh came from that hood.&nbsp; \"Well... <em>would</em> you prefer a life entirely free of pain and sorrow, having sex all day long?\"</p>\n<p>\"Not... really,\" Akon said.</p>\n<p>The shoulders of the robe shrugged.&nbsp; \"You have judged.&nbsp; What else is there?\"</p>\n<p>Akon stared straight at that anonymizing robe, the hood containing a holo of dark mist, a shadow that always obscured the face inside.&nbsp; The voice was also anonymized - altered slightly, not in any obtrusive way, but you wouldn't know your own Confessor to hear him speak.&nbsp; Akon had no idea who the Confessor might be, outside that robe.&nbsp; There were rumors of Confessors who had somehow arranged to be seen <em>in the company</em> of their own secret identity...</p>\n<p>Akon drew a breath.&nbsp; \"You said that you, <em>of all people,</em> could not say whether humanity had gone down the wrong path.&nbsp; The simple fact of being a Confessor should have no bearing on that; rationalists are also human.&nbsp; And you told the Lady 3rd that you were <em>too old</em> to make decisions for your species.&nbsp; Just how old <em>are </em>you... honorable ancestor?\"</p>\n<p>There was a silence.</p>\n<p>It didn't last long.</p>\n<p>As though the decision had already been foreseen, premade and preplanned, the Confessor's hands moved easily upward and drew back the hood - revealing an <em>unblended</em> face, strangely colored skin and shockingly distinctive features.&nbsp; A face out of forgotten history, which could only have come from a time before the genetic mixing of the 21st century, untouched by DNA insertion or diaspora.</p>\n<p>Even though Akon had been half-expecting it, he still gasped out loud.&nbsp; Less than one in a million:&nbsp; That was the percentage of the current human population that had been born on Earth before the invention of antiagathics or star travel, five hundred years ago.</p>\n<p>\"Congratulations on your guess,\" the Confessor said.&nbsp; The unaltered voice was only slightly different; but it was stronger, more masculine.</p>\n<p>\"Then you <em>were </em>there,\" Akon said.&nbsp; He felt almost breathless, and tried not to show it.&nbsp; \"You were alive - all the way back in the days of the initial biotech revolution!&nbsp; That would have been when humanity first debated whether to go down the Super Happy path.\"</p>\n<p>The Confessor nodded.</p>\n<p>\"Which side did you argue?\"</p>\n<p>The Confessor's face froze for a moment, and then he emitted a brief chuckle, one short laugh.&nbsp; \"You have <em>entirely</em> the wrong idea about how things were done, back then.&nbsp; I suppose it's natural.\"</p>\n<p>\"I don't understand,\" Akon said.</p>\n<p>\"And there are <em>no</em> words that I can speak to make you understand.&nbsp; It is beyond your imagining.&nbsp; But you should not imagine that a violent thief whose closest approach to industry was selling uncertified hard drugs - you should not imagine, my lord, my honorable descendant, that I was ever asked to <em>take sides</em>.\"</p>\n<p>Akon's eyes slid away from the hot gaze of the unmixed man; there was something <em>wrong</em> about the thread of anger still there in the memory after five hundred years.</p>\n<p>\"But time passed,\" the Confessor said, \"time moved forward, and things changed.\"&nbsp; The eyes were no longer focused on Akon, looking now at something far away.&nbsp; \"There was an old saying, to the effect that while someone with a <em>single </em>bee sting will pay much for a remedy, to someone with <em>five </em>bee stings, removing just one sting seems less attractive.&nbsp; That was humanity in the ancient days.&nbsp; There was so much wrong with the world that the small resources of altruism were splintered among ten thousand urgent charities, and none of it ever seemed to go anywhere.&nbsp; And yet... and yet...\"</p>\n<p>\"There was a threshold crossed somewhere,\" said the Confessor, \"without a single apocalypse to mark it.&nbsp; Fewer wars.&nbsp; Less starvation.&nbsp; Better technology.&nbsp; The economy kept growing.&nbsp; People had more resource to spare for charity, and the altruists had fewer and fewer causes to choose from.&nbsp; They came even to me, in my time, and rescued me.&nbsp; Earth cleaned itself up, and whenever something threatened to go drastically wrong again, the whole attention of the planet turned in that direction and took care of it.&nbsp; Humanity finally got its act together.\"</p>\n<p>The Confessor worked his jaws as if there were something stuck in his throat.&nbsp; \"I doubt you can even imagine, my honorable descendant, just how much of an impossible dream that once was.&nbsp; But I will not call this path mistaken.\"</p>\n<p>\"No, I can't imagine,\" Akon said quietly.&nbsp; \"I once tried to read some of the pre-Dawn Net.&nbsp; I thought I wanted to know, I really did, but I - just couldn't handle it.&nbsp; I doubt anyone on this ship can handle it except you.&nbsp; Honorable ancestor, shouldn't we be asking you how to deal with the Babyeaters and the Superhappies?&nbsp; You are the only one here who's ever dealt with that level of emergency.\"</p>\n<p>\"<em>No,</em>\" said the Confessor, like an absolute order handed down from outside the universe.&nbsp; \"<em>You</em> are the world that we wanted to create.&nbsp; Though I can't say <em>we.</em>&nbsp; That is just a distortion of memory, a romantic gloss on history fading into mist.&nbsp; I wasn't one of the dreamers, back then.&nbsp; I was just wrapped up in my private blanket of hurt.&nbsp; But if my pain <em>meant </em>anything, Akon, it is as part of the long price of a better world than <em>that </em>one.&nbsp; If you look back at ancient Earth, and are horrified - then that means it was all <em>for </em>something, don't you see?&nbsp; You are the beautiful and shining children, and this is <em>your</em> world, and you are the ones who must decide what to do with it now.\"</p>\n<p>Akon started to speak, to demur -</p>\n<p>The Confessor held up a hand.&nbsp; \"I <em>mean </em>it, my lord Akon.&nbsp; It is not polite idealism.&nbsp; We ancients <em>can't</em> steer.&nbsp; We remember too much disaster.&nbsp; We're too <em>cautious</em> to dare the bold path forward.&nbsp; Do you know there was a time when nonconsensual sex was illegal?\"</p>\n<p>Akon wasn't sure whether to smile or grimace.&nbsp; \"The Prohibition, right?&nbsp; During the first century pre-Net?&nbsp; I expect everyone was glad to have <em>that</em> law taken off the books.&nbsp; I can't imagine how boring your sex lives must have been up until then - flirting with a woman, teasing her, leading her on, <em>knowing</em> the whole time that you were perfectly safe because she <em>couldn't</em> take matters into her own hands if you went a little too far -\"</p>\n<p>\"You need a history refresher, my Lord Administrator.&nbsp; At some suitably abstract level.&nbsp; What I'm trying to tell you - and this is <em>not</em> public knowledge - is that we nearly tried to overthrow your government.\"</p>\n<p><em>\"What?\"</em> said Akon.&nbsp; \"The <em>Confessors?</em>\"</p>\n<p>\"No, <em>us.</em>&nbsp; The ones who remembered the ancient world.&nbsp; Back then we still had our hands on a large share of the capital and tremendous influence in the grant committees.&nbsp; When our children legalized rape, we thought that the Future had gone wrong.\"</p>\n<p>Akon's mouth hung open.&nbsp; \"You were <em>that</em> prude?\"</p>\n<p>The Confessor shook his head.&nbsp; \"There aren't any words,\" the Confessor said, \"there aren't any words at all, by which I ever could explain to you.&nbsp; No, it wasn't prudery.&nbsp; It was a memory of disaster.\"</p>\n<p>\"Um,\" Akon said.&nbsp; He was trying not to smile.&nbsp; \"I'm trying to visualize what sort of disaster could have been caused by too much nonconsensual sex -\"</p>\n<p>\"Give it up, my lord,\" the Confessor said.&nbsp; He was finally laughing, but there was an undertone of pain to it.&nbsp; \"Without, shall we say, <em>personal experience,</em> you can't possibly imagine, and there's no point in trying.\"</p>\n<p>\"Well, out of curiosity - how much did you lose?\"</p>\n<p>The Confessor seemed to freeze, for a moment.&nbsp; \"What?\"</p>\n<p>\"How much did you lose in the legislative prediction markets, betting on whatever dreadful outcome you thought would happen?\"</p>\n<p>\"You really wouldn't ever understand,\" the Confessor said.&nbsp; His smile was entirely real, now.&nbsp; \"But now you know, don't you?&nbsp; You know, after speaking to me, that I can't ever be allowed to make decisions for humankind.\"</p>\n<p>Akon hesitated.&nbsp; It was odd... he did know, on some gut level.&nbsp; And he couldn't have explained on any verbal level why.&nbsp; Just - that hint of <em>wrongness</em>.</p>\n<p>\"So now you know,\" the Confessor repeated.&nbsp; \"And because we <em>do </em>remember so much disaster - and because it <em>is</em> a profession that benefits from being five hundred years old - many of us became Confessors.&nbsp; Being the voice of pessimism comes easily to us, and few indeed are those among the human kind who must rationally be nudged <em>upward</em>...&nbsp; We advise, but do not lead.&nbsp; Debate, but do not decide.&nbsp; We're going along for your ride, and trying not to be <em>too </em>shocked so that we can be almost as delighted as you.&nbsp; <em>You </em>might find yourself in a similar situation in five hundred years... if humanity survives this week.\"</p>\n<p>\"Ah, yes,\" Akon said dryly.&nbsp; \"The aliens.&nbsp; The current problem of discourse.\"</p>\n<p>\"Yes.&nbsp; Have you had any thoughts on the subject?\"</p>\n<p>\"Only that I really do wish that humanity had been alone in the universe.\"&nbsp; Akon's hand suddenly formed a fist and smashed hard against the bed.&nbsp; \"<em>Fuck</em> it!&nbsp; I know how the Superhappies felt when they discovered that we and the Babyeaters hadn't 'repaired ourselves'.&nbsp; You understand what this implies about what the rest of the universe looks like, statistically speaking?&nbsp; Even if it's just a sample of two?&nbsp; I'm sure that somewhere out there are likable neighbors.&nbsp; Just as somewhere out there, if we go far enough through the infinite universe, there's a person who's an exact duplicate of me down to the atomic level.&nbsp; But every other species we ever actually <em>meet</em> is probably going to be -\"&nbsp; Akon drew a breath.&nbsp; \"It wasn't supposed to be like this, damn it!&nbsp; All three of our species have empathy, we have sympathy, we have a sense of fairness - the Babyeaters even tell <em>stories</em> like we do, they have <em>art.</em>&nbsp; Shouldn't that be enough?&nbsp; Wasn't that supposed to be enough?&nbsp; But all it does is put us into enough of the same reference frame that we can be <em>horrible</em> by each others' standards.\"</p>\n<p>\"Don't take this the wrong way,\" the Confessor said, \"but I'm glad that we ran across the Babyeaters.\"</p>\n<p>Words stuck in Akon's throat.&nbsp; <em>\"What?\"</em></p>\n<p>A half-smile twisted up one corner of the Confessor's face.&nbsp; \"Because if we hadn't run across the Babyeaters, we couldn't possibly rescue the babies, now could we?&nbsp; Not <em>knowing </em>about their existence wouldn't mean they weren't there.&nbsp; The Babyeater children would still exist.&nbsp; They would still die in horrible agony.&nbsp; We just wouldn't be able to help them.&nbsp; If we didn't know it wouldn't be our <em>fault</em>, our <em>responsibility</em> - but <em>that's</em> not something you're supposed to optimize for.\"&nbsp; The Confessor paused.&nbsp; \"Of course I understand how you feel.&nbsp; But on this vessel I am humanity's token attempt at sanity, and it is my duty to think certain strange yet logical thoughts.\"</p>\n<p>\"And the Superhappies?\" Akon said.&nbsp; \"The race with superior technology that may decide to exterminate us, or keep us in prison, or take our children away?&nbsp; Is there any silver lining to <em>that?</em>\"</p>\n<p>\"The Superhappies aren't so far from us,\" the Confessor said.&nbsp; \"We <em>could </em>have gone down the Super Happy path.&nbsp; We nearly <em>did </em>- you might have trouble imagining just how <em>attractive </em>the absence of pain can sound, under certain circumstances.&nbsp; In a sense, you could say that I <em>tried </em>to go down that path - though I wasn't a very competent neuroengineer.&nbsp; If human nature had been only slightly different, we could easily have been within that attractor.&nbsp; And the Super Happy civilization is not hateful to <em>us</em>, whatever we are to them.&nbsp; That's<em> </em>good news<em> </em>at least, for how the rest of the universe might look.\"&nbsp; The Confessor paused.&nbsp; \"And...\"</p>\n<p>\"And?\"</p>\n<p>The Confessor's voice became harder.&nbsp; \"And the Superhappies will rescue the Babyeater children no matter what, I think, even if humanity should fail in the task.&nbsp; Considering how many Babyeater children are dying, and in what pain - that could outweigh even our own extermination.&nbsp; Shut up and multiply, as the saying goes.\"</p>\n<p>\"Oh, come <em>on!</em>\" Akon said, too surprised to be shocked.&nbsp; \"If the Superhappies hadn't shown up, we would have - well, we would have done <em>something</em> about the Babyeaters, once we decided what.&nbsp; We wouldn't have just let the, the -\"</p>\n<p>\"Holocaust,\" the Confessor offered.</p>\n<p>\"Good word for it.&nbsp; We wouldn't have just let the Holocaust go on.\"</p>\n<p>\"You would be <em>astounded,</em> my lord, at what human beings will <em>just let go on</em>.&nbsp; Do you realize the expenditure of capital, labor, maybe even <em>human lives</em> required to invade every part of the Babyeater civilization?&nbsp; To trace out every part of their starline network, push our technological advantage to its limit to build faster ships that can hunt down every Babyeater ship that tries to flee?&nbsp; Do you realize -\"</p>\n<p>\"I'm sorry.&nbsp; You are simply mistaken as a question of fact.\"&nbsp; <em>Boy,</em> thought Akon, <em>you don't often get to say </em>that <em>to a Confessor.</em>&nbsp; \"This is not your birth era, honorable ancestor.&nbsp; <em>We </em>are the humanity that <em>has its shit together.</em><em>&nbsp; </em>If the Superhappies had never come along, humanity would have done whatever it took to rescue the Babyeater children.&nbsp; You saw the Lord Pilot, the Lady Sensory; they were ready to secede from civilization if that's what it took to get the job done.&nbsp; And that, honorable ancestor, is how <em>most</em> people would react.\"</p>\n<p>\"For a moment,\" said the Confessor.&nbsp; \"In the moment of first hearing the news.&nbsp; When talk was cheap.&nbsp; When they hadn't yet visualized the costs.&nbsp; But once they <em>did</em>, there would be an uneasy pause, while everyone waited to see if someone <em>else</em> might act first.&nbsp; And faster than you imagine possible, people would adjust to that state of affairs.&nbsp; It would no longer sound quite so shocking as it did at first.&nbsp; Babyeater children are dying horrible, agonizing deaths in their parents' stomachs?&nbsp; Deplorable, of course, but things have always been that way.&nbsp; It would no longer be <em>news.</em>&nbsp; It would <em>all be part of the plan.</em>\"</p>\n<p>\"Are you high on something?\" Akon said.&nbsp; It wasn't the most polite way he could have phrased it, but he couldn't help himself.</p>\n<p>The Confessor's voice was as cold and hard as an iron sun, after the universe had burned down to embers.&nbsp; \"Innocent youth, when you have watched your older brother beaten almost to death before your eyes, and seen how little the police investigate - when you have watched all four of your grandparents wither away like rotten fruit and cease to exist, while you spoke not <em>one word</em> of protest because <em>you thought it was normal</em> - <em>then</em> you may speak to me of what human beings will tolerate.\"</p>\n<p>\"I don't believe <em>we </em>would do that,\" Akon said as mildly as possible.</p>\n<p>\"Then you fail as a rationalist,\" the Confessor said.&nbsp; His unhooded head turned toward the false walls, to look out at the accurately represented stars.&nbsp; \"But I - <em>I will not fail again.</em>\"</p>\n<p>\"Well, you're damn right about one thing,\" Akon said.&nbsp; He was too exhausted to be tactful.&nbsp; \"You can't ever be allowed to make decisions for the human species.\"</p>\n<p>\"I know.&nbsp; Believe me, I know.&nbsp; Only youth can Administrate.&nbsp; That is the pact of immortality.\"</p>\n<p>Akon stood up from the bed.&nbsp; \"Thank you, Confessor.&nbsp; You have helped me.\"</p>\n<p>With an easy, practiced motion, the Confessor slid the hood of his robe over his head, and the stark features vanished into shadow.&nbsp; \"I have?\" the Confessor said, and his recloaked voice sounded strangely mild, after that earlier masculine power.&nbsp; \"How?\"</p>\n<p>Akon shrugged.&nbsp; He didn't think he could put it into words.&nbsp; It had something to do with the terrible vast sweep of Time across the centuries, and so much true change that had already happened, deeper by far than anything he had witnessed in his own lifetime; the requirement of courage to face the future, and the sacrifices that had been made for it; and that not everyone had been saved, once upon a time.</p>\n<p>\"I guess you reminded me,\" Akon said, \"that you can't always get everything you want.\"</p>\n<p><a href=\"/lw/y9/three_worlds_decide_58/\"><em>To be continued...</em></a></p>\n<hr />\n<p><em>A reminder: &nbsp;This is a work of fiction. &nbsp;In real life, continuing to attempt to have sex with someone after they say 'no' and before they say 'yes', whether or not they offer forceful resistance and whether or not any visible injury occurs, is (in the USA) defined as rape and considered a federal felony. &nbsp;I agree with and support that this is the correct place for society to draw the line. &nbsp;Some people have worked out a safeword system in which they explicitly and verbally agree, with each other or on a signed form, that 'no' doesn't mean stop but e.g. 'red' or 'safeword' does mean stop. &nbsp;I agree with and support this as carving out a safe exception whose existence does not endanger innocent bystanders. &nbsp;If either of these statements come to you as a surprise then you should look stuff up. &nbsp;Thank you and remember, your safeword should be at least 10 characters and contain a mixture of letters and numbers. &nbsp;We now return you to your regularly scheduled reading. &nbsp;Yours, the author.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bojLBvsYck95gbKNM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 55, "extendedScore": null, "score": 8.3e-05, "legacy": true, "legacyId": "1232", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 96, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HawFh7RvDM4RyoJ2d", "Z263n4TXJimKn6A8Z"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-03T09:14:02.000Z", "modifiedAt": null, "url": null, "title": "Three Worlds Decide (5/8)", "slug": "three-worlds-decide-5-8", "viewCount": null, "lastCommentedAt": "2020-02-13T05:23:21.655Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Z263n4TXJimKn6A8Z/three-worlds-decide-5-8", "pageUrlRelative": "/posts/Z263n4TXJimKn6A8Z/three-worlds-decide-5-8", "linkUrl": "https://www.lesswrong.com/posts/Z263n4TXJimKn6A8Z/three-worlds-decide-5-8", "postedAtFormatted": "Tuesday, February 3rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Three%20Worlds%20Decide%20(5%2F8)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThree%20Worlds%20Decide%20(5%2F8)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ263n4TXJimKn6A8Z%2Fthree-worlds-decide-5-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Three%20Worlds%20Decide%20(5%2F8)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ263n4TXJimKn6A8Z%2Fthree-worlds-decide-5-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ263n4TXJimKn6A8Z%2Fthree-worlds-decide-5-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4516, "htmlBody": "<p style=\"text-align: right; font-size: 11px;\">(Part 5 of 8 in &quot;<a href=\"/lw/y4/three_worlds_collide_08/\">Three Worlds Collide</a>&quot;)</p><p>Akon strode into the main Conference Room; and though he walked like a physically exhausted man, at least his face was determined.&#0160; Behind\nhim, the shadowy Confessor followed.</p><p>The Command Conference looked up at him, and exchanged glances.</p><p>&quot;You look better,&quot; the Ship&#39;s Master of Fandom ventured.</p><p>Akon put a hand on the back of his seat, and paused.&#0160; Someone was absent.&#0160; &quot;The Ship&#39;s Engineer?&quot;</p><p>The\nLord Programmer frowned.&#0160; &quot;He said he had an experiment to run, my\nlord.&#0160; He refused to clarify further, but I suppose it must have\nsomething to do with the Babyeaters&#39; data -&quot;</p><p>&quot;You&#39;re joking,&quot; Akon said.&#0160; &quot;Our Ship&#39;s Engineer is off Nobel-hunting?&#0160; <em>Now?</em>&#0160; With the fate of the <em>human species</em> at stake?<em>&quot;</em></p><p>The Lord Programmer shrugged.&#0160; &quot;He seemed to think it was important, my lord.&quot;</p><p>Akon\nsighed.&#0160; He pulled his chair back and half-slid, half-fell into it.&#0160; &quot;I\ndon&#39;t suppose that the ship&#39;s markets have settled down?&quot;</p><p>The Lord Pilot grinned sardonically.&#0160; &quot;Read for yourself.&quot;\n</p><a id=\"more\"></a>\n<p>Akon\ntwitched, calling up a screen.&#0160; &quot;Ah, I see.&#0160; The ship&#39;s Interpreter of\nthe Market&#39;s Will reports, and I quote, &#39;Every single one of the\nunderlying assets in my market is going up and down like a fucking\nyo-yo while the ship&#39;s hedgers try to adjust to a Black Swan that&#39;s\ngoing to wipe out ninety-eight percent of their planetside risk\ncapital.&#0160; Even the spot prices on this ship are going crazy; either\nwe&#39;ve got bubble traders coming out of the woodwork, or someone\nseriously believes that sex is overvalued relative to orange juice.&#0160;\nOne derivatives trader says she&#39;s working on a contract that will have\na clearly defined value in the event that aliens wipe out the entire\nhuman species, but she says it&#39;s going to take a few hours and I say\nshe&#39;s on crack.&#0160; Indeed I believe an actual majority of the people\nstill trying to trade in this environment are higher than the\nheliopause.&#0160; Bid-ask spreads are so wide you could kick a fucking\nfootball <em>stadium</em> through them, nothing is clearing, and I have\nunisolated conditional dependencies coming out of my ass.&#0160; I have no\nfucking clue what the market believes.&#0160; Someone get me a drink.&#39;&#0160;\nUnquote.&quot;&#0160; Akon looked at the Master of Fandom.&#0160; &quot;Any suggestions get\nreddited up from the rest of the crew?&quot;</p><p>The Master cleared his throat.&#0160; &quot;My\nlord, we took the liberty of filtering out everything that was\nphysically impossible, based on pure wishful thinking, or displayed a\nclear misunderstanding of naturalistic metaethics.&#0160; I can show you the\nraw list, if you&#39;d like.&quot;</p><p>&quot;And what&#39;s left?&quot; Akon said.&#0160; &quot;Oh, never mind, I get it.&quot;</p><p>&quot;Well, not quite,&quot; said the Master.&#0160; &quot;To summarize the best ideas -&quot;&#0160; He gestured a small holo into existence.</p><p style=\"margin-left: 40px;\"><em>Ask\nthe Superhappies if their biotechnology is capable of in vivo cognitive\nalterations of Babyeater children to ensure that they don&#39;t grow up\nwanting to eat their own children.&#0160; Sterilize the current adults.&#0160; If\nBabyeater adults cannot be sterilized and will not surrender, imprison\nthem.&#0160; If that&#39;s too expensive, kill most of them, but leave enough in\nprison to preserve their culture for the children.&#0160; Offer the\nSuperhappies an alliance to invade the Babyeaters, in which we provide\nthe capital and labor and they provide the technology.</em></p><p><span style=\"font-style: italic;\"></span>&quot;Not\ntoo bad,&quot; Akon said.&#0160; His voice grew somewhat dry.&#0160; &quot;But it doesn&#39;t\nseem to address the question of what the Superhappies are supposed to\ndo with <em>us.</em>&#0160; The <em>analogous </em>treatment -&quot;</p><p>&quot;Yes, my\nlord,&quot; the Master said.&#0160; &quot;That was extensively pointed out in the\ncomments, my lord.&#0160; And the other problem is that the Superhappies\ndon&#39;t really <em>need </em>our labor <em>or</em> our capital.&quot;&#0160; The Master looked in the direction of the Lord Programmer, the Xenopsychologist, and the Lady Sensory.</p><p>The\nLord Programmer said, &quot;My lord, I believe the Superhappies think much\nfaster than we do.&#0160; If their cognitive systems are really based on\nsomething more like DNA than like neurons, that shouldn&#39;t be\nsurprising.&#0160; In fact, it&#39;s surprising that the speedup is as little as\n-&quot;&#0160; The Lord Programmer stopped, and swallowed.&#0160; &quot;My lord.&#0160; The\nSuperhappies responded to most of our transmissions extremely quickly.&#0160;\nThere was, however, a finite delay.&#0160; And that delay was roughly\nproportional to the length of the response, plus an additive constant.&#0160;\nGoing by the proportion, my lord, I believe they think between fifteen\nand thirty times as fast as we do, to the extent such a comparison can\nbe made.&#0160; If I try to use Moore&#39;s Law type reasoning on some of the\nobservable technological parameters in their ship - Alderson flux,\npower density, that sort of thing - then I get a reasonably convergent\nestimate that the aliens are two hundred years ahead of us <em>in human-equivalent subjective time.</em>&#0160;\nWhich means it would be twelve hundred equivalent years since their\nScientific Revolution.&quot;</p><p>&quot;If,&quot; the Xenopsychologist said, &quot;their history went as slowly as ours.&#0160; It probably didn&#39;t.&quot;&#0160; The Xenopsychologist took a breath.&#0160; &quot;My lord, my suspicion is that the\naliens are literally able to run their entire ship using only three\n<em>kiritsugu </em>as sole crew.&#0160; My lord, this may represent, not only the\nsuperior programming ability that translated their communications to\nus, but also the highly probable case that Superhappies can trade\nknowledge and skills among themselves by having sex.&#0160; Every individual\nof their species might contain the memory of their Einsteins and\nNewtons and a thousand other areas of expertise, no more conserved than\nDNA is conserved among humans.&#0160; My lord, I suspect their version of Galileo was something like thirty objective years ago, as the stars count time,\nand that they&#39;ve been in space for maybe twenty years.&quot;</p><p>The Lady Sensory said, &quot;Their ship has a plane of symmetry, and it&#39;s been getting wider on the axis through that plane, as it sucks up\nnova dust and energy.&#0160; It&#39;s growing on a smooth exponential at 2% per\nhour, which means it can split every thirty-five hours in this environment.&quot;</p><p>&quot;I\nhave no idea,&quot; the Xenopsychologist said, &quot;how fast the Superhappies\ncan reproduce themselves - how many children they have per generation,\nor how fast their children sexually mature.&#0160; But all things considered,\nI don&#39;t think we can count on their kids taking twenty years to get\nthrough high school.&quot;</p><p>There was silence.</p><p>When Akon could speak again, he said, &quot;Are you all quite finished?&quot;</p><p>&quot;If\nthey let us live,&quot; the Lord Programmer said, &quot;and if we can work out a\ntrade agreement with them under Ricardo&#39;s Law of Comparative Advantage, interest rates will -&quot;</p><p>&quot;Interest rates can fall into an open sewer and die.&#0160; Any further transmissions from the Superhappy ship?&quot;</p><p>The Lady Sensory shook her head.</p><p>&quot;All right,&quot; Akon said.&#0160; &quot;Open a transmission channel to them.&quot;</p><p>There was a stir around the table.&#0160; &quot;My lord -&quot; said the Master of Fandom.&#0160; &quot;My lord, what are you going to say?&quot;</p><p>Akon smiled wearily.&#0160; &quot;I&#39;m going to ask them if they have any options to offer us.&quot;</p><p>The Lady Sensory looked at the Ship&#39;s Confessor.&#0160; The hood silently nodded:&#0160; <em>He&#39;s still sane.</em></p><p>The Lady Sensory swallowed, and opened a channel.&#0160; On the holo there first appeared, as a screen:</p><p style=\"margin-left: 40px;\"><em>The Lady 3rd Kiritsugu<br /></em>&#0160;&#0160;&#0160; <span style=\"font-style: italic;\">temporary co-chair</span><em> of the </em>Gameplayer<em><br /></em>&#0160;&#0160;&#0160; &#0160;&#0160;&#0160; <span style=\"font-style: italic;\">Language </span><em>Translator version 9<br /></em>&#0160;&#0160;&#0160; &#0160;&#0160;&#0160; <em>Cultural Translator version 16</em></p><p>The\nLady 3rd in this translation was slightly less pale, and looked a bit\nmore concerned and sympathetic.&#0160; She took in Akon&#39;s appearance at a\nglance, and her eyes widened in alarm.&#0160; &quot;My lord, you&#39;re hurting!&quot;</p><p>&quot;Just\ntired, milady,&quot; Akon said.&#0160; He cleared his throat.&#0160; &quot;Our ship&#39;s\ndecision-making usually relies on markets and our markets are behaving\nerratically.&#0160; I&#39;m sorry to inflict that on you as shared pain, and I&#39;ll\ntry to get this over with quickly.&#0160; Anyway -&quot;</p><p>\n</p><p>Out of the corner of his eye, Akon saw the Ship&#39;s Engineer\nre-enter the room; the Engineer looked as if he had something to say,\nbut froze when he saw the holo.</p><p>There was no time for that now.</p><p>\n&quot;Anyway,&quot; Akon said, &quot;we&#39;ve worked out that the key decisions depend\nheavily on your level of technology.&#0160; What do you think you can\nactually <em>do</em> with us or the Babyeaters?&quot;</p><p>The Lady 3rd sighed.&#0160; &quot;I really should get your independent component before giving you ours - you should at least <em>think</em> of it first - but I suppose we&#39;re out of luck on that.&#0160; How about if I just tell you what we&#39;re currently planning?&quot;</p><p>Akon\nnodded.&#0160; &quot;That would be much appreciated, milady.&quot;&#0160; Some of his muscles\nthat had been tense, started to relax.&#0160; Cultural Translator version 16\nwas a lot easier on his brain.&#0160; Distantly, he wondered if some\ntransformed avatar of himself was making skillful love to the Lady 3rd -</p><p>&quot;All\nright,&quot; the Lady 3rd said.&#0160; &quot;We consider that the obvious starting\npoint upon which to build further negotiations, is to combine and\ncompromise the utility functions of the three species until we mutually\nsatisfice, providing compensation for all changes demanded.&#0160; The\nBabyeaters must compromise their values to eat their children at a\nstage where they are not sentient - we might accomplish this most\neffectively by changing the lifecycle of the children themselves.&#0160; We\ncan even give the unsentient children an instinct to flee and scream,\nand generate simple spoken objections, but prevent their brain from developing self-awareness until after the\nhunt.&quot;</p><p>Akon straightened.&#0160; That actually sounded - quite compassionate - sort of -</p><p>&quot;Our\nown two species,&quot; the Lady 3rd said, &quot;which desire this change of the\nBabyeaters, will compensate them by adopting Babyeater values, making\nour own civilization of greater utility in their sight: we will both\nchange to spawn additional infants, and eat most of them at almost the\nlast stage before they become sentient.&quot;</p><p>The Conference room was frozen.&#0160; No one moved.&#0160; Even their faces didn&#39;t change expression.</p><p>Akon&#39;s mind suddenly flashed back to those writhing, interpenetrating, visually <em>painful</em> blobs he had seen before.</p><p>A cultural translator could change the image, but not the reality.</p><p>&quot;It\nis nonetheless probable,&quot; continued the Lady 3rd, &quot;that the Babyeaters\nwill not accept this change as it stands; it will be necessary to\nimpose these changes by force.&#0160; As for you, humankind, we hope you will\nbe more reasonable.&#0160; But both your species, and the Babyeaters, must\nrelinquish bodily pain, embarrassment, and romantic troubles.&#0160; In\nexchange, we will change our own values in the direction of yours.&#0160; We\nare willing to change to desire pleasure obtained in more complex ways,\nso long as the total amount of our pleasure does not significantly\ndecrease.&#0160; We will learn to create art you find pleasing.&#0160; We will\nacquire a sense of humor, though we will not lie.&#0160; From the perspective\nof humankind and the Babyeaters, our civilization will obtain much\nutility in your sight, which it did not previously possess.&#0160; This is\nthe compensation we offer you.&#0160; We furthermore request that you accept\nfrom us the gift of <em>untranslatable 2</em>, which we believe will\nenhance, on its own terms, the value that you name &#39;love&#39;.&#0160; This will also enable our kinds to have sex using mechanical aids, which we greatly desire.&#0160; At the end\nof this procedure, all three species will satisfice each other&#39;s values\nand possess great common ground, upon which we may create a\ncivilization together.&quot;</p><p>Akon slowly nodded.&#0160; It was all quite\nunbelievably civilized.&#0160; It might even be the categorically best\ngeneral procedure when worlds collided.</p><p>The Lady 3rd brightened.&#0160; &quot;A nod - is that assent, humankind?&quot;</p><p>&quot;It&#39;s acknowledgment,&quot; Akon said.&#0160; &quot;We&#39;ll have to think about this.&quot;</p><p>&quot;I\nunderstand,&quot; the Lady 3rd said.&#0160; &quot;Please think as swiftly as you can.&#0160;\nBabyeater children are dying in horrible agony as you think.&quot;</p><p>&quot;I understand,&quot; Akon said in return, and gestured to cut the transmission.</p><p>The holo blinked out.</p><p>There was a long, terrible silence.</p><p>&quot;No.&quot;</p><p>The Lord Pilot said it.&#0160; Cold, flat, absolute.</p><p>There was another silence.</p><p>&quot;My\nlord,&quot; the Xenopsychologist said, very softly, as though afraid the\nmessenger would be torn apart and dismembered, &quot;I do not think they\nwere offering us that option.&quot;</p><p>&quot;Actually,&quot; Akon said, &quot;The Superhappies offered us more than we were going to offer the Babyeaters<span style=\"font-style: italic;\">.&#0160; </span><em>We </em>weren&#39;t\nexactly thinking about how to compensate them.&quot;&#0160; It was strange, Akon\nnoticed, his voice was very calm, maybe even deadly calm.&#0160; &quot;The\nSuperhappies really are a very fair-minded people.&#0160; You get the\nimpression they would have proposed exactly the same solution whether\nor not they happened to hold the upper hand.<span style=\"font-style: italic;\">&#0160; </span><em>We </em>might have just enforced our own will on the Babyeaters and told the Superhappies to take a hike.&#0160; If <em>we&#39;d </em>held the upper hand.&#0160; But we don&#39;t.&#0160; And that&#39;s that, I guess.&quot;</p><p>&quot;<em>No!&quot;</em> shouted the Lord Pilot.&#0160; &quot;That&#39;s not -&quot;</p><p>Akon looked at him, still with that deadly calm.</p>\n<p>The\nLord Pilot was breathing deeply, not as if quieting himself, but as if\npreparing for battle on some ancient savanna plain that no longer\nexisted.&#0160; &quot;They want to turn us into something inhuman.&#0160; It - it <em>cannot</em> - we <em>cannot</em> - we <em>must not allow </em>-&quot;</p><p>&quot;Either\ngive us a better option or shut up,&quot; the Lord Programmer said flatly.&#0160;\n&quot;The Superhappies are smarter than us, have a technological advantage,\nthink faster, and probably reproduce faster.&#0160; We have no hope of\nholding them off militarily.&#0160; If our ships flee, the Superhappies will\nsimply follow in faster ships.&#0160; There&#39;s no way to shut a starline once\nopened, and no way to conceal the fact that it is open -&quot;</p><p>&quot;Um,&quot; the Ship&#39;s Engineer said.</p><p>Every eye turned to him.</p><p>&quot;Um,&quot; the Ship&#39;s Engineer said.&#0160; &quot;My Lord Administrator, I must report to you in private.&quot;</p><p>The Ship&#39;s Confessor shook his head.&#0160; &quot;You could have handled that better, Engineer.&quot;</p><p>Akon\nnodded to himself.&#0160; It was true.&#0160; The Ship&#39;s Engineer had already\nbetrayed the fact that a secret existed.&#0160; Under the circumstances, easy\nto deduce that it had come from the Babyeater data.&#0160; That was eighty\npercent of the secret right there.&#0160; And if it was relevant to starline\nphysics, that was half of the remainder.</p><p>&quot;Engineer,&quot; Akon said,\n&quot;since you have already revealed that a secret exists, I suggest you\ntell the full Command Conference.&#0160; We need to stay in sync with each\nother.&#0160; Two minds are not a committee.&#0160; We&#39;ll worry later about keeping\nthe secret classified.&quot;</p><p>The Ship&#39;s Engineer hesitated.&#0160; &quot;Um, my lord, I suggest that I report to you first, before you decide -&quot;</p><p>&quot;There&#39;s no time,&quot; Akon said.&#0160; He pointed to where the holo had been.</p><p>&quot;Yes,&quot; the Master of Fandom said, &quot;we can always slit our own throats afterward, if the secret is <em>that</em> awful.&quot;&#0160; The Master of Fandom gave a small laugh -</p><p>- then stopped, at the look on the Engineer&#39;s face.</p><p>&quot;At your will, my lord,&quot; the Engineer said.</p><p>He\ndrew a deep breath.&#0160; &quot;I asked the Lord Programmer to compare any\nidentifiable equations and constants in the Babyeater&#39;s scientific\narchive, to the analogous scientific data of humanity.&#0160; Most of the\nidentified analogues were equal, of course.&#0160; In some places we have\nmore precise values, as befits our, um, superior technological level.&#0160;\nBut one anomaly did turn up: the Babyeater figure for Alderson&#39;s\nCoupling Constant was <em>ten orders of magnitude</em> larger than our own.&quot;</p><p>The Lord Pilot whistled.&#0160; &quot;Stars above, how did they manage to make <em>that</em> mistake -&quot;</p><p>Then the Lord Pilot stopped abruptly.</p><p>&quot;Alderson&#39;s\nCoupling Constant,&quot; Akon echoed.&#0160; &quot;That&#39;s the... coupling between Alderson interactions and the...&quot;</p><p>&quot;Between Alderson interactions and the nuclear strong force,&quot;\nthe Lord Pilot said.&#0160; He was beginning to smile, rather grimly.&#0160; &quot;It\nwas a free parameter in the standard model, and so had to be\nestablished experimentally.&#0160; But because the interaction is so\nincredibly... weak... they had to build an <em>enormous</em> Alderson generator to find the value.&#0160; The size of a very small moon, just to give us that one number.&#0160; Definitely <em>not </em>something you could check at home.&#0160; That&#39;s the story in the physics textbooks, my lords, my lady.&quot;</p><p>The\nMaster of Fandom frowned.&#0160; &quot;You&#39;re saying... the physicists faked the\nresult in order to... fund a huge project...?&quot;&#0160; He looked puzzled.</p><p>&quot;No,&quot;\nthe Lord Pilot said.&#0160; &quot;Not for the love of power.&#0160; Engineer, the\nBabyeater value should be testable using our own ship&#39;s Alderson drive,\nif the coupling constant is that strong.&#0160; This you have done?&quot;</p><p>The Ship&#39;s Engineer nodded.&#0160; &quot;The Babyeater value is correct, my lord.&quot;</p><p>The Ship&#39;s Engineer was pale.&#0160; The Lord Pilot was clenching his jaw into a sardonic grin.</p><p>&quot;Please\nexplain,&quot; Akon said.&#0160; &quot;Is the universe going to end in another billion\nyears, or something?&#0160; Because if so, the issue can wait -&quot;</p><p>&quot;My\nlord,&quot; the Ship&#39;s Confessor said, &quot;suppose the laws of physics in our\nuniverse had been such that the ancient Greeks could invent the\nequivalent of nuclear weapons from materials just lying around.&#0160;\nImagine the laws of physics had permitted a way to destroy whole\ncountries with no more difficulty than mixing gunpowder.&#0160; History would\nhave looked quite different, would it not?&quot;</p><p>Akon nodded, puzzled.&#0160; &quot;Well, yes,&quot; Akon said.&#0160; &quot;It would have been shorter.&quot;</p><p>&quot;Aren&#39;t we lucky that physics <em>didn&#39;t</em> happen to turn out that way, my lord?&#0160; That in our own time, the laws of physics <em>don&#39;t</em> permit cheap, irresistable superweapons?&quot;</p><p>Akon furrowed his brow -</p><p>&quot;But my lord,&quot; said the Ship&#39;s Confessor, &quot;do we really know what we <em>think </em>we know?&#0160; What <em>different</em> evidence would we see, if things were otherwise?&#0160; After all - if <em>you </em>happened to be a physicist, and <em>you </em>happened to notice an easy way to wreak enormous destruction using off-the-shelf hardware - would <em>you </em>run out and tell you?&quot;</p><p>&quot;No,&quot;\nAkon said.&#0160; A sinking feeling was dawning in the pit of his stomach.&#0160;\n&quot;You would try to conceal the discovery, and create a cover story that\ndiscouraged anyone else from looking there.&quot;</p><p>The Lord Pilot emitted a bark that was half laughter, and half something much darker.&#0160; &quot;It was perfect.&#0160; I&#39;m a Lord Pilot and I never suspected until now.&quot;</p><p>&quot;So?&quot; Akon said.&#0160; &quot;What is it, actually?&quot;</p><p>&quot;Um,&quot; the Ship&#39;s Engineer said.&#0160; &quot;Well... basically... to skip over the technical details...&quot;</p><p>The Ship&#39;s Engineer drew a breath.</p><p>&quot;Any ship with a medium-sized Alderson drive can make a star go supernova.&quot;</p><p>Silence.</p><p>&quot;Which might seem like bad news in general,&quot; the Lord Pilot said, &quot;but from our perspective, right here, right now, it&#39;s just what we need.&#0160; A mere nova wouldn&#39;t do it.&#0160; But blowing up the <em>whole </em>star\n- &quot;&#0160; He gave that bitter bark of laughter, again.&#0160; &quot;No star, no starlines.&#0160; We can make the main\nstar of this system go supernova - not the white dwarf, the companion.&#0160;\nAnd then the Superhappies won&#39;t be able to get to us.&#0160; That is, they\nwon&#39;t be able to get to the human starline network.&#0160; <em>We</em> will be dead.&#0160; If you care about tiny irrelevant details like that.&quot;&#0160; The Lord Pilot looked around the Conference Table.&#0160; &quot;<em>Do</em> you care?&#0160; The correct answer is no, by the way.&quot;</p><p>&quot;I\ncare,&quot; the Lady Sensory said softly.&#0160; &quot;I care a whole lot.&#0160; But...&quot;&#0160;\nShe folded her hands atop the table and bowed her head.</p><p>There were nods from around the Table.</p><p>The Lord Pilot looked at the Ship&#39;s Engineer.&#0160; &quot;How long will it take for you to modify the ship&#39;s Alderson Drive -&quot;</p><p>&quot;It&#39;s\ndone,&quot; said the Ship&#39;s Engineer.&#0160; &quot;But... we should, um, wait until the\nSuperhappies are gone, so they don&#39;t detect us doing it.&quot;</p><p>The Lord Pilot nodded.&#0160; &quot;Sounds like a plan.&#0160; Well, <em>that&#39;s</em> a relief.&#0160; And here I thought the whole human race was doomed, instead of just us.&quot;&#0160; He looked inquiringly at Akon.&#0160; &quot;My lord?&quot;</p><p>Akon\nrested his head in his hands, suddenly feeling more weary than he had\never felt in his life.&#0160; From across the table, the Confessor watched\nhim - or so it seemed; the hood was turned in his direction, at any\nrate.</p><p><em>I told you so,</em> the Confessor did not say.</p><p>&quot;There is a certain problem with your plan,&quot; Akon said.</p><p>&quot;Such as?&quot; the Lord Pilot said.</p><p>&quot;You&#39;ve forgotten something,&quot; Akon said.&#0160; &quot;Something terribly important.&#0160; Something you once swore you would protect.&quot;</p><p>Puzzled faces looked at him.</p><p>&quot;If you say something bloody ridiculous like &#39;the safety of the ship&#39; -&quot; said the Lord Pilot.</p><p>The Lady Sensory gasped.&#0160; &quot;Oh, no,&quot; she murmured.&#0160; &quot;Oh, no.&#0160; The Babyeater children.&quot;</p><p>The\nLord Pilot looked like he had been punched in the stomach.&#0160; The grim\nsmiles that had begun to spread around the table were replaced with\nhorror.</p><p>&quot;Yes,&quot; Akon said.&#0160; He looked away from the Conference\nTable.&#0160; He didn&#39;t want to see the reactions.&#0160; &quot;The Superhappies\nwouldn&#39;t be able to get to us.&#0160; And they couldn&#39;t get to the Babyeaters\neither.&#0160; Neither could we.&#0160; So the Babyeaters would go on eating their\nown children indefinitely.&#0160; And the children would go on dying over\ndays in their parents&#39; stomachs.&#0160; Indefinitely.&#0160; Is the human race\nworth that?&quot;</p><p>Akon looked back at the Table, just once.&#0160; The\nXenopsychologist looked sick, tears were running down the Master&#39;s\nface, and the Lord Pilot looked like he were being slowly torn in\nhalf.&#0160; The Lord Programmer looked abstracted, the Lady Sensory was\ncovering her face with her hands.&#0160; (And the Confessor&#39;s face still lay\nin shadow, beneath the silver hood.)</p><p>Akon closed his eyes.&#0160; &quot;The\nSuperhappies will transform us into something not human,&quot; Akon said.&#0160;\n&quot;No, let&#39;s be frank.&#0160; Something <em>less </em>than human.&#0160; But not all <em>that </em>much\nless than human.&#0160; We&#39;ll still have art, and stories, and love.&#0160; I&#39;ve\ngone entire hours without being in pain, and on the whole, it wasn&#39;t <em>that</em>\nbad an experience -&quot;&#0160; The words were sticking in his throat, along with\na terrible fear.&#0160; &quot;Well.&#0160; Anyway.&#0160; If remaining whole is <em>that</em>\nimportant to us - we have the option.&#0160; It&#39;s just a question of whether\nwe&#39;re willing to pay the price.&#0160; Sacrifice the Babyeater children -&quot;</p><p><em>They&#39;re a lot like human children, really.</em></p><p>&quot;- to save humanity.&quot;</p><p>Someone\nin the darkness was screaming, a thin choked wail that sounded like\nnothing Akon had ever heard or wanted to hear.&#0160; Akon thought it might\nbe the Lord Pilot, or the Master of Fandom, or maybe the Ship&#39;s\nEngineer.&#0160; He didn&#39;t open his eyes to find out.</p><p>There was a chime.</p><p>&quot;In-c-c-coming c-call from the <em>Super Happy</em>,&quot; the Lady Sensory spit out the words like acid, &quot;ship, my lord.&quot;</p><p>Akon\nopened his eyes, and felt, somehow, that he was still in darkness.</p><p>&quot;Receive,&quot; Akon said.</p><p>The Lady 3rd Kiritsugu appeared before him.&#0160; Her eyes widened once, as she took in his appearance, but she said nothing.</p><p><em>That&#39;s right, my lady, I don&#39;t look super happy.</em></p><p>&quot;Humankind, we must have your answer,&quot; she said simply.</p><p>The\nLord Administrator pinched the bridge of his nose, and rubbed his\neyes.&#0160; Absurd, that one human being should have to answer a question\nlike that.&#0160; He wanted to foist off the decision on a committee, a\nmajority vote of the ship, a market - something that wouldn&#39;t demand\nthat anyone accept full responsibility.&#0160; But a ship run that way didn&#39;t\nwork well under ordinary circumstances, and there was no reason to\nthink that things would change under extraordinary circumstances.&#0160; He\nwas an Administrator; he had to accept all the advice, integrate it,\nand decide.&#0160; Experiment had shown that no organizational structure of non-Administrators could\nmatch what he was trained to do, and <em>motivated</em> to do; anything that worked was simply absorbed into the Administrative weighting of advice.</p><p>Sole\ndecision.&#0160; Sole responsibility if he got it wrong.&#0160; Absolute power and\nabsolute accountability, and never forget the second half, my lord, or\nyou&#39;ll be fired the moment you get home.&#0160; Screw up <em>indefensibly</em>,\nmy lord, and all your hundred and twenty years of accumulated salary in\nescrow, producing that lovely steady income, will vanish before you\ndraw another breath.</p><p>Oh - and <em>this </em>time the whole human species will pay for it, too.</p><p>&quot;I\ncan&#39;t speak for all humankind,&quot; said the Lord Administrator.&#0160; &quot;I can decide, but others may decide differently.&#0160; Do you\nunderstand?&quot;</p><p>The Lady 3rd made a light gesture, as if it were of no consequence.&#0160; &quot;Are you an exceptional case of a human decision-maker?&quot;</p><p>Akon tilted his head.&#0160; &quot;Not... <em>particularly</em>...&quot;</p><p>&quot;Then\nyour decision is strongly indicative of what other human decisionmakers\nwill decide,&quot; she said.&#0160; &quot;I find it hard to imagine that the options\nexactly balance in your decision mechanism, whatever your inability to\nadmit your own preferences.&quot;</p><p>Akon slowly nodded.&#0160; &quot;Then...&quot;</p><p>He drew a breath.</p><p><em>Surely, any species that reached the stars would understand the Prisoner&#39;s Dilemma.</em>&#0160;\nIf you couldn&#39;t cooperate, you&#39;d just destroy your own stars.&#0160; A very\neasy thing to do, as it had turned out.&#0160; By that standard, humanity\nmight be something of an impostor next to the Babyeaters and the\nSuperhappies.&#0160; Humanity had kept it a secret from itself.&#0160; The other\ntwo races - just managed not to do the stupid thing.&#0160; You wouldn&#39;t meet anyone out among the stars, otherwise.</p><p>The Superhappies had done their very best to press C.&#0160; Cooperated as fairly as they could.</p><p>Humanity could only do the same.</p><p>&quot;For myself, I am inclined to accept your offer.&quot;</p><p>He didn&#39;t look around to see how anyone had reacted to that.</p><p>&quot;There\nmay be other things,&quot; Akon added, &quot;that humanity would like to ask of\nyour kind, when our representatives meet.&#0160; Your technology is advanced\nbeyond ours.&quot;</p><p>The Lady 3rd smiled.&#0160; &quot;We will, of course, be quite\npositively inclined toward any such requests.&#0160; As I believe our first\nmessage to you said - &#39;we love you and we want you to be super happy&#39;.&#0160;\nYour joy will be shared by us, and we will be pleasured together.&quot;</p><p>Akon couldn&#39;t bring himself to smile.&#0160; &quot;Is that all?&quot;</p><p>&quot;This\nBabyeater ship,&quot; said the Lady 3rd, &quot;the one that did not fire on you,\neven though they saw you first.&#0160; Are you therefore allied with them?&quot;</p><p>&quot;What?&quot; Akon said without thinking.&#0160; &quot;No -&quot;</p><p>&quot;<em>My lord!</em>&quot; shouted the Ship&#39;s Confessor -</p><p>Too late.</p><p>&quot;My lord,&quot; the Lady Sensory said, her voice breaking, &quot;the Superhappy ship has fired on the Babyeater vessel and destroyed it.&quot;</p><p>Akon stared at the Lady 3rd in horror.</p><p>&quot;I&#39;m\nsorry,&quot; the Lady 3rd Kiritsugu said.&#0160; &quot;But our negotiations with them\nfailed, as predicted.&#0160; Our own ship owed them nothing and promised them\nnothing.&#0160; This will make it considerably easier to sweep through their\nstarline network when we return.&#0160; Their children would be the ones to\nsuffer from any delay.&#0160; You understand, my lord?&quot;</p><p>&quot;Yes,&quot; Akon said, his voice trembling.&#0160; &quot;I understand, my lady <em>kiritsugu</em>.&quot;&#0160; He wanted to protest, to scream out.&#0160; But the war was only beginning, and this - <em>would</em> admittedly save -</p><p>&quot;Will you warn them?&quot; the Lady 3rd asked.</p><p>&quot;No,&quot; Akon said.&#0160; It was the truth.</p><p>&quot;Transforming the Babyeaters will take precedence over transforming your own species.&#0160; We estimate the Babyeater operation may take several weeks of your time to conclude.&#0160; We hope you do not mind waiting.&#0160; That is all,&quot; the Lady 3rd said.</p><p>And the holo faded.</p><p>&quot;The\nSuperhappy ship is moving out,&quot; the Lady Sensory said.&#0160; She was crying,\nsilently, as she steadily performed her duty of reporting.&#0160; &quot;They&#39;re\nheading back toward their starline origin.&quot;</p><p>&quot;All right,&quot; Akon said.&#0160; &quot;Take us home.&#0160; We need to report on the negotiations -&quot;</p><p>There\nwas an inarticulate scream, like that throat was trying to burst the\nwalls of the Conference chamber, as the Lord Pilot burst out of his\nchair, burst all restraints he had placed on himself, and lunged\nforward.</p><p>But standing behind his target, unnoticed, the Ship&#39;s Confessor had\nproduced from his sleeve the tiny stunner - the weapon which he alone\non the ship was authorized to use, if he made a determination of\noutright mental breakdown.&#0160; With a sudden motion, the Confessor&#39;s arm swept out...</p><ol>\n<li style=\"font-family: inherit;\"><a href=\"/lw/ya/normal_ending_last_tears_68/\">... and anesthetized the Lord Pilot.</a></li>\n<li><a href=\"/lw/yb/true_ending_sacrificial_fire_78/\">...</a>&#0160; [This option will become the True Ending only if someone suggests it in the comments before the previous ending is posted tomorrow.&#0160; Otherwise, the first ending is the True one.]</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Z263n4TXJimKn6A8Z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 60, "extendedScore": null, "score": 9e-05, "legacy": true, "legacyId": "1233", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 60, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 140, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HawFh7RvDM4RyoJ2d", "HWH46whexsoqR3yXk", "6Ls6f5PerERJmsTGB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-04T08:45:35.000Z", "modifiedAt": null, "url": null, "title": "Normal Ending: Last Tears (6/8)", "slug": "normal-ending-last-tears-6-8", "viewCount": null, "lastCommentedAt": "2018-12-26T16:29:02.297Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HWH46whexsoqR3yXk/normal-ending-last-tears-6-8", "pageUrlRelative": "/posts/HWH46whexsoqR3yXk/normal-ending-last-tears-6-8", "linkUrl": "https://www.lesswrong.com/posts/HWH46whexsoqR3yXk/normal-ending-last-tears-6-8", "postedAtFormatted": "Wednesday, February 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Normal%20Ending%3A%20Last%20Tears%20(6%2F8)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANormal%20Ending%3A%20Last%20Tears%20(6%2F8)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHWH46whexsoqR3yXk%2Fnormal-ending-last-tears-6-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Normal%20Ending%3A%20Last%20Tears%20(6%2F8)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHWH46whexsoqR3yXk%2Fnormal-ending-last-tears-6-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHWH46whexsoqR3yXk%2Fnormal-ending-last-tears-6-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 917, "htmlBody": "<p style=\"text-align: right; font-size: 11px;\">(Part 6 of 8 in \"<a href=\"/lw/y4/three_worlds_collide_08/\">Three Worlds Collide</a>\")</p>\n<p>Today was the day.</p>\n<p>The streets of ancient Earth were crowded to overbursting with people looking up at the sky, faces crowded up against windows.</p>\n<p>Waiting for their sorrows to end.</p>\n<p>Akon was looking down at their faces, from the balcony of a room in a well-guarded hotel.&nbsp; There were many who wished to initiate violence against him, which was understandable.&nbsp; Fear showed on most of the faces in the crowd, rage in some; a very few were smiling, and Akon suspected they might have simply given up on holding themselves together.&nbsp; Akon wondered what his own face looked like, right now.</p>\n<p>The streets were less crowded than they might have been, only a few weeks earlier.</p>\n<p>No one had told the Superhappies about that part.&nbsp; They'd sent an ambassadorial ship \"in case you have any urgent requests we can help with\", arriving hard on the heels of the <em>Impossible</em>.&nbsp; That ship had not been given any of the encryption keys to the human Net, nor allowed to land.&nbsp; It had made the Superhappies <em>extremely </em>suspicious, and the ambassadorial ship had disgorged a horde of tiny daughters to observe the rest of the human starline network -</p>\n<p>But if the Superhappies <em>knew</em>, they would have tried to stop it.&nbsp; Somehow.</p>\n<p><em>That </em>was a price that no one was willing to include into the bargain, no matter what.&nbsp; There <em>had </em>to be that - alternative.</p>\n<p><a id=\"more\"></a></p>\n<p>A quarter of the <em>Impossible Possible World's</em> crew had committed suicide, when the pact and its price became known.&nbsp; Others, Akon thought, had waited only to be with their families.&nbsp; The percentage on Earth... would probably be larger.&nbsp; The government, what was left of it, had refused to publish statistics.&nbsp; All you saw was the bodies being carried out of the apartments - in plain, unmarked boxes, in case the Superhappy ship was using optical surveillance.</p>\n<p>Akon swallowed.&nbsp; The fear was already drying his own throat, the fear of changing, of becoming something else that wasn't quite <em>him.</em>&nbsp; He understood the urge to end that fear, at any price.&nbsp; And yet at the same time, he didn't, couldn't understand the suicides.&nbsp; Was being dead a <em>smaller</em> change?&nbsp; To die was <em>not </em>to leave the world, <em>not </em>to escape somewhere else; it was the simultaneous change of every piece of yourself into nothing.</p>\n<p>Many parents had made that choice for their children.&nbsp; The government <em>had</em> tried to stop it.&nbsp; The Superhappies weren't going to like it, when they found out.&nbsp; And it <em>wasn't</em> right, when the children themselves wouldn't be so afraid of a world without pain.&nbsp; It wasn't as if the parents and children were <em>going</em> somewhere together.&nbsp; The government had done its best, issued orders, threatened confiscations - but there was only so much you could do to coerce someone who was going to die anyway.</p>\n<p>So more often than not, they carried away the mother's body with her daughter's, the father with the son.</p>\n<p>The survivors, Akon knew, would regret that <em>far </em>more vehemently, once they were closer to the Superhappy point of view.</p>\n<p>Just as they would regret not eating the tiny bodies of the infants.</p>\n<p>A hiss went up from the crowd, the intake of a thousand breaths.&nbsp; Akon looked up, and he saw in the sky the cloud of ships, dispersing from the direction of the Sun and the Huygens starline.&nbsp; Even at this distance they twinkled faintly.&nbsp; Akon guessed - and as one ship grew closer, he knew that he was right - that the Superhappy ships were no longer things of pulsating ugliness, but gently shifting iridescent crystal, designs that both a human and a Babyeater would find beautiful.&nbsp; The Superhappies had been swift to follow through on their own part of the bargain.&nbsp; Their new aesthetic senses would already be an intersection of three worlds' tastes.</p>\n<p>The ship drew closer, overhead.&nbsp; It was quieter in the air than even the most efficient human ships, twinkling brightly and silently; the way that someone might imagine a star in the night sky would look close up, if they had no idea of the truth.</p>\n<p>The ship stopped, hovering above the roads, between the buildings.</p>\n<p>Other bright ships, still searching for their destinations, slid by overhead like shooting stars.</p>\n<p>Long, graceful iridescent tendrils extended from the ship, down toward the crowd.&nbsp; One of them came toward his own balcony, and Akon saw that it was marked with the curves of a door.</p>\n<p>The crowd didn't break, didn't run, didn't panic.&nbsp; The screams failed to spread, as the strong hugged the weak and comforted them.&nbsp; That was something to be proud of, in the last moments of the old humanity.</p>\n<p>The tendril reaching for Akon halted just before him.&nbsp; The door marked at its end dilated open.</p>\n<p>And wasn't it strange, now, the crowd was looking up at <em>him.</em></p>\n<p>Akon took a deep breath.&nbsp; He was afraid, but -</p>\n<p>There wasn't much point in standing here, going <em>on </em>being afraid, experiencing futile disutility.</p>\n<p>He stepped through the door, into a neat and well-lighted transparent capsule.</p>\n<p>The door slid shut again.</p>\n<p>Without a lurch, without a sound, the capsule moved up toward the alien ship.</p>\n<p>One last time, Akon thought of all his fear, of the sick feeling in his stomach and the burning that was becoming a pain in his throat.&nbsp; He pinched himself on the arm, hard, very hard, and felt the warning signal telling him to stop.</p>\n<p><em>Goodbye,</em> Akon thought; and the tears began falling down his cheek, as though that one silent word had, for the very last time, broken his heart.</p>\n&nbsp;\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>And he lived happily ever after.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HWH46whexsoqR3yXk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 58, "extendedScore": null, "score": 8.7e-05, "legacy": true, "legacyId": "1234", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 60, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 69, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HawFh7RvDM4RyoJ2d"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-05T10:57:12.000Z", "modifiedAt": null, "url": null, "title": "True Ending: Sacrificial Fire (7/8)", "slug": "true-ending-sacrificial-fire-7-8", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:11.262Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6Ls6f5PerERJmsTGB/true-ending-sacrificial-fire-7-8", "pageUrlRelative": "/posts/6Ls6f5PerERJmsTGB/true-ending-sacrificial-fire-7-8", "linkUrl": "https://www.lesswrong.com/posts/6Ls6f5PerERJmsTGB/true-ending-sacrificial-fire-7-8", "postedAtFormatted": "Thursday, February 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20True%20Ending%3A%20Sacrificial%20Fire%20(7%2F8)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATrue%20Ending%3A%20Sacrificial%20Fire%20(7%2F8)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Ls6f5PerERJmsTGB%2Ftrue-ending-sacrificial-fire-7-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=True%20Ending%3A%20Sacrificial%20Fire%20(7%2F8)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Ls6f5PerERJmsTGB%2Ftrue-ending-sacrificial-fire-7-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Ls6f5PerERJmsTGB%2Ftrue-ending-sacrificial-fire-7-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2366, "htmlBody": "<p><em>(Part 7 of 8 in &quot;<a href=\"https://www.lesswrong.com/lw/y4/three_worlds_collide_08/\">Three Worlds Collide</a>&quot;)</em></p><p>Standing behind his target, unnoticed, the Ship&#x27;s Confessor had produced from his sleeve the tiny stunner - the weapon which he alone on the ship was authorized to use, if he made a determination of outright mental breakdown.  With a sudden motion, his arm swept outward -</p><p>- and anesthetized the Lord Akon.</p><p>Akon crumpled almost instantly, as though most of his strings had already been cut, and only a few last strands had been holding his limbs in place.</p><p>Fear, shock, dismay, sheer outright surprise: that was the Command Conference staring aghast at the Confessor.</p><p>From the hood came words absolutely forbidden to originate from that shadow: the voice of command.  &quot;Lord Pilot, take us through the starline back to the Huygens system.  Get us moving <em>now,</em> you are on the critical path.  Lady Sensory, I need you to enforce an absolute lockdown on all of this ship&#x27;s communication systems except for a single channel under your direct control.  Master of Fandom, get me proxies on the assets of every being on this ship.  We are going to need capital.&quot;</p><p>For a moment, the Command Conference was frozen, voiceless and motionless, as everyone waited for someone else do to something.</p><p>And then -</p><p>&quot;Moving the <em>Impossible</em> now, my lord,&quot; said the Lord Pilot.  His face was sane once again.  &quot;What&#x27;s your plan?&quot;</p><p>&quot;He is <em>not</em> your lord!&quot; cried the Master of Fandom.  Then his voice dropped.  &quot;Excuse me.  <em>Confessor</em> - it did <em>not</em> appear to me that our Lord Administrator was insane.  And <em>you</em>, of all people, cannot just seize power -&quot;</p><p>&quot;True,&quot; said the one, &quot;Akon was sane.  But he was also an honest man who would keep his word once he gave it, and <em>that </em>I could not allow.  As for me - I have betrayed my calling three times over, and am no longer a Confessor.&quot;  With that same response, the once-Confessor swept back the hood -</p><p>At any other time, the words and the move and the revealed face would have provoked shock to the point of fainting.  On this day, with the whole human species at stake, it seemed merely interesting.  Chaos had already run loose, madness was already unleashed into the world, and a little more seemed of little consequence.</p><p>&quot;Ancestor,&quot; said the Master, &quot;you are <em>twice </em>prohibited from exercising any power here.&quot;</p><p>The former Confessor smiled dryly.  &quot;Rules like that only exist within our own minds, you know.  Besides,&quot; he added, &quot;I am not <em>steering</em> the future of humanity in any real sense, just stepping in front of a bullet.  That is not even advice, let alone an order.  And it is... <em>appropriate... </em>that I, and not any of you, be the one who orders this thing done -&quot;</p><p>&quot;Fuck that up the ass with a hedge trimmer,&quot; said the Lord Pilot.  &quot;Are we going to save the human species or not?&quot;</p><p>There was a pause while the others figured out the correct answer.</p><p>Then the Master sighed, and inclined his head in assent to the once-Confessor.  &quot;I shall follow your orders... <em>kiritsugu</em>.&quot;</p><p>Even the Kiritsugu flinched at that, but there was work to be done, and not much time in which to do it.</p><p></p><p>In the Huygens system, the <em>Impossible Possible World</em> was observed to return from its much-heralded expedition, appearing on the starline that had shown the unprecedented anomaly.  Instantly, without a clock tick&#x27;s delay, the <em>Impossible</em> broadcast a market order.</p><p>That was already a dozen ways illegal.  If the <em>Impossible</em> had made a scientific discovery, it should have broadcast the experimental results openly before attempting to trade on them.  Otherwise the result was not profit but chaos, as traders throughout the market refused to deal with you; just conditioning on the fact that you <em>wanted </em>to sell or buy from them, was reason enough for them <em>not </em>to.  The whole market seized up as hedgers tried to guess what the hidden experimental results could have been, and which of their counterparties had private information.</p><p>The <em>Impossible</em> ignored the rules.  It broadcast the specification of a new prediction contract, signed with EMERGENCY OVERRIDE and IMMINENT HARM and CONFESSOR FLAG - signatures that carried extreme penalties, up to total confiscation, for misuse; but any one of which ensured that the contract would appear on the prediction markets at almost the speed of the raw signal.</p><p>The <em>Impossible</em> placed an initial order on the contract backed by nearly the entire asset base of its crew.</p><p>The prediction&#x27;s plaintext read:</p><blockquote><em>In three hours and forty-one minutes, the starline between Huygens and Earth will become impassable.</em></blockquote><blockquote><em>Within thirty minutes after, every human being remaining in this solar system will die.</em></blockquote><blockquote><em>All passage through this solar system will be permanently denied to humans thereafter.</em></blockquote><blockquote><em>(The following plaintext is not intended to describe the contract&#x27;s terms, but justifies why a probability estimate on the underlying proposition is of great social utility:</em></blockquote><blockquote><em>ALIENS.  ANYONE WITH A STARSHIP, FILL IT WITH CHILDREN AND GO!  GET OUT OF HUYGENS, NOW!)</em></blockquote><p>In the Huygens system, there was almost enough time to draw a single breath.</p><p>And then the markets went mad, as every single trader tried to calculate the odds, and every married trader abandoned their positions and tried to get their children to a starport.</p><p></p><p>&quot;Six,&quot; murmured the Master of Fandom, &quot;seven, eight, nine, ten, eleven -&quot;</p><p>A holo appeared within the Command Conference, a signal from the President of the Huygens Central Clearinghouse, requesting (or perhaps &quot;demanding&quot; would have been a better word) an interview with the Lord Administrator of the <em>Impossible Possible World.</em></p><p>&quot;Put it through,&quot; said the Lord Pilot, now sitting in Akon&#x27;s chair as the figurehead anointed by the Kiritsugu.</p><p>&quot;<em>Aliens?</em>&quot; the President demanded, and then her eye caught the Pilot&#x27;s uniform.  &quot;<em>You&#x27;re</em> not an Administrator -&quot;</p><p>&quot;Our Lord Administrator is under sedation,&quot; said the Kiritsugu beside; he was wearing his Confessor&#x27;s hood again, to save on explanations.  &quot;He placed himself under more stress than any of us -&quot;</p><p>The President made an abrupt cutting gesture.  &quot;Explain this - <em>contract</em>.  And if this is a market manipulation scheme, I&#x27;ll see you all tickled until the last sun grows cold!&quot;</p><p>&quot;We followed the starline that showed the anomalous behavior,&quot; the Lord Pilot said, &quot;and found that a nova had just occurred in the originating system.  In other words, my Lady President, it was a direct effect of the nova and thus occurred on <em>all</em> starlines leading out of that system.  We&#x27;ve never found aliens before now - but that&#x27;s reflective of the probability of any <em>single</em> system we explore having been colonized.  There might even be a starline leading out of this system that leads to an alien domain - but we have no way of knowing <em>which </em>one, and opening a new starline is expensive.  The nova acted as a common rendezvous signal, my Lady President.  It reflects the probability, not that we and the aliens encounter each other by direct exploration, but the probability that we have at least one <em>neighboring</em> world in common.&quot;</p><p>The President was pale.  &quot;And the aliens are hostile.&quot;</p><p>The Lord Pilot involuntarily looked to the Kiritsugu.</p><p>&quot;Our values are incompatible,&quot; said the Kiritsugu.</p><p>&quot;Yes, that&#x27;s one way of putting it,&quot; said the Lord Pilot.  &quot;And unfortunately, my Lady President, their technology is considerably in advance of ours.&quot;</p><p>&quot;Lord... Pilot,&quot; the President said, &quot;are you certain that the aliens intend to wipe out the human species?&quot;</p><p>The Lord Pilot gave a very thin, very flat smile.  &quot;Incompatible values, my Lady President.  They&#x27;re quite skilled with biotechnology.  Let&#x27;s leave it at that.&quot;</p><p>Sweat was running down the President&#x27;s forehead.  &quot;And why did they let <em>you </em>go, then?&quot;</p><p>&quot;We arranged for them to be told a plausible lie,&quot; the Lord Pilot said simply.  &quot;One of the reasons they&#x27;re more advanced than us is that they&#x27;re not very good at deception.&quot;</p><p>&quot;None of this,&quot; the President said, and now her voice was trembling, &quot;none of this explains why the starline <em>between Huygens and Earth</em> will become impassable.  Surely, if what you say is true, the aliens will pour through our world, and into Earth, and into the human starline network.  Why do you think that this one starline will <em>luckily</em> shut down?&quot;</p><p>The Lord Pilot drew a breath.  It was good form to tell the exact truth when you had something to hide.  &quot;My Lady President, we encountered <em>two</em> alien species at the nova.  The first species exchanged scientific information with us.  It is the second species that we are running from.  But, from the <em>first </em>species, we learned a fact which this ship can use to shut down the Earth starline.  For obvious reasons, my Lady President, we do not intend to share this fact publicly.  That portion of our final report will be encrypted to the Chair of the Interstellar Association for the Advancement of Science, and to no other key.&quot;</p><p>The President started laughing.  It was wild, hysterical laughter that caused the Kiritsugu&#x27;s hood to turn toward her.  From the corner of the screen, a gloved hand entered the view; the hand of the President&#x27;s own Confessor.  &quot;My lady...&quot; came a soft female voice.</p><p>&quot;Oh, <em>very</em> good,&quot; the President said.  &quot;Oh, marvelous.  So it&#x27;s <em>your</em> ship that&#x27;s going to be responsible for this catastrophe.  You admit that, eh?  I&#x27;m amazed.  You probably managed to avoid telling a single direct lie.  You plan to blow up our star and kill fifteen billion people, and you&#x27;re trying to stick to the literal truth.&quot;</p><p>The Lord Pilot slowly nodded.  &quot;When we compared the first aliens&#x27; scientific database to our own -&quot;</p><p>&quot;No, don&#x27;t tell me.  I was told it could be done by a single ship, but I&#x27;m not supposed to know how.  Astounding that an alien species could be so peaceful they don&#x27;t even consider <em>that </em>a secret.  I think I would like to meet these aliens.  They sound much nicer than the other ones - why are you laughing?&quot;</p><p>&quot;My Lady President,&quot; the Lord Pilot said, getting a grip on himself, &quot;forgive me, we&#x27;ve been through a lot.  Excuse me for asking, but are you evacuating the planet or what?&quot;</p><p>The President&#x27;s gaze suddenly seemed sharp and piercing like the fire of stars.  &quot;It was set in motion instantly, of course.  No <em>comparable </em>harm done, if you&#x27;re wrong.  But three hours and forty-one minutes is not enough time to evacuate <em>ten percent</em> of this planet&#x27;s <em>children.</em>&quot;  The President&#x27;s eyes darted at something out of sight.  &quot;With eight hours, we could call in ships from the Earth nexus and evacuate the whole planet.&quot;</p><p>&quot;My lady,&quot; a soft voice came from behind the President, &quot;it is the whole human species at stake.  Not just the entire starline network beyond Earth, but the entire future of humanity.  <em>Any </em>incrementally higher probability of the aliens arriving within that time -&quot;</p><p>The President stood in a single fluid motion that overturned her chair, moving so fast that the viewpoint bobbed as it tried to focus on her and the shadow-hooded figure standing beside.  &quot;Are you telling me,&quot; she said, and her voice rose to a scream, &quot;to <em>shut up and multiply?</em>&quot;</p><p>&quot;Yes.&quot;</p><p>The President turned back to the camera angle, and said simply, &quot;No.  You don&#x27;t <em>know</em> the aliens are following that close behind you - do you?  We don&#x27;t even <em>know </em>if you can shut down the starline!  No matter what your <em>theory </em>predicts, it&#x27;s never been tested - right?  What if you create a flare bright enough to roast our planet, but not explode the whole sun?  Billions would die, for <em>nothing!</em>  So if you do not promise me a minimum of - let&#x27;s call it nine hours to finish evacuating this planet - then I will order your ship destroyed before it can act.&quot;</p><p>No one from the <em>Impossible </em>spoke.</p><p>The President&#x27;s fist slammed her desk.  &quot;Do you understand me?  <em>Answer!</em>  Or in the name of Huygens, I will destroy your ship -&quot;</p><p>Her Confessor caught her President&#x27;s body, very gently supporting it as it collapsed.</p><p>Even the Lord Pilot was pale and silent.  But <em>that</em>, at least, had been within law and tradition; no one could have called that thinking sane.</p><p>On the display, the Confessor bowed her hood.  &quot;I will inform the markets that the Lady President was driven unstable by your news,&quot; she said quietly, &quot;and recommend to the government that they carry out the evacuation without asking further questions of your ship.  Is there anything else you wish me to tell them?&quot;  Her hood turned slightly, toward the Kiritsugu.  &quot;Or tell me?&quot;</p><p>There was a strange, quick pause, as the shadows from within the two hoods stared at each other.</p><p>Then:  &quot;No,&quot; replied the Kiritsugu.  &quot;I think it has all been said.&quot;</p><p>The Confessor&#x27;s hood nodded.  &quot;Goodbye.&quot;</p><p></p><p>&quot;There it goes,&quot; the Ship&#x27;s Engineer said.  &quot;We have a complete, stable positive feedback loop.&quot;</p><p>On screen was the majesty that was the star Huygens, of the inhabited planet Huygens IV.  Overlaid in false color was the recirculating loop of Alderson forces which the <em>Impossible</em> had steadily fed.</p><p>Fusion was now increasing in the star, as the Alderson forces encouraged nuclear barriers to break down; and the more fusions occurred, the more Alderson force was generated.  Round and round it went.  All the work of the <em>Impossible</em>, the full frantic output of their stardrive, had only served to subtly steer the vast forces being generated; nudge a fraction into a circle rather than a line.  But now -</p><p>Did the star brighten?  It was only their imagination, they knew.  Photons take centuries to exit a sun, under normal circumstances.  The star&#x27;s core was trying to expand, but it was expanding too slowly - all too slowly - to outrun the positive feedback that had begun.</p><p>&quot;Multiplication factor one point oh five,&quot; the Engineer said.  &quot;It&#x27;s climbing faster now, and the loop seems to be intact.  I think we can conclude that this operation is going to be... successful.   One point two.&quot;</p><p>&quot;Starline instability detected,&quot; the Lady Sensory said.</p><p>Ships were still disappearing in frantic waves on the starline toward Earth.  Still connected to the Huygens civilization, up to the last moment, by tiny threads of Alderson force.</p><p>&quot;Um, if anyone has anything they want to add to our final report,&quot; the Ship&#x27;s Engineer said, &quot;they&#x27;ve got around ten seconds.&quot;</p><p>&quot;Tell the human species from me -&quot; the Lord Pilot said.</p><p>&quot;Five seconds.&quot;</p><p>The Lord Pilot shouted, fist held high and triumphant:  &quot;<em>To live, and occasionally be unhappy!</em>&quot;</p><p><u>This concludes the full and final report of the <em>Impossible Possible World.</em></u></p><p><em>(<a href=\"https://www.lesswrong.com/lw/yc/epilogue_atonement_88/\">To be completed.</a>)</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6Ls6f5PerERJmsTGB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 38, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "1235", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 91, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HawFh7RvDM4RyoJ2d", "4pov2tL6SEC23wrkq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-06T11:52:42.000Z", "modifiedAt": "2020-07-12T01:23:09.489Z", "url": null, "title": "Epilogue: Atonement (8/8)", "slug": "epilogue-atonement-8-8", "viewCount": null, "lastCommentedAt": "2020-08-09T22:40:56.508Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4pov2tL6SEC23wrkq/epilogue-atonement-8-8", "pageUrlRelative": "/posts/4pov2tL6SEC23wrkq/epilogue-atonement-8-8", "linkUrl": "https://www.lesswrong.com/posts/4pov2tL6SEC23wrkq/epilogue-atonement-8-8", "postedAtFormatted": "Friday, February 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Epilogue%3A%20Atonement%20(8%2F8)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEpilogue%3A%20Atonement%20(8%2F8)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4pov2tL6SEC23wrkq%2Fepilogue-atonement-8-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Epilogue%3A%20Atonement%20(8%2F8)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4pov2tL6SEC23wrkq%2Fepilogue-atonement-8-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4pov2tL6SEC23wrkq%2Fepilogue-atonement-8-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1941, "htmlBody": "<p style=\"text-align: right; font-size: 11px;\">(Part 8 of 8 in \"<a href=\"/lw/y4/three_worlds_collide_08/\">Three Worlds Collide</a>\")</p>\n<p>Fire came to Huygens.</p>\n<p>The star erupted.</p>\n<p>Stranded ships, filled with children doomed by a second's last delay, still milled around the former Earth transit point.&nbsp; Too many doomed ships, far too many doomed ships.&nbsp; They should have left a minute early, just to be <em>sure</em>; but the temptation to load in that one last child must have been irresistable.&nbsp; To do the warm and fuzzy thing just this one time, instead of being cold and calculating.&nbsp; You couldn't blame them, could you...?</p>\n<p>Yes, actually, you could.</p>\n<p>The Lady Sensory switched off the display.&nbsp; It was too painful.</p>\n<p>On the Huygens market, the price of a certain contract spiked to 100%.&nbsp; They were all rich in completely worthless assets for the next nine minutes, until the supernova blast front arrived.</p>\n<p>\"So,\" the Lord Pilot finally said.&nbsp; \"What kind of asset retains its value in a market with nine minutes to live?\"</p>\n<p><a id=\"more\"></a></p>\n<p>\"Booze for immediate delivery,\" the Master of Fandom said promptly.&nbsp; \"That's what you call a -\"</p>\n<p>\"Liquidity preference,\" the others chorused.</p>\n<p>The Master laughed.&nbsp; \"All right, that was too obvious.&nbsp; Well... chocolate, sex -\"</p>\n<p>\"Not necessarily,\" said the Lord Pilot.&nbsp; \"If you can use up the whole supply of chocolate at once, <em>does </em>demand outstrip supply?&nbsp; Same with sex - the value could actually <em>drop</em> if everyone's suddenly willing.&nbsp; Not to mention:&nbsp; <em>Nine minutes?</em>\"</p>\n<p>\"All right then, expert oral sex from experienced providers.&nbsp; And hard drugs with dangerous side effects; the demand would rise hugely relative to supply -\"</p>\n<p>\"This is inane,\" the Ship's Engineer commented.</p>\n<p>The Master of Fandom shrugged.&nbsp; \"What <em>do </em>you say in the unrecorded last minutes of your life that is <em>not</em> inane?\"</p>\n<p>\"It doesn't matter,\" said the Lady Sensory.&nbsp; Her face was strangely tranquil.&nbsp; \"Nothing that we do now matters.&nbsp; We won't have to live with the consequences.&nbsp; No one will.&nbsp; All this time will be obliterated when the blast front hits.&nbsp; The role I've always played, the picture that I have of <em>me...</em> it doesn't matter.&nbsp; There's... a peace... in not having to be Dalia Ancromein any more.\"</p>\n<p>The others looked at her.&nbsp; Talk about killing the mood.</p>\n<p>\"Well,\" the Master of Fandom said, \"since you raise the subject, I suppose it <em>would </em>be peaceful if not for the screaming terror.\"</p>\n<p>\"You don't <em>have </em>to feel the screaming terror,\" the Lady Sensory said.&nbsp; \"That's just a picture you have in your head of how it <em>should </em>be.&nbsp; The <em>role </em>of someone facing imminent death.&nbsp; But I don't <em>have </em>to play any more roles.&nbsp; I don't <em>have </em>to feel screaming terror.&nbsp; I don't <em>have </em>to frantically pack in a few last moments of fun.&nbsp; There are no more obligations.\"</p>\n<p>\"Ah,\" the Master of Fandom said, \"so I guess this is when we find out who we really are.\"&nbsp; He paused for a moment, then shrugged.&nbsp; \"I don't seem to be anyone in particular.&nbsp; Oh well.\"</p>\n<p>The Lady Sensory stood up, and walked across the room to where the Lord Pilot stood looking at the viewscreen.</p>\n<p>\"My Lord Pilot,\" the Lady Sensory said.</p>\n<p>\"Yes?\" the Lord Pilot said.&nbsp; His face was expectant.</p>\n<p>The Lady Sensory smiled.&nbsp; It was bizarre, but not frightening.&nbsp; \"Do you know, my Lord Pilot, that I had often thought how wonderful it would be to kick you very hard in the testicles?\"</p>\n<p>\"Um,\" the Lord Pilot said.&nbsp; His arms and legs suddenly tensed, preparing to block.</p>\n<p>\"But now that I could do it,\" the Lady Sensory said, \"I find that I don't really <em>want</em> to.&nbsp; It seems... that I'm not as awful a person as I thought.\"&nbsp; She gave a brief sigh.&nbsp; \"I wish that I had realized it earlier.\"</p>\n<p>The Lord Pilot's hand swiftly darted out and groped the Lady Sensory's breast.&nbsp; It was so unexpected that no one had time to react, least of all her.&nbsp; \"Well, what do you know,\" the Pilot said, \"I'm just as much of a pervert as I thought.&nbsp; My self-estimate was more accurate than yours, nyah nyah -\"</p>\n<p>The Lady Sensory kneed him in the groin, hard enough to drop him moaning to the floor, but not hard enough to require medical attention.</p>\n<p>\"Okay,\" the Master of Fandom said, \"can we please not go down this road?&nbsp; I'd like to die with at least <em>some</em> dignity.\"</p>\n<p>There was a long, awkward silence, broken only by a quiet \"Ow ow ow ow...\"</p>\n<p>\"Would you like to hear something amusing?\" asked the Kiritsugu, who had once been a Confessor.</p>\n<p>\"If you're going to ask that question,\" said the Master of Fandom, \"when the answer is obviously yes, thus wasting a few more seconds -\"</p>\n<p>\"Back in the ancient days that none of you can imagine, when I was seventeen years old - which was underage even then - I stalked an underage girl through the streets, slashed her with a knife until she couldn't stand up, and then had sex with her before she died.&nbsp; It was probably even worse than you're imagining.&nbsp; And deep down, in my very core, I enjoyed every minute.\"</p>\n<p>Silence.</p>\n<p>\"I don't think of it often, mind you.&nbsp; It's been a long time, and I've taken a lot of intelligence-enhancing drugs since then.&nbsp; But still - I was just thinking that maybe what I'm doing now <em>finally </em>makes up for that.\"</p>\n<p>\"Um,\" said the Ship's Engineer.&nbsp; \"What we just did, in fact, was kill fifteen billion people.\"</p>\n<p>\"Yes,\" said the Kiritsugu, \"that's the amusing part.\"</p>\n<p>Silence.</p>\n<p>\"It seems to me,\" mused the Master of Fandom, \"that I should feel a lot worse about that than I actually do.\"</p>\n<p>\"We're in shock,\" the Lady Sensory observed distantly.&nbsp; \"It'll hit us in about half an hour, I expect.\"</p>\n<p>\"I think it's starting to hit me,\" the Ship's Engineer said.&nbsp; His face was twisted.&nbsp; \"I - I was so worried I <em>wouldn't</em> be able to destroy my home planet, that I didn't get around to feeling unhappy about <em>succeeding</em> until now.&nbsp; It... hurts.\"</p>\n<p>\"I'm mostly just numb,\" the Lord Pilot said from the floor.&nbsp; \"Well, except down there, unfortunately.\"&nbsp; He slowly sat up, wincing.&nbsp; \"But there was this absolute unalterable thing inside me, screaming so loud that it overrode everything.&nbsp; I never knew there was a place like that within me.&nbsp; There wasn't room for anything else until humanity was safe.&nbsp; And now my brain is worn out.&nbsp; So I'm just numb.\"</p>\n<p>\"Once upon a time,\" said the Kiritsugu, \"there were people who dropped a U-235 fission bomb, on a place called Hiroshima.&nbsp; They killed perhaps seventy thousand people, and ended a war.&nbsp; And if the good and decent officer who pressed that button had needed to walk up to a man, a woman, a child, and slit their throats one at a time, he would have broken long before he killed seventy thousand people.\"</p>\n<p>Someone made a choking noise, as if trying to cough out something that had suddenly lodged deep in their throat.</p>\n<p>\"But pressing a button is different,\" the Kiritsugu said.&nbsp; \"You don't see the results, then.&nbsp; Stabbing someone with a knife has an impact on you.&nbsp; The first time, anyway.&nbsp; Shooting someone with a gun is easier.&nbsp; Being a few meters further away makes a surprising difference.&nbsp; Only needing to pull a trigger changes it a lot.&nbsp; As for pressing a button on a spaceship - that's the easiest of all.&nbsp; Then the part about 'fifteen billion' just gets flushed away.&nbsp; And more importantly - you think it was the <em>right</em> thing to do.&nbsp; The noble, the moral, the honorable thing to do.&nbsp; For the safety of your tribe.&nbsp; You're <em>proud </em>of it -\"</p>\n<p>\"Are you saying,\" the Lord Pilot said, \"that it was <em>not</em> the right thing to do?\"</p>\n<p>\"No,\" the Kiritsugu said.&nbsp; \"I'm saying that, right or wrong, the <em>belief </em>is all it takes.\"</p>\n<p>\"I see,\" said the Master of Fandom.&nbsp; \"So you can kill billions of people without feeling much, so long as you do it by pressing a button, and you're sure it's the right thing to do.&nbsp; That's human nature.\"&nbsp; The Master of Fandom nodded.&nbsp; \"What a valuable and important lesson.&nbsp; I shall remember it all the rest of my life.\"</p>\n<p>\"Why <em>are </em>you saying all these things?\" the Lord Pilot asked the Kiritsugu.</p>\n<p>The Kiritsugu shrugged.&nbsp; \"When I have no reason left to do anything, I am someone who tells the truth.\"</p>\n<p>\"It's wrong,\" said the Ship's Engineer in a small, hoarse voice, \"I <em>know </em>it's wrong, but - I keep wishing the supernova would hurry up and get here.\"</p>\n<p>\"There's no reason for you to hurt,\" said the Lady Sensory in a strange calm voice.&nbsp; \"Just ask the Kiritsugu to stun you.&nbsp; You'll never wake up.\"</p>\n<p>\"...no.\"</p>\n<p>\"Why not?\" asked the Lady Sensory, in a tone of purely abstract curiosity.</p>\n<p>The Ship's Engineer clenched his hands into fists.&nbsp; \"Because if hurting is that much of a crime, then the Superhappies are right.\"&nbsp; He looked at the Lady Sensory.&nbsp; \"You're wrong, my lady.&nbsp; These moments are as real as every other moment of our lives.&nbsp; The supernova can't make them not exist.\"&nbsp; His voice lowered.&nbsp; \"That's what my cortex says.&nbsp; My diencephalon wishes we'd been closer to the sun.\"</p>\n<p>\"It could be worse,\" observed the Lord Pilot.&nbsp; \"You could <em>not</em> hurt.\"</p>\n<p>\"For myself,\" the Kiritsugu said quietly, \"I had already visualized and accepted this, and then it was just a question of watching it play out.\"&nbsp; He sighed.&nbsp; \"The most dangerous truth a Confessor knows is that the rules of society are just consensual hallucinations.&nbsp; Choosing to wake up from the dream means choosing to end your life.&nbsp; I knew that when I stunned Akon, even apart from the supernova.\"</p>\n<p>\"Okay, look,\" said the Master of Fandom, \"call me a gloomy moomy, but does anyone have something <em>uplifting</em> to say?\"</p>\n<p>The Lord Pilot jerked a thumb at the expanding supernova blast front, a hundred seconds away.&nbsp; \"What, about <em>that?</em>\"</p>\n<p>\"Yeah,\" the Master of Fandom said.&nbsp; \"I'd like to end my life on an up note.\"</p>\n<p>\"We saved the human species,\" offered the Lord Pilot.&nbsp; \"Man, that's the sort of thing you could just repeat to yourself over and over and over again -\"</p>\n<p>\"Besides that.\"</p>\n<p>\"Besides <em>WHAT?</em>\"</p>\n<p>The Master managed to hold a straight face for a few seconds, and then had to laugh.</p>\n<p>\"You know,\" the Kiritsugu said, \"I don't think there's anyone in modern-day humanity, who would regard my past self as anything but a poor, abused victim.&nbsp; I'm pretty sure my mother drank during pregnancy, which, back then, would give your child something called Fetal Alcohol Syndrome.&nbsp; I was poor, uneducated, and in an environment so entrepreneurially hostile you can't even imagine it -\"</p>\n<p>\"This is <em>not sounding uplifting,</em>\" the Master said.</p>\n<p>\"But somehow,\" the Kiritsugu said, \"all those wonderful excuses - I could never quite believe in them <em>myself</em>, afterward.&nbsp; Maybe because I'd also thought of some of the same excuses <em>before</em>.&nbsp; It's the part about not doing <em>anything</em> that got to me.&nbsp; Others fought the war to save the world, far over my head.&nbsp; Lightning flickering in the clouds high above me, while I hid in the basement and suffered out the storm.&nbsp; And by the time I was rescued and healed and educated, in any shape to help others - the battle was essentially over.&nbsp; Knowing that I'd been a victim for someone <em>else </em>to save, one more point in someone else's high score - that just stuck in my craw, all those years...\"</p>\n<p>\"...anyway,\" the Kiritsugu said, and there was a small, slight smile on that ancient face, \"I feel better now.\"</p>\n<p>\"So does that mean,\" asked the Master, \"that now your life is finally complete, and you can die without any regrets?\"</p>\n<p>The Kiritsugu looked startled for a moment.&nbsp; Then he threw back his head and laughed.&nbsp; True, pure, honest laughter.&nbsp; The others began to laugh as well, and their shared hilarity echoed across the room, as the supernova blast front approached at almost exactly the speed of light.</p>\n<p>Finally the Kiritsugu stopped laughing, and said:</p>\n<p>\"Don't be ridicu-\"</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4pov2tL6SEC23wrkq", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 65, "extendedScore": null, "score": 9.7e-05, "legacy": true, "legacyId": "1236", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": true, "maxBaseScore": 65, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 194, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["HawFh7RvDM4RyoJ2d"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-02-06T11:52:42.000Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-07T19:18:44.000Z", "modifiedAt": null, "url": null, "title": "The Thing That I Protect", "slug": "the-thing-that-i-protect", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:59.419Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3wyMbgeFfQWntFxTf/the-thing-that-i-protect", "pageUrlRelative": "/posts/3wyMbgeFfQWntFxTf/the-thing-that-i-protect", "linkUrl": "https://www.lesswrong.com/posts/3wyMbgeFfQWntFxTf/the-thing-that-i-protect", "postedAtFormatted": "Saturday, February 7th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Thing%20That%20I%20Protect&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Thing%20That%20I%20Protect%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3wyMbgeFfQWntFxTf%2Fthe-thing-that-i-protect%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Thing%20That%20I%20Protect%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3wyMbgeFfQWntFxTf%2Fthe-thing-that-i-protect", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3wyMbgeFfQWntFxTf%2Fthe-thing-that-i-protect", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1544, "htmlBody": "<p><strong>Followup to</strong>:&#0160; <a href=\"/lw/nb/something_to_protect/\">Something to Protect</a>, <a href=\"/lw/y3/value_is_fragile/\">Value is Fragile</a></p><p>&quot;<a href=\"/lw/nb/something_to_protect/\">Something to Protect</a>&quot; discursed on the idea of wielding rationality in the service of something other than &quot;rationality&quot;.&#0160; Not just that rationalists ought to pick out a Noble Cause as a hobby to keep them busy; but rather, that rationality itself is generated by having something that you care about more than your current ritual of cognition.</p><p>So what is it, then, that <em>I</em> protect?</p><p>I quite deliberately did not discuss that in &quot;<a href=\"/lw/nb/something_to_protect/\">Something to Protect</a>&quot;, leaving it only as a hanging implication.&#0160; In the unlikely event that we ever run into aliens, I don&#39;t expect their version of Bayes&#39;s Theorem to be mathematically different from ours, even if they generated it in the course of protecting different and incompatible values.&#0160; Among humans, the idiom of having &quot;something to protect&quot; is not bound to any one cause, and therefore, to mention my own cause in that post would have harmed its integrity.&#0160; Causes are dangerous things, whatever their true importance; I have <a href=\"/lw/ln/resist_the_happy_death_spiral/\">written</a> <a href=\"/lw/lv/every_cause_wants_to_be_a_cult/\">somewhat</a> on this, and will write more about it.</p><p>But still - what is it, then, the thing that I protect?</p><p>Friendly AI?&#0160; No - a thousand times no - a thousand times <a href=\"/lw/ty/my_childhood_death_spiral/\">not anymore</a>.&#0160; It&#39;s not thinking <em>of the AI</em> that gives me strength to <a href=\"/lw/y2/rationality_quotes_25/\">carry on even in the face of inconvenience</a>.\n</p><a id=\"more\"></a><p>I would be a strange and dangerous AI wannabe if <em>that</em> were my cause - the image in my mind of a perfected being, an existence greater than humankind.&#0160; Maybe someday I&#39;ll be able to imagine such a child and try to build one, but for now <a href=\"/lw/x7/cant_unbirth_a_child/\">I&#39;m too young to be a father</a>.</p><p>Those of you who&#39;ve been following along recent discussions, particularly &quot;<a href=\"/lw/y3/value_is_fragile/\">Value is Fragile</a>&quot;, might have noticed something else that I might, perhaps, hold precious.&#0160; Smart agents want to protect the physical representation of their utility function for almost the same reason that male organisms are built to be protective of their testicles.&#0160; From the standpoint of the <a href=\"/lw/kr/an_alien_god/\">alien god</a>, natural selection, losing the germline - the gene-carrier that propagates the pattern into the next generation - means losing almost everything that <em>natural selection</em> cares about.&#0160; Unless you already have children to protect, can protect relatives, etcetera - few are the absolute and unqualified statements that can be made in evolutionary biology - but still, if you happen to be a male human, you will find yourself rather protective of your testicles; that one, centralized vulnerability is why a kick in the testicles hurts more than being hit on the head.</p><p>To lose the <em>pattern </em>of human value - which, for now, is physically embodied <em>only </em>in the human brains that care about those values - would be to lose the Future itself; <span style=\"text-decoration: underline;\">if there&#39;s no agent with those values, there&#39;s nothing to</span><a href=\"/lw/y3/value_is_fragile/\"> shape a valuable Future</a>.</p><p>And this pattern, this one most vulnerable and precious pattern, is indeed at risk to be distorted or destroyed.&#0160;&#0160;<a href=\"/lw/xd/growing_up_is_hard/\">Growing up is a hard problem either way</a>, whether you try to <a href=\"/lw/xe/changing_emotions/\">edit existing brains</a>, or build <em>de novo</em> Artificial Intelligence that <a href=\"/lw/tb/mirrors_and_paintings/\">mirrors</a> human values.&#0160; If something more powerful than humans, and not sharing human values, comes into existence - whether by de novo AI gone wrong, or augmented humans gone wrong - then we can expect to <a href=\"/lw/qk/that_alien_message/\">lose</a>, <a href=\"/lw/wf/hard_takeoff/\">hard</a>.&#0160; And value is fragile; <a href=\"/lw/y3/value_is_fragile/\">losing just one dimension of human value can destroy nearly all of the utility we expect from the future</a>.</p><p>So is <em>that</em>, then, the thing that I protect?</p><p>If it were - then what inspired me when times got tough would be, say, thinking of people being nice to each other.&#0160; Or thinking of people laughing, and contemplating how humor probably exists among only an infinitesimal fraction of evolved intelligent species and their descendants.&#0160; I would marvel at the power of sympathy to make us feel what others feel -</p><p>But that&#39;s not quite it either.</p><p>I once attended a small gathering whose theme was &quot;This I Believe&quot;.&#0160; You could interpret that phrase in a number of ways; I chose &quot;What do you believe that most other people don&#39;t believe which makes a corresponding difference in your behavior?&quot;&#0160; And it seemed to me that most of how I behaved differently from other people boiled down to two unusual beliefs.&#0160; The first belief could be summarized as &quot;<a href=\"/lw/vm/lawful_creativity/\">intelligence is a manifestation of order rather than chaos</a>&quot;; this accounts both for my attempts to master rationality, and my attempt to wield the power of AI.</p><p>And the second unusual belief could be summarized as:&#0160; &quot;Humanity&#39;s future can be a WHOLE LOT better than its past.&quot;</p><p>Not desperately darwinian robots surging out to eat as much of the cosmos as possible, mostly ignoring their own internal values to try and grab as many stars as possible, with most of the remaining matter going into making paperclips.</p><p>Not some bittersweet ending where you and I fade away on Earth while the inscrutable robots ride off into the unknowable sunset, having grown beyond such merely human values as love or sympathy.</p><p><em>Screw</em> bittersweet.&#0160; To <em>hell </em>with that melancholy-tinged crap.&#0160; Why leave <em>anyone </em>behind?&#0160; Why surrender a single thing that&#39;s precious?</p><p>(And the compromise-futures are all fake anyway; at this difficulty level, <a href=\"/lw/ul/my_bayesian_enlightenment/\">you steer precisely or you crash</a>.)</p><p><a href=\"/lw/y0/31_laws_of_fun/\">The pattern of fun is also lawful</a>.&#0160; And, though I do not know all the law - I do think that written in humanity&#39;s value-patterns is the implicit potential of a <em>happy </em>future.&#0160; A seriously goddamn FUN future.&#0160; A genuinely GOOD outcome.&#0160; <em>Not </em>something you&#39;d accept with a sigh of resignation for nothing better being possible.&#0160; Something that would make you go &quot;WOOHOO!&quot;</p><p>In the <a href=\"/lw/xy/the_fun_theory_sequence/\">sequence on Fun Theory</a>, I have given you, I hope, some small reason to believe that such a possibility might be consistently describable, if only it could be made real.&#0160; How to read that potential out of humans and project it into reality... might or might not be as simple as &quot;<a href=\"/lw/tb/mirrors_and_paintings/\">superpose our extrapolated reflected equilibria</a>&quot;.&#0160; But that&#39;s one way of looking at what I&#39;m <em>trying</em> to do - to reach the potential of the GOOD outcome, not the melancholy bittersweet compromise.&#0160; Why settle for less?</p><p>To really have something to protect, it has to be able to bring tears to your eyes.&#0160; That, generally, requires something concrete to visualize - not just abstract laws.&#0160; Reading the Laws of Fun doesn&#39;t bring tears to my eyes.&#0160; I can visualize a possibility or two that makes sense to me, but I don&#39;t know if it would make sense to others the same way.</p><p>What does bring tears to my eyes?&#0160; Imagining a future where humanity has its act together.&#0160; Imagining children who grow up never knowing our world, who don&#39;t even understand it.&#0160; Imagining the rescue of those now in sorrow, the end of nightmares great and small.&#0160; Seeing in reality the real sorrows that happen now, so many of which are <a href=\"/lw/wq/you_only_live_twice/\">unnecessary</a> even now.&#0160; Seeing in reality the signs of progress toward a humanity that&#39;s at least <em>trying </em>to get its act together and become something more - even if the signs are mostly just symbolic: a space shuttle launch, a march that protests a war.</p><p>(And of course these are not the only things that move me.&#0160; Not everything that moves me has to be a Cause.&#0160; When I&#39;m listening to e.g. Bach&#39;s <em>Jesu: Joy of Man&#39;s Desiring,</em> I don&#39;t think about how every extant copy might be vaporized if things go wrong.&#0160; That may be true, but it&#39;s not the point.&#0160; It would be as bad as refusing to listen to that melody because it was once inspired by belief in the supernatural.)</p><p>To really have something to protect, you have to be able to <em>protect </em>it, not just <em>value </em>it.&#0160; My battleground for that better Future is, indeed, the fragile pattern of value.&#0160; Not to keep it in stasis, but to keep it <a href=\"/lw/vm/lawful_creativity/\">improving under its own criteria rather than randomly losing information</a>.&#0160; And then to project that through more powerful <a href=\"/lw/rk/optimization_and_the_singularity/\">optimization</a>, to materialize the valuable future.&#0160; Without surrendering a single thing that&#39;s precious, because <a href=\"/lw/y3/value_is_fragile/\">losing a single dimension of value could lose it all</a>.</p><p>There&#39;s no easy way to do this, whether by <em>de novo</em> AI or by editing brains.&#0160; But with a de novo AI, cleanly and correctly designed, I think it should at least be <em>possible</em> to get it truly right and win completely.&#0160; It seems, for all its danger, the safest and easiest and shortest way (yes, the alternatives really are <em>that bad</em>).&#0160; And so that is my project.</p><p>That, then, is the service in which I wield rationality.&#0160; To protect the Future, on the battleground of the physical representation of value.&#0160; And my weapon, if I can master it, is the ultimate hidden technique of Bayescraft - to explicitly and fully know the structure of rationality, to such an extent that you can shape the pure form outside yourself - what some call &quot;Artificial General Intelligence&quot; and I call &quot;Friendly AI&quot;.&#0160; Which is, itself, a major unsolved research problem, and so it calls into play the more informal methods of merely human rationality.&#0160; That is the purpose of my art and the wellspring of my art.</p><p>That&#39;s pretty much all I wanted to say here about this Singularity business...</p><p>...except for one last thing; so after tomorrow, I plan to go back to posting about plain old rationality on Monday.</p><p></p><p></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"pGqRLe9bFDX2G2kXY": 1, "xexCWMyds6QLWognu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3wyMbgeFfQWntFxTf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 32, "extendedScore": null, "score": 4.9e-05, "legacy": true, "legacyId": "1237", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SGR4GxFK7KmW7ckCB", "GNnHHmm8EzePmKzPk", "hwi8JQjspnMWyWs4g", "yEjaj7PWacno5EvWa", "uD9TDHPwQ5hx4CgaX", "iRop8WinYW4B8KYiK", "gb6zWstjmkYHLrbrg", "pLRogvJLPPg6Mrvg4", "EQkELCGiGQwvrrp3L", "QZs4vkC7cbyjL9XA9", "jNAAZ9XNyt82CXosr", "5wMcKNAwB6X4mp9og", "tjH8XPxAnr6JRbh7k", "KKLQp934n77cfZpPn", "Ti3Z7eZtud32LhGZT", "qZJBighPrnv9bSqTZ", "K4aGvLnHvYgX9pZHS", "yKXKcyoBzWtECzXrE", "HFTn3bAT6uXSNwv4m"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-09T00:15:35.000Z", "modifiedAt": null, "url": null, "title": "...And Say No More Of It", "slug": "and-say-no-more-of-it", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:55.651Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SqNvmwDxRibLXjMZN/and-say-no-more-of-it", "pageUrlRelative": "/posts/SqNvmwDxRibLXjMZN/and-say-no-more-of-it", "linkUrl": "https://www.lesswrong.com/posts/SqNvmwDxRibLXjMZN/and-say-no-more-of-it", "postedAtFormatted": "Monday, February 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20...And%20Say%20No%20More%20Of%20It&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A...And%20Say%20No%20More%20Of%20It%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSqNvmwDxRibLXjMZN%2Fand-say-no-more-of-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=...And%20Say%20No%20More%20Of%20It%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSqNvmwDxRibLXjMZN%2Fand-say-no-more-of-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSqNvmwDxRibLXjMZN%2Fand-say-no-more-of-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1450, "htmlBody": "<p><strong>Followup to</strong>:&#0160; <a href=\"/lw/yd/the_thing_that_i_protect/\">The Thing That I Protect</a></p><p>Anything done with an ulterior motive has to be done with a pure heart.&#0160; You cannot serve your ulterior motive, without faithfully prosecuting your <em>overt </em>purpose as a thing in its own right, that has its own integrity.&#0160; If, for example, you&#39;re writing about rationality with the intention of recruiting people to your utilitarian Cause, then you cannot talk too much about your Cause, or you will fail to successfully write about rationality.</p><p>This doesn&#39;t mean that you never say <em>anything</em> about your Cause, but there&#39;s a balance to be struck.&#0160; &quot;A fanatic is someone who can&#39;t change his mind and won&#39;t change the subject.&quot;</p><p>In previous months, I&#39;ve pushed this balance too far toward talking about Singularity-related things.&#0160; And this was for (first-order) selfish reasons on my part; I was finally GETTING STUFF SAID that had been building up painfully in my brain for FRICKIN&#39; YEARS.&#0160; And so I just kept writing, because it was finally coming <em>out</em>.&#0160; For those of you who have not the slightest interest, I&#39;m sorry to have polluted your blog with that.</p><p>When Less Wrong starts up, it will, by my own request, impose a two-month moratorium on discussion of &quot;<span style=\"text-decoration: underline;\"></span><a href=\"http://yudkowsky.net/singularity/ai-risk\">Friendly AI</a>&quot; and other <a href=\"http://yudkowsky.net/singularity/schools\">Singularity/intelligence explosion</a>-related topics.</p><p>There&#39;s a number of reasons for this.&#0160; One of them is simply to restore the balance.&#0160; Another is to make sure that a forum intended to have a more general audience, doesn&#39;t narrow itself down and disappear.</p><p>But more importantly - there are certain subjects which tend to drive people crazy, <em>even if</em> there&#39;s truth behind them.&#0160; <a href=\"/lw/r5/the_quantum_physics_sequence/\">Quantum mechanics</a> would be the paradigmatic example; you don&#39;t <em>have</em> to go funny in the head but a lot of people <em>do.</em>&#0160; Likewise Godel&#39;s Theorem, consciousness, Artificial Intelligence -</p><p>The concept of &quot;Friendly AI&quot; can be <em>poisonous </em>in certain ways.&#0160; True or false, it carries risks to mental health.&#0160; And <em>not</em> just the obvious liabilities of praising a<span style=\"text-decoration: underline;\"> </span><a href=\"/lw/lo/uncritical_supercriticality/\">Happy Thing</a>.&#0160; Something stranger and subtler that <em>drains </em>enthusiasm.</p><a id=\"more\"></a>\n<p>If there were no such problem as Friendly AI, I would probably be\ndevoting more or less my entire life to cultivating human rationality; I would have already been doing it for years.</p><p>And though I could be mistaken - I&#39;m guessing that I would have been <em>much further along</em> by now.</p><p>Partially, of course, because it&#39;s easier to tell people things that\nthey&#39;re already prepared to hear.&#0160; &quot;Rationality&quot; doesn&#39;t command <span style=\"font-style: italic;\"></span><em>universal </em>respect,\nbut it commands wide respect and recognition.&#0160; There is already the New Atheist movement, and the\nBayesian revolution; there are already currents flowing in that\ndirection.</p><p>One has to be wary, in life, of substituting easy problems for hard problems.&#0160; This is a form of <a href=\"/lw/un/on_doing_the_impossible/\">running away</a>.&#0160; &quot;Life is what happens to you while you are making other plans&quot;, and it takes a very strong and non-distractable focus to avoid that...</p><p>But I&#39;d been working on <em>directly </em>launching a Singularity movement for years, and it just wasn&#39;t getting traction.&#0160; At <a href=\"/lw/gx/just_lose_hope_already/\">some point</a> you also have to say, &quot;This isn&#39;t working the way I&#39;m doing it,&quot; and try something different.</p><p>There are many ulterior motives behind my participation in Overcoming Bias / Less Wrong.&#0160; One of the simpler ones is the idea of &quot;First, produce rationalists - people who can shut up and multiply - and then, try to recruit <em>some</em> of them.&quot;&#0160; <em>Not </em>all.&#0160; You do have to care about the rationalist community for its own sake.&#0160; You have to be willing <em>not </em>to recruit all the rationalists you create.&#0160; The first rule of acting with ulterior motives is that it must be done with a pure heart, faithfully serving the overt purpose.</p><p>But more importantly - the whole thing only works if the strange intractability of the <em>direct </em>approach - the mysterious slowness of trying to build an organization <em>directly </em>around the Singularity - does not <em>contaminate</em> the new rationalist movement.</p><p>There&#39;s an old saw about the lawyer who works in a soup kitchen for an hour in order to <a href=\"/lw/hw/scope_insensitivity/\">purchase moral satisfaction</a>, rather than work the same hour at the law firm and donate the money to hire 5 people to work at the soup kitchen.&#0160; Personal involvement isn&#39;t just pleasurable, it keeps people involved; the lawyer is more likely to donate real money to the soup kitchen later.&#0160; Research problems don&#39;t have a lot of opportunity for outsiders to get personally involved, including FAI research.&#0160; (This is why scientific research isn&#39;t usually supported by individuals, I suspect; instead scientists fight over the division of money that has been block-allocated by governments and foundations.&#0160; I should write about this later.)</p><p>If it were the Cause of human rationality - if that had always been the purpose I&#39;d been pursuing - then there would have been all <em>sorts</em> of things people could have done to personally help out, to keep their spirits high and encourage them to stay involved.&#0160; Writing letters to the editor, trying to get heuristics and biases taught in organizations and in classrooms; holding events, handing out flyers; starting a magazine, increasing the number of subscribers; students handing out copies of the &quot;<a href=\"http://yudkowsky.net/rational/virtues\">Twelve Virtues of Rationality</a>&quot; at campus events...</p><p>It might not be too late to start going down that road - <em>but only if</em> the &quot;Friendly AI&quot; meme doesn&#39;t take over and suck out the life and motivation.</p><p>In a purely utilitarian sense - the sort of thinking that would lead a lawyer to actually work that extra hour at the law firm and donate the money - someone who thinks that handing out flyers is important to the Cause of human rationality, should be strictly <em>less</em> enthusiastic than someone who thinks that handing out flyers for human rationality has directly rationality-related benefits <em>and</em> might help a Friendly AI project.&#0160; It&#39;s a strictly added benefit; it should result in strictly more enthusiasm...</p><p>But in practice - it&#39;s as though the idea of &quot;Friendly AI&quot; exerts an attraction that sucks the emotional energy <em>out of its own subgoals.</em></p><p>You only press the &quot;Run&quot; button after you finish coding and teaching a Friendly AI; which happens after the theory has been worked out; which happens after theorists have been recruited; which happens after (a) mathematically smart people have comprehended cognitive naturalism on a deep gut level and (b) a regular flow of funding exists to support these professional specialists; which first requires that the whole project get sufficient traction; for which handing out flyers may be involved...</p><p>But something about the fascination of finally building the AI, seems to make all the <em>mere preliminaries</em> pale in emotional appeal.&#0160; Or maybe it&#39;s that the actual researching takes on an <a href=\"/lw/qs/einsteins_superpowers/\">aura of the sacred magisterium</a>, and then it&#39;s impossible to scrape up enthusiasm for any work outside the sacred magisterium.</p><p>If you&#39;re handing out flyers for the Cause <em>of human rationality</em>... it&#39;s not about a faraway final goal that makes the mere <em>work </em>seem all too mundane by comparison, and there isn&#39;t a sacred magisterium that you&#39;re not part of.</p><p>And this is only a brief gloss on the mental health risks of &quot;Friendly AI&quot;; there are others I haven&#39;t even touched on, though the others are relatively more obvious.&#0160; Import <a href=\"/lw/lp/fake_fake_utility_functions/\">morality</a>.crazy, import <a href=\"/lw/sk/changing_your_metaethics/\">metaethics</a>.crazy, import <a href=\"/lw/qu/a_premature_word_on_ai/\">AI</a>.crazy, import <a href=\"/lw/lv/every_cause_wants_to_be_a_cult/\">Noble Cause</a>.crazy, import <a href=\"/lw/lo/uncritical_supercriticality/\">Happy Agent</a>.crazy, import <a href=\"http://www.overcomingbias.com/2009/01/a-tale-of-two-tradeoffs.html\">Futurism</a>.crazy, etcetera.</p><p>But it boils down to this:&#0160; From my perspective, my participation in Overcoming Bias / Less Wrong has many different ulterior motives, and many different helpful potentials, many potentially useful paths leading out of it.&#0160; But the <em>meme</em> of Friendly AI potentially <em>poisons </em>many of those paths, if it interacts in the wrong way; and so the ability to shut up about the Cause is more than usually important, here.&#0160; Not shut up <em>entirely </em>- but the rationality part of it needs to have its own integrity.&#0160; Part of protecting that integrity is to <em>not</em> inject comments about &quot;Friendly AI&quot; into any post that isn&#39;t directly about &quot;Friendly AI&quot;.</p><p>I would like to see &quot;Friendly AI&quot; be <em>a</em> rationalist Cause sometimes\ndiscussed on Less Wrong, alongside other rationalist Causes whose\nmembers likewise hang out there for companionship and skill acquisition.&#0160; This is\nas much as is necessary to recruit a <em>fraction</em> of the rationalists created.&#0160; Anything more would poison the community, I think.&#0160; Trying to find hooks to steer every arguably-related conversation toward your own Cause is <em>not</em> virtuous, it is dangerously and destructively greedy.&#0160; <em>All</em> Causes represented on LW will have to bear this in mind, on pain of their clever conversational hooks being downvoted to oblivion.</p><p>And when Less Wrong starts up, its integrity will be protected in a simpler way: shut up about the Singularity entirely for two months.</p><p>...and that&#39;s it.</p><p>Back to rationality.</p><p>WHEW.</p><p>(This would be a great time to announce that Less Wrong is ready to go, but they&#39;re still working on it.&#0160; Possibly later this week, possibly not.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "MXcpQvaPGtXpB6vkM": 1, "Xw6pxiicjuv6NJWjf": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SqNvmwDxRibLXjMZN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 37, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "1238", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3wyMbgeFfQWntFxTf", "hc9Eg6erp6hk9bWhn", "NCefvet6X3Sd4wrPc", "fpecAJLG9czABgCe9", "waqC6FihC2ryAZuAq", "2ftJ38y9SRBCBsCzy", "5o4EZJyqmHY4XgRCY", "D6rsNhHM4pBCpDzSb", "LhP2zGBWR5AdssrdJ", "iD5baT42zYAkWJPMB", "yEjaj7PWacno5EvWa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-09T17:26:12.000Z", "modifiedAt": null, "url": null, "title": "(Moral) Truth in Fiction?", "slug": "moral-truth-in-fiction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:30.427Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XnPpn2uSRxvmGfxHq/moral-truth-in-fiction", "pageUrlRelative": "/posts/XnPpn2uSRxvmGfxHq/moral-truth-in-fiction", "linkUrl": "https://www.lesswrong.com/posts/XnPpn2uSRxvmGfxHq/moral-truth-in-fiction", "postedAtFormatted": "Monday, February 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20(Moral)%20Truth%20in%20Fiction%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A(Moral)%20Truth%20in%20Fiction%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXnPpn2uSRxvmGfxHq%2Fmoral-truth-in-fiction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=(Moral)%20Truth%20in%20Fiction%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXnPpn2uSRxvmGfxHq%2Fmoral-truth-in-fiction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXnPpn2uSRxvmGfxHq%2Fmoral-truth-in-fiction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1963, "htmlBody": "<p>A comment by <a href=\"/lw/y9/three_worlds_decide_58/qtv\">Anonymous</a> on <a href=\"/lw/y4/three_worlds_collide_08/\">Three Worlds Collide</a>:</p>\n<blockquote>\n<p>After reading this story I feel myself agreeing with Eliezer more on his views and that seems to be a sign of manipulation and not of rationality.</p>\n<p>Philosophy expressed in form of fiction seems to have a very strong effect on people - even if the fiction isn't very good (ref. Ayn Rand).</p>\n</blockquote>\n<p>Robin has <a href=\"/lw/yc/epilogue_atonement_88/qyw\">similar qualms</a>:</p>\n<blockquote>\n<p>Since people are inconsistent but reluctant to admit that fact, their moral beliefs can be influenced by which moral dilemmas they consider in what order, especially when written by a good writer. I expect Eliezer chose his dilemmas in order to move readers toward his preferred moral beliefs, but why should I expect those are better moral beliefs than those of all the other authors of fictional moral dilemmas?</p>\n<p>If I'm going to read a literature that might influence my moral beliefs, I'd rather read professional philosophers and other academics making more explicit arguments.</p>\n</blockquote>\n<p>I <a href=\"/lw/yc/epilogue_atonement_88/qyz\">replied</a> that I <em>had</em> taken considerable pains to set out the <a href=\"/lw/xi/serious_stories/\">explicit arguments</a> before daring to publish the story.&nbsp; And moreover, I had gone to considerable length to present the Superhappy argument in the best possible light.&nbsp; (The opposing viewpoint is the counterpart of the villain; you want it to look <em>as reasonable as possible</em> for purposes of dramatic conflict, the same principle whereby Frodo confronts the Dark Lord Sauron rather than a cockroach.)</p>\n<p>Robin <a href=\"/lw/yc/epilogue_atonement_88/szu\">didn't</a> find this convincing:</p>\n<blockquote>\n<p>I don't think readers should much let down their guard against communication modes where sneaky persuasion is more feasible simply because the author has made some more explicit arguments elsewhere...&nbsp; Academic philosophy offers exemplary formats and styles for low-sneak ways to argue about values.</p>\n</blockquote>\n<p>I think that this understates the power and utility of fiction.&nbsp; I once read a book that was called something like \"How to Read\" (no, not \"How to Read a Book\") which said that nonfiction was about communicating <em>knowledge,</em> while fiction was about communicating <em>experience.</em></p>\n<p><a id=\"more\"></a></p>\n<p>If I want to communicate something about the <em>experience</em> of being a rationalist, I can best do it by writing a short story with a rationalist character.&nbsp; Not only would identical abstract statements about proper responses have <em>less impact</em>, they wouldn't even communicate the <em>same thought</em>.</p>\n<p>From <a href=\"/lw/q9/the_failures_of_eld_science/\">The Failures of Eld Science</a>:</p>\n<p style=\"margin-left: 40px;\">\"...Work expands to fill the time allotted, as the saying goes.&nbsp; But people can think important thoughts in far less than thirty years, if they <em>expect</em> speed of themselves.\"&nbsp; Jeffreyssai suddenly slammed down a hand on the arm of Brennan's chair.&nbsp; <em></em>\"<em>How long do you have to dodge a thrown knife?</em>\"</p>\n<p style=\"font-style: normal; margin-left: 40px;\">\"Very little time, sensei!\"</p>\n<p style=\"margin-left: 40px;\">\"<em>Less than a second!&nbsp; Two opponents are attacking you!&nbsp; How long do you have to guess who's more dangerous?</em>\"</p>\n<p style=\"font-style: normal; margin-left: 40px;\">\"Less than a second, sensei!\"</p>\n<p style=\"margin-left: 40px;\">\"<em>The two opponents have split up and are attacking two of your girlfriends!&nbsp; How long do you have to decide which one you truly love?</em>\"</p>\n<p style=\"margin-left: 40px;\">\"Less than a second, sensei!\"</p>\n<p style=\"margin-left: 40px;\">\"<em>A new argument shows your precious theory is flawed!&nbsp; How long does it take you to change your mind?</em>\"</p>\n<p style=\"font-style: normal; margin-left: 40px;\">\"Less than a second, sensei!\"</p>\n<p style=\"margin-left: 40px;\">\"<em>WRONG! DON'T GIVE ME THE WRONG ANSWER JUST BECAUSE IT FITS A CONVENIENT PATTERN AND I SEEM TO EXPECT IT OF YOU!&nbsp; </em>How long does it really take, Brennan?\"</p>\n<p style=\"font-style: normal; margin-left: 40px;\">Sweat was forming on Brennan's back, but he stopped and actually thought about it -</p>\n<p style=\"margin-left: 40px;\">\"<em>ANSWER, BRENNAN!\"</em></p>\n<p style=\"margin-left: 40px;\"><em>\"No sensei!&nbsp; I'm not finished thinking sensei!&nbsp; An answer would be premature!&nbsp; Sensei!</em>\"</p>\n<p style=\"margin-left: 40px;\">\"<em>Very good!&nbsp; Continue!&nbsp; But don't take thirty years!</em>\"</p>\n<p>This is an experience about how to avoid <a href=\"/lw/k5/cached_thoughts/\">completing the pattern</a> when the pattern happens to be blatantly wrong, and how to think quickly without thinking <em>too </em>quickly.</p>\n<p>Forget the question of whether you can write the equivalent abstract argument that communicates the same thought in <em>less space.</em>&nbsp; Can you do it at <em>all?</em>&nbsp; Is there <em>any </em>series of abstract arguments that creates the same learning experience in the reader?&nbsp; Entering a series of believed propositions into your belief pool is not the same as feeling yourself in someone else's shoes, and reacting to the experience, and forming an experiential skill-memory of how to do it next time.</p>\n<p>And it seems to me that to communicate experience is a <em>valid form of moral argument</em> as well.</p>\n<p><em>Uncle Tom's Cabin</em> was not just a historically powerful argument against slavery, it was a <em>valid </em> argument against slavery.&nbsp; If human beings were constructed without <a href=\"/lw/xs/sympathetic_minds/\">mirror neurons</a>, if we didn't hurt when we see a nonenemy hurting, then we would exist in the reference frame of a different morality, and we would decide what to do by asking a different question, \"What should* we do?\"&nbsp; Without that ability to sympathize, we might think that it was perfectly all right* to keep slaves.&nbsp; (See <a href=\"/lw/sx/inseparably_right_or_joy_in_the_merely_good/\">Inseparably Right</a> and <a href=\"/lw/t9/no_license_to_be_human/\">No License To Be Human</a>.)</p>\n<p>Putting someone into the shoes of a slave and letting their mirror neurons feel the suffering of a husband separated from a wife, a mother separated from a child, a man whipped for refusing to whip a fellow slave - it's not just persuasive, it's <em>valid</em>.&nbsp; It fires the mirror neurons that physically implement that part of our moral frame.</p>\n<p>I'm sure many have turned against slavery without reading <em>Uncle Tom's Cabin</em> - maybe even due to purely abstract arguments, without ever seeing the carving \"<a href=\"http://www.pbs.org/wgbh/aia/part2/2h67.html\">Am I Not a Man and a Brother?</a>\"&nbsp; But for some people, or for a not-much-different intelligent species, reading <em>Uncle Tom's Cabin</em> might be the <em>only</em> argument that can turn you against slavery.&nbsp; Any amount of abstract argument that <em>didn't</em> fire the experiential mirror neurons, would not activate the part of your implicit should-function that disliked slavery.&nbsp; You would just seem to be making a good profit on something you owned.</p>\n<p>Can fiction be abused?&nbsp; Of course.&nbsp; Suppose that blacks had <a href=\"/lw/pn/zombies_the_movie/\">no subjective experiences</a>.&nbsp; Then <em>Uncle Tom's Cabin</em> would have been a lie in a deeper sense than being fictional, and anyone moved by it would have been deceived.</p>\n<p>Or to give a more subtle case not involving a direct \"lie\" of this sort:&nbsp; On the SL4 mailing list, Stuart Armstrong posted an argument against TORTURE in the infamous <a href=\"/lw/kn/torture_vs_dust_specks/\">Torture vs. Dust Specks</a> debate, consisting of <a href=\"http://www.sl4.org/archive/0810/19404.html\">a short story describing the fate of the person to be tortured</a>.&nbsp; My <a href=\"http://www.sl4.org/archive/0810/19407.html\">reply</a> was that the appropriate counterargument would be <a href=\"http://www.sl4.org/archive/0810/19407.html\">3^^^3</a> stories about someone getting a dust speck in their eye.&nbsp; I actually did try to send a long message consisting only of</p>\n<p style=\"margin-left: 40px;\">DUST SPECK<br /> DUST SPECK<br />DUST SPECK<br /> DUST SPECK<br />DUST SPECK<br /> DUST SPECK</p>\n<p>for a thousand lines or so, but the mailing software stopped it.&nbsp; (Ideally, I should have created a webpage using Javascript and bignums, that, if run on a sufficiently large computer, would print out exactly 3^^^3 copies of a brief story about someone getting a dust speck in their eye.&nbsp; It probably would have been the world's longest finite webpage.&nbsp; Alas, I lack time for many of my good ideas.)</p>\n<p>Then there's the sort of standard polemic used in e.g. <em>Atlas Shrugged</em> (as well as many less famous pieces of science fiction) in which Your Beliefs are put into the minds of strong empowered noble heroes, and the Opposing Beliefs are put into the mouths of evil and contemptible villains, and then the consequences of Their Way are depicted as uniformly disastrous while Your Way offers butterflies and apple pie.&nbsp; That's not even subtle, but it works on people predisposed to hear the message.</p>\n<p>But to entirely turn your back on fiction is, I think, taking it too far.&nbsp; Abstract argument can be abused too.&nbsp; In fact, I would say that abstract argument is if anything <em>easier</em> to abuse because it has <em>more</em> degrees of freedom.&nbsp; Which is easier, to say \"Slavery is good for the slave\", or to write a believable story about slavery benefiting the slave?&nbsp; You <em>can </em>do both, but the second is at least more <em>difficult</em>; your brain is more likely to notice the non-sequiturs when they're played out as a written experience.</p>\n<p>Stories may not get us completely into <a href=\"/lw/xq/getting_nearer/\">Near mode</a>, but they get us <em>closer </em>into Near mode than abstract argument.&nbsp; If it's words on paper, you can end up believing that you <em>ought</em> to do just about anything.&nbsp; If you're in the shoes of a character encountering the experience, your reactions may be harder to twist.</p>\n<p>Contrast a verbal argument against the verbal belief that \"non-Catholics go to Hell\"; versus reading a story about a good and decent person, who happens to be a Protestant, and dies trying to save a child's life, who is condemned to hell and has molten lead poured down her throat; versus the South Park episode where a crowd of newly dead souls is at the entrance to hell, and the Devil says, \"Sorry, it was the Mormons\" and everyone goes \"Awwwww...\"</p>\n<p>Yes, abstraction done <em>right</em> can keep you going where concrete visualization breaks down - the torture vs. dust specks thing being an archetypal example; you can't actually visualize that many dust specks, but if you try to choose SPECKS you'll end up with <a href=\"/lw/n3/circular_altruism/\">circular preferences</a>.&nbsp; But so far as I can organize my metaethics, the ground level of morality lies in our preferences over particular, concrete situations - and when these can be comprehended as concrete images at all, it's best to visualize them as concretely as possible.&nbsp; Unless we know specifically where the concrete image is going wrong, and have to apply an abstract correction.&nbsp; The moral abstraction is built on <em>top</em> of the ground level.</p>\n<p>I am also, of course, worried about the idea that stories aren't \"respectable\" because they don't look sufficiently solemn and dull; or the idea that something isn't \"respectable\" if can be <em>understood</em> by a mere popular audience.&nbsp; Yes, there are technical fields that are genuinely impossible to explain to your grandmother in an hour; but ceteris paribus, people who can write at a more popular level <em>without</em> distorting technical reality are performing a huge service to that field.&nbsp; I've heard that Carl Sagan was held in some disrepute by his peers for the crime of speaking to the general public.&nbsp; If true, this is merely stupid.</p>\n<p>Explaining things is <em>hard.</em>&nbsp; Explainers need every tool they can get their hands on - as a matter of public interest.</p>\n<p>And in <em>moral </em>philosophy - well, I <em>suppose </em>it could be the case that moral philosophers have discovered moral truths that are deductive consequences of most humans' moral frames, but which are so difficult and technical that they simply can't be explained to a popular audience within a one-hour lecture.&nbsp; But it would be a tad more suspicious than the corresponding case in, say, physics.</p>\n<p>I realize that I speak as someone who does a lot of popularizing, but even so - fiction ought to be a respectable form of moral argument.&nbsp; And a respectable way of communicating <em>experiences</em>, in particular the experience of applying certain types of thinking skills.</p>\n<p>I've always been of two minds about publishing longer fiction pieces about the future and its consequences.&nbsp; Not so much because of the potential for abuse, but because even when <em>not </em>abused, fiction can <em>still</em> bypass critical faculties and end up poured directly into the brains of at least some readers.&nbsp; Telling people about <a href=\"/lw/k9/the_logical_fallacy_of_generalization_from/\">the logical fallacy of generalization from fictional evidence</a> doesn't make it go away; people may just go on generalizing from the story as though they had actually seen it happen.&nbsp; And you simply can't have a story that's a rational projection; it's not just a matter of plot, it's a matter of the story needing to be <em>specific</em>, rather than depicting a state of epistemic uncertainty.</p>\n<p>But to make shorter philosophical points?&nbsp; Sure.</p>\n<p>And... oh, what the hell.&nbsp; Just on the off-chance, are there any OB readers who could get a <em>good</em> movie made?&nbsp; Either outside Hollywood, or able to bypass the usual dumbing-down process that creates a money-losing flop?&nbsp; The probabilities are infinitesimal, I know, but I thought I'd check.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 1, "ouT6wKhACJRouGokM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XnPpn2uSRxvmGfxHq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 21, "extendedScore": null, "score": 3.7e-05, "legacy": true, "legacyId": "1239", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 82, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HawFh7RvDM4RyoJ2d", "6qS9q5zHafFXsB6hf", "ZxR8P8hBFQ9kC8wMy", "2MD3NMLBPCqPfnfre", "NLMo5FZWFFq652MNe", "JynJ6xfnpq9oN3zpb", "YrhT7YxkRJoRnr7qD", "fsDz6HieZJBu54Yes", "3wYTFWY3LKQCnAptN", "4KSWmJm6K3EEvHBkd", "4ZzefKQwAtMo5yp99", "rHBdcHGLJ7KvLJQPk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-10T20:22:55.000Z", "modifiedAt": null, "url": null, "title": "Informers and Persuaders", "slug": "informers-and-persuaders", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:30.473Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/njb9cyyzqLTHewups/informers-and-persuaders", "pageUrlRelative": "/posts/njb9cyyzqLTHewups/informers-and-persuaders", "linkUrl": "https://www.lesswrong.com/posts/njb9cyyzqLTHewups/informers-and-persuaders", "postedAtFormatted": "Tuesday, February 10th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Informers%20and%20Persuaders&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInformers%20and%20Persuaders%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnjb9cyyzqLTHewups%2Finformers-and-persuaders%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Informers%20and%20Persuaders%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnjb9cyyzqLTHewups%2Finformers-and-persuaders", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnjb9cyyzqLTHewups%2Finformers-and-persuaders", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1516, "htmlBody": "<p>Suppose we lived in this <em>completely</em> alternate universe where <em>nothing</em> in academia was about status, and no one had any concept of style.&#0160; A universe where people wrote journal articles, and editors approved them, without the <em>tiniest</em> shred of concern for what &quot;impression&quot; it gave - without trying to look serious or solemn or sophisticated, and without being afraid of looking silly or even stupid.&#0160; We shall even suppose that readers, correspondingly, have no such impressions.</p><p>In this simpler world, academics write papers from only two possible motives:</p><p>First, they may have some theory of which they desire to persuade others; this theory may or may not be true, and may or may not be believed for virtuous reasons or with very strong confidence, but the writer of the paper desires to gain adherents for it.</p><p>Second, there will be those who write with an utterly pure and virtuous love of the truthfinding <em>process;</em> they desire solely to give people more unfiltered evidence and to see evidence correctly added up, without a shred of attachment to their or anyone else&#39;s theory.</p><p>People in the first group may want to <em>signal </em>membership in the second group, but people in the second group <em>only </em>want their readers to be well-informed.&#0160; In any case, to first order we must suppose that <em>none </em>of this is about signaling - that all such motives are just blanked out.</p><p>What do journal articles in this world look like, and how do the Persuaders&#39; articles differ from the Informers&#39;?\n</p><a id=\"more\"></a><p>First, I would argue that <em>both</em> groups write much less formal journal articles than our own.&#0160; I&#39;ve read probably around a hundred books on writing (they&#39;re addictive); and they all treated formality as <em>entropy </em>to be fought - a state of disorder into which writing slides.&#0160; It is <em>easier </em> to use big words than small words, <em>easier</em> to be abstract than concrete, <em>easier</em> to use passive -ation words than their active counterparts.&#0160; Perhaps formality <em>first </em>became associated with Authority, back in the dawn of time, because Authorities put in less effort and forced their audience to read anyway.&#0160; Formality became associated with Wisdom by being hard to understand.&#0160; Why suppose that scientific formality was <em>ever</em> about preventing propaganda?\n</p><p>\nBoth groups still use technical language, because they both care about being precise.&#0160; They even use big words or phrases for their technical concepts:&#0160; To carve out <a href=\"/lw/ic/the_virtue_of_narrowness/\">ever-finer slices through reality</a>, you need <em>new </em>words, <em>more </em>words, hence <em>bigger </em>words (so you don&#39;t run out of namespace).</p><p>However, since neither group has a care for their image, they use the simplest\nwords they can apart from that, and sentences as easy as\npossible apart from the big words.&#0160; From our standpoint, their style would seem inconsistent, discongruous.&#0160; A sentence might start with small words that just <em>anyone </em>could\nread, and then terminate in an exceptionally precise structure of\ntechnically sophisticated concepts accessible to only advanced\naudiences.</p><p>In this world it&#39;s not just eminent physicists who - secure in their reputation as Real Scientists - invent labels like &quot;gluon&quot;, &quot;quark&quot;, &quot;black hole&quot; and &quot;Big Bang&quot;.</p><p>Other aspects of scientific taboo may still carry over.&#0160; A Persuader might use vicious insults and character assassination.&#0160; An Informer never would.&#0160; But an Informer might point out - evenhandedly, wherever it happened to be true - that a supposedly relevant paper came from a small unheard-of organization and hadn&#39;t yet been replicated, or that the author of an exciting new paper had previously retracted other results...</p><p>If Persuaders want to <em>look </em>like Informers, they will, of course, restrain their ad-hominem attacks to <em>sounding like</em> the sort of things an Informer might point out; but this is a second-order phenomenon.&#0160; First-order Persuaders would use all-out invective against their opponents.</p><p>What about emotions in general?</p><p>Suppose that there were <em>only</em> Informers and that they weren&#39;t concerned about preventing invasion by Persuaders.&#0160; The Informers might well make a value-laden remark or two in the <em>conclusions</em> of their papers - after balancing the probability that the conclusion would later need to be retracted and that the emotion might interfere, versus the importance of the values in question.&#0160; Even an Informer might say, in the conclusion of a paper on asteroid strikes, &quot;We can probably breathe a sigh of relief about getting hit in the immediate future, but when you consider the sheer size of the catastrophe and the millions of dead and injured, we really ought to have a spacewatch program.&quot;</p><p>But Persuaders have an <em>immensely</em> stronger first-order drive to invoke powerful affective emotions and lade the reader&#39;s judgments with value.&#0160; To second order, Persuaders will try to <em>disguise</em> this method as much as possible - let the reader draw conclusions, so long as they&#39;re the <em>desired</em> conclusions - try to <em>pretend</em> to abstract dispassionate <em>language </em>so that they can look like Informers, while still lingering on the emotion-arousing <em>facts</em>.&#0160; Formality is a very <em>easy</em> disguise to wear, which is one reason I give it so little credit.</p><p>Informers, who have no desire to look like Informers, might go ahead and leave in a value judgment or two that seemed really unlikely to interfere with anyone&#39;s totting up the evidence.&#0160; If Informers trusted their own judgment about that sort of thing, that is.</p><p>(Persuaders and Informers writing about <em>policy </em>arguments or <em>moral </em>arguments would be a whole &#39;nother class of issue.&#0160; Then both types are offering value-laden arguments and dealing in facts that trigger emotions, and the question is who&#39;s collecting them <em>evenhandedly </em>versus lopsidedly.)</p><p>How about writing short stories?</p><p>Persuaders obviously have a motive to do it.&#0160; Do Informers ever do it, if they&#39;re not worried about looking like Persuaders?</p><p>If you try to blank out the conventions of our own world, and imagine what would really be <em>useful...</em></p><p>Then I can well imagine that it would be <em>de rigueur</em> to write small stories - story fragments, rather - describing the elements of your experimental procedure.</p><p>&quot;The subjects were admininistered Progenitorivox&quot; actually gives you very little information, just the dull sensation of having been told an authoritative fact.</p><p>Compare:&#0160; &quot;James is one of my typical subjects.&#0160; Every Wednesday, he would visit me in my lab at 2pm, and, grimacing, swallow down two yellow pills from his bottle, while I watched.&#0160; At the end of the study, I watched James and the other students file into the classroom, sit down, and fill out the surveys on each desk; as they left, I gave each of them a check for $50.&quot;</p><p>This, which conveys something of the <em>experience </em>of running the experiment and just begs you to go out and do your own... also gives you valuable information: that the Progenitorivox or placebo was taken at regular intervals with the investigator watching, and when and where and how the survey data was collected.</p><p>Maybe this is the most efficient way to communicate that information, and maybe not.&#0160; To me it actually does seem efficient, and I would guess that the only reason people don&#39;t do this more often is that they would look insufficiently Distant and Authoritative.&#0160; I have no trouble imagining an Informer writing a story fragment or two into their journal article.</p><p>Robin <a href=\"http://www.overcomingbias.com/2009/02/against-propaganda-.html\">says</a>:&#0160; &quot;I thus tend to avoid emotion, color, flash, stories, vagueness, repetition, rambling, and even eloquence.&quot;</p><p>I would guess that, to first order and before all signaling:</p><p>Persuaders actively seek out emotion, color, flash, and eloquence.&#0160; They are vague when they have something to hide.&#0160; They rehearse their favored arguments, but not to where it becomes annoying.&#0160; They try to avoid rambling because no one wants to read that.&#0160; They use stories where they expect stories to be persuasive - which, by default, they are - and <em>avoid</em> stories where they don&#39;t want their readers visualizing things in too much detail.</p><p>Informers avoid emotions that they fear may bias their readers.&#0160; If they can&#39;t actually avoid the emotion - e.g., the paper is about slavery - they&#39;ll explicitly point it out, along with its potential biasing effect, and they&#39;ll go to whatever lengths they can to avoid provoking or inflaming the emotion further (short of actually obscuring the subject matter).&#0160; Informers may use color to highlight the most important parts of their article, but won&#39;t usually give extra color to a single piece of specific evidence.&#0160; Informers have no use for flash.&#0160; They won&#39;t <em>avoid </em>being eloquent when their discussion happens to have an elegant logical structure.&#0160; Informers use the appropriate level of abstraction or maybe a little lower; they are vague when details are unknowable or almost certainly irrelevant.&#0160; Informers don&#39;t <a href=\"/lw/ik/one_argument_against_an_army/\">rehearse evidence</a>, but they might find it useful to repeat some details of the experimental procedure.&#0160; Informers use stories when they have an important experience to communicate, or when a set of details is most easily conveyed in story form.&#0160; In papers that are about judgments of simple fact, Informers never use a story to arouse emotion.</p><p>I finally note, with regret, that in a world containing Persuaders, it may make sense for a second-order Informer to be deliberately eloquent if the issue has already been obscured by an eloquent Persuader - just exactly <em>as</em> elegant as the previous Persuader, no more, no less.&#0160; It&#39;s a pity that this wonderful excuse exists, but in the real world, well...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZXFpyQWPB5ideFbEG": 2, "P64rmDCvTBAehmkoi": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "njb9cyyzqLTHewups", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 29, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "1240", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yDfxTj9TKYsYiWH5o", "WN73eiLQkuDtSC8Ag"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-11T15:06:44.000Z", "modifiedAt": null, "url": null, "title": "Cynicism in Ev-Psych (and Econ?)", "slug": "cynicism-in-ev-psych-and-econ", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:30.471Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KC5qGJiWSxt9zpyDy/cynicism-in-ev-psych-and-econ", "pageUrlRelative": "/posts/KC5qGJiWSxt9zpyDy/cynicism-in-ev-psych-and-econ", "linkUrl": "https://www.lesswrong.com/posts/KC5qGJiWSxt9zpyDy/cynicism-in-ev-psych-and-econ", "postedAtFormatted": "Wednesday, February 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cynicism%20in%20Ev-Psych%20(and%20Econ%3F)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACynicism%20in%20Ev-Psych%20(and%20Econ%3F)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKC5qGJiWSxt9zpyDy%2Fcynicism-in-ev-psych-and-econ%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cynicism%20in%20Ev-Psych%20(and%20Econ%3F)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKC5qGJiWSxt9zpyDy%2Fcynicism-in-ev-psych-and-econ", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKC5qGJiWSxt9zpyDy%2Fcynicism-in-ev-psych-and-econ", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1077, "htmlBody": "<p>Though I know more about the former than the latter, I begin to suspect that different styles of cynicism prevail in <a href=\"/lw/l1/evolutionary_psychology/\">evolutionary psychology</a> than in microeconomics.</p>\n<p>Evolutionary psychologists are absolutely and uniformly cynical about the real reason why humans are universally wired with a chunk of complex purposeful functional circuitry X (e.g. an emotion) - we have X because it increased inclusive genetic fitness in the ancestral environment, full stop.</p>\n<p>Evolutionary psychologists are mildly cynical about the environmental circumstances that activate and maintain an emotion.&nbsp; For example, if you fall in love with the body, mind, and soul of some beautiful mate, an evolutionary psychologist would like to check up on you in ten years to see whether the degree to which you think your mate's mind is still beautiful, correlates with independent judges' ratings of how physically attractive that mate still is.</p>\n<p>But it wouldn't be<em> conventionally ev-psych cynicism </em>to suppose that you don't <em>really</em> love your mate, and that you were actually just attracted to their body all along, but that instead you told yourself a self-deceiving story about virtuously loving them for their mind, in order to falsely signal commitment.</p>\n<p>Robin, on the other hand, often seems to think that this general type of cynicism is the default explanation and that anything else bears a burden of proof - why suppose an explanation that invokes a genuine virtue, when a selfish desire will do?</p>\n<p>Of course my experience with having deep discussions with economists mostly consists of talking to Robin, but I suspect that this is at least partially reflective of a difference between the ev-psych and economic notions of <em>parsimony.</em></p>\n<p>Ev-psychers are trying to be parsimonious with how complex of an adaptation they postulate, and how cleverly complicated they are supposing natural selection to have been.</p>\n<p>Economists... well, it's not my field, but maybe they're trying be parsimonious by having just a few simple motives that play out in complex ways via consequentialist calculations?</p>\n<p><a id=\"more\"></a></p>\n<p>Quoth Leda Cosmides and John Tooby (famous EPers):&nbsp;</p>\n<p style=\"margin-left: 40px;\">\"The science of understanding living organization is very different from physics or chemistry, where parsimony makes sense as a theoretical criterion.&nbsp; The study of organisms is more like reverse engineering, where one may be dealing with a large array of very different components whose heterogeneous organization is explained by the way in which they interact to produce a functional outcome.&nbsp; Evolution, the constructor of living organisms, has no privileged tendency to build into designs principles of operation that are simple and general.\"</p>\n<p>One consequence of this is that it's more <em>parsimonious</em> - under the evolutionary prior - to postulate many smaller simpler adaptations than one big clever complicated adaptation.</p>\n<p>One simple way to signal quality X is by <em>having</em> quality X.&nbsp; But then other simple modifications might accrete around that.</p>\n<p>So cynicism in the style of evolutionary psychology might be, \"Why yes, so far as your explicit cognition is concerned, you love them for their beautiful mind.&nbsp; It's just that without the beautiful body, you probably wouldn't find yourself loving their mind so much.&nbsp; And once the beautiful body fades, you may find their ideas appearing less attractive too.\"&nbsp; Mind you, this is not an actual experimental result.&nbsp; It's just the sort of thing that a cynical evolutionary psychologist would look for - a cross-wiring between a couple of emotional circuits.</p>\n<p>Even then, the cynic would bear a burden of proof, because a devil's-advocate parsimonious evolutionary psychologist would say, \"How do you know the extra circuit is there?&nbsp; Maybe evolution wasn't that clever.&nbsp; Mates looked for signals of long-term commitment, so their partners evolved an actual long-term commitment mechanism when a mate was of high enough quality - can you show me an experiment that demonstrates it's any more complicated than that?\"</p>\n<p>By and large, evolutionary psychologists don't expect <em>people</em> to be clever, just evolution.&nbsp; It's a foundational assumption that there's no explicit cognitive desire to increase inclusive genetic fitness, and no reason to think that anyone (except a professional evolutionary psychologist) would <em>explicitly know in advance which behaviors</em> increased fitness in the ancestral environment.&nbsp; The organism, rather than being programmed with machiavellian subconscious long-term knowledge, is programmed with (genuine) emotions that activate under the right circumstances to steer them the right way (in the ancestral environment).</p>\n<p>In economics, perhaps, it is more conventional and less alarming to suppose that people are doing <em>explicitly </em>clever and complicated things in the pursuit of explicit goals.&nbsp; But of this it is not really my place to speak; I'm just trying to describe my own side of the contrast I see.</p>\n<p>Now it makes sense to suppose that we have certain general faculties - simple emotional circuits - that make us seek high status: that we are magnetically attracted to behaviors whenever we imagine that behavior will make others look on us fondly.</p>\n<p>And it makes to suppose that we have a general faculty - a relatively simple emotional circuit - that makes us flinch away from explanations and views of our own behavior that put us in a negative light, and flinch <em>toward </em>explanations that put ourselves in a positive light.</p>\n<p>So from an ev-psych standpoint, we can expect a lot of cynicism to be, in general, justified.&nbsp; It wouldn't even be surprising if people were relatively more attracted to bodies, and relatively less attracted to minds, on a purely psychological level, than they said/thought they were.</p>\n<p>But to modify the emotional ontology by entirely deleting virtuous emotions... to say that, even on a psychological level, no human being was ever attracted to a mate's mind, nor ever wanted to <em>be honest</em> in a business transaction and not just <em>signal honesty</em>... is not quite what evolutionary psychologists <em>do,</em> most of the time.&nbsp; They are out to come up with an evolutionary explanation of why humans have the standard emotions, rather than telling us that we have nonstandard emotions instead.&nbsp; Maybe in economics this sounds less alarming because people routinely come up with simplified models?&nbsp; But in ev-psych it seems to hearken back to the <a href=\"/lw/qz/living_in_many_worlds/\">bad old Freudian days of counterintuitiveness</a> - we're excited that the intuitive view of human emotion turns out to <em>be</em> evolutionarily explainable.&nbsp; Including a lot of things that people would rather not talk about but which they <em>do</em> recognize as realistic.&nbsp; And including a lot of phenomena that go on behind the scenes but which don't much change our view of <em>which </em>emotions we have, just our view of <em>when</em> emotions activate, in what real context.&nbsp; On <em>that </em>score we are happy to be cynical and challenge intuition.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"exZi6Bing5AiM4ZQB": 7, "Q6P8jLn8hH7kbuXRr": 1, "5f5c37ee1b5cdee568cfb16a": 1, "iP2X4jQNHMWHRNPne": 6}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KC5qGJiWSxt9zpyDy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 29, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "1241", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["epZLSoNvjW53tqNj9", "qcYCAxYZT4Xp9iMZY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-12T16:44:43.000Z", "modifiedAt": null, "url": null, "title": "The Evolutionary-Cognitive Boundary", "slug": "the-evolutionary-cognitive-boundary", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:30.470Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o8Bh82hKGpRNA2q36/the-evolutionary-cognitive-boundary", "pageUrlRelative": "/posts/o8Bh82hKGpRNA2q36/the-evolutionary-cognitive-boundary", "linkUrl": "https://www.lesswrong.com/posts/o8Bh82hKGpRNA2q36/the-evolutionary-cognitive-boundary", "postedAtFormatted": "Thursday, February 12th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Evolutionary-Cognitive%20Boundary&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Evolutionary-Cognitive%20Boundary%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8Bh82hKGpRNA2q36%2Fthe-evolutionary-cognitive-boundary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Evolutionary-Cognitive%20Boundary%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8Bh82hKGpRNA2q36%2Fthe-evolutionary-cognitive-boundary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8Bh82hKGpRNA2q36%2Fthe-evolutionary-cognitive-boundary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 871, "htmlBody": "<p>I tend to draw a <em>very</em> sharp line between anything that happens inside a brain and anything that happened in evolutionary history.&nbsp; There are good reasons for this!&nbsp; Anything <em>originally</em> computed in a brain can be expected to be <em>recomputed</em>, on the fly, in response to changing circumstances.</p>\n<p>Consider, for example, the hypothesis that managers behave rudely toward subordinates \"to signal their higher status\".&nbsp; This hypothesis then has two natural subdivisions:</p>\n<p>If <em>rudeness</em> is an executing adaptation <em>as such</em> - something <em>historically</em> linked to the fact it signaled high status, but not <em>psychologically</em> linked to status drives - then we might experiment and find that, say, the rudeness of high-status men to lower-status men depended on the number of desirable women watching, but that they weren't aware of this fact.&nbsp; Or maybe that people are just as rude when posting completely anonymously on the Internet (or more rude; they can now indulge their adapted penchant to be rude <em>without</em> worrying about the now-nonexistent reputational consequences).</p>\n<p>If rudeness is a conscious or subconscious strategy to signal high status (which is itself a universal adapted desire), then we're more likely to expect the style of rudeness to be culturally variable, like clothes or jewelry; different kinds of rudeness will send different signals in different places.&nbsp; People will be most likely to be rude (in the culturally indicated fashion) in front of those whom they have the greatest psychological desire to impress with their own high status.</p>\n<p><a id=\"more\"></a></p>\n<p>When someone says, \"People do X to signal Y\", I tend to hear, \"People do X when they consciously or subconsciously expect it to signal Y\", not, \"Evolution built people to do X as an adaptation that executes given such-and-such circumstances, because in the ancestral environment, X signaled Y.\"</p>\n<p>I apologize, Robin, if this means I misunderstood you.&nbsp; But I think it <em>really is</em> important to use different words that draw a hard boundary between the evolutionary computation and the cognitive computation - \"People are adapted to do X because it signaled Y\", versus \"People do X because they expect it to signal Y\".</p>\n<p>\"Distal cause\" and \"proximate cause\" doesn't seem good enough, when there's such a sharp boundary within the causal network about what gets computed, how it got computed, and when it will be recomputed.&nbsp; Yes, we have epistemic leakage across this boundary - we can try to fill in our leftover uncertainty about psychology using evolutionary predictions - but it's epistemic leakage between two very different subjects.</p>\n<p>I've noticed that I am, in general, less cynical than Robin, and I would <em>offer up the guess for refutation</em> (it is dangerous to reason about other people's psychologies) that Robin doesn't draw a sharp boundary <em>across his cynicism</em> at the evolutionary-cognitive boundary.&nbsp; When Robin asks \"Are people doing X mostly for the sake of Y?\" he seems to answer the same \"Yes\", and feel more or less the same way about that answer, whether or not the reasoning goes through an evolutionary step along the way.</p>\n<p>I would be very disturbed to learn that parents, in general, showed no grief for the loss of a child who they consciously believed to be sterile.&nbsp; The <em>actual </em>experiment which shows that parental grief correlates strongly to the expected reproductive potential of a child of that age in a <em>hunter-gatherer</em> society - <em>not</em> the different reproductive curve in a modern society - does <em>not </em>disturb me.</p>\n<p>There was a point more than a decade ago when I would have seen that as a puppeteering of human emotions by evolutionary selection pressures, and hence something to be cynical about.&nbsp; Yet how could parental grief come into existence <em>at all, </em>without a strong enough selection pressure to <em>carve it into the genome from scratch?</em>&nbsp; All that should matter for saying \"The parent truly cares about the child\" is that the grief in the parent's mind is cognitively real and unconditional and not even subconsciously for the sake of any ulterior motive; and so it does not update for modern reproductive curves.</p>\n<p><em>Of course</em> the emotional circuitry is ultimately there for evolutionary-historical reasons.&nbsp; But only conscious or subconscious computations can gloom up my day; natural selection is an alien thing whose 'decisions' can't be the target of my cynicism or admiration.</p>\n<p>I suppose that is a merely <em>moral </em>consequence - albeit it's one that I care about quite a lot.&nbsp; Cynicism <em>does </em>have hedonic effects.&nbsp; Part of my grand agenda that I have to put forward about rationality, has to do with arguing against many various propositions \"Rationality should make us cynical about X\" (e.g. \"<a href=\"/lw/r0/thou_art_physics/\">physical lawfulness -&gt; choice is a meaningless illusion</a>\") that I happen to disagree with.&nbsp; So you can see why I'm concerned about drawing the proper boundary of cynicism around evolutionary psychology (especially since I think the proper boundary is a sharp full stop).</p>\n<p>But the same boundary also has major consequences for what we can expect people to recompute or not recompute - for the way that future behaviors will change as the environment changes.&nbsp; So once again, I advocate for language that separates out evolutionary causes and clearly labels them, <em>especially </em>in discussions of signaling.&nbsp; It has major effects, not just on how cynical I end up about human nature, but on what 'signaling' behaviors to expect, when.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"exZi6Bing5AiM4ZQB": 1, "5f5c37ee1b5cdee568cfb16a": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o8Bh82hKGpRNA2q36", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 46, "extendedScore": null, "score": 7e-05, "legacy": true, "legacyId": "1242", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 46, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NEeW7eSXThPz7o4Ne"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-13T14:58:18.000Z", "modifiedAt": "2020-03-25T00:03:18.019Z", "url": null, "title": "An Especially Elegant Evpsych Experiment", "slug": "an-especially-elegant-evpsych-experiment", "viewCount": null, "lastCommentedAt": "2021-03-11T19:39:47.056Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J4vdsSKB7LzAvaAMB/an-especially-elegant-evpsych-experiment", "pageUrlRelative": "/posts/J4vdsSKB7LzAvaAMB/an-especially-elegant-evpsych-experiment", "linkUrl": "https://www.lesswrong.com/posts/J4vdsSKB7LzAvaAMB/an-especially-elegant-evpsych-experiment", "postedAtFormatted": "Friday, February 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20Especially%20Elegant%20Evpsych%20Experiment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20Especially%20Elegant%20Evpsych%20Experiment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ4vdsSKB7LzAvaAMB%2Fan-especially-elegant-evpsych-experiment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20Especially%20Elegant%20Evpsych%20Experiment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ4vdsSKB7LzAvaAMB%2Fan-especially-elegant-evpsych-experiment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ4vdsSKB7LzAvaAMB%2Fan-especially-elegant-evpsych-experiment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1150, "htmlBody": "<blockquote>In a 1989 Canadian study, adults were asked to imagine the death of children of various ages and estimate which deaths would create the greatest sense of loss in a parent. The results, plotted on a graph, show grief growing until just before adolescence and then beginning to drop. When this curve was compared with a curve showing changes in reproductive potential over the life cycle (a pattern calculated from Canadian demographic data), the correlation was fairly strong. But much stronger - nearly perfect, in fact - was the correlation between the grief curves of these modern Canadians and the reproductive-potential curve of a hunter-gatherer people, the !Kung of Africa. In other words, the pattern of changing grief was almost exactly what a Darwinian would predict, given demographic realities in the ancestral environment...  The first correlation was .64, the second an extremely high .92.</blockquote><p>(Robert Wright, summarizing:  &quot;Human Grief:  Is Its Intensity Related to the Reproductive Value of the Deceased?&quot;  Crawford, C. B., Salter, B. E., and Lang, K.L.  <em>Ethology and Sociobiology</em> 10:297-307.)</p><p>Disclaimer:  I haven&apos;t read this paper because it (a) isn&apos;t online and (b) is not specifically relevant to my actual real job.  But going on the given description, it seems like a reasonably awesome experiment.  [Gated version <a href=\"https://www.sciencedirect.com/science/article/abs/pii/016230958990006X\">here</a>, thanks Benja Fallenstein.  Odd, I thought I searched for that.  Reading now... seems to check out on the basics.  Correlations are as described, N=221.]</p><p>The most obvious <em>in</em>elegance of this study, as described, is that it was conducted by asking human adults to imagine parental grief, rather than asking real parents with children of particular ages.  (Presumably that would have cost more / allowed fewer subjects.)  However, my understanding is that the results here squared well with the data from closer studies of parental grief that were looking for other correlations (i.e., a raw correlation between parental grief and child age).</p><p>That said, consider some of this experiment&apos;s elegant aspects:</p><ul><li>A correlation of .92(!)  This may sound suspiciously high - could evolution really do such exact fine-tuning? - until you realize that this selection pressure was not only great enough to <em>fine-tune</em> parental grief, but, in fact, <em>carve it out of existence from scratch in the first place.</em></li><li>People who say that evolutionary psychology hasn&apos;t made any advance predictions are (ironically) mere victims of &quot;<a href=\"http://www.overcomingbias.com/2007/10/no-one-knows-wh.html\">no one knows what science doesn&apos;t know</a>&quot; syndrome.  You wouldn&apos;t even <em>think of this as an experiment to be performed</em> if not for evolutionary psychology.</li><li>The experiment illustrates as beautifully and as cleanly as any I have ever seen, the distinction between a <em>conscious or subconscious ulterior motive</em> and an <em><a href=\"http://www.overcomingbias.com/2007/11/adaptation-exec.html\">executing adaptation</a> with no realtime sensitivity to the original selection pressure that created it.</em></li></ul><p>The parental grief is <em>not even subconsciously</em> about reproductive value - otherwise it would update for Canadian reproductive value instead of !Kung reproductive value.  Grief is an adaptation that now simply exists, real in the mind and continuing under its own inertia.</p><p>Parents do <em>not</em> care about children for the sake of their reproductive contribution.  Parents care about children for their own sake; and the <em>non-cognitive, evolutionary-historical</em> reason <a href=\"http://www.overcomingbias.com/2008/07/moral-miracle-o.html\">why such minds exist in the universe in the first place</a>, is that children carry their parents&apos; genes.</p><p>Indeed, evolution is the reason why there are any minds in the universe at all.  So you can see why I&apos;d want to draw a sharp line through my <a href=\"http://www.overcomingbias.com/2009/02/cynicism-in-evpsych-and-econ.html\">cynicism about ulterior motives</a> at the <a href=\"http://www.overcomingbias.com/2009/02/the-evolutionarycognitive-boundary.html\">evolutionary-cognitive boundary</a>; otherwise, I might as well stand up in a supermarket checkout line and say, &quot;Hey!  You&apos;re only correctly processing visual information while bagging my groceries in order to maximize your inclusive genetic fitness!&quot;</p><p>&#x2022; 0.92 is, I think, the highest correlation I&apos;ve ever seen in any ev-psych experiment, and indeed, one of the highest correlations I&apos;ve seen in any psychology experiment.  (Albeit I&apos;ve seen e.g. a correlation of 0.98 reported for asking one group of subjects &quot;<a href=\"http://www.overcomingbias.com/2007/09/burdensome-deta.html\">How similar is A to B?</a>&quot; and another group &quot;What is the probability of A given B?&quot; on questions like &quot;How likely are you to draw 60 red balls and 40 white balls from this barrel of 800 red balls and 200 white balls?&quot; - in other words, these are simply processed as the same question.)</p><p>Since we are all Bayesians here, we may take our priors into account and ask if at least <em>some </em>of this unexpectedly high correlation is due to luck.  The evolutionary fine-tuning we can probably take for granted; this is a huge selection pressure we&apos;re talking about.  The remaining sources of suspiciously low variance are, (a) whether a large group of adults could correctly envision, on average, relative degrees of parental grief (apparently they can).  And, (b), whether the surviving !Kung are <em>typical </em>ancestral hunter-gatherers in this dimension, or whether variance <em>between </em>hunter-gatherer tribal types should have been too high to allow a correlation of .92.</p><p>But even after taking into account any skeptical priors, correlation .92 and N=221 is pretty strong evidence, and our posteriors should be less skeptical on all these counts.</p><p>&#x2022; You might think it an inelegance of the experiment that it was performed <em>prospectively</em> on imagined grief, rather than retrospectively on real grief.  But it is <em>prospectively </em> imagined grief that will actually operate to steer parental behavior <em>away</em> from losing the child!  From an evolutionary standpoint, an actual dead child is a sunk cost; evolution &quot;wants&quot; the parent to learn from the pain, not do it again, adjust back to their <a href=\"http://www.overcomingbias.com/2009/01/better-and-better.html\">hedonic set point</a>, and go on raising other children.</p><p>&#x2022; Similarly, the graph that correlates to parental grief is for <em>the future reproductive potential of a child that has survived to a given age,</em> and not the <em>sunk cost of raising the child which has survived to that age.</em>  (Might we get an even <em>higher </em>correlation if we tried to take into account the reproductive opportunity cost of raising a child of age X to independent maturity, while discarding all sunk costs to raise a child to age X?)</p><p>Humans usually do notice sunk costs - this is presumably either an adaptation to prevent us from switching strategies too often (compensating for an overeager opportunity-noticer?) or an unfortunate spandrel of pain felt on wasting resources.</p><p>Evolution, on the other hand - it&apos;s not that evolution &quot;doesn&apos;t care about sunk costs&quot;, but that evolution doesn&apos;t even remotely &quot;think&quot; that way; &quot;evolution&quot; is just a macrofact about the real historical reproductive consequences.</p><p>So - of course - the parental grief adaptation is fine-tuned in a way that has nothing to do with past investment in a child, and everything to do with the future reproductive consequences of losing that child.  Natural selection isn&apos;t crazy about sunk costs the way we are.</p><p>But - of course - the parental grief adaptation goes on functioning as if the parent were living in a !Kung tribe rather than Canada.  Most humans would notice the difference.</p><p>Humans and natural selection are insane in <em>different stable complicated ways</em>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"exZi6Bing5AiM4ZQB": 7}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J4vdsSKB7LzAvaAMB", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 64, "extendedScore": null, "score": 9.7e-05, "legacy": true, "legacyId": "1243", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "MH2b8NfWv22dBtrs8", "canonicalCollectionSlug": "rationality", "canonicalBookId": "wAXodw6LPScjrdnkR", "canonicalNextPostSlug": "superstimuli-and-the-collapse-of-western-civilization", "canonicalPrevPostSlug": "evolutionary-psychology", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 64, "bannedUserIds": null, "commentsLocked": false, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-14T16:14:57.000Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes 26", "slug": "rationality-quotes-26", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:04.878Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3Jn4voRWzqboq2M38/rationality-quotes-26", "pageUrlRelative": "/posts/3Jn4voRWzqboq2M38/rationality-quotes-26", "linkUrl": "https://www.lesswrong.com/posts/3Jn4voRWzqboq2M38/rationality-quotes-26", "postedAtFormatted": "Saturday, February 14th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%2026&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%2026%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3Jn4voRWzqboq2M38%2Frationality-quotes-26%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%2026%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3Jn4voRWzqboq2M38%2Frationality-quotes-26", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3Jn4voRWzqboq2M38%2Frationality-quotes-26", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 372, "htmlBody": "<p>\"Poets, philosophers, acidheads, salesmen: everybody wants to know, 'What is Reality?'&nbsp; Some say it's a vast Unknowable so astounding and raw and naked that it grips the human mind and shakes it like a puppy shakes a rag doll.&nbsp; A lot of good that does us.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- The Book of the SubGenius</p>\n<p>\"When they discovered that reality was more complicated than they thought, they just swept the complexity under a carpet of epicycles.&nbsp; That is, they created unnecessary complexity.&nbsp; This is an important point.&nbsp; The universe is complex, but it's usefully complex.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <a href=\"http://www.oreilly.com/catalog/opensources/book/larry.html\">Larry Wall</a></p>\n<p>\"I can't imagine a more complete and precise answer to the question 'for what reason...?' than 'none'.&nbsp; The fact that you don't like the answer is your problem, not the universe's.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Lee Daniel Crocker</p>\n<p>\"In the end they all moved in fantasies and not in the daily tide of their seemingly useless lives.&nbsp; Souls forever lost in the terrifying freedom of their existence.\"<br /> &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <a href=\"http://www.fanfiction.net/s/3886999/Shinji_and_Warhammer40k\">Shinji and Warhammer40k</a></p>\n<p>\"Thus the <em>freer </em>the judgement of a man is in regard to a definite issue, with so much greater <em>necessity</em> will the substance of this judgement be determined.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Friedrich Engels, <em>Anti-D&uuml;hring</em></p>\n<p><a id=\"more\"></a></p>\n<p>\"There will always be some that cannot be saved.<br />&nbsp;It is impossible to save everyone.<br />&nbsp;If I have to lose five hundred to earn one thousand,<br />&nbsp;I will abandon one hundred and save the lives of nine hundred.<br />&nbsp;That is the most efficient method.<br />&nbsp;That is the ideal----<br />&nbsp;Kiritsugu once said that.<br />&nbsp;Of course I got mad.<br />&nbsp;I really got mad.<br />&nbsp;Because I knew that without being told.<br />&nbsp;Because I myself was saved like that.<br />&nbsp;I don't even need to be told something as obvious as that.<br />&nbsp;But still----I believed that someone would be a superhero if they saved everyone even though they think like that.<br />&nbsp;It may be an idealistic thought or an impossible pipe dream, but a superhero is someone who tries to save everyone in spite of that.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Emiya Shirou, in <em>Fate/stay night</em><br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (visual novel by Kinoko Nasu; Unlimited Blade Works path, Mirror Moon translation)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3Jn4voRWzqboq2M38", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 4, "extendedScore": null, "score": 4.7550739243254777e-07, "legacy": true, "legacyId": "1244", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-16T01:00:51.000Z", "modifiedAt": null, "url": null, "title": "An African Folktale", "slug": "an-african-folktale", "viewCount": null, "lastCommentedAt": "2021-07-26T15:31:29.006Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yyteh3qwD6kjhLiCb/an-african-folktale", "pageUrlRelative": "/posts/yyteh3qwD6kjhLiCb/an-african-folktale", "linkUrl": "https://www.lesswrong.com/posts/yyteh3qwD6kjhLiCb/an-african-folktale", "postedAtFormatted": "Monday, February 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20African%20Folktale&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20African%20Folktale%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyyteh3qwD6kjhLiCb%2Fan-african-folktale%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20African%20Folktale%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyyteh3qwD6kjhLiCb%2Fan-african-folktale", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyyteh3qwD6kjhLiCb%2Fan-african-folktale", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 809, "htmlBody": "<p>This is a folktale of the Hausa, a farming culture of around 30 million people, located primarily in Nigeria and Niger but with other communities scattered around Africa.&nbsp; I find the different cultural assumptions revealed to be... attention-catching; you wouldn't find a tale like this in Aesop.&nbsp; From <a href=\"http://books.google.com/books?id=-qAz2YUf0qoC&amp;pg=PA146&amp;lpg=PA146#PPA145,M1\">Hausa Tales and Traditions</a> by Frank Edgar and Neil Skinner; HT Robert Greene.</p>\n<p style=\"text-align: center; margin-left: 40px;\">The Farmer, the Snake and the Heron</p>\n<p style=\"margin-left: 40px;\">&nbsp;&nbsp;&nbsp; There was once a man hoeing away on his farm, when along came some people chasing a snake, meaning to kill it.&nbsp; And the snake came up to the farmer.<br />&nbsp;&nbsp;&nbsp; Says the snake \"Farmer, please hide me.\"&nbsp; \"Where shall I hide you?\" said the farmer, and the snake said \"All I ask is that you save my life.\"&nbsp; The farmer couldn't think where to put the snake, and at last bent down and opened his anus, and the snake entered.<br />&nbsp;&nbsp;&nbsp; Presently the snake's pursuers arrived and said to the farmer \"Hey, there!&nbsp; Where's the snake we were chasing and intend to kill?&nbsp; As we followed him, he came in your direction.\"&nbsp; Says the farmer \"I haven't seen him.\"&nbsp; And the people went back again.<br />&nbsp;&nbsp;&nbsp; Then the farmer said to the snake \"Righto - come out now.&nbsp; They've gone.\"&nbsp; \"Oh no\" said the snake, \"I've got me a home.\"&nbsp; And there was the farmer, with his stomach all swollen, for all the world like a pregnant woman!</p>\n<p><a id=\"more\"></a></p>\n<p style=\"margin-left: 40px;\">&nbsp;&nbsp;&nbsp; And the farmer set off, and presently, as he passed, he saw a heron.&nbsp; They put their heads together and whispered, and the heron said to the farmer \"Go and excrete.&nbsp; Then, when you have finished, move forward a little, but don't get up.&nbsp; Stay squatting, with your head down and your buttocks up, until I come.\"<br />&nbsp;&nbsp;&nbsp; So the man went off and did just exactly as the heron had told him, everything.&nbsp; And the snake put out his head and began to catch flies.&nbsp; Then the heron struck and seized the snake's head.&nbsp; Then he pulled and he pulled until he had got him out, and the man tumbled over.&nbsp; And the heron finished off the snake with his beak.<br />&nbsp;&nbsp;&nbsp; The man rose and went over to the heron.&nbsp; \"Heron\" says he, \"You've got the snake out for me, now please give me some medicine to drink, for the poison where he was lying.\"<br />&nbsp;&nbsp;&nbsp; Says the heron \"Go and find yourself some white fowls, six of them.&nbsp; Cook them and eat them - they are the medicine.\"&nbsp; \"Oho\" said the man, \"White fowl?&nbsp; But that's you\" and he grabbed the heron and tied it up and went off home.&nbsp; There he took him into a hut and hung him up, the heron lamenting the while.<br />&nbsp;&nbsp;&nbsp; Then the man's wife said \"Oh, husband!&nbsp; The bird did you a kindness.&nbsp; He saved your life, by getting the trouble out of your stomach for you.&nbsp; And now you seize him and say that you are going to slaughter him!\"<br />&nbsp;&nbsp;&nbsp; So the man's wife loosed the heron, but as he was going out, he pecked out one of her eyes.&nbsp; And so passed and went his way.&nbsp; That's all.&nbsp; For so it always has been - if you see the dust of a fight rising, you will know that a kindness is being repaid!&nbsp; That's all.&nbsp; The story's finished.</p>\n<p>I wonder if this has something to do with why Africa stays poor.</p>\n<p>I was slightly shocked, reading this story.&nbsp; It seems to reveal a cultural gloominess deeper than its Western analogue of fashionable cynicism.&nbsp; The cynical Western tale would at least have an innocent, virtuous, idealistic fool to be exploited, since our cynicism is mostly about feeling superior to those less sophisticated.&nbsp; <em>This </em>tale has so much defection that you can't even call the characters hypocrites.&nbsp; This isn't a story told to make the listener feel superior to the fools who still believe in the essential goodness of human nature.&nbsp; This is a tribe whose children are being warned to expect cooperation to be met with defection the same way our own kids are told \"slow but steady wins the race\".</p>\n<p>It's occasionally debated on this blog whether <a href=\"/lw/rl/the_psychological_unity_of_humankind/\">the psychological unity of humankind</a> is real and how much room it leaves for humans to be deeply different.&nbsp; Someone might look at this tale and say, \"Gratitude in Africa isn't like gratitude in the West.\"&nbsp; But to me it looks like the people who pass on this tale must have pretty much the same chunk of brain circuitry somewhere that implements the emotion of <em>gratitude </em>- that's what creates the background against which this story is told.&nbsp; It wouldn't be viewed as important wisdom, otherwise.</p>\n<p>But cultural gloominess this deep may be a self-fulfilling prophecy, as powerful as if the emotion of gratitude itself had diminished, or failed.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 1, "gHCNhqxuJq2bZ2akb": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yyteh3qwD6kjhLiCb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 26, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "1245", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 63, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Cyj6wQLW6SeF6aGLy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-17T00:49:26.000Z", "modifiedAt": null, "url": null, "title": "Cynical About Cynicism", "slug": "cynical-about-cynicism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:06.833Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/R2crzhnqrytPycc6C/cynical-about-cynicism", "pageUrlRelative": "/posts/R2crzhnqrytPycc6C/cynical-about-cynicism", "linkUrl": "https://www.lesswrong.com/posts/R2crzhnqrytPycc6C/cynical-about-cynicism", "postedAtFormatted": "Tuesday, February 17th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cynical%20About%20Cynicism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACynical%20About%20Cynicism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR2crzhnqrytPycc6C%2Fcynical-about-cynicism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cynical%20About%20Cynicism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR2crzhnqrytPycc6C%2Fcynical-about-cynicism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR2crzhnqrytPycc6C%2Fcynical-about-cynicism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 613, "htmlBody": "<p>I&#39;m cynical about cynicism.&#0160; I don&#39;t believe that most cynicism is <em>really</em> about knowing better.&#0160; When I see someone being cynical, my first thought is that they&#39;re trying to show off their sophistication and assert superiority over the naive.&#0160; As opposed to, say, sharing their uncommon insight about not-widely-understood flaws in human nature.</p><p>There are two obvious exceptions to this rule.&#0160; One is if the speaker has something serious and realistic to say about how to <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">improve matters</a>.&#0160; Claiming that problems can be <em>fixed </em>will instantly lose you all your world-weary street cred and mark you as another starry-eyed idealistic fool.&#0160; (Conversely, any &quot;solution&quot; that manages <em>not</em> to disrupt the general atmosphere of doom, does not make me less skeptical:&#0160; &quot;Humans are evil creatures who slaughter and destroy, but eventually we&#39;ll die out from poisoning the environment, so it&#39;s all to the good, really.&quot;)</p><p>No, not every problem is solvable.&#0160; But by and large, if someone\nachieves uncommon insight into darkness - if they know more than I do\nabout human nature and its flaws - then it&#39;s not unreasonable to expect\nthat they <em>might</em> have a suggestion or two to make about remedy,\npatching, or minor deflection.&#0160; If, you know, the problem is\none that they really would <em>prefer solved,</em> rather than gloom being milked for a feeling of superiority to the naive herd.</p><p>The other obvious exception is for science that has something to say about human nature.&#0160; A testable hypothesis is a testable hypothesis and the thing to do with it is test it.&#0160; Though here one must be very careful not to go <em>beyond</em> the letter of the experiment for the sake of signaling hard-headed realism:\n</p><a id=\"more\"></a>\n<p>Consider the hash that some people make of evolutionary psychology in trying to be cynical - assuming that humans have a subconscious motive to promote their inclusive genetic fitness.&#0160; Consider the hash that some neuroscientists make of the results of their brain scans, supposing that if a brain potential is visible before the moment of reported decision, this proves the nonexistence of <a href=\"/lw/r1/timeless_control/\">free will</a>.&#0160; <a href=\"/lw/r1/timeless_control/\">It&#39;s not you who chooses, it&#39;s your brain!</a></p><p>The facts are one thing, but <em><a href=\"/lw/hp/feeling_rational/\">feeling</a> </em><em>cynical</em> about those facts is another matter entirely.&#0160; In some cases it can lead people to overrun the facts - to construct new, unproven, or even outright disproven glooms in the name of signaling realism.&#0160; <a href=\"/lw/sr/the_comedy_of_behaviorism/\">Behaviorism</a> probably had this problem - signaling hardheaded realism about human nature was probably one of the reasons they asserted <a href=\"/lw/sr/the_comedy_of_behaviorism/\">we don&#39;t have minds</a>.<br />\n</p>\n<p>I&#39;m especially on guard against cynicism because it seems to be a\nstandard corruption of rationality in particular.&#0160; If many people are\noptimists, then true rationalists will occasionally have to say things\nthat sound pessimistic by contrast.&#0160; If people are trying to signal\nvirtue through their beliefs, then a rationalist may have to advocate\ncontrasting beliefs that don&#39;t signal virtue.</p><p>Which in turn means that\nrationalists, and especially apprentice rationalists watching other rationalists at work, are especially at-risk for absorbing cynicism as though it were a virtue in its own right - assuming that whosoever speaks of ulterior motives is probably a wise rationalist with uncommon insight; or believing that it is an entitled benefit of realism to feel superior to the naive herd that still has a shred of hope.</p><p>And this is a fearsome mistake indeed, because you can&#39;t propose ways to <em>meliorate </em>problems and still come off as world-weary.</p><p>TV Tropes proposes a <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/SlidingScaleOfIdealismVersusCynicism\">Sliding Scale of Idealism Versus Cynicism</a>.&#0160; It looks to me like Robin tends to focus his suspicions on that which might be <em>signaling </em>idealism, virtue, or righteousness; while I tend to focus my own skepticism on that which might <em>signal </em>cynicism, world-weary sophistication, or sage maturity.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q6P8jLn8hH7kbuXRr": 1, "LDTSbmXtokYAsEq8e": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "R2crzhnqrytPycc6C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 42, "extendedScore": null, "score": 6.4e-05, "legacy": true, "legacyId": "1246", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DoLQN5ryZ9XkZjq5h", "YYLmZFEGKsjCKQZut", "SqF8cHjJv43mvJJzx", "9fpWoXpNv83BAHJdc"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-17T18:41:21.000Z", "modifiedAt": null, "url": null, "title": "Good Idealistic Books are Rare", "slug": "good-idealistic-books-are-rare", "viewCount": null, "lastCommentedAt": "2013-11-07T18:49:53.389Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YdXMZX5HbZTvvNy84/good-idealistic-books-are-rare", "pageUrlRelative": "/posts/YdXMZX5HbZTvvNy84/good-idealistic-books-are-rare", "linkUrl": "https://www.lesswrong.com/posts/YdXMZX5HbZTvvNy84/good-idealistic-books-are-rare", "postedAtFormatted": "Tuesday, February 17th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Good%20Idealistic%20Books%20are%20Rare&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGood%20Idealistic%20Books%20are%20Rare%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYdXMZX5HbZTvvNy84%2Fgood-idealistic-books-are-rare%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Good%20Idealistic%20Books%20are%20Rare%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYdXMZX5HbZTvvNy84%2Fgood-idealistic-books-are-rare", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYdXMZX5HbZTvvNy84%2Fgood-idealistic-books-are-rare", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 481, "htmlBody": "<p>Saith Robin in &quot;<a href=\"http://www.overcomingbias.com/2009/02/seeking-a-cynics-library.html\">Seeking a Cynic&#39;s Library</a>&quot;:</p><div style=\"margin-left: 40px;\">Cynicism and Idealism are a classic yin and yang, a contradictory pair\nwhere we all seem to need both sides...<p>Books on education, medicine, government, charity, religion,\ntechnology, travel, relationships, etc. mostly present relatively\nidealistic views, though of course no view is entirely one way or the\nother.&#0160; So one reason the young tend to be idealistic is that most reading material they can easily find and understand is idealistic.&#0160; </p></div><p>My impression of this differs somewhat from Robin&#39;s (what a surprise).</p><p>I think that what we see in most books of the class Robin describes, are <em>official</em> views.&#0160; These official views may leave out many unpleasant elements of the story.&#0160; But because officialism also tries to signal authority and maturity, it&#39;s hardly likely to permit itself any real hope or enthusiasm.&#0160; Perhaps an obligatory if formal nod in the direction of some popular good cause, because this is expected of officialdom.&#0160; But this is hardly an idealistic voice.</p><p>What does a full-blown nonfictional idealism look like?&#0160; Some examples that I remember from my own youth: </p><ul>\n<li>Jerry Pournelle&#39;s <em>A Step Farther Out,</em> an idealistic view of\nspace travel and more general technological advancement, and the\npossibility of rising standards of living as opposed to Ehrlichian\ngloomsaying.</li>\n<li>Brown, Keating, Mellinger, Post, Smith, and Tudor&#39;s <em><a href=\"http://www.theincrediblebreadmachine.com/\">The Incredible Bread Machine</a>,</em> my childhood introduction to <a href=\"/lw/ux/traditional_capitalist_values/\">traditional capitalist values</a>.</li>\n<li>Eric Drexler&#39;s <em>Engines of Creation</em> (and to a lesser extent Ed Regis&#39;s <em>Great Mambo Chicken</em>), my introduction to <a href=\"http://yudkowsky.net/singularity/simplified\">transhumanism</a>.</li>\n<li>Richard Feynman&#39;s <em>Surely You&#39;re Joking, Mr. Feynman</em> (for traditional rationalist values).</li>\n</ul>\n<p>Supposing you <em>wanted</em> your child to grow up an idealist - what\nnonfiction books like these could you find to give them?&#0160; I don&#39;t find\nit easy to think of many - most nonfiction books are <em>not</em> like this.</p><a id=\"more\"></a><p>On the other hand, I suspect that idealistic <em>fiction</em> aimed <em>specifically</em> at children is a far greater cultural force than anything they pick up from their school textbooks.&#0160; Textbooks are marketed to adult textbook-selectors; juvenile fiction and children&#39;s television are actually aimed at children.</p><p>Of course this just implies a chicken-and-egg problem; why do children\nenjoy idealism more than cynicism?&#0160; On this score I would suggest that children in the hunter-gatherer EEA have no chance of successful rebellion, and so haven&#39;t yet developed certain emotions that will come\ninto play after puberty.&#0160; But children will be actively engaged in absorbing tribal mores during their maturation, and may benefit from signaling such absorption.</p><p>Teenagers who act as if they could still get together with their friends and split off to form their own tribe, enjoy cynicism aimed at current authority figures and idealism aimed at their new tribe.</p><p>When such forces have petered out, I suggest we are left with a mostly socially-determined adult equilibrium: idealism about some distant subjects is used to signal virtue, cynicism about other distant subjects is used to signal sophistication.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "4Kcm4etxAJjmeDkHP": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YdXMZX5HbZTvvNy84", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 8, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "1247", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3bfWCPfu9AFspnhvf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-18T23:34:51.000Z", "modifiedAt": null, "url": null, "title": "Against Maturity", "slug": "against-maturity", "viewCount": null, "lastCommentedAt": "2019-06-30T15:00:51.547Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rM7hcz67N7WtwGGjq/against-maturity", "pageUrlRelative": "/posts/rM7hcz67N7WtwGGjq/against-maturity", "linkUrl": "https://www.lesswrong.com/posts/rM7hcz67N7WtwGGjq/against-maturity", "postedAtFormatted": "Wednesday, February 18th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Against%20Maturity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAgainst%20Maturity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrM7hcz67N7WtwGGjq%2Fagainst-maturity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Against%20Maturity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrM7hcz67N7WtwGGjq%2Fagainst-maturity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrM7hcz67N7WtwGGjq%2Fagainst-maturity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1461, "htmlBody": "<p>I remember the moment of my first break with Judaism.&#0160; It was in kindergarten, when I was being forced to memorize and recite my first prayer.&#0160; It was in Hebrew.&#0160; We were given a transliteration, but not a translation.&#0160; I asked what the prayer meant.&#0160; I was told that I didn&#39;t need to know - so long as I prayed in Hebrew, it would work even if I didn&#39;t understand the words.&#0160; (Any resemblance to <a href=\"/lw/iq/guessing_the_teachers_password/\">follies inveighed against in my writings</a> is <em>not </em>coincidental.)</p><p>Of course I didn&#39;t accept this, since it was blatantly stupid, and I figured that God had to be at least as smart as I was.&#0160; So when I got home, I asked my parents, and they didn&#39;t bother arguing with me.&#0160; They just said, &quot;You&#39;re too young to argue with; we&#39;re older and wiser; adults know best; you&#39;ll understand when you&#39;re older.&quot;</p><p>They were right about that last part, anyway.</p>\n<p>Of course there were plenty of places my parents really <em>did </em>know better, even in the realms of abstract reasoning.&#0160; They were doctorate-bearing folks and not stupid.&#0160; I remember, at age nine or something silly like that, showing my father a diagram full of filled circles and trying to convince him that the indeterminacy of particle collisions was because they had a fourth-dimensional cross-section and they were bumping or failing to bump in the fourth dimension.</p><p>My father shot me down flat.&#0160; (Without making the slightest effort to humor me or encourage me.&#0160; This seems to have worked out just fine.&#0160; He did buy me books, though.)</p><p>But he <em>didn&#39;t</em> just say, &quot;You&#39;ll understand when you&#39;re older.&quot;&#0160; He said that physics was math and couldn&#39;t even be talked about\nwithout math.&#0160; He talked about how everyone he met tried to invent\ntheir own theory of physics and how <em>annoying </em>this was.&#0160; He may even\nhave talked about the futility of &quot;providing a mechanism&quot;, though I&#39;m\nnot actually sure if I originally got that off him or <a href=\"http://math.ucr.edu/home/baez/crackpot.html\">Baez</a>.</p><p>You see the pattern developing here.&#0160; &quot;Adulthood&quot; was what my parents appealed to when they couldn&#39;t verbalize any <em>object-level</em> justification.&#0160; They had doctorates and were smart; if there <em>was </em>a good reason, they usually <em>would </em>at least <em>try </em>to explain it to me.&#0160; And it gets worse...<br />\n</p><a id=\"more\"></a>\n<p></p><p></p><p></p><p>The most fearsome damage wreaked upon my parents by their concept of &quot;adulthood&quot;, was the idea that being &quot;adult&quot; meant that you were <em>finished</em> - that &quot;maturity&quot; marked the place where you declared yourself done, needing to go no further.</p><p>This was displayed most clearly in the matter of religion, where I would try to talk about a question I had, and my parents would smile and say:&#0160; &quot;Only children ask questions like that; when you&#39;re adult, you know that it&#39;s pointless to argue about it.&quot;&#0160; They actually said that outright!&#0160; To ask questions was a manifestation of earnest, childish enthusiasm, earning a smile and a pat on the head.&#0160; An adult knew better than to waste effort on pointless things.</p><p>We never really know our parents; we only know the face of our parents that they turn to us, their children.&#0160; I don&#39;t know if my parents ever thought about the child-adult dichotomy when they <em>weren&#39;t</em> talking to me.</p><p>But this is what I think my parents were thinking:&#0160; If they had tried to answer a question as children, and then given up as adults - a quite common pattern in their religious decay - they labeled &quot;mature&quot; the place and act of giving up, by way of consolation.&#0160; They&#39;d asked the question as children and stopped asking as adults - and the story they told themselves about that was that only children asked that question, and now they had succeeded into the sage maturity of knowing not to argue.</p><p>\n</p><p>To this very day, I constantly remind myself that, no matter\nwhat I do in this world, I will doubtlessly be considered an infant by\nthe standards of future intergalactic civilization, and so there is no\npoint in pretending to be a grown-up.&#0160; I try to maintain a mental\npicture of myself as someone who is <em>not</em> mature, so that I can go on <em>maturing</em>.</p><p>And more...</p><p>From my parents I learned the observational lesson that &quot;adulthood&quot; was something sort\nof like &quot;peer acceptance&quot;, that is, its pursuit made you do stupid things that\nyou wouldn&#39;t have done if you were just trying to get it <em>right.</em></p><p>At\nthat age I couldn&#39;t have given you a very good definition of &quot;right&quot;\noutside the realm of pure epistemic accuracy -</p><p>-\nbut I understood the concept of asking the wrong question.&#0160; &quot;Does this\nhelp people?&quot;&#0160; &quot;Will this make anyone happy?&quot;&#0160; &quot;Is this belief true?&quot;&#0160;\nThose were the sorts of questions to ask, not, &quot;Is this the adult thing\nto do?&quot;</p><p>So I did not divide up the universe into the childish way\nversus the adult way, nor ever tell myself that I had completed anything by getting older, nor congratulate myself on having stopped being a child.&#0160; Instead I\nlearned that there were various stereotypes and traps that could take\npeople&#39;s attention off the important questions, and instead make them\ntry to match certain unimportant concepts that existed in their minds.&#0160;\nOne of these attractor-traps was called &quot;teenager&quot;, and one of these attractor-traps was\ncalled &quot;adult&quot;, and both were to be avoided.</p><p>I&#39;ve <a href=\"/lw/s5/rebelling_within_nature/\">previously touched</a> on the massive effect on my youthful psychology of reading a book of advice to parents with teenagers, years before I was actually a teenager; I took one look at the description of the stupid things teenagers did, and said to myself with quiet revulsion, &quot;I&#39;m not going to do that&quot;; and then I actually didn&#39;t.&#0160; I never drank and drove, never drank, never tried a single drug, never lost control to hormones, never paid any attention to peer pressure, and never once thought my parents didn&#39;t love me.&#0160; In a safer world, I would have wished for my parents to have hidden that book better...</p><p>...but I had a picture in my mind of what it meant to be a &quot;teenager&quot;; and I determined to avoid it; and I did.</p><p>Of course there are a lot of children in this world who don&#39;t like being &quot;children&quot; and who try to appear as &quot;adult&quot; or as &quot;mature&quot; as possible.&#0160; That&#39;s why they start smoking, right?&#0160; So that was also part of the picture that I had in my mind of a &quot;stupid teenager&quot;:<em> stupid teenagers deliberately try to be mature.</em></p><p>My parents had a picture in their mind of what it meant to be a &quot;kid&quot;, which included &quot;kids desperately want to be adult&quot;.&#0160; I presume, though I don&#39;t exactly know, that my parents had a picture of &quot;childishness&quot; which was formed by their <em>own </em>childhood and not updated.</p><p>In any case my parents were constantly trying to get me to do things by telling me about how it would make me <em>look</em> adult.</p><p>That was their appeal - not, &quot;Do this because it <em>is </em>older and wiser,&quot; but, &quot;Do this, because it will make you <em>look </em>adult.&quot;&#0160; To this day I wonder what they could have possibly been thinking.&#0160; Would a stereotypical teenager <em>listen to their parents&#39; advice</em> about that sort of thing?</p><p>Not surprisingly, being constantly urged to do things because they would <em>signal </em>adulthood, which I had no particular desire to do, had the effect of making me strongly notice things that <em>signaled </em>adulthood as mere signals.</p><p>I think that a lot of difference in the individual style of\nrationalists comes down to which signaling behaviors strike us as dangerous, harmful, or, perhaps, personally annoying; and which signaling behaviors seem relatively harmless, or possibly even useful paths to accomplishment (perhaps because they don&#39;t annoy us quite so much).&#0160; Robin is willing to tolerate formality in journals, viewing it as possibly even a useful path to filtering out certain kinds of noise; to me formality seems like a strong net danger to rationality that filters nothing, just making it more difficult to read.&#0160; I&#39;m willing to tolerate behaviors that signal idealism or caring for others, viewing it as an important pathway to real altruism later on; Robin seems to think such behaviors are relatively more harmful and that they ought to be stopped outright.</p><p>It&#39;s not that Robin is a cynic or I&#39;m an idealist, but that I&#39;m relatively more annoyed by cynical errors and Robin is relatively more annoyed by idealistic errors.&#0160; And fitting the same dimension, Robin seems relatively more annoyed by the errors in the style of youth, where I seem relatively more annoyed by errors in the style of maturity.</p><p>And so I take a certain dark delight in <a href=\"/lw/y2/rationality_quotes_25/\">quoting anime fanfiction</a> at people who expect me to behave like a wise sage of rationality.&#0160; Why should I pretend to be mature when practically every star in the night sky is older than I am?</p><p></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"irYLXtT9hkPXoZqhH": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rM7hcz67N7WtwGGjq", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 48, "extendedScore": null, "score": 7.2e-05, "legacy": true, "legacyId": "1248", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 48, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NMoLJuDJEms7Ku9XS", "YhNGY6ypoNbLJvDBu", "iRop8WinYW4B8KYiK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-02-18T23:34:51.000Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-19T22:30:22.000Z", "modifiedAt": "2021-08-12T19:28:32.549Z", "url": null, "title": "Pretending to be Wise", "slug": "pretending-to-be-wise", "viewCount": null, "lastCommentedAt": "2021-01-30T02:36:55.220Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jeyvzALDbjdjjv5RW/pretending-to-be-wise", "pageUrlRelative": "/posts/jeyvzALDbjdjjv5RW/pretending-to-be-wise", "linkUrl": "https://www.lesswrong.com/posts/jeyvzALDbjdjjv5RW/pretending-to-be-wise", "postedAtFormatted": "Thursday, February 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pretending%20to%20be%20Wise&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APretending%20to%20be%20Wise%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjeyvzALDbjdjjv5RW%2Fpretending-to-be-wise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pretending%20to%20be%20Wise%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjeyvzALDbjdjjv5RW%2Fpretending-to-be-wise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjeyvzALDbjdjjv5RW%2Fpretending-to-be-wise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1202, "htmlBody": "<blockquote><p>The hottest place in Hell is reserved for those who in time of crisis remain neutral.</p><p>\u2014<s>Dante Alighieri, famous hell expert</s></p><p>\u2014John F. Kennedy, <a href=\"http://www.bartleby.com/73/1211.html\">misquoter</a></p></blockquote><p>&nbsp;</p><p>Belief is quantitative, and just as it is possible to make overconfident assertions relative to ones anticipations, it is possible to make <i>under</i> confident assertions relative to ones anticipations. One can wear the attire of uncertainty, or profess an agnosticism that isn\u2019t really there. Here, I'll single out a special case of improper uncertainty: the display of <i>neutrality</i> or <i>suspended judgment</i> in order to signal maturity, impartiality, or a superior vantage point.</p><p>An example would be <a href=\"http://lesswrong.com/lw/yo/against_maturity/\">the case of my parents</a>, who respond to theological questions like \u201cWhy does ancient Egypt, which had good records on many other matters, lack any records of Jews having ever been there?\u201d with \u201cOh, when I was your age, I also used to ask that sort of question, but now I\u2019ve grown out of it.\u201d</p><p>Another example would be the principal who, faced with two children who were caught fighting on the playground, sternly says: \u201cIt doesn\u2019t matter who started the fight, it only matters who ends it.\u201d Of course it matters who started the fight. The principal may not have access to good <i>information</i> about this critical fact, but if so, the principal should <i>say</i> so, not <i>dismiss the importance</i> of who threw the first punch. Let a parent try punching the principal, and we\u2019ll see how far \u201cIt doesn\u2019t matter who started it\u201d gets in front of a judge. But to adults it is just <i>inconvenient</i> that children fight, and it matters not at all to their <i>convenience</i> which child started it. It is only <i>convenient</i> that the fight end as rapidly as possible.</p><p>A similar dynamic, I believe, governs the occasions in international diplomacy where Great Powers sternly tell smaller groups to stop that fighting <i>right now</i>. It doesn\u2019t matter to the Great Power who started it\u2014who provoked, or who responded disproportionately to provocation\u2014because the Great Power\u2019s ongoing <i>inconvenience</i> is only a function of the ongoing conflict. Oh, can\u2019t Israel and Hamas just get along?</p><p>This I call \u201cpretending to be Wise.\u201d Of course there are many ways to try and signal wisdom. But trying to signal wisdom by refusing to make guesses\u2014refusing to&nbsp;sum up evidence\u2014refusing to pass judgment\u2014refusing to take sides\u2014staying above the fray and looking down with a lofty and condescending gaze\u2014which is to say, signaling wisdom by saying and doing nothing\u2014well, that I find particularly pretentious.</p><p>Paolo Freire said, \u201cWashing one\u2019s hands of the conflict between the powerful and the powerless means to side with the powerful, not to be neutral.\u201d<sup>1</sup> A playground is a great place to be a bully, and a terrible place to be a victim, if the teachers don\u2019t care <i>who started it.</i> And likewise in international politics: A world where the Great Powers refuse to take sides and only demand immediate truces is a great world for aggressors and a terrible place for the aggressed. But, of course, it is a very convenient world in which to be a Great Power or a school principal. So part of this behavior can be chalked up to sheer selfishness on the part of the Wise.</p><p>But part of it also has to do with signaling a superior vantage point. After all\u2014what would the <i>other adults</i> think of a principal who actually seemed to be <i>taking sides</i> in a fight between mere <i>children?</i> Why, it would lower the principal\u2019s status to a mere <i>participant in the fray</i>!</p><p>Similarly with the revered elder\u2014who might be a CEO, a prestigious academic, or a founder of a mailing list\u2014whose reputation for fairness depends on their refusal to pass judgment themselves, when others are choosing sides. Sides appeal to them for support, but almost always in vain; for the Wise are revered judges on the condition that they almost never actually judge\u2014 <i>then</i> they would just be another disputant in the fray, no better than any mere arguer.<sup>2</sup></p><p>There <i>are</i> cases where it is rational to suspend judgment, where people leap to judgment only because of their biases. As <a href=\"http://lesswrong.com/lw/he/knowing_about_biases_can_hurt_people/dza\">Michael Rooney said</a>:</p><blockquote><p>The error here is similar to one I see all the time in beginning philosophy students: when confronted with reasons to be skeptics, they instead become relativists. That is, when the rational conclusion is to suspend judgment about an issue, all too many people instead conclude that any judgment is as plausible as any other.</p></blockquote><p>But then how can we avoid the (related but distinct) pseudo-rationalist behavior of signaling your unbiased impartiality by falsely claiming that the current balance of evidence is neutral? \u201cOh, well, of course you have a lot of passionate Darwinists out there, but I think the evidence we have doesn\u2019t really enable us to make a definite endorsement of natural selection over intelligent design.\u201d</p><p>On this point I\u2019d advise remembering that <i>neutrality is a definite judgment</i>. It is not staying <i>above</i> anything. It is putting forth the definite and particular position that the balance of evidence in a particular case licenses <i>only</i> one summation, which happens to be neutral. This belief, too, must pay rent in anticipated experiences,&nbsp;and it can be wrong; propounding neutrality is just as attackable as propounding any particular side.</p><p>Likewise with policy questions. If someone says that both pro-life and pro-choice sides have good points and that they really should try to compromise and respect each other more, they are not taking a position <i>above</i> the two standard sides in the abortion debate. They are putting forth a definite judgment, every bit as particular as saying \u201cpro-life!\u201d or \u201cpro-choice!\u201d</p><p>It may be useful to initially avoid using issues like abortion or the Israeli-Palestinian conflict for your rationality practice, and so build up skill on less emotionally charged topics. But it\u2019s <i>not</i> that a rationalist is too mature to talk about politics. It\u2019s <i>not</i> that a rationalist is above this foolish fray in which only mere political partisans and youthful enthusiasts would stoop to participate.</p><p>As Robin Hanson describes it, the ability to have potentially divisive conversations is a limited resource. If you can think of ways to pull the rope sideways, you are justified in expending your limited resources on relatively less common issues where marginal discussion offers relatively higher marginal payoffs.<sup>3</sup></p><p>But then the responsibilities that you deprioritize are a matter of your limited resources. <i>Not</i> a matter of floating high above, serene and Wise.</p><p>In sum, there\u2019s a difference between:</p><ul><li>Passing neutral judgment;</li><li>Declining to invest marginal resources;</li><li>Pretending that either of the above is a mark of deep wisdom, maturity, and a superior vantage point; with the corresponding implication that the original sides occupy lower vantage points that are not importantly different from up there.</li></ul><hr><p><sup>1</sup> Paulo Freire, <i>The Politics of Education: Culture, Power, and Liberation</i> (Greenwood Publishing Group, 1985), 122.</p><p><sup>2</sup> Oddly, judges in the actual legal system can repeatedly hand down real verdicts without <i>automatically</i> losing their reputation for impartiality. Maybe because of the understood norm that they <i>have</i> to judge, that it\u2019s their job. Or maybe because judges don\u2019t have to repeatedly rule on issues that have split a tribe on which they depend for their reverence.</p><p><sup>3</sup> See Hanson, \u201cPolicy Tug-O-War\u201d (<a href=\"http://www.overcomingbias.com/2007/05/policy_tugowar.html\">http://www.overcomingbias.com/2007/05/policy_ tugowar.html</a>) and \u201cBeware Value Talk\u201d (<a href=\"http://www.overcomingbias.com/2009/02/the-cost-of-talking-values.html\">http://www.overcomingbias.com/2009/02/the-cost-of-talking-values.html</a>).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "Q6P8jLn8hH7kbuXRr": 2, "2EFq8dJbxKNzforjM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jeyvzALDbjdjjv5RW", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 98, "baseScore": 108, "extendedScore": null, "score": 0.000161, "legacy": true, "legacyId": "1249", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2021-04-01T23:36:16.660Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": "7gRSERQZbqTuLX5re", "canonicalCollectionSlug": "rationality", "canonicalBookId": "PYQ2izdfDPTe5uJTG", "canonicalNextPostSlug": "applause-lights", "canonicalPrevPostSlug": "belief-as-attire", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 108, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "rPsASnzcvaBdBEaSJ", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 77, "af": false, "version": "2.4.0", "pingbacks": {"Posts": ["rM7hcz67N7WtwGGjq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-02-19T22:30:22.000Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-20T17:02:28.000Z", "modifiedAt": null, "url": null, "title": "Wise Pretensions v.0", "slug": "wise-pretensions-v-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:30.957Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/i97ohcwLugt5oQvMy/wise-pretensions-v-0", "pageUrlRelative": "/posts/i97ohcwLugt5oQvMy/wise-pretensions-v-0", "linkUrl": "https://www.lesswrong.com/posts/i97ohcwLugt5oQvMy/wise-pretensions-v-0", "postedAtFormatted": "Friday, February 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Wise%20Pretensions%20v.0&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWise%20Pretensions%20v.0%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi97ohcwLugt5oQvMy%2Fwise-pretensions-v-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Wise%20Pretensions%20v.0%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi97ohcwLugt5oQvMy%2Fwise-pretensions-v-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi97ohcwLugt5oQvMy%2Fwise-pretensions-v-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1472, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/yp/pretending_to_be_wise/\">Pretending to be Wise</a></p>\n<p>For comparison purposes, here's an essay with similar content to yesterday's \"Pretending to be Wise\", which I wrote in 2006 in a completely different style, edited down slightly (content has been deleted but not added).&nbsp; Note that the 2006 concept of \"pretending to be Wise\" hasn't been narrowed down as much compared to the 2009 version; also when I wrote it, I was in more urgent need of persuasive force.</p>\n<p>I thought it would be an interesting data point to check whether this essay seems more <em>convincing </em>than yesterday's, following <a href=\"http://www.overcomingbias.com/2009/02/against-propaganda-.html\">Robin's injuction</a> \"to avoid emotion, color, flash, stories, vagueness, repetition, rambling, and even eloquence\" - this seems like rather the sort of thing he might have had in mind.</p>\n<p>And conversely the stylistic change also seems like the sort of thing Orwell might have had in mind, when <a href=\"http://www.mtholyoke.edu/acad/intrel/orwell46.htm\">Politics and the English Language</a> compared:&nbsp; \"I returned and saw under the sun, that the race is not to the swift, nor the battle to the strong, neither yet bread to the wise, nor yet riches to men of understanding, nor yet favour to men of skill; but time and chance happeneth to them all.\"&nbsp; Versus:&nbsp; \"Objective considerations of contemporary phenomena compel the conclusion that success or failure in competitive activities exhibits no tendency to be commensurate with innate capacity, but that a considerable element of the unpredictable must invariably be taken into account.\"&nbsp; That would be the other side of it.</p>\n<p>At any rate, here goes Eliezer<sub>2006</sub>...</p>\n<p style=\"margin-left: 40px;\">I do not fit the stereotype of the Wise. I am not Gandalf, Ged, or Gandhi. I do not sit amidst my quiet garden, staring deeply into the truths engraved in a flower or a drop of dew; speaking courteously to all who come before me, and answering them gently regardless of how they speak to me.</p>\n<p style=\"margin-left: 40px;\">If I tried to look Wise, and succeeded, I would receive more respect from my fellows. But there would be a price.</p>\n<p style=\"margin-left: 40px;\">To pretend to be Wise means that you must always appear to give people the benefit of the doubt. Thus people will admire you for your courtesy. But this is not always true.</p>\n<p style=\"margin-left: 40px;\">To pretend to be Wise, you must always pretend that both sides have merit, and solemnly refuse to judge between them. For if you took one side or another, why then, you would no longer be one of the aloof Wise, but merely another partisan, on a level with all the other mere bickerers.</p>\n<p style=\"margin-left: 40px;\">As one of the Wise, you are omnipotent on the condition that you never exercise your power. Otherwise people would start thinking that you were no better than they; and they would no longer hold you in awe.</p>\n<p><a id=\"more\"></a></p>\n<p style=\"margin-left: 40px;\">Ofttimes it is greatly convenient, to pretend to be Wise. When any conflict breaks out, you can sternly chide both sides, saying: \"You are equally at fault; you must learn to see each other's viewpoints. I am older and more mature, and I say to you: stop this pointless bickering, children, for you begin to annoy me. Ponder well the wisdom of having everyone get along!\" You do not need to examine the dispute, nor wonder if perhaps one side does have more merit than the other. You need not judge between two sides, and risk having your judgment turn out to be embarrassingly wrong, or risk having your judgment questioned as though you were only another ordinary mortal. Indeed you must not ask questions, you must not judge; for if you take sides, you will at once lose your reputation for being Wise, which requires that you stand forever above the fray.</p>\n<p style=\"margin-left: 40px;\">But truth is not handed out in equal parts before the start of a dispute.</p>\n<p style=\"margin-left: 40px;\">And I am not one of the Wise. Even if I wished to be, it is not within my nature. I do not hesitate to place my reputation in jeopardy to aid the side I believe is right. Even if it makes me seem but an ordinary mortal, no better than any other in the fray. The respect I have earned, I have earned by other ways than by appearing gravely solemn; and respect has no purpose but what it can accomplish. Respect is not to be hoarded, but spent. Even when those pretending to be Wise chide me, saying: \"Stop this bickering!\" - yet I will not pretend to neutrality, nor rise above the fray.</p>\n<p style=\"margin-left: 40px;\">For not all conflicts are balanced; indeed an exactly balanced conflict is very rare. Sometimes - indeed often - I have struck out against both sides in a dispute, saying: \"You are both wrong; here is the third way.\" But never have I told both sides of a dispute: \"It doesn't matter who started it, just end it.\" This is the path of convenience to yourself, and it comes at a cost to others; it is selfish. There are aggressors and aggressed, in wars. When some small nation is invaded by another, or is provoked endlessly, or when one nation provokes another and that other responds disproportionately; then it may prove convenient indeed to the Great Powers, to pretend that all violence is equally wrong and equally the fault of all sides, and selfishly seek a truce for this year, this election. The Great Powers have no need to take sides, when they can more easily tell the two edges of the gaping wound: \"Oh, just stop fighting, you foolish children!\" Ignoring the rottenness inside... but that is pragmatism to a Great Power, which only wishes that the boat should go unrocked, and does not truly care for the health of lesser nations.</p>\n<p style=\"margin-left: 40px;\">And so too with those who pretend to be Wise: who pretend that there is no aggrieved, that there is no long-term problem to be addressed, that no side is ever in the right nor another in the wrong; that there are no causes for conflicts, only fools who are not Wise and who will spontaneously strike out at each other for no reason. It only takes one to start a war. But the Wise cannot acknowledge this in any particular case, for then they would be taking sides, and they would not be above the fray, merely another combatant. They would lose the awe, in which the Wise are held, and which they most earnestly desire.</p>\n<p style=\"margin-left: 40px;\">I do not say that the Wise do this deliberately; but it is the constraint that settles around them, the invisible chain that governs their behavior. No doubt the Wise truly believe that the combatants are but spoiled children; for if the Wise ceased to believe this, they would have to act, and no longer appear Wise.</p>\n<p style=\"margin-left: 40px;\">Have you not met them? the principal who cares not which child started it? the Chair who is above the mere fray of corporate politics? the Great Power who demands only an immediate truce? the priest who says that all alike are sinners and all must repent? the boss who sternly dictates that the conflict end now? have you not met them, the Wise?</p>\n<p style=\"margin-left: 40px;\">To care about the public image of any virtue - whether that virtue be charity, or wisdom, or rationality itself - is to limit your performance of that virtue to what all others already understand of it. Therefore those who would walk the Way of any virtue must first relinquish all desire to appear virtuous before others, for this will limit and constrain their understanding of the virtue. To earn the respect of others is not hard, if you have a little talent, if that is the limit of your desire. But to know what is true, or to do what is right - that is far harder than convincing an audience of your wisdom. I am not Wise, and I will not be Wise, and no one can be Wise if they would follow the Way of rationality.</p>\n<p style=\"margin-left: 40px;\">For the eye of the Wise is blinded, and it may sometimes miss the gaping obvious.</p>\n<p>I prefer yesterday's post (which is why I wrote it).&nbsp; But I also suspect that yesterday's post is more <em>persuasive,</em> signaling more maturity and deliberately avoiding flashes of eloquence that might provoke skepticism here... while containing pretty much exactly the same message-payload.</p>\n<p>On the other hand, this version seems easier to read, and you might find it more persuasive if you had just encountered it on the Net - if you weren't <em>used </em>to a different style from me.</p>\n<p>(Nowadays if I have something to say that sounds <em>suspiciously </em>eloquent, I'll create a character and have <em>them </em>say it in dialogue or fiction; this lets me have my cake and eat it too.&nbsp; Though - important disclaimer - many of my characters are also there to say eloquent things that I <em>disagree</em> with, c.f. the <a href=\"/lw/y7/the_super_happy_people_38/\">Superhappies</a>.)</p>\n<p>What think you?&nbsp; Criticism can be addressed to me personally, I guess; Eliezer<sub>2006</sub> is still someone who I'd talk about as \"<a href=\"/lw/v4/which_parts_are_me/\">me</a>\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q6P8jLn8hH7kbuXRr": 1, "wzgcQCrwKfETcBpR9": 1, "Ng8Gice9KNkncxqcj": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "i97ohcwLugt5oQvMy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 14, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "1250", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jeyvzALDbjdjjv5RW", "qCsxiojX7BSLuuBgQ", "vjmw8tW6wZAtNJMKo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-22T01:55:48.000Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes 27", "slug": "rationality-quotes-27", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:46.889Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/adSXR7Lnyok9ZMWcR/rationality-quotes-27", "pageUrlRelative": "/posts/adSXR7Lnyok9ZMWcR/rationality-quotes-27", "linkUrl": "https://www.lesswrong.com/posts/adSXR7Lnyok9ZMWcR/rationality-quotes-27", "postedAtFormatted": "Sunday, February 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%2027&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%2027%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FadSXR7Lnyok9ZMWcR%2Frationality-quotes-27%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%2027%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FadSXR7Lnyok9ZMWcR%2Frationality-quotes-27", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FadSXR7Lnyok9ZMWcR%2Frationality-quotes-27", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 289, "htmlBody": "<p>\"Believing this statement will make you happier.\"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- Ryan Lortie</p>\n<p>\"Make changes based on your strongest opportunities, not your most convenient ones.\"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- <a href=\"http://www.megatokyo.com/index.php?strip_id=232\">MegaTokyo </a></p>\n<p>\"The mind is a cruel, lying, unreliable bastard that can't be trusted with even an ounce of responsibility.&nbsp; If you were dating the mind, all your friends would take you aside, and tell you that you can really do better, and being alone isn't all that bad, anyway.&nbsp; If you hired the mind as a babysitter, you would come home to find all but one of your children in critical condition, and the remaining one crowned 'King of the Pit'.\"<br /> &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Lore Sjoberg</p>\n<p>\"Getting bored is a non-trivial cerebral transformation that doubtlessly took many millions of years for nature to perfect.\"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- Lee Corbin</p>\n<p>\"The views expressed here do not necessarily represent the unanimous view of all parts of my mind.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Malcolm McMahon</p>\n<p><a id=\"more\"></a></p>\n<p>\"The boundary between these two classes is more porous than I've made it sound.&nbsp; I'm always running into regular dudes--construction workers, auto mechanics, taxi drivers, galoots in general--who were largely aliterate until something made it necessary for them to become readers and start actually thinking about things.&nbsp; Perhaps they had to come to grips with alcoholism, perhaps they got sent to jail, or came down with a disease, or suffered a crisis in religious faith, or simply got bored.&nbsp; Such people can get up to speed on particular subjects quite rapidly.&nbsp; Sometimes their lack of a broad education makes them over-apt to go off on intellectual wild goose chases, but, hey, at least a wild goose chase gives you some exercise.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- Neal Stephenson, <a href=\"http://steve-parker.org/articles/others/stephenson/interface.shtml\">In the Beginning was the Command Line</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "adSXR7Lnyok9ZMWcR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 4.770148048379293e-07, "legacy": true, "legacyId": "1251", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-22T20:22:00.000Z", "modifiedAt": null, "url": null, "title": "Fairness vs. Goodness", "slug": "fairness-vs-goodness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:03.241Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rKGvgRiEu5qYechNT/fairness-vs-goodness", "pageUrlRelative": "/posts/rKGvgRiEu5qYechNT/fairness-vs-goodness", "linkUrl": "https://www.lesswrong.com/posts/rKGvgRiEu5qYechNT/fairness-vs-goodness", "postedAtFormatted": "Sunday, February 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fairness%20vs.%20Goodness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFairness%20vs.%20Goodness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrKGvgRiEu5qYechNT%2Ffairness-vs-goodness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fairness%20vs.%20Goodness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrKGvgRiEu5qYechNT%2Ffairness-vs-goodness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrKGvgRiEu5qYechNT%2Ffairness-vs-goodness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 423, "htmlBody": "<p>It seems that back when the Prisoner&#39;s Dilemma was still being worked out, Merrill Flood and Melvin Drescher tried <a href=\"http://econ161.berkeley.edu/Economists/prisoners_dilemma.html\">a 100-fold iterative PD on two smart but unprepared subjects</a>, Armen Alchian of UCLA and John D. Williams of RAND.</p><p>The kicker being that the payoff matrix was <em>asymmetrical,</em> with dual cooperation awarding JW twice as many points as AA:</p>\n\n<table align=\"center\" border=\"1\" cellpadding=\"5\" cellspacing=\"2\"><tbody><tr><td>(AA, JW)</td><td>JW: D</td><td>JW: C</td></tr><tr><td>AA: D</td><td><span style=\"color: #c00000; font-family: Trebuchet MS;\"><strong>(0, 0.5)</strong></span></td><td>(1, -1)</td></tr><tr><td>AA: C</td><td>(-1, 2)</td><td><span style=\"color: #407f00; font-family: Trebuchet MS;\"><strong>(0.5, 1)</strong></span></td></tr></tbody></table><p>The resulting 100 iterations, with a log of comments written by both players, make for <a href=\"http://econ161.berkeley.edu/Economists/prisoners_dilemma.html\">fascinating reading</a>.</p><p>JW spots the possibilities of cooperation right away, while AA is slower to catch on.</p><p>But once AA does catch on to the possibilities of cooperation, AA goes on throwing in an occasional D... because AA thinks the natural meeting point for cooperation is a <em>fair</em> outcome, where both players get around the same number of total points.</p><p>JW goes on trying to enforce (C, C) - the option that maximizes total utility for both players - by punishing AA&#39;s attempts at defection.&#0160; JW&#39;s log shows comments like &quot;He&#39;s crazy.&#0160; I&#39;ll teach him the hard way.&quot;</p><p>Meanwhile, AA&#39;s log shows comments such as &quot;He won&#39;t share.&#0160; He&#39;ll punish me for trying!&quot;\n</p><a id=\"more\"></a><p>I confess that my own sympathies lie with JW, and I don&#39;t <em>think </em>I would have played AA&#39;s game in AA&#39;s shoes.&#0160; This would seem to indicate that I&#39;m more of a utilitarian than a fair-i-tarian.&#0160; Life doesn&#39;t always hand you <em>fair</em> games, and the best we can do for each other is play them positive-sum.</p><p>Though I might have been <em>somewhat </em>more sympathetic to AA, if the (C, C) outcome had actually <em>lost</em> him points, and only (D, C) had made it possible for him to gain them back.&#0160; For example, this is also a Prisoner&#39;s Dilemma:</p><p></p><table align=\"center\" border=\"1\" cellpadding=\"5\" cellspacing=\"2\"><tbody><tr><td>(AA, JW)</td><td>JW: D</td><td>JW: C</td></tr><tr><td>AA: D</td><td><span style=\"color: #c00000; font-family: Trebuchet MS;\"><strong>(-2, 2)</strong></span></td><td>(2, 0)</td></tr><tr><td>AA: C</td><td>(-5, 6)</td><td><span style=\"color: #407f00; font-family: Trebuchet MS;\"><strong>(-1, 4)</strong></span></td></tr></tbody></table><p>Theoretically, of course, utility functions are invariant up to affine transformation, so a utility&#39;s absolute sign is not meaningful.&#0160; But this is not always a good metaphor for real life.</p><p>Of course what <em>we </em>want in this case, societally speaking, is for JW to slip AA a bribe under the table.&#0160; That way we can maximize social utility while letting AA go on making a profit.&#0160; But if AA starts out with a negative number in (C,\nC), how much do we want AA to demand in bribes - from our global, societal perspective?</p><p>The whole affair makes for an interesting reminder of the different worldviews that people invent for themselves - seeming so natural and <em>uniquely obvious</em> from the inside - to make themselves the heroes of their own stories.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"be2Mh2bddQ6ZaBcti": 1, "b8FHrKqyXuYGWc6vn": 1, "nSHiKwWyMZFdZg5qt": 1, "AHK82ypfxF45rqh9D": 1, "5A5ZGTQovxbay6fpr": 6}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rKGvgRiEu5qYechNT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 15, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "1252", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-23T20:20:31.000Z", "modifiedAt": null, "url": null, "title": "On Not Having an Advance Abyssal Plan", "slug": "on-not-having-an-advance-abyssal-plan", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:06.408Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nYEcgJe8LyqB4qqCL/on-not-having-an-advance-abyssal-plan", "pageUrlRelative": "/posts/nYEcgJe8LyqB4qqCL/on-not-having-an-advance-abyssal-plan", "linkUrl": "https://www.lesswrong.com/posts/nYEcgJe8LyqB4qqCL/on-not-having-an-advance-abyssal-plan", "postedAtFormatted": "Monday, February 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20Not%20Having%20an%20Advance%20Abyssal%20Plan&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20Not%20Having%20an%20Advance%20Abyssal%20Plan%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnYEcgJe8LyqB4qqCL%2Fon-not-having-an-advance-abyssal-plan%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20Not%20Having%20an%20Advance%20Abyssal%20Plan%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnYEcgJe8LyqB4qqCL%2Fon-not-having-an-advance-abyssal-plan", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnYEcgJe8LyqB4qqCL%2Fon-not-having-an-advance-abyssal-plan", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1089, "htmlBody": "<p style=\"margin-left: 40px;\">&quot;Even though he could foresee the problem <em>then,</em> we can see it equally well <em>now.</em>&#0160; Therefore, if he could foresee the solution <em>then,</em> we should be able to see it <em>now.</em>&#0160; After all, Seldon was not a magician.&#0160; There are no trick methods of escaping a dilemma that he can see and we can&#39;t.&quot;<br />&#0160;&#0160;&#0160; &#0160;&#0160;&#0160; -- Salvor Hardin</p><p>Years ago at the Singularity Institute, the Board was entertaining a proposal to expand somewhat.&#0160; I wasn&#39;t sure our funding was able to support the expansion, so I insisted that - if we started running out of money - we decide in advance who got fired and what got shut down, in what order.</p><p>Even over the electronic aether, you could hear the uncomfortable silence.</p><p>&quot;Why can&#39;t we decide that at the time, if the worst happens?&quot; they said, or something along those lines.</p><p>&quot;For the same reason that when you&#39;re buying a stock you think will go up, you decide how far it has to decline before it means you were wrong,&quot; I said, or something along those lines; this being far back enough in time that I would still have used stock-trading in a rationality example.&#0160; &quot;If we can make that decision during a crisis, we ought to be able to make it now.&#0160; And if I can&#39;t trust that we can make this decision in a crisis, I can&#39;t trust this to go forward.&quot;</p><p>People are really, really reluctant to plan in advance for the abyss.&#0160; But what good reason is there <em>not</em> to?&#0160; How can you be worse off from knowing in <em>advance </em>what you&#39;ll do in the worse cases?</p><p></p><p></p><p></p><p></p><p>I have been trying fairly hard to keep my mouth shut about the current economic crisis.&#0160; But still -</p><p>Why didn&#39;t various governments create and publish a plan for what they would do in the event of various forms of financial collapse, <em>before</em> it actually happened?\n</p><a id=\"more\"></a><p>Never mind hindsight on the real-estate bubble - there are <em>lots</em> of things that could potentially trigger financial catastrophes.&#0160; I&#39;m willing to bet the American government knows what it will do in terms of immediate rescue operations if an atomic bomb goes off in San Francisco.&#0160; But if the US government had any advance idea of under which circumstances it would nationalize Fannie Mae or guarantee Bear Stearns&#39;s counterparties, this plan was not very much in evidence as various government officials gave every appearance of trying to figure everything out on the fly.</p><p>A<em> published, believable</em> advance plan for the worst case - one that you could actually believe the government would <em>carry out</em>, instead of junking the plan to try to keep the top spinning a little longer - would have made the markets that much less uncertain.</p><p>If you don&#39;t publish a plan for catastrophe - or can&#39;t publish a <em>believable</em> plan - then the market just tries to guess what a realistic, believable plan would look like.&#0160; If that realistic, believable plan involves frantically attempting to bail out the large financial entities in order to keep the whole system from melting down further, you have moral hazard.&#0160; If they actually do it, that&#39;s lemon socialism (privatized upside, public downside).</p><p>If that&#39;s what happens in the abyssal case - then <em>not</em> publishing that fact, doesn&#39;t prevent anyone from foreseeing it.&#0160; If you <em>publish</em> that plan, maybe it will start a debate about whether to break up Bear Stearns into smaller entities, or change the plan to give counterparties a predictable 10% haircut, or claw back executive bonuses no matter what their contracts read (because you <em>really aren&#39;t</em> supposed to screw up so badly that the government has to get involved)...</p><p>But if you can&#39;t <em>publish </em>a <em>realistic, believable </em>advance abyssal plan that <em>doesn&#39;t</em> call for rescuing the huge entities - then <em>who are you even kidding?</em></p><p>Governmental agencies failing to stare into the abyss in advance gives us a double problem: moral hazard as counterparties and investors try to guess what the government will <em>realistically</em> do; and fear and uncertainty in the market when the worst does happen.</p><p>It&#39;s questionable whether the government should be in the position of trying to <em>forecast </em>the abyss - to put a <em>probability</em> on financial meltdown in any given year due to any given cause.&#0160; But advance abyssal planning isn&#39;t about the <em>probability</em>, as it would be in investing.&#0160; It&#39;s about the <em>possibility</em>.&#0160; If you can realistically imagine global financial meltdowns of various types being possible, there&#39;s no excuse for not war-gaming them.&#0160; If your brain doesn&#39;t literally cease to exist upon facing systemic meltdowns <em>at the time,</em> you ought to be able to imagine plausible systemic meltdowns <em>in advance.</em></p><p>Sure, you might have to make some modifications on-the-fly because you didn&#39;t get the exact causes and circumstances right.&#0160; But it shouldn&#39;t be obvious and predictable that the modifications will consist of &quot;Oh dear it&#39;s more awful than we planned for and the systemic hazard is worse and now we really do have to bail out everyone even though we said we wouldn&#39;t.&quot;&#0160; Then the plan is not believable.</p><p>So long as the plan is not wrong in the stupidly obvious directions, it&#39;s hard to see how we&#39;d be <em>worse </em>off if the governors of the Federal Reserve had taken a week once per year to play through scenarios <em>more nightmarish than this one</em> in their minds, deciding in <em>advance</em> what to do about it, <em>realistically.</em></p><p>I suppose the main argument against <em>publishing </em>the plan would be that the uninformed public (i.e. Congress) would revolt against the emergency plans, demanding that unbelievable plans be substituted (let the banks burn! don&#39;t bail out GM!) and then changing their tune as soon as the worst actually happened.</p><p>But at least having the Federal Reserve <em>privately</em> visualizing all sorts of hideous possibilities in advance, war-gaming them with the Board of Governors, and planning for them <em>realistically</em> - so that when the worst starts happening, you don&#39;t have everyone running around being <em>vague</em> and visibly unprepared and refusing to talk about what happens if things get even worse - instead you just take out folder #37-B and figure out what needs tweaking - for the lack of <em>that </em>preparedness, there seems to me to be very little excuse.&#0160; The Federal Reserve should not be in the business of forecasting <em>probabilities</em> - they&#39;ve already demonstrated that they can&#39;t, and they&#39;re not investors.&#0160; They should just be always staring into the abyss.</p><p>Of course the Federal Reserve doesn&#39;t read this blog, so far as I know.&#0160; But it&#39;s the sort of thing that doesn&#39;t require a majority vote for individuals to use in their personal lives.</p><p></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KoXbd2HmbdRfqLngk": 1, "Lgy35Xh222bwgeGTL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nYEcgJe8LyqB4qqCL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 28, "extendedScore": null, "score": 3.9e-05, "legacy": true, "legacyId": "1253", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-23T23:30:48.747Z", "modifiedAt": "2022-04-20T02:47:39.200Z", "url": null, "title": "About Less Wrong", "slug": "about-less-wrong", "viewCount": null, "lastCommentedAt": "2019-02-08T15:56:58.365Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2om7AHEHtbogJmT5s/about-less-wrong", "pageUrlRelative": "/posts/2om7AHEHtbogJmT5s/about-less-wrong", "linkUrl": "https://www.lesswrong.com/posts/2om7AHEHtbogJmT5s/about-less-wrong", "postedAtFormatted": "Monday, February 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20About%20Less%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAbout%20Less%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2om7AHEHtbogJmT5s%2Fabout-less-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=About%20Less%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2om7AHEHtbogJmT5s%2Fabout-less-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2om7AHEHtbogJmT5s%2Fabout-less-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 711, "htmlBody": "<p><em><strong>Edit:&nbsp;</strong>This post refers to the original version of LessWrong, which ran between February 2009 and March 2018. The About page referring to the period from March 2018 to the present can be found <a href=\"https://www.lesswrong.com/about\">here</a>.</em></p>\n<p>&nbsp;</p>\n<p>Over the last decades, new experiments have changed science's picture of the way we think - the ways we succeed or fail to obtain the truth, or fulfill our goals. The heuristics and biases program, in cognitive psychology, has exposed dozens of major flaws in human reasoning. Social psychology shows how we succeed or fail in groups. Probability theory and decision theory have given us new mathematical foundations for understanding minds.</p>\n<p>Less Wrong is devoted to refining the art of human <a href=\"/lw/31/what_do_we_mean_by_rationality/\">rationality</a> - the art of thinking. The new math and science deserves to be applied to our daily lives, and heard in our public voices.</p>\n<p>Less Wrong consists of three areas: The <a href=\"/\">main community blog</a>, the <a href=\"http://wiki.lesswrong.com/\">Less Wrong wiki</a> and the <a href=\"/r/discussion/\">Less Wrong discussion area</a>.</p>\n<p>Less Wrong is a partially moderated community blog that allows general authors to contribute posts as well as comments. Users vote posts and comments up and down (with code based on <a href=\"http://code.reddit.com/\">Reddit's open source</a>). \"Promoted\" posts (appearing on the front page) are chosen by the editors on the basis of substantive new content, clear argument, good writing, popularity, and importance.</p>\n<p>We suggest submitting links with a <em>short</em> description. Recommended books should have longer descriptions. Links will not be promoted unless they are truly excellent - the \"promoted\" posts are intended as a filtered stream for the casual/busy reader.</p>\n<p>The Less Wrong discussion area is for topics not yet ready or not suitable for normal top level posts. To post a new discussion, select \"Post to: Less Wrong Discussion\" from the Create new article page. Comment on discussion posts as you would elsewhere on the site.</p>\n<p>Votes on posts are worth &plusmn;10 points on the main site and &plusmn;1 point in the discussion area. Votes on comments are worth &plusmn;1 point. Users with sufficient karma can publish posts. You need 20+ points to post to the main area and 2+ points to post to the discussion area. You can only down vote up to four times your current karma (thus if you never comment, you cannot downvote). Comments voted to -3 or lower will be collapsed by default for most readers (if you log in, you can change this setting in your Preferences). Please keep this in mind before writing long, thoughtful, intelligent responses to trolls: most readers will never see your work, and your effort may be better spent elsewhere, in more visible threads. Similarly, if many of your comments are heavily downvoted, please take the hint and change your approach, or choose a different venue for your comments. (Failure to take the hint may lead to moderators deleting future comments.) Spam comments will be deleted immediately. Off-topic top-level posts may be removed.</p>\n<p>We reserve the right for moderators to change contributed posts or comments to fix HTML problems or other misfeatures. Moderators may add or remove tags.</p>\n<p>Less Wrong is brought to you by the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute at Oxford University</a>. Neither FHI nor Oxford University necessarily endorses any specific views appearing anywhere on Less Wrong. Copyright is retained by each author, but we reserve the non-exclusive right to move, archive, or otherwise reprint posts and comments.<a id=\"more\"></a></p>\n<p>Sample posts:</p>\n<table border=\"0\" cellspacing=\"10\">\n<tbody>\n<tr valign=\"top\">\n<td><a href=\"/lw/gw/politics_is_the_mindkiller/\">Politics is the Mind-Killer</a></td>\n<td><a href=\"http://www.overcomingbias.com/2007/05/policy_tugowar.html\">Policy Tug-O-War</a></td>\n<td><a href=\"/lw/i0/are_your_enemies_innately_evil/\">Are Your Enemies Innately Evil?</a></td>\n</tr>\n<tr valign=\"top\">\n<td><a href=\"/lw/lg/the_affect_heuristic/\">The Affect Heuristic</a></td>\n<td><a href=\"/lw/jx/we_change_our_minds_less_often_than_we_think/\">We Change Our Minds Less Often Than We Think</a></td>\n<td><a href=\"/lw/jj/conjunction_controversy_or_how_they_nail_it_down/\">Conjunction Controversy</a></td>\n</tr>\n<tr valign=\"top\">\n<td><a href=\"/lw/hp/feeling_rational/\">Feeling Rational</a></td>\n<td><a href=\"/lw/he/knowing_about_biases_can_hurt_people/\">Knowing About Biases Can Hurt You</a></td>\n<td><a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomb's Problem</a></td>\n</tr>\n<tr valign=\"top\">\n<td><a href=\"/lw/oj/probability_is_in_the_mind/\">Probability is in the Mind</a></td>\n<td><a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">Absence of Evidence is Evidence of Absence</a></td>\n<td><a href=\"/lw/jo/einsteins_arrogance/\">Einstein's Arrogance</a></td>\n</tr>\n<tr valign=\"top\">\n<td><a href=\"/lw/js/the_bottom_line/\">The Bottom Line</a></td>\n<td><a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">Making Beliefs Pay Rent</a></td>\n<td><a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">Tsuyoku Naritai</a></td>\n</tr>\n</tbody>\n</table>\n<p><strong>To read through Less Wrong systematically, use the <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">Sequences</a>.</strong></p>\n<p>Less Wrong was established as a sister site to the blog <a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a>, where Eliezer Yudkowsky originally began blogging under the chief editorship of Robin Hanson, and contains many old posts imported from OB.</p>\n<p>To some extent, the software of LW is still under development. The code that runs this site is forked from the open source <a href=\"http://code.reddit.com/\" target=\"_blank\">Reddit base</a>, and is <a href=\"http://github.com/tricycle/lesswrong/\" target=\"_blank\">hosted on Github</a>. Our public issue tracker is hosted at <a href=\"http://code.google.com/p/lesswrong/\" target=\"_blank\">Google Code</a>. Contributions of <a href=\"http://github.com/tricycle/lesswrong/\" target=\"_blank\">code</a> or <a href=\"http://code.google.com/p/lesswrong/\" target=\"_blank\">issues</a> are welcome and volunteer Python developers are cordially solicited (see <a href=\"/lw/1t/wanted_python_open_source_volunteers/\">this post</a>). More information on how to contribute is available at the <a href=\"https://github.com/tricycle/lesswrong/wiki\">LW Github wiki</a>.</p>\n<p>Less Wrong is hosted and maintained by <a href=\"http://trikeapps.com/\" target=\"_blank\">Trike Apps</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "5f5c37ee1b5cdee568cfb1a0": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2om7AHEHtbogJmT5s", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 57, "baseScore": 56, "extendedScore": null, "score": 0.00011861334392850679, "legacy": true, "legacyId": "1", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bJ2haLkcGeLtTWaD5", "RcZCwxFiZzE6X7nsv", "9weLK2AJ9JEt2Tt8f", "28bAMAxhoX3bwbAKC", "Kow8xRzpfkoY7pa69", "buixYfcXBah9hbSNZ", "cXzTpSiCrNGzeoRAz", "SqF8cHjJv43mvJJzx", "AdYdLP2sRqPMoe8fb", "6ddcsdA2c2XpNpE5x", "f6ZLxEWaankRZ2Crv", "mnS2WYLCGJP2kQkRn", "MwQRucYo6BZZwjKE7", "34XxbRFe54FycoCDw", "a7n8GdKiAZRX86T5A", "DoLQN5ryZ9XkZjq5h", "J2sr4tdC4ThrbJRR3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2009-02-23T23:30:48.747Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-24T23:02:35.000Z", "modifiedAt": null, "url": null, "title": "Formative Youth", "slug": "formative-youth", "viewCount": null, "lastCommentedAt": "2017-07-12T22:08:01.821Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hwbopYqniG9iDqGDH/formative-youth", "pageUrlRelative": "/posts/hwbopYqniG9iDqGDH/formative-youth", "linkUrl": "https://www.lesswrong.com/posts/hwbopYqniG9iDqGDH/formative-youth", "postedAtFormatted": "Tuesday, February 24th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Formative%20Youth&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFormative%20Youth%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhwbopYqniG9iDqGDH%2Fformative-youth%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Formative%20Youth%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhwbopYqniG9iDqGDH%2Fformative-youth", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhwbopYqniG9iDqGDH%2Fformative-youth", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 989, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/yo/against_maturity/\">Against Maturity</a></p>\n<p style=\"margin-left: 40px;\">\"Rule of thumb:&nbsp; Be skeptical of things you learned before you could read.&nbsp; E.g., religion.\"<br />&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; -- <a href=\"http://ben.casnocha.com/2009/01/quote-of-.html\">Ben Casnocha </a></p>\n<p>Looking down on others is fun, and if there's one group we adults can <em>all </em>enjoy looking down on, it's children.&nbsp; At least I <em>assume </em>this is one of the driving forces behind the <em>incredible </em>disregard for... but don't get me started.</p>\n<p>Inconveniently, though, most of us were children at one point or another during our lives.&nbsp; Furthermore, many of us, as adults, still believe or choose certain things that we happened to believe or choose as children.&nbsp; This fact is incongruent with the general fun of condescension - it means that your life is being run by a child, even if that particular child happens to be your own past self.</p>\n<p>I suspect that most of us therefore underestimate the degree to which our youths were formative - because to admit that your youth was formative is to admit that the course of your life was not all steered by Incredibly Deep Wisdom and <a href=\"/lw/rc/the_ultimate_source/\">uncaused free will</a>.</p>\n<p>To give a concrete example, suppose you asked me, \"Eliezer, where does your altruism <em>originally</em> come from?&nbsp; What was the very <em>first </em>step in the chain that made you amenable to helping others?\"</p>\n<p>Then my best guess would be \"Watching <em>He-Man</em> and similar TV shows as a very young and impressionable child, then failing to compartmentalize the way my contemporaries did.\"&nbsp; (Same reason <a href=\"/lw/yo/against_maturity/\">my Jewish education didn't take</a>; I either genuinely believed something, or didn't believe it at all.&nbsp; (Not that I'm saying that I believed <em>He-Man</em> was fact; just that the altruistic behavior I picked up wasn't compartmentalized off into some safely harmless area of my brain, then or later.))</p>\n<p>It's my understanding that most people would be reluctant to admit this sort of historical fact, because it makes them sound childish - in the sense that they're still being governed by the causal history of a child.</p>\n<p>But I find myself skeptical that others are governed by their childhood causal histories <em>so much</em> less than myself - especially when there's a simple alternative explanation: they're too embarrassed to admit it.</p>\n<p><a id=\"more\"></a></p>\n<p>A lovely excuse, of course, is that we at first ended up in a certain place for childish reasons, and then we went back and redid the calculations as adults, and what do you know, it <a href=\"/lw/s3/the_genetic_fallacy/\">magically ended up</a> with the <a href=\"/lw/js/the_bottom_line/\">same bottom line</a>.</p>\n<p>Well - of course that <em>can</em> happen.&nbsp; If you ask me why I'm out to save the world, then there's a sense in which I can defend that as a sober utilitarian calculation, \"Shut up and multiply\", that has nothing to do with spending my childhood reading science fiction about protagonists who saved the world.&nbsp; But if you ask me why I <em>listen</em> to that sober utilitarian calculation, why it <em>actually </em>has the capacity to move me - then yes, the fact that the first \"grownup\" book I read was <em>Dragonflight</em> may have played a role.&nbsp; It's what F'lar and Lessa would do.</p>\n<p>Why not really start over from scratch - throw away our childhoods and redo everything?</p>\n<p>For <em>epistemic </em>beliefs that might be sorta-possible, which is why I didn't name an epistemic belief that I think I inherited from the chaos of childhood.&nbsp; <em>That </em>wouldn't be tolerable, and when I look back, I really <em>have </em><a href=\"/lw/u2/the_sheer_folly_of_callow_youth/\">rejected</a> a lot of what I once believed epistemically.</p>\n<p>But matters of taste?&nbsp; Of personality?&nbsp; Of deeply held ideals and values?</p>\n<p>Well, yes, I reformulated my whole metaethics at a certain point and that had a definite <em>influence </em>on my values... but despite that, I think you could draw an obvious line back from where I am now, to factors like reading <em>Dragonlance</em> at age nine and vowing never to end up like Raistlin Majere.&nbsp; (Bitter genius archetype.)</p>\n<p>If you can't look back and draw a line between your current adult self and factors like that, I have to wonder if your self-history is really accurate.</p>\n<p>In particular, I have to wonder if you're thinking right now of a deceptively obvious-seeming line that someone <em>else </em>might be tempted to draw, but which of course isn't the <em>real</em> reason why you still...</p>\n<p><strong>PS:</strong>&nbsp; Of course I don't directly justify any of my decisions, these days, by saying \"That's what the Thundercats did, therefore it is right.\"&nbsp; The question is more like whether I ended up finding developed <em>altruistic </em>philosophies more appealing as an adult because, sometime back in my youth, I was bombarded with <em>altruistic</em> messages.</p>\n<p>If there are many different stores selling developed philosophies, then which store you <em>walk into</em> to buy your sophisticated adult judgments might depend on a factor like that.</p>\n<p><strong>PPS</strong>:&nbsp; Several commenters asked why I focused on fiction.&nbsp; I could point to several real-life events in my childhood that I still remember and that seem promisingly characteristic of \"me\" - for example, the only time I remember my kindergarten classmates ever praising me or liking me was the time I used wooden blocks to build a complicated track that they could \"ski\" along.&nbsp; Making something clever = peer approval, says this memory.</p>\n<p>But because this was a <em>one-off</em> event, I doubt it would have quite as much influence as messages repeated over and over, through many different TV shows with similar themes, or many different books written by science-fiction authors who influenced one another.&nbsp; I couldn't recite the plot of even a single episode of <em>He-Man</em>, but I have some memory of what the opening theme song was, because it was <em>recurring.</em>&nbsp; That's the power of a fictional corpus, relative to any single moment of real life no matter how significant it seems - fictions can repeat the same message over and over.</p>\n<p>My childhood universe was very much a universe of books.&nbsp; The nonfiction I read (like the <em>Childcraft</em> books) might have been formative in a sense - but factual beliefs you really <em>can </em>recheck and redo.&nbsp; Hence my citation of fiction as a lingering influence on values and personality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q55STnFh6gbSezRuR": 1, "irYLXtT9hkPXoZqhH": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hwbopYqniG9iDqGDH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 32, "extendedScore": null, "score": 4.1e-05, "legacy": true, "legacyId": "1254", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rM7hcz67N7WtwGGjq", "EsMhFZuycZorZNRF5", "KZLa74SzyKhSJ3M55", "34XxbRFe54FycoCDw", "Yicjw6wSSaPdb83w9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-25T17:16:11.626Z", "modifiedAt": "2021-08-12T19:27:37.182Z", "url": null, "title": "Tell Your Rationalist Origin Story", "slug": "tell-your-rationalist-origin-story", "viewCount": null, "lastCommentedAt": "2017-08-31T05:05:02.665Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BHMBBFupzb4s8utts/tell-your-rationalist-origin-story", "pageUrlRelative": "/posts/BHMBBFupzb4s8utts/tell-your-rationalist-origin-story", "linkUrl": "https://www.lesswrong.com/posts/BHMBBFupzb4s8utts/tell-your-rationalist-origin-story", "postedAtFormatted": "Wednesday, February 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tell%20Your%20Rationalist%20Origin%20Story&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATell%20Your%20Rationalist%20Origin%20Story%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHMBBFupzb4s8utts%2Ftell-your-rationalist-origin-story%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tell%20Your%20Rationalist%20Origin%20Story%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHMBBFupzb4s8utts%2Ftell-your-rationalist-origin-story", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHMBBFupzb4s8utts%2Ftell-your-rationalist-origin-story", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 222, "htmlBody": "<p>To break up the awkward silence at the start of a recent Overcoming Bias meetup, I asked everyone present to tell their rationalist origin story - a key event or fact that played a role in their first beginning to aspire to rationality.&nbsp; This worked surprisingly well (and I would recommend it for future meetups).</p>\n<p>I think I've already told enough of my own origin story on <em>Overcoming Bias:</em> how I was digging in my parents' yard as a kid and found a tarnished silver amulet inscribed with Bayes's Theorem, and how I wore it to bed that night and dreamed of a woman in white, holding an ancient leather-bound book called <em>Judgment Under Uncertainty: Heuristics and Biases</em> (eds. D. Kahneman, P. Slovic, and A. Tversky, 1982)... but there's no need to go into that again.</p>\n<p>So, seriously... how did <em>you </em>originally go down that road?</p>\n<p><strong>Added:</strong>&nbsp; For some odd reason, many of the commenters here seem to have had a single experience in common - namely, at some point, encountering <em>Overcoming Bias</em><em>...</em>&nbsp; But I'm especially interested in what it takes to get the transition <em>started </em>- crossing the <em>first </em>divide.&nbsp; This would be very valuable knowledge if it can be generalized.&nbsp; If that did happen at <em>OB,</em> please try to specify what was the crucial \"Aha!\" insight (down to the specific post if possible).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"irYLXtT9hkPXoZqhH": 1, "ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BHMBBFupzb4s8utts", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 47, "baseScore": 37, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "2", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 416, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-02-27T04:28:14.083Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-26T00:12:38.613Z", "modifiedAt": null, "url": null, "title": "Image Test", "slug": "image-test", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wmoore", "createdAt": "2009-02-17T05:49:50.396Z", "isAdmin": false, "displayName": "wmoore"}, "userId": "EgQZcMBqxf6sGmKfi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZtckZmtwWxhwgpt8Y/image-test", "pageUrlRelative": "/posts/ZtckZmtwWxhwgpt8Y/image-test", "linkUrl": "https://www.lesswrong.com/posts/ZtckZmtwWxhwgpt8Y/image-test", "postedAtFormatted": "Thursday, February 26th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Image%20Test&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AImage%20Test%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZtckZmtwWxhwgpt8Y%2Fimage-test%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Image%20Test%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZtckZmtwWxhwgpt8Y%2Fimage-test", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZtckZmtwWxhwgpt8Y%2Fimage-test", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3, "htmlBody": "<p><img src=\"http://images.lesswrong.com/t3_3_0.png\" alt=\"\" width=\"12\" height=\"12\" /></p>\n<p><img src=\"http://images.lesswrong.com/t3_3_1.png?v=239ccf11aa69938d19117d327d7bcbfe\" alt=\"\" width=\"333\" height=\"50\" /></p>\n<p>Editing in IE6!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZtckZmtwWxhwgpt8Y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 1, "extendedScore": null, "score": 4.778181029939537e-07, "legacy": true, "legacyId": "3", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-26T00:55:33.000Z", "modifiedAt": null, "url": null, "title": "Markets are Anti-Inductive", "slug": "markets-are-anti-inductive", "viewCount": null, "lastCommentedAt": "2019-12-18T10:56:51.374Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h24JGbmweNpWZfBkM/markets-are-anti-inductive", "pageUrlRelative": "/posts/h24JGbmweNpWZfBkM/markets-are-anti-inductive", "linkUrl": "https://www.lesswrong.com/posts/h24JGbmweNpWZfBkM/markets-are-anti-inductive", "postedAtFormatted": "Thursday, February 26th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Markets%20are%20Anti-Inductive&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMarkets%20are%20Anti-Inductive%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh24JGbmweNpWZfBkM%2Fmarkets-are-anti-inductive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Markets%20are%20Anti-Inductive%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh24JGbmweNpWZfBkM%2Fmarkets-are-anti-inductive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh24JGbmweNpWZfBkM%2Fmarkets-are-anti-inductive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1065, "htmlBody": "<p>I suspect there&#39;s a <em>Pons Asinorum</em> of probability between the bettor who thinks that you make money on horse races by betting on the horse you think will win, and the bettor who realizes that you can only make money on horse races if you find horses whose odds seem <em>poorly calibrated</em> relative to superior probabilistic guesses.</p><p>There is, I think, a second <em>Pons Asinorum</em> associated with more advanced finance, and it is the concept that markets are an <em>anti</em>-inductive environment.</p><p>Let&#39;s say you see me flipping a coin.&#0160; It is not necessarily a fair coin.&#0160; It&#39;s a biased coin, and you don&#39;t know the bias.&#0160; I flip the coin nine times, and the coin comes up &quot;heads&quot; each time.&#0160; I flip the coin a tenth time.&#0160; What is the probability that it comes up heads?</p><p>If you answered &quot;ten-elevenths, by Laplace&#39;s Rule of Succession&quot;, you are a fine scientist in ordinary environments, but you will lose money in finance.</p><p>In finance the correct reply is, &quot;Well... if everyone <em>else</em> also saw the coin coming up heads... then by <em>now</em> the odds are probably back to fifty-fifty.&quot;</p><p>Recently on Hacker News I saw a commenter insisting that stock prices had nowhere to go but down, because the economy was in such awful shape.&#0160; If stock prices have nowhere to go but down, and everyone knows it, then trades won&#39;t clear - remember, for every seller there must be a buyer - until prices have gone down far enough that there is once again a possibility of prices going <em>up</em>.</p><p>So you can see the bizarreness of someone saying, &quot;Real estate prices\nhave gone up by 10% a year for the last N years, and we&#39;ve never\nseen a drop.&quot;&#0160; This treats the market like it was the mass of an\nelectron or something.&#0160; <em>Markets are anti-inductive.</em>&#0160; <em>If, historically, real estate prices have <strong>always gone up</strong>, they will\nkeep rising <strong>until they can go down</strong>.</em></p><p>\n</p><a id=\"more\"></a><p>\n</p><p>\n</p><p>To get an excess return - a return that pays premium interest over the going rate for that level of riskiness - you need to know something that other market participants don&#39;t, or they will rush in and bid up whatever you&#39;re buying (or bid down whatever you&#39;re selling) until the returns match prevailing rates.</p><p>If the economy is awful and everyone knows it, no one&#39;s going to buy at\na price that doesn&#39;t take into account that knowledge.</p><p>If there&#39;s an\nobvious possibility of prices dropping further, then the market must\nalso believe there&#39;s a probability of prices rising to make up for it, or the trades won&#39;t clear.</p><p>This elementary point has all sorts of caveats I&#39;m not bothering to include here, like the fact that &quot;up&quot; and &quot;down&quot; is relative to the <a href=\"/lw/hy/riskfree_bonds_arent/\">risk-free</a> interest rate and so on.&#0160; Nobody believes the market is really &quot;efficient&quot;, and recent events suggest it is less efficient than previously believed, and I have a certain friend who says it&#39;s even less efficient than that... but still, the market does not leave hundred-dollar-bills on the table if <em>everyone believes in them.</em></p><p>There was a time when the Dow systematically tended to drop on Friday and rise on Monday, and once this was noticed and published, the effect went away.</p><p><em>Past history</em>, e.g. &quot;real estate prices have always gone up&quot;, <em>is not private info.</em></p><p>And the same also goes for more complicated regularities.&#0160; Let&#39;s say two stock prices are historically anticorrelated - the variance in their returns moves in opposite directions.&#0160; <em>As soon as everyone believes this,</em> hedge-fund managers will leverage up and buy both stocks.&#0160; Everyone will do this, meaning that both stocks will rise.&#0160; As the stocks rise, their returns get more expensive.&#0160; The hedge-fund managers book profits, though, because their stocks are rising.&#0160; Eventually the stock prices rise to the point they can go down.&#0160; Once they do, hedge-fund managers who got in late will have to liquidate some of their assets to cover margin calls.&#0160; This means that both stock prices will go down - at the same time, even though they were originally anticorrelated.&#0160; Other hedge funds may lose money on the same two stocks and also sell or liquidate, driving the price down further, etcetera.&#0160; The correlative structure behaves anti-inductively, because other people can observe it too.</p><p>If mortage defaults are historically uncorrelated, so that you can get an excess return on risk by buying lots of mortages and pooling them together, then people will rush in and buy lots of mortgages until (a) rates on mortgages are bid down (b) individual mortgage failure rates rise (c) mortgage failure rates become more correlated, possibly looking uncorrelated in the short-term but having more future scenarios where they all fail at once.</p><p>Whatever is believed in, stops being real.&#0160; The market is literally <em>anti-inductive</em> rather than <em>anti-regular</em> - it&#39;s the regularity that enough participants <em>induce,</em> which therefore goes away.</p>\n<p>\n\n\n</p><p>This, as I understand it, is the <em>standard theory</em> of\n&quot;efficient markets&quot;, which should perhaps have been called\n&quot;inexploitable markets&quot; or &quot;markets that are not easy to exploit\nbecause others are already trying to exploit them&quot;.&#0160; Should I have made\na mistake thereof, let me be corrected.</p>\n<p>Now it&#39;s not surprising, on the one hand, to see this screwed up in random internet discussions where a gold bug argues from well-known observations about the <em>past history</em> of gold.&#0160; (This is the equivalent of trying to make money at horse-racing by betting on the horse that you think will win - failing to cross the <em>Pons Asinorum.</em>)</p><p>But it <em>is</em> surprising is to hear histories of the financial crisis in which prestigious actors argued in <em>crowded auditoriums</em> that, <em>previously</em>, real-estate prices had always gone up, or that <em>previously</em> mortage defaults had been uncorrelated.&#0160; This is naive inductive reasoning of the sort that only works on falling apples and rising suns and human behavior and everything else in the universe except markets.&#0160; Shouldn&#39;t everyone have frowned and said, &quot;But isn&#39;t the marketplace an anti-inductive environment?&quot;</p><p>Not that this is standard terminology - but perhaps &quot;efficient market&quot; doesn&#39;t convey quite the same warning as &quot;anti-inductive&quot;.&#0160; We would appear to need stronger warnings.</p><p><strong>PS:</strong>&#0160; To clarify, the coin example is a humorous exaggeration of what the world would be like if most physical systems behaved the same way as market price movements, illustrating the point, &quot;An exploitable pricing regularity that is easily inducted degrades into inexploitable noise.&quot;&#0160; Here the coin coming up &quot;heads&quot; is analogous to getting an above-market return on a publicly traded asset.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jgcAJnksReZRuvgzp": 20, "pnSDArjzAjkvAF5Jo": 8, "PvridmTCj2qsugQCH": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h24JGbmweNpWZfBkM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 74, "extendedScore": null, "score": 0.000111, "legacy": true, "legacyId": "1255", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 74, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mgmvs6BT3dSNxmyP2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-26T16:45:28.803Z", "modifiedAt": null, "url": null, "title": "Issues, Bugs, and Requested Features", "slug": "issues-bugs-and-requested-features", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:04.210Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qoFhFhGuFDRkb4GTq/issues-bugs-and-requested-features", "pageUrlRelative": "/posts/qoFhFhGuFDRkb4GTq/issues-bugs-and-requested-features", "linkUrl": "https://www.lesswrong.com/posts/qoFhFhGuFDRkb4GTq/issues-bugs-and-requested-features", "postedAtFormatted": "Thursday, February 26th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Issues%2C%20Bugs%2C%20and%20Requested%20Features&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIssues%2C%20Bugs%2C%20and%20Requested%20Features%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqoFhFhGuFDRkb4GTq%2Fissues-bugs-and-requested-features%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Issues%2C%20Bugs%2C%20and%20Requested%20Features%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqoFhFhGuFDRkb4GTq%2Fissues-bugs-and-requested-features", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqoFhFhGuFDRkb4GTq%2Fissues-bugs-and-requested-features", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 70, "htmlBody": "<p><strong>[Edit:</strong> Issues<span style=\"font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse; -webkit-border-horizontal-spacing: 2px; -webkit-border-vertical-spacing: 2px;\">,&nbsp;</span>Bugs, and Requested Features should be tracked at <a title=\"Google Code Issue tracker for LW\" href=\"http://code.google.com/p/lesswrong/issues/list\">Google Code</a>, not here&nbsp;-- matt, 2010-04-23<strong>]&nbsp;</strong></p>\n<p>&nbsp;</p>\n<p>Less Wrong is still under construction.&nbsp; Please post any bugs or issues with Less Wrong to this thread.&nbsp; Try to keep each comment thread a clean discussion of each bug or issue.</p>\n<p>Requested features... sure, go ahead, but bear in mind we may not be able to implement for a while.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qoFhFhGuFDRkb4GTq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 9, "extendedScore": null, "score": 4.779591925874291e-07, "legacy": true, "legacyId": "5", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 674, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-27T04:06:15.000Z", "modifiedAt": null, "url": null, "title": "Tell Your Rationalist Origin Story... at Less Wrong", "slug": "tell-your-rationalist-origin-story-at-less-wrong", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fwWyQgAFRuNpQJDPm/tell-your-rationalist-origin-story-at-less-wrong", "pageUrlRelative": "/posts/fwWyQgAFRuNpQJDPm/tell-your-rationalist-origin-story-at-less-wrong", "linkUrl": "https://www.lesswrong.com/posts/fwWyQgAFRuNpQJDPm/tell-your-rationalist-origin-story-at-less-wrong", "postedAtFormatted": "Friday, February 27th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tell%20Your%20Rationalist%20Origin%20Story...%20at%20Less%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATell%20Your%20Rationalist%20Origin%20Story...%20at%20Less%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfwWyQgAFRuNpQJDPm%2Ftell-your-rationalist-origin-story-at-less-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tell%20Your%20Rationalist%20Origin%20Story...%20at%20Less%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfwWyQgAFRuNpQJDPm%2Ftell-your-rationalist-origin-story-at-less-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfwWyQgAFRuNpQJDPm%2Ftell-your-rationalist-origin-story-at-less-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 232, "htmlBody": "<div style=\"margin-left: 40px;\">(<a href=\"http://lesswrong.com/\">A beta version of Less Wrong is now live</a>, no old posts imported as yet.&#0160; Some of the plans for what to do with Less Wrong relative to OB have been revised by further discussion among Robin, Nick, and myself, but for now we&#39;re just seeing what happens once LW is up - whether it&#39;s stable, what happens to the tone of comments once threading and voting is enabled, etcetera.<br /><br />Posting by non-admins is disabled for now - today we&#39;re just testing out registration, commenting, threading, etcetera.)<br /></div>\n \n <p>To\nbreak up the awkward silence at the start of a recent Overcoming Bias\nmeetup, I asked everyone present to tell their rationalist origin story\n- a key event or fact that played a role in their becoming rationalists.&#0160; This worked surprisingly well.</p><div class=\"content clear\" id=\"entry_t3_2\"><div class=\"md\"><div>\n<p>I think I&#39;ve already told enough of my own origin story on <em>Overcoming Bias:</em>\nhow I was digging in my parents&#39; yard as a kid and found a tarnished\nsilver amulet inscribed with Bayes&#39;s Theorem, and how I wore it to bed\nthat night and dreamed of a woman in white, holding a leather-bound\nbook called <em>Judgment Under Uncertainty: Heuristics and Biases</em> (eds. D. Kahneman, P. Slovic, and A. Tversky, 1982)... but there&#39;s no need to go into that again.</p>\n<p>So, seriously... how did <em>you </em>originally go down that road?</p><p><a href=\"http://lesswrong.com/lw/2/tell_your_rationalist_origin_story/\">Continue reading &quot;Tell Your Rationalist Origin Story&quot; at Less Wrong \u00bb</a></p></div></div></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "iTe27Ced8s8bGuvMK": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fwWyQgAFRuNpQJDPm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 4.780562218305668e-07, "legacy": true, "legacyId": "1256", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BHMBBFupzb4s8utts"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-27T20:15:59.430Z", "modifiedAt": null, "url": null, "title": "The Most Important Thing You Learned", "slug": "the-most-important-thing-you-learned", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:34.181Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bsdxuNdSGbfEKZREP/the-most-important-thing-you-learned", "pageUrlRelative": "/posts/bsdxuNdSGbfEKZREP/the-most-important-thing-you-learned", "linkUrl": "https://www.lesswrong.com/posts/bsdxuNdSGbfEKZREP/the-most-important-thing-you-learned", "postedAtFormatted": "Friday, February 27th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Most%20Important%20Thing%20You%20Learned&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Most%20Important%20Thing%20You%20Learned%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbsdxuNdSGbfEKZREP%2Fthe-most-important-thing-you-learned%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Most%20Important%20Thing%20You%20Learned%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbsdxuNdSGbfEKZREP%2Fthe-most-important-thing-you-learned", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbsdxuNdSGbfEKZREP%2Fthe-most-important-thing-you-learned", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 273, "htmlBody": "<p>My current plan <em>does</em> still call for me to write a rationality book - at some point, and despite all delays - which means I have to decide what goes in the book, and what doesn't.&nbsp; Obviously the vast majority of my OB content can't go into the book, because there's so much of it.</p>\n<p>So let me ask - what was the <em>one</em> thing you learned from my posts on <em>Overcoming Bias,</em> that stands out as most important in your mind?&nbsp; If you like, you can also list your numbers 2 and 3, but it will be understood that any upvotes on the comment are just agreeing with the #1, not the others.&nbsp; If it was striking enough that you remember the exact post where you \"got it\", include that information.&nbsp; If you think the most important thing is for me to rewrite a post from Robin Hanson or another contributor, go ahead and say so.&nbsp; To avoid recency effects, you might want to take a quick glance at <a href=\"http://www.cs.auckland.ac.nz/~andwhay/postlist.html\">this list of all my OB posts</a> before naming anything from just the last month - on the other hand, if you can't remember it even after a year, then it's probably not the most important thing.</p>\n<p>Please also distinguish this question from \"What was the most <em>frequently </em>useful thing you learned, and how did you use it?\" and \"What one thing has to go into the book that would (actually) make you buy a copy of that book for someone <em>else </em>you know?\"&nbsp; I'll ask those on Saturday and Sunday.</p>\n<p><strong>PS:</strong>&nbsp; Do please think of your answer before you read the others' comments, of course.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JMD7LTXTisBzGAfhX": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bsdxuNdSGbfEKZREP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 15, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "9", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 98, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-02-28T18:43:56.156Z", "modifiedAt": null, "url": null, "title": "The Most Frequently Useful Thing", "slug": "the-most-frequently-useful-thing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:38.767Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Gjb5fJuDBWamPoCoz/the-most-frequently-useful-thing", "pageUrlRelative": "/posts/Gjb5fJuDBWamPoCoz/the-most-frequently-useful-thing", "linkUrl": "https://www.lesswrong.com/posts/Gjb5fJuDBWamPoCoz/the-most-frequently-useful-thing", "postedAtFormatted": "Saturday, February 28th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Most%20Frequently%20Useful%20Thing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Most%20Frequently%20Useful%20Thing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGjb5fJuDBWamPoCoz%2Fthe-most-frequently-useful-thing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Most%20Frequently%20Useful%20Thing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGjb5fJuDBWamPoCoz%2Fthe-most-frequently-useful-thing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGjb5fJuDBWamPoCoz%2Fthe-most-frequently-useful-thing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 74, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/9/the_most_important_thing_you_learned\">The Most Important Thing You Learned</a></p>\n<p>What's the most <em>frequently useful</em> thing you've learned on OB - not the most memorable or most valuable, but the thing you <em>use </em>most <em>often?</em>&nbsp; What influences your behavior, factors in more than one decision?&nbsp; Please give a concrete example if you can.&nbsp; This isn't limited to archetypally \"mundane\" activities: if your daily life involves difficult research or arguing with philosophers, go ahead and describe that too.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "fkABsGCJZ6y9qConW": 1, "izp6eeJJEg9v5zcur": 1, "JMD7LTXTisBzGAfhX": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Gjb5fJuDBWamPoCoz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 12, "extendedScore": null, "score": 4.783861814711293e-07, "legacy": true, "legacyId": "15", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bsdxuNdSGbfEKZREP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-01T12:04:39.721Z", "modifiedAt": null, "url": null, "title": "That You'd Tell All Your Friends", "slug": "that-you-d-tell-all-your-friends", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:34.192Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ajYePZpAM4FYMwrqT/that-you-d-tell-all-your-friends", "pageUrlRelative": "/posts/ajYePZpAM4FYMwrqT/that-you-d-tell-all-your-friends", "linkUrl": "https://www.lesswrong.com/posts/ajYePZpAM4FYMwrqT/that-you-d-tell-all-your-friends", "postedAtFormatted": "Sunday, March 1st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20That%20You'd%20Tell%20All%20Your%20Friends&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThat%20You'd%20Tell%20All%20Your%20Friends%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FajYePZpAM4FYMwrqT%2Fthat-you-d-tell-all-your-friends%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=That%20You'd%20Tell%20All%20Your%20Friends%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FajYePZpAM4FYMwrqT%2Fthat-you-d-tell-all-your-friends", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FajYePZpAM4FYMwrqT%2Fthat-you-d-tell-all-your-friends", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 109, "htmlBody": "<div id=\"entry_t3_f\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/f/the_most_frequently_useful_thing\">The Most Frequently Useful Thing</a><a href=\"/lw/f/the_most_frequently_useful_thing\"></a></p>\n<p>What's the number one thing that goes into a book on rationality, which would make you buy a copy of that book for a friend?&nbsp; We can, of course, talk about all the ways that the rationality of the Distant World At Large needs to be improved.&nbsp; But in this case - I think the more useful data might be the <a href=\"http://www.overcomingbias.com/2009/01/a-tale-of-two-tradeoffs.html\">Near</a> question, \"With respect to the people I actually know, what do I want to see in that book, so that I can give the book to them to explain it?\"</p>\n<p>(And again, please think of your own answer-component before reading others' comments.)</p>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "mip7tdAN87Jarkcew": 1, "aHjTRDkGypPqbXWpN": 1, "JMD7LTXTisBzGAfhX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ajYePZpAM4FYMwrqT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 8, "extendedScore": null, "score": 4.785345417128503e-07, "legacy": true, "legacyId": "16", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Gjb5fJuDBWamPoCoz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-01T13:21:34.375Z", "modifiedAt": null, "url": null, "title": "Test Your Rationality", "slug": "test-your-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:35.755Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinHanson", "createdAt": "2009-02-26T13:46:26.443Z", "isAdmin": false, "displayName": "RobinHanson"}, "userId": "P4HT9AG3PuXjZv5Mw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Kn6H8Tk6EPT4Atq4k/test-your-rationality", "pageUrlRelative": "/posts/Kn6H8Tk6EPT4Atq4k/test-your-rationality", "linkUrl": "https://www.lesswrong.com/posts/Kn6H8Tk6EPT4Atq4k/test-your-rationality", "postedAtFormatted": "Sunday, March 1st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Test%20Your%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATest%20Your%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKn6H8Tk6EPT4Atq4k%2Ftest-your-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Test%20Your%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKn6H8Tk6EPT4Atq4k%2Ftest-your-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKn6H8Tk6EPT4Atq4k%2Ftest-your-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 318, "htmlBody": "<p>So you think you want to be rational, to believe what is true even when sirens tempt you?&nbsp; Great, get to work; there's lots you can do.&nbsp; Do you want to justifiably believe that you <em>are</em> more rational than others, smugly knowing your beliefs are more accurate?&nbsp; Hold on; this is <em>hard</em>.&nbsp; <br /><br />Humans nearly universally find excuses to believe that they are more correct that others, at least on the important things. They point to others' incredible beliefs, to biases afflicting others, and to estimation tasks where they are especially skilled.&nbsp; But they forget most everyone can point to such things.&nbsp;&nbsp; <br /><br />But shouldn't you get more rationality credit if you spend more time studying common biases, statistical techniques, and the like?&nbsp; Well this would be good evidence of your rationality <strong>if</strong> you were in fact pretty <em>rational about your rationality</em>, i.e., if you knew that when you read or discussed such issues your mind would then systematically, broadly, and reasonably incorporate those insights into your reasoning processes.&nbsp; <br /><br />But what if your mind is far from rational?&nbsp; What if your mind is likely to just go through the motions of studying rationality to allow itself to smugly believe it is more accurate, or to bond you more closely to your social allies?&nbsp; <br /><br />It seems to me that if you are serious about actually <em>being</em> rational, rather than just believing in your rationality or joining a group that thinks itself rational, you should try hard and often to <strong>test</strong> your rationality.&nbsp; But how can you do that?&nbsp; <br /><br />To test the rationality of your beliefs, you could sometimes declare beliefs, and later score those beliefs via tests where high scoring beliefs tend to be more rational.&nbsp; Better tests are those where scores are more tightly and reliably correlated with rationality.&nbsp; So, what are good rationality tests?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "32DdRimdM7sB5wmKu": 1, "5f5c37ee1b5cdee568cfb163": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Kn6H8Tk6EPT4Atq4k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 43, "extendedScore": null, "score": 6.9e-05, "legacy": true, "legacyId": "17", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 87, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-02T15:33:22.933Z", "modifiedAt": null, "url": null, "title": "Unteachable Excellence", "slug": "unteachable-excellence", "viewCount": null, "lastCommentedAt": "2021-11-18T08:54:04.726Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/34Tu4SCK5r5Asdrn3/unteachable-excellence", "pageUrlRelative": "/posts/34Tu4SCK5r5Asdrn3/unteachable-excellence", "linkUrl": "https://www.lesswrong.com/posts/34Tu4SCK5r5Asdrn3/unteachable-excellence", "postedAtFormatted": "Monday, March 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Unteachable%20Excellence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUnteachable%20Excellence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F34Tu4SCK5r5Asdrn3%2Funteachable-excellence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Unteachable%20Excellence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F34Tu4SCK5r5Asdrn3%2Funteachable-excellence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F34Tu4SCK5r5Asdrn3%2Funteachable-excellence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 741, "htmlBody": "<p>There's a whole genre of literature whose authors want to sell you the secret success sauce behind Gates's Microsoft or Buffett's Berkshire Hathaway - the common theme being that <em>you,</em> yes, <em>you</em> can be the next Larry Page.</p>\n<p>But probably not even Warren Buffett can teach you to be the next Warren Buffett.&nbsp; That kind of <em>extraordinary</em> success is <em>extraordinary</em> because no one has yet figured out how to teach it reliably.</p>\n<p>And so mostly these books are <a href=\"http://www.overcomingbias.com/2007/04/lotteries_a_was.html\">a waste of hope</a>, feeding off the excitement from dangling the possibility of the glorious yet unattainable; which is why I call them \"excellence pornography\", with subgenres like investment pornography and business pornography, telling every barista how to run the next Starbucks and every MBA student how to be the best CEO in the Fortune 500.&nbsp; Calling this \"excellence pornography\" might be too unkind to pornography, which is at least overtly fiction.</p>\n<p>Now, there <em>are</em> incredibly powerful techniques that civilization has figured out how to teach: techniques like \"test your ideas by experiment\" or \"reinvest your wealth to generate more wealth\".&nbsp; You, yes, <em>you</em> can be a scientist!&nbsp; Or maybe not everyone - but <em>enough </em>people can become scientists by using <em>learnable </em>techniques and <em>communicable </em>knowledge, to support our technological civilization.</p>\n<p>\"You, yes, <em>you</em> can reinvest the proceeds of your earlier investments!\"&nbsp; You may not <em>beat</em> the market like Warren Buffett.&nbsp; But if you think about a <em>whole civilization</em> practicing that rule, we do better nowadays than historical societies with no banks or stock markets.&nbsp; (No, really, we still do better on net.)&nbsp; Because the trick of Reinvestment <em>can</em> be taught, <em>can</em> be described in words, <em>can</em> work for ordinary people without extraordinary luck... we don't think of it as an extraordinary triumph.&nbsp; Just <em>anyone</em> can do it, so <a href=\"http://www.overcomingbias.com/2008/10/mundane-magic.html\">it must not be important</a>.<a id=\"more\"></a></p>\n<p>Warren Buffett did manage to turn on a lot of people to value investing.&nbsp; He's given out a lot of advice, and it looks like good advice to me on the occasions I've read it.&nbsp; The impression I get, at least, is that if he knew how to <em>communicate </em>what was left, he would just <em>tell </em>you.</p>\n<p>But Berkshire Hathaway, and Buffett personally, still spend huge amounts of time <em>looking </em>for managerial excellence.&nbsp; Why?&nbsp; Because they don't know any systematically reliable process to start with bright kids and turn them into Fortune 500 CEOs.</p>\n<p>There <em>are </em>things you can learn from the superstars.&nbsp; But you can't expect to eat their <em>whole </em>soul, and the last mile of their extraordinariness will be the hardest for them to <em>teach</em>.&nbsp; You will, at best, learn a few useful tricks that a lot of other people can learn as well, and that won't put you anywhere near the delicious high status of the superstar.&nbsp; Unless, of course, you yourself have the right mix of raw genetic talents and you put years and years into training yourself and have a lot of luck along the way, etcetera, but the point is, you won't get there by reading pornography.</p>\n<p>(If someone actually does come up with a new <em>teachable</em> supertrick, so that civilization itself is about to take another lurching step forward, then you should expect to have a lot of fellow superstars by the time you're done learning!)&nbsp;</p>\n<p>There's a number of lessons that I draw from this point; but one of the main ones is that much of the most important information we can learn from history is about <a href=\"http://www.iwillteachyoutoberich.com/blog/how-to-not-lose/\">how to not lose</a>, rather than how to win.</p>\n<p>It's easier to <em>avoid </em>duplicating spectacular failures than to duplicate spectacular successes.&nbsp; And it's often easier to generalize failure between domains.&nbsp; The instructions for \"how to be a superstar\" tend to be highly specific and domain-specialized (Buffett &ne; Einstein) but the lessons for \"how not to be an idiot\" have a lot more in common between professions.</p>\n<p>Ken Lay can teach you how <em>not </em>to be the next Enron a lot more easily than Warren Buffett can teach you to be the next Berkshire Hathaway.&nbsp; <a href=\"http://www.overcomingbias.com/2007/02/a_time_to_lose_.html\">Casey Serin</a> can teach you how to lose hope, <a href=\"http://www.overcomingbias.com/2007/08/mysterious-answ.html\">Lord Kelvin</a> can teach you not to worship your own ignorance...</p>\n<p>But that kind of lesson won't make you a glorious superstar.&nbsp; It may prevent your life from becoming miserable, but this is not as glamorous.&nbsp; And even worse - this kind of lesson may end up showing you that you're doing something <em>wrong,</em> that <em>you</em> yes <em>you</em> are about to join the ranks of fools.</p>\n<p>It's a lot easier to sell excellence pornography.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YQW2DxpZFTrqrxHBJ": 1, "x3zyEPFaJANB2BHmP": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "34Tu4SCK5r5Asdrn3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 40, "extendedScore": null, "score": 6.1e-05, "legacy": true, "legacyId": "22", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-03T18:13:17.465Z", "modifiedAt": null, "url": null, "title": "The Costs of Rationality", "slug": "the-costs-of-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.899Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinHanson", "createdAt": "2009-02-26T13:46:26.443Z", "isAdmin": false, "displayName": "RobinHanson"}, "userId": "P4HT9AG3PuXjZv5Mw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9Z3pezjiWLfNANg9P/the-costs-of-rationality", "pageUrlRelative": "/posts/9Z3pezjiWLfNANg9P/the-costs-of-rationality", "linkUrl": "https://www.lesswrong.com/posts/9Z3pezjiWLfNANg9P/the-costs-of-rationality", "postedAtFormatted": "Tuesday, March 3rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Costs%20of%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Costs%20of%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Z3pezjiWLfNANg9P%2Fthe-costs-of-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Costs%20of%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Z3pezjiWLfNANg9P%2Fthe-costs-of-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Z3pezjiWLfNANg9P%2Fthe-costs-of-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 374, "htmlBody": "<p>The word \"rational\" is overloaded with associations, so let me be clear: to me [here], more \"rational\" means better believing what is true, given one's limited info and analysis resources.&nbsp; <br /><br />Rationality certainly can have instrumental advantages.&nbsp; There are plenty of situations where being more rational helps one achieve a wide range of goals.&nbsp; In those situtations, \"winnners\", i.e., those who better achieve their goals, should tend to be more rational.&nbsp; In such cases, we might even estimate someone's rationality by looking at his or her \"residual\" belief-mediated success, i.e., after explaining that success via other observable factors.</p>\n<p>But note: we humans were designed in many ways <em>not</em> to be rational, because believing the truth often got in the way of achieving goals evolution had for us.&nbsp; So it is important for everyone who intends to seek truth to clearly understand: rationality has <em>costs</em>, not only in time and effort to achieve it, but also in conflicts with other common goals.</p>\n<p>Yes, rationality <em>might</em> help you win that game or argument, get promoted, or win her heart.&nbsp; <em>Or</em> more rationality for you might hinder those outcomes.&nbsp; If what you really want is love, respect, beauty, inspiration, meaning, satisfaction, or success, as commonly understood, we just cannot assure you that rationality is your best approach toward those ends.&nbsp; In fact we often know it is not.</p>\n<p>The truth may well be messy, ugly, or dispriting; knowing it make you <em>less</em> popular, loved, or successful.&nbsp; These are actually pretty likely outcomes in many identifiable situations.&nbsp; You may think you want to know the truth no matter what, but how sure can you really be of that?&nbsp; Maybe you just like the heroic image of someone who wants the truth no matter what; or maybe you only really want to know the truth if it is the bright shining glory you hope for.&nbsp;</p>\n<p>Be warned; the truth just is what it is.&nbsp; If just knowing the truth is not reward enough, perhaps you'd be better off not knowing.&nbsp; Before you join us in this quixotic quest, ask yourself: do you <em>really</em> want to be <em>generally</em> rational, on all topics?&nbsp; Or might you be better off limiting your rationality to the usual practical topics where rationality is respected and welcomed?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"9YFoDPFwMoWthzgkY": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9Z3pezjiWLfNANg9P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 47, "baseScore": 37, "extendedScore": null, "score": 5.8e-05, "legacy": true, "legacyId": "19", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 81, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-03T23:14:39.495Z", "modifiedAt": null, "url": null, "title": "Teaching the Unteachable", "slug": "teaching-the-unteachable", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:01.112Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9SaAyq7F7MAuzAWNN/teaching-the-unteachable", "pageUrlRelative": "/posts/9SaAyq7F7MAuzAWNN/teaching-the-unteachable", "linkUrl": "https://www.lesswrong.com/posts/9SaAyq7F7MAuzAWNN/teaching-the-unteachable", "postedAtFormatted": "Tuesday, March 3rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Teaching%20the%20Unteachable&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATeaching%20the%20Unteachable%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9SaAyq7F7MAuzAWNN%2Fteaching-the-unteachable%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Teaching%20the%20Unteachable%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9SaAyq7F7MAuzAWNN%2Fteaching-the-unteachable", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9SaAyq7F7MAuzAWNN%2Fteaching-the-unteachable", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1704, "htmlBody": "<p><strong>Previously in series</strong>:&nbsp; <a href=\"/lw/m/unteachable_excellence/\">Unteachable Excellence</a><br /><strong>Followup to</strong>:&nbsp; <a href=\"https://www.lesswrong.com/posts/YhgjmCxcQXixStWMC/artificial-addition/\">Artificial Addition<br /></a></p>\n<p>The literary industry that I called \"<a href=\"/lw/m/unteachable_excellence/\">excellence pornography</a>\" isn't very good at what it does.&nbsp; But it is failing at a very important job.&nbsp; When you consider the net benefit to civilization of Warren Buffett's superstar skills, versus the less glamorous but more <em>communicable </em>trick of \"reinvest wealth to create more wealth\" - there's hardly any comparison.&nbsp; You can see how much it <em>would</em> matter, if you could figure out how to communicate just <em>one more skill</em> that used to be a secret sauce.&nbsp; Not the pornographic promise of consuming the entire soul of a superstar.&nbsp; Just figuring out how to reliably teach <em>one more thing,</em> even if it wasn't <em>everything</em>...</p>\n<p>What makes a success hard to duplicate?</p>\n<p>Naked statistical chance is always incommunicable.&nbsp; No matter what you say about your historical luck, you can't teach someone else to have it.&nbsp; The arts of seizing opportunity, and <a href=\"http://ben.casnocha.com/2007/05/expose_yourself.html\">exposing yourself to positive randomness</a>, are commonly underestimated; I've seen people stopped in their tracks by \"bad luck\" that a Silicon Valley entrepreneur would drive over like a steamroller flattening speed bumps...&nbsp; Even so, there is still an element of genuine chance left over.</p>\n<p><a href=\"http://www.overcomingbias.com/2008/05/einsteins-super.html\">Einstein's superstardom</a> depended on his genetics that gave him the <em>potential</em> to learn his skills.&nbsp; If a skill relies on having that much brainpower, you can't teach it to most people... Though if the <em>potential </em>is one-in-a-million, then six thousand Einsteins around the world would be an <em>improvement</em>.&nbsp; (And if we're going to be really creative, who says genes are <em>incommunicable?</em>&nbsp; It just takes more advanced technology than a blackboard, that's all.)</p>\n<p>So when we factor out the genuinely unteachable - what's left?&nbsp; Where you can you push the border?&nbsp; What is it that might be <em>possible </em>to teach - albeit perhaps very difficult - and isn't being taught?</p>\n<p>I was once told that half of Nobel laureates were the students of other Nobel laureates.&nbsp; <a href=\"http://www.careerchem.com/NAMED/NobelAnecdotes1.pdf\">This source</a> seems to assert 155 out of 503.&nbsp; (Interestingly, the same source says that the number of Nobel laureates with Nobel \"grandparents\" (teachers of teachers) is just 60.)&nbsp; Even after discounting for cherry-picking of students and political pull, this suggests to me that you can learn things by apprenticeship - close supervision, free-form discussion, ongoing error correction over a long period of time - that no Nobel laureate has <em>yet </em>succeeding in putting into any of their many books.</p>\n<p>What is it that the students of Nobel laureates learn, but can't put into words?<a id=\"more\"></a></p>\n<p>This subject holds a fascination for me, because how it delves into the meta, the source behind, <a href=\"http://www.overcomingbias.com/2007/11/artificial-addi.html\">the gap between the output and the generator</a>.&nbsp; We can explain Einstein's General Relativity to students, but we can't make them Einstein.&nbsp; (If you look at it from the right angle, the whole trick of human intelligence is just an incommunicable insight that humans have and can't explain to a computer.)</p>\n<p>The amount of wordless intelligence in our work tends to be underestimated because <a href=\"http://www.overcomingbias.com/2008/07/detached-lever.html\">the words themselves are so much easier to introspect on</a>.&nbsp; But when I'm paying attention, I can see how much of my searchpower takes place in fast flashes of perception that tell me what's <em>important,</em> which thought to think next.</p>\n<p>When I met my apprentice Marcello he was already better at mathematical proof than myself, certainly much faster.&nbsp; He'd competed at the national level - but in competitions like that you get <em>told</em> which problems are important.&nbsp; (And also in competitions, you instantly hand in the problem when you're done, and rush on to the next one; without looking over your proof to see if you can simplify it, see it at a glance, learn something more.)&nbsp; But the really critical thing I was trying to teach him - testing to see if it could even be taught at all - was this sense of <em>which</em> AI problems led somewhere.&nbsp; \"You can pedal as well as I can,\" I said to him early on when he asked how he was doing, \"but I'm still doing ninety percent of the <em>steering.</em>\"&nbsp; And it was a constant, tremendous struggle to put anything into words about <em>why </em>I thought&nbsp; that we hadn't yet found the really important insight that was lurking somewhere in a problem, and so we were going to discard Marcello's current proof and reformulate the problem and try <em>again </em>from another angle, to see if this time we would really <em>understand</em> something.</p>\n<p>We go through our life events, and our brain uses an opaque algorithm to grind the experiences to grist, and outputs yet another opaque neural net of circuitry: the procedural skill, the source of wordless intuitions that you know so fast you can't see yourself knowing them.&nbsp; \"The zeroth step\", I called it, the step in reasoning that comes before the first step and goes by so quickly that you don't realize it's there.</p>\n<p>I pride myself on being good at putting things into words, at being able to introspect on the momentary flashes and see their pattern and trend, even if I can't print out the circuitry that is their source.&nbsp; But when I tried to communicate my cutting edge, the borderline where I advanced my knowledge - then my words were defeated, and I was left working with Marcello on problem after problem, hoping his brain would pick up that unspoken rhythm of the steering:&nbsp; <em>Turn left, turn right; this is probably worth pursuing, this is not; this seems like a valuable insight, this is just a black box around our ignorance.</em></p>\n<p>I'd expected it to go like that; I'd never had the delusion that the most important parts of thought would be easy to put in words.&nbsp; If it were that simple we really would have had Artificial Intelligence in the 1970s.</p>\n<p>Civilization gets by on teaching the output of the generator without teaching the generator.&nbsp; Einstein output his various discoveries, and then the <em>generated </em>knowledge is verbal enough to be passed on to students in university.&nbsp; When another Einstein is needed, civilization just holds its breath and hopes.</p>\n<p>But if these wordless skills are the product of experience - then why not <a href=\"http://www.overcomingbias.com/2009/02/truth-from-fiction.html\">communicate the experiences</a>?&nbsp; Or if fiction isn't good enough, and it probably isn't even close, then why not duplicate the experiences - put people through the same events?</p>\n<p>(1)&nbsp; Superstars may not <em>know </em>what their critical experiences were.</p>\n<p>(2)&nbsp; The critical experiences may be difficult to duplicate - for example, everyone already knows the answer to Special Relativity, and now we can't train people by giving them the problem of Special Relativity.&nbsp; Just knowing that it has something to do with space and time shifting around, is already too much of a spoiler.&nbsp; The really important part of the problem is the one where you stare at a blank sheet of paper until drops of blood form on your forehead, trying to figure out what to think next.&nbsp; The skills of genius are rare, I've <a href=\"http://www.overcomingbias.com/2008/05/eld-science.html\">suggested</a>, because there is not enough opportunity to practice them.</p>\n<p>(3)&nbsp; There may be luck or genetic talent involved in your brain hitting on the right thing to learn - finding a solution of high quality in the space of wordless procedural skills.&nbsp; Even if we put you through the same experiences, there's components of true chance and genetic talent left over in having your brain <em>learn</em> the same wordless skill.</p>\n<p>But I think there's still reason to go on trying to describe the indescribable and teach the unteachable.</p>\n<p>Consider the transition in <em>gambling skill</em> associated with the invention of <em>probability theory</em> a few centuries back.&nbsp; There's still a leftover art to poker, wordless skills that poker superstars can only partially describe in words.&nbsp; But go back far enough, and no one would have any idea how to <em>calculate </em>the odds of rolling three dice and coming up with all ones.&nbsp; And maybe an experienced enough gambler would have a wordless intuition that some things were likelier than others, but they couldn't have put it into words - couldn't have <em>told </em>anyone else what they'd learned about the chances; except, maybe, through a long process of watching over an apprentice's shoulder and supervising their bets.</p>\n<p>The more we learn about a domain, and the more we systematically observe the stars at work, and the more we learn about the human mind in general, the more we can hope for new skills to make the transition from unteachable to apprenticeable to publishable.</p>\n<p>And you can hope to <em>trailblaze</em> certain paths, even if you can't set down all the path in words.&nbsp; Even if you <em>yourself </em>got somewhere through luck (including genetic luck), you can hope to <em>diminish</em> the role of luck on future occasions:</p>\n<p>(A)&nbsp; Warning against blind alleys that delayed you, is one obvious sort of help.</p>\n<p>(B)&nbsp; If you lay down a set of thoughts that are the <em>product</em> of wordless skills, someone reading through the set of thoughts may find their brain <em>picking up the rhythm, </em>making the leap to the unspoken thing behind; and this might require <em>less</em> luck than the events that led to your own original acquisition of those wordless skills.</p>\n<p>(C)&nbsp; There are good <em>attractors </em>in the solution-space - clustered sub-solutions which make it easier to reach other solutions in the same attractor.&nbsp; Then - even if some of the thoughts can't be put into words, and even if it took a lot of luck to wander into the attractor the first time through - describing everything that <em>can</em> be put into words, may be enough to anchor the attractor.</p>\n<p>(D)&nbsp; Some important experiences <em>are </em>duplicable: for example, you can advise people what areas to study, what books to read.</p>\n<p>(E)&nbsp; And finally, the simple advance of science may just describe a domain better, so that you realize what it is you know, and are suddenly able to communicate it outright.</p>\n<p>And of course the punchline is that this is the transition I hope to see in certain aspects of <em>human rationality</em> - skills which have been up until now unteachable, or only passed down from master to apprentice.&nbsp; We've learned a lot about the domain in the past few decades, and I think it's time to take another shot at systematizing it.</p>\n<p>I aspire to <em>diminish the role of luck and talent</em> in producing rationalists of a higher grade.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 1, "YQW2DxpZFTrqrxHBJ": 1, "EdDGrAxYcrXnKkDca": 1, "x3zyEPFaJANB2BHmP": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9SaAyq7F7MAuzAWNN", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 49, "extendedScore": null, "score": 8.1e-05, "legacy": true, "legacyId": "21", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 49, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["34Tu4SCK5r5Asdrn3", "YhgjmCxcQXixStWMC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-04T23:29:50.910Z", "modifiedAt": null, "url": null, "title": "No, Really, I've Deceived Myself", "slug": "no-really-i-ve-deceived-myself", "viewCount": null, "lastCommentedAt": "2022-05-12T15:12:30.492Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rZX4WuufAPbN6wQTv/no-really-i-ve-deceived-myself", "pageUrlRelative": "/posts/rZX4WuufAPbN6wQTv/no-really-i-ve-deceived-myself", "linkUrl": "https://www.lesswrong.com/posts/rZX4WuufAPbN6wQTv/no-really-i-ve-deceived-myself", "postedAtFormatted": "Wednesday, March 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20No%2C%20Really%2C%20I've%20Deceived%20Myself&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANo%2C%20Really%2C%20I've%20Deceived%20Myself%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrZX4WuufAPbN6wQTv%2Fno-really-i-ve-deceived-myself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=No%2C%20Really%2C%20I've%20Deceived%20Myself%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrZX4WuufAPbN6wQTv%2Fno-really-i-ve-deceived-myself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrZX4WuufAPbN6wQTv%2Fno-really-i-ve-deceived-myself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 466, "htmlBody": "<p>I recently spoke with a person who... it's difficult to describe.&nbsp; Nominally, she was an Orthodox Jew.&nbsp; She was also highly intelligent, conversant with some of the archaeological evidence against her religion, and the shallow standard arguments against religion that religious people know about.&nbsp; For example, she knew that Mordecai, Esther, Haman, and Vashti were not in the Persian historical records, but that there was a corresponding old Persian legend about the Babylonian gods Marduk and Ishtar, and the rival Elamite gods Humman and Vashti.&nbsp; She <em>knows </em>this, and she still celebrates Purim.&nbsp; One of those highly intelligent religious people who stew in their own contradictions for years, elaborating and tweaking, until their minds look like the inside of an M. C. Escher painting.</p>\n<p>Most people like this will <a href=\"http://www.overcomingbias.com/2009/02/pretending-to-be-wise.html\">pretend that they are much too wise</a> to talk to atheists, but she was willing to talk with me for a few hours.</p>\n<p>As a result, I now understand at least one more thing about self-deception that I didn't explicitly understand before&mdash;namely, that you don't have to <em>really</em> deceive yourself so long as you <em>believe</em> you've deceived yourself.&nbsp; Call it \"belief in self-deception\".<a id=\"more\"></a></p>\n<p>When this woman was in high school, she thought she was an atheist.&nbsp; But she decided, at that time, that she should act as if she believed in God.&nbsp; And then&mdash;she told me earnestly&mdash;over time, she came to really believe in God.</p>\n<p>So far as I can tell, she is completely wrong about that.&nbsp; Always throughout our conversation, she said, over and over, \"I <em>believe</em> in God\", never once, \"There <em>is</em> a God.\"&nbsp; When I asked her why she was religious, she never once talked about the consequences of God existing, only about the consequences of believing in God.&nbsp; Never, \"God will help me\", always, \"my belief in God helps me\".&nbsp; When I put to her, \"Someone who just wanted the truth and looked at our universe would not even invent God as a hypothesis,\" she agreed outright.</p>\n<p>She hasn't <em>actually </em>deceived herself into believing that God exists or that the Jewish religion is true.&nbsp; Not even close, so far as I can tell.</p>\n<p>On the other hand, I think she really <em>does</em> believe she has deceived herself.</p>\n<p>So although she does not receive any benefit of believing in God&mdash;because she doesn't&mdash;she honestly <em>believes</em> she has deceived herself into believing in God, and so she honestly <em>expects</em> to receive the benefits that she associates with deceiving oneself into believing in God; and <em>that,</em> I suppose, ought to produce much the same placebo effect as <em>actually </em>believing in God.</p>\n<p>And this may explain why she was motivated to earnestly defend the statement that she <em>believed</em> in God from my skeptical questioning, while never saying \"Oh, and by the way, God actually does exist\" or even seeming the slightest bit interested in the proposition.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "YTCrHWYHAsAD74EHo": 4, "br65ewZbmvPA6GKPi": 2, "NSMKfa8emSbGNXRKD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rZX4WuufAPbN6wQTv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 99, "baseScore": 109, "extendedScore": null, "score": 0.000164, "legacy": true, "legacyId": "27", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "qqFS6Kw5fmPyzkLby", "canonicalCollectionSlug": "rationality", "canonicalBookId": "coGq3LC5Yn4vZiu6k", "canonicalNextPostSlug": "belief-in-self-deception", "canonicalPrevPostSlug": "doublethink-choosing-to-be-biased", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 109, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 89, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-05T04:28:28.528Z", "modifiedAt": "2020-08-25T02:57:16.430Z", "url": null, "title": "The ethic of hand-washing and community epistemic practice", "slug": "the-ethic-of-hand-washing-and-community-epistemic-practice", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:34.370Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "AnnaSalamon", "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZP2om2oWHPhvWP2Q3/the-ethic-of-hand-washing-and-community-epistemic-practice", "pageUrlRelative": "/posts/ZP2om2oWHPhvWP2Q3/the-ethic-of-hand-washing-and-community-epistemic-practice", "linkUrl": "https://www.lesswrong.com/posts/ZP2om2oWHPhvWP2Q3/the-ethic-of-hand-washing-and-community-epistemic-practice", "postedAtFormatted": "Thursday, March 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20ethic%20of%20hand-washing%20and%20community%20epistemic%20practice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20ethic%20of%20hand-washing%20and%20community%20epistemic%20practice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZP2om2oWHPhvWP2Q3%2Fthe-ethic-of-hand-washing-and-community-epistemic-practice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20ethic%20of%20hand-washing%20and%20community%20epistemic%20practice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZP2om2oWHPhvWP2Q3%2Fthe-ethic-of-hand-washing-and-community-epistemic-practice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZP2om2oWHPhvWP2Q3%2Fthe-ethic-of-hand-washing-and-community-epistemic-practice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1037, "htmlBody": "<p>Related to: <a href=\"http://www.overcomingbias.com/2008/08/use-the-native.html\">Use the Native Architecture</a></p>\n<p>When cholera moves through countries with poor drinking water sanitation, it <a href=\"http://www.scielo.br/scielo.php?pid=S0074-02761998000500002&amp;script=sci_arttext&amp;tlng=en\">apparently</a> becomes more virulent. When it moves through countries that have clean drinking water (more exactly, countries that reliably keep fecal matter out of the drinking water), it becomes less virulent. The theory is that cholera faces a tradeoff between rapidly copying within its human host (so that it has more copies to spread) and keeping its host well enough to wander around infecting others. If person-to-person transmission is cholera&rsquo;s only means of spreading, it will evolve to keep its host well enough to spread it. If it can instead spread through the drinking water (and thus spread even from hosts who are too ill to go out), it will evolve toward increased lethality. (Critics <a href=\"http://www.sciencemag.org/cgi/content/summary/300/5624/1362\">here</a>.)</p>\n<p>I&rsquo;m stealing this line of thinking from my friend Jennifer Rodriguez-Mueller, but: I&rsquo;m curious whether anyone&rsquo;s gotten analogous results for the progress and mutation of ideas, among communities with different communication media and/or different habits for deciding which ideas to adopt and pass on. Are there differences between religions that are passed down vertically (parent to child) vs. horizontally (peer to peer), since the former do better when their bearers raise more children? Do mass media such as radio, TV, newspapers, or printing presses decrease the functionality of the average person&rsquo;s ideas, by allowing ideas to spread in a manner that is less dependent on their average host&rsquo;s prestige and influence? (The intuition here is that prestige and influence might be positively correlated with the functionality of the host&rsquo;s ideas, at least in some domains, while the contingencies determining whether an idea spreads through mass media instruments might have less to do with functionality.)</p>\n<p>Extending this analogy -- most of us were taught as children to wash our hands. We were given the rationale, not only of keeping ourselves from getting sick, but also of making sure we don&rsquo;t infect others. There&rsquo;s an ethic of sanitariness that draws from the ethic of being good community members.</p>\n<p><a id=\"more\"></a>Suppose we likewise imagine that each of us contain a variety of beliefs, some well-founded and some not. Can we make an ethic of &ldquo;epistemic hygiene&rdquo; to describe practices that will selectively cause our more accurate beliefs to spread, and cause our less accurate beliefs to stay con tained, even in cases where the individuals spreading those beliefs don&rsquo;t know which is which? That is: (1) is there a set of simple, accessible practices (analogous to hand-washing) that will help good ideas spread and bad ideas stay contained; and (2) is there a nice set of metaphors and moral intuitions that can keep the practices alive in a community? Do we have such an ethic already, on OB or in intellectual circles more generally? (Also, (3) we would like some other term besides &ldquo;epistemic hygiene&rdquo; that would be less Orwellian and/or harder to abuse -- any suggestions? Another wording we&rsquo;ve <a href=\"http://bloggingheads.tv/diavlogs/17359\">heard</a> is &ldquo;good cognitive citizenship&rdquo;, which sounds relatively less prone to abuse.)</p>\n<p>Honesty is an obvious candidate practice, and honesty has much support from human moral intuitions. But &ldquo;honesty&rdquo; is too vague to pinpoint the part that&rsquo;s actually useful. Being honest about one&rsquo;s <span style=\"font-style: italic;\">evidence</span> and about the <span style=\"font-style: italic;\">actual causes of one&rsquo;s beliefs</span> is valuable for distinguishing accurate from mistaken beliefs. However, a habit of <span style=\"font-style: italic;\">focussing attention on</span> evidence and on the actual causes of one&rsquo;s own as well as one&rsquo;s interlocutor&rsquo;s beliefs would be just as valuable, and such a practice is not part of the traditional requirements of &ldquo;honesty&rdquo;. Meanwhile, I see little reason to expect a socially-endorsed practice of &ldquo;honesty&rdquo; about one&rsquo;s &ldquo;sincere&rdquo; but carelessly assembled opinions (about politics, religion, the neighbors&rsquo; character, or anything else) to selectively promote accurate ideas.</p>\n<p>Another candidate practice is the practice of only passing on ideas one has oneself verified from empirical evidence (as in the ethic of traditional rationality, where arguments from authority are banned, and one attains virtue by checking everything for oneself). This practice sounds plausibly useful against group failure modes where bad ideas are kept in play, and passed on, in large part because so many others believe the idea (e.g. religious beliefs, or the persistence of Aristotelian physics in medieval scholasticism; this is the motivation for the scholarly norm of citing primary literature such as historical documents or original published experiments). But limiting individuals&rsquo; sharing to the (tiny) set of beliefs they can themselves check sounds extremely costly. Rolf Nelson&rsquo;s <a href=\"http://www.overcomingbias.com/2008/04/naming-beliefs.html\">suggestion</a> that we find words to explicitly separate &ldquo;individual impressions&rdquo; (impressions based only on evidence we&rsquo;ve ourselves verified) from &ldquo;beliefs&rdquo; (which include evidence from others&rsquo; impressions) sounds promising as a means of avoiding circular evidence while also benefiting from others&rsquo; evidence. I&rsquo;m curious how many here are habitually distinguishing impressions from beliefs. (I am. I find it useful.)</p>\n<p>Are there other natural ideas? Perhaps social norms that accord status for reasoned opinion-change in the face of new good evidence, rather than norms that dock status from the &ldquo;losers&rdquo; of debates? Or social norms that take care to leave one&rsquo;s interlocutor a line of retreat in <span style=\"font-style: italic;\">all</span> directions -- to take care to avoid setting up consistency and commitment pressures that might wedge them toward <span style=\"font-style: italic;\">either</span> your ideas <span style=\"font-style: italic;\">or their own</span>? (I&rsquo;ve never seen this strategy implemented as a community norm. Some people conscientiously avoid &ldquo;rhetorical tricks&rdquo; or &ldquo;sales techniques&rdquo; for getting their interlocutor to adopt <span style=\"font-style: italic;\">their</span> ideas; but I&rsquo;ve never seen a social norm of carefully preventing one&rsquo;s interlocutor from having status- or consistency pressures toward entrenchedly <span style=\"font-style: italic;\">keeping their own pre-existing</span> ideas.) These norms strike me as plausibly helpful, if we could manage to implement them. However, they appear difficult to integrate with human instincts and moral intuitions around purity and hand-washing, whereas honesty and empiricism fit comparatively well into human purity intuitions. Perhaps this is why these social norms are much less practiced.</p>\n<p>In any case:</p>\n<p>(1) Are ethics of &ldquo;epistemic hygiene&rdquo;, and of the community impact of one&rsquo;s speech practices, worth pursuing? Are they already in place? Are there alternative moral frames that one might pursue instead? Are human instincts around purity too dangerously powerful and inflexible for sustainable use in community epistemic practice?</p>\n<p>(2) What community practices do you actually find useful, for creating community structures where accurate ideas are selectively promoted?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LNsEBXoFdAy8yzvbw": 2, "Ng8Gice9KNkncxqcj": 2, "izp6eeJJEg9v5zcur": 2, "x5TtBDjRg9egvg9gm": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZP2om2oWHPhvWP2Q3", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": 52, "extendedScore": null, "score": 8e-05, "legacy": true, "legacyId": "30", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": ["M9dDE5vD9y2ZpgwuZ"], "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 53, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-05T15:20:27.590Z", "modifiedAt": null, "url": null, "title": "Belief in Self-Deception", "slug": "belief-in-self-deception", "viewCount": null, "lastCommentedAt": "2019-11-10T19:01:37.181Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wP2ymm44kZZwaFPYh/belief-in-self-deception", "pageUrlRelative": "/posts/wP2ymm44kZZwaFPYh/belief-in-self-deception", "linkUrl": "https://www.lesswrong.com/posts/wP2ymm44kZZwaFPYh/belief-in-self-deception", "postedAtFormatted": "Thursday, March 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Belief%20in%20Self-Deception&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABelief%20in%20Self-Deception%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwP2ymm44kZZwaFPYh%2Fbelief-in-self-deception%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Belief%20in%20Self-Deception%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwP2ymm44kZZwaFPYh%2Fbelief-in-self-deception", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwP2ymm44kZZwaFPYh%2Fbelief-in-self-deception", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1329, "htmlBody": "<p>I spoke yesterday of my conversation with a nominally Orthodox Jewish woman who vigorously defended the assertion that she believed in God, while seeming not to actually believe in God at all.</p>\n<p>While I was questioning her about the benefits that she thought came from believing in God, I introduced the <a href=\"http://www.overcomingbias.com/2007/10/curiosity.html\">Litany of Tarski</a>&mdash;which is actually an infinite family of litanies, a specific example being:</p>\n<p><em>&nbsp; If the sky is blue<br />&nbsp;&nbsp; &nbsp;&nbsp; I desire to believe \"the sky is blue\"<br />&nbsp; If the sky is not blue<br />&nbsp;&nbsp; &nbsp;&nbsp; I desire to believe \"the sky is not blue\".</em></p>\n<p>\"This is not my philosophy,\" she said to me.</p>\n<p>\"I didn't think it was,\" I replied to her.&nbsp; \"I'm just asking&mdash;assuming that God does <em>not </em>exist, and this is known, then should you still believe in God?\"</p>\n<p>She hesitated.&nbsp; She seemed to really be trying to think about it, which surprised me.</p>\n<p>\"So it's a counterfactual question...\" she said slowly.</p>\n<p>I thought at the time that she was having difficulty allowing herself to visualize the world where God does not exist, because of her attachment to a God-containing world.</p>\n<p>Now, however, I suspect she was having difficulty visualizing a contrast between the way the <em>world</em> would look if God existed or did not exist, because all her thoughts were about her <em>belief in God</em>, but her causal network modelling the world did not contain God as a node.&nbsp; So she could easily answer \"How would the world look different if I didn't believe in God?\", but not \"How would the world look different if there was no God?\"</p>\n<p>She didn't answer that question, at the time.&nbsp; But she did produce a <em>counterexample </em>to the Litany of Tarski:</p>\n<p>She said, \"I believe that people are nicer than they really are.\"</p>\n<p><a id=\"more\"></a></p>\n<p>I tried to explain that if you say, \"People are bad,\" that means you believe people are bad, and if you say, \"I believe people are nice\", that means you believe you believe people are nice.&nbsp; So saying \"People are bad and I believe people are nice\" means you believe people are bad but you believe you believe people are nice.</p>\n<p>I quoted to her:</p>\n<p style=\"padding-left: 30px;\">&nbsp; \"If there were a verb meaning 'to believe falsely', it would not have any<br />&nbsp; significant first person, present indicative.\"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &mdash;Ludwig Wittgenstein</p>\n<p>She said, smiling, \"Yes, I believe people are nicer than, in fact, they are.&nbsp; I just thought I should put it that way for you.\"</p>\n<p style=\"padding-left: 30px;\">&nbsp; \"I reckon Granny ought to have a good look at you, Walter,\" said Nanny.&nbsp; \"I reckon<br />&nbsp; your mind's all tangled up like a ball of string what's been dropped.\"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &mdash;Terry Pratchett, <em>Maskerade</em></p>\n<p>And I can type out the words, \"Well, I guess she didn't believe that her reasoning ought to be <a href=\"http://www.overcomingbias.com/2007/09/the-lens-that-s.html\">consistent under reflection</a>,\" but I'm still having trouble <a href=\"http://www.overcomingbias.com/2007/05/think_like_real.html\">coming to grips</a> with it.</p>\n<p>I can see the pattern in the words coming out of her lips, but I can't understand the mind behind on an empathic level.&nbsp; I can imagine myself into the shoes of <a href=\"http://www.overcomingbias.com/2009/01/the-babyeating-aliens.html\">baby-eating aliens</a> and <a href=\"http://www.overcomingbias.com/2009/02/super-happy-people.html\">the Lady 3rd Kiritsugu</a>, but I cannot imagine what it is like to be her.&nbsp; Or maybe I just don't <em>want</em> to?</p>\n<p>This is why intelligent people only have a certain amount of time (measured in subjective time spent thinking about religion) to become atheists.&nbsp; After a certain point, if you're smart, have spent time thinking about and defending your religion, and still haven't escaped the grip of <a href=\"http://www.overcomingbias.com/2008/10/the-dark-side.html\">Dark Side Epistemology</a>, the inside of your mind ends up as an Escher painting.</p>\n<p>(One of the other few moments that gave her pause&mdash;I mention this, in case you have occasion to use it&mdash;is when she was talking about how it's good to believe that someone cares whether you do right or wrong&mdash;<em>not</em>, of course, talking about how there actually <em>is </em>a God who cares whether you do right or wrong, this proposition is not part of her religion&mdash;</p>\n<p>And I said, \"But <em>I</em> care whether you do right or wrong.&nbsp; So what you're saying is that this isn't enough, and you also need to believe in something <em>above</em> humanity that cares whether you do right or wrong.\"&nbsp; So that stopped her, for a bit, because of course she'd never thought of it in those terms before.&nbsp; Just <a href=\"http://www.overcomingbias.com/2007/10/how-to-seem-and.html\">a standard application of the nonstandard toolbox</a>.)</p>\n<p>Later on, at one point, I was asking her if it would be good to do <em>anything</em> differently if there definitely was no God, and this time, she answered, \"No.\"</p>\n<p>\"So,\" I said incredulously, \"if God exists or doesn't exist, that has absolutely no effect on how it would be good for people to think or act?&nbsp; I think even a rabbi would look a little askance at that.\"</p>\n<p>Her religion seems to now consist <em>entirely </em>of the worship of worship.&nbsp; As the true believers of older times might have believed that an all-seeing father would save them, she now believes that belief in God will save her.</p>\n<p>After she said \"I believe people are nicer than they are,\" I asked, \"So, are you consistently surprised when people undershoot your expectations?\"&nbsp; There was a long silence, and then, slowly:&nbsp; \"Well... am I <em>surprised</em> when people... undershoot my expectations?\"</p>\n<p>I didn't understand this pause at the time.&nbsp; I'd intended it to suggest that if she was constantly disappointed by reality, then this was a downside of believing falsely. &nbsp; But she seemed, instead, to be taken aback at the implications of <em>not</em> being surprised.</p>\n<p>I now realize that the whole essence of her philosophy was <em>her belief that she had deceived herself,</em> and the possibility that her estimates of other people were <em>actually accurate</em>, threatened the <a href=\"http://www.overcomingbias.com/2008/10/the-dark-side.html\">Dark Side Epistemology</a> that she had built around beliefs such as \"I benefit from believing people are nicer than they actually are.\"</p>\n<p>She has taken the old idol off its throne, and replaced it with an explicit worship of the Dark Side Epistemology that was once invented to defend the idol; she worships her own attempt at self-deception.&nbsp; The attempt failed, but she is honestly unaware of this.</p>\n<p>And so humanity's token guardians of sanity (motto: \"pooping your deranged little party since Epicurus\") must now fight the active worship of self-deception&mdash;the worship <em>of the supposed benefits of faith</em>, in place of God.</p>\n<p>This actually explains a fact about <em>myself</em> that I didn't really understand earlier&mdash;the reason why I'm annoyed when people talk as if self-deception is <em>easy,</em> and why I write <a href=\"http://www.overcomingbias.com/2007/09/doublethink-cho.html\">entire blog posts</a> arguing that making a deliberate choice to believe the sky is green, is harder to get away with than people seem to think.</p>\n<p>It's because&mdash;while you <em>can't</em> just choose to believe the sky is green&mdash;if you don't <em>realize</em> this fact, then you actually <em>can</em> fool yourself into believing that you've successfully deceived yourself.</p>\n<p>And since you then sincerely <em>expect</em> to receive the benefits that you think come from self-deception, you get the same sort of placebo benefit that would actually come from a successful self-deception.</p>\n<p>So by going around explaining how <em>hard </em>self-deception is, I'm actually taking direct aim at the placebo benefits that people get from believing that they've deceived themselves, and targeting the new sort of religion that worships only the worship of God.</p>\n<p>Will this battle, I wonder, generate a new list of reasons why, not belief, but <a href=\"http://www.overcomingbias.com/2007/07/belief-in-belie.html\">belief in belief</a>, is <em>itself</em> a good thing?&nbsp; Why people derive great benefits from worshipping their worship?&nbsp; Will we have to do this over again with belief in belief in belief and worship of worship of worship?&nbsp; Or will intelligent theists finally just give up on that line of argument?</p>\n<p>I wish I could believe that no one could possibly believe in belief in belief in belief, but <a href=\"http://www.overcomingbias.com/2008/04/zombies.html\">the Zombie World argument in philosophy has gotten even more tangled than this</a> and its proponents still haven't abandoned it.</p>\n<p>I await the eager defenses of belief in belief in the comments, but I wonder if anyone would care to jump ahead of the game and defend belief in belief in belief?&nbsp; Might as well go ahead and get it over with.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "YTCrHWYHAsAD74EHo": 2, "br65ewZbmvPA6GKPi": 2, "E6qP9r9xxM4LCxaFk": 2, "SJFsFfFhE6m2ThAYJ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wP2ymm44kZZwaFPYh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 81, "baseScore": 84, "extendedScore": null, "score": 0.000127, "legacy": true, "legacyId": "28", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "qqFS6Kw5fmPyzkLby", "canonicalCollectionSlug": "rationality", "canonicalBookId": "coGq3LC5Yn4vZiu6k", "canonicalNextPostSlug": "moore-s-paradox", "canonicalPrevPostSlug": "no-really-i-ve-deceived-myself", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 84, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 112, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-05T15:31:16.803Z", "modifiedAt": null, "url": null, "title": "Rationality and Positive Psychology", "slug": "rationality-and-positive-psychology", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:07.572Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "badger", "createdAt": "2009-02-27T06:50:31.697Z", "isAdmin": false, "displayName": "badger"}, "userId": "w3rzcs3GwLDqgRpwo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w6QGyqyoL8k5GwuPu/rationality-and-positive-psychology", "pageUrlRelative": "/posts/w6QGyqyoL8k5GwuPu/rationality-and-positive-psychology", "linkUrl": "https://www.lesswrong.com/posts/w6QGyqyoL8k5GwuPu/rationality-and-positive-psychology", "postedAtFormatted": "Thursday, March 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20and%20Positive%20Psychology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20and%20Positive%20Psychology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw6QGyqyoL8k5GwuPu%2Frationality-and-positive-psychology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20and%20Positive%20Psychology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw6QGyqyoL8k5GwuPu%2Frationality-and-positive-psychology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw6QGyqyoL8k5GwuPu%2Frationality-and-positive-psychology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 790, "htmlBody": "<p>&nbsp;&nbsp;&nbsp; Robin recently had us consider the costs of rationality, but I have been thinking about the benefits. I typically think of rationality as having instrumental value, but after reading <a class=\"mw-redirect\" title=\"Mih&aacute;ly Cs&iacute;kszentmih&aacute;lyi\" href=\"http://en.wikipedia.org/wiki/Mih%C3%A1ly_Cs%C3%ADkszentmih%C3%A1lyi\">Mih&aacute;ly Cs&iacute;kszentmih&aacute;lyi</a><span class=\"mw-redirect\">'s work on flow, I began pondering its status as an intrinsically fulfilling activity. </span>Cognitive and evolutionary psychology are major components in the study of rationality, but I haven't seen connections drawn between rationality and positive psychology before. <span class=\"mw-redirect\">Cs&iacute;kszentmih&aacute;lyi (said </span>cheek-sent-me-high-ee) defines \"flow\" as a state of intense focus where you lose track of yourself and become completely involved in what you are doing. After some consideration, I am intrigued by the similarities between the practice of rationality and a flow-like state of mind.</p>\n<p><a id=\"more\"></a><span class=\"mw-redirect\">Cs&iacute;kszentmih&aacute;lyi identifies the following components of flow:<br /></span></p>\n<blockquote>\n<p>1. Clear goals and expectations about the task at hand.</p>\n<p>2. Direct and immediate feedback.</p>\n<p>3. A high degree of concentration.</p>\n<p>4. Loss of self-consciousness.</p>\n<p>5. Altered perception of time.</p>\n<p>6. Challenge proportionate to your skill level.</p>\n<p>7. Feeling of control over the situation.</p>\n<p>8. Activity is intrinsically rewarding.</p>\n<p>(summarized from <a href=\"http://en.wikipedia.org/wiki/Flow_(psychology)\">Flow (psychology)</a>)</p>\n</blockquote>\n<p>&nbsp;&nbsp;&nbsp; The first component appears directly tied to the concepts of conservation of expectation and making predictions in advance. By clearly defining your goals, how you will respond to new evidence, and what your current predictions are, you will know how to react in advance. Clear goals and expectations allow you to better recognize successes and failures. A well-defined scoring function is important to guide intelligence as an optimization process, but also allows you to be fulfilled when you do encounter success.</p>\n<p>&nbsp;&nbsp;&nbsp; The second component depends on the first, but it also entails finding reliable feedback to evaluate those expectations against. Careful rationalists uses precise quantitative measures and math where possible. The precision of math can give detailed and immediate feedback, so long as it does not introduce additional unjustified complexity. As rationalists, we should actively be seeking out feedback on our beliefs and predictions. Feedback assists calibration, and if correctly used, prevents us from being trapped in bad beliefs and with poor methods.</p>\n<p>&nbsp;&nbsp;&nbsp; Components three, four, and five are all closely related. These three are more of a mixed bag than the first two. On one hand, this aspect of flow represents the directive to shut up and multiply: forget yourself and your own feelings and focus only on the issue at hand. However, until your rationality instincts are sufficiently honed, becoming absorbed in a task means that you could be forgetting about unconscious biases. Like any skill though, with practice, checking biases can become automatic.</p>\n<p>&nbsp;&nbsp;&nbsp; The ability to automatically check biases leads into the sixth part. The challenge of a task should be proportionate to your skill level. A disproportionately difficult task leads to frustration, and an insufficiently difficult one leads to relaxation or boredom. Relaxation is not a bad thing, but is distinct from flow and antithetical to rationality. If curiosity should lead to its own destruction, we shouldn't allow ourselves to always be in a state of relaxation. As beginning rationalists, the challenge can simply be becoming aware of our biases and the principles of rationality. As this becomes easier and our skill increases, we can focus on more and more difficult issues to apply ourselves to. I am interested in the possibility of developing simple standard challenges that allow rationalists to build awareness without being overwhelming. Reading the origins thread, it appears religion played this role for many of us, but I think the issue is too fraught with emotion to be a reliable standard challenge.</p>\n<p>&nbsp;&nbsp;&nbsp; The seventh component is a little more difficult for rationalists. Being open and willing to relinquish beliefs and acknowledge mistakes seems contrary to being in control. Nevertheless, dispelling biases and reflecting on our values means that we can be in better control of our minds and behaviors.</p>\n<p>&nbsp;&nbsp;&nbsp; Finally, for many of us, curiosity is an intrinsic desire. In my case, I only need to give it more of a chance to express itself.</p>\n<p>&nbsp;&nbsp;&nbsp; While I began by considering whether rationality is instrinsically fulfulling, this has really been a discussion of the practice of rationality. And even then, to be careful I should say the practice of rationality is still only instrumentally valuable; just less so than commonly thought. <a href=\"/lw/j/the_costs_of_rationality/e9\">Eliezer</a> thinks that preferences should be neutral to rituals of cognition, which I am inclined to agree with. That rationality tends to produce a state of flow in me&nbsp; is a highly contingent fact. Is it possible to lessen the costs of rationality and increase its benefits?</p>\n<p>&nbsp;&nbsp;&nbsp; Does anyone else have any thoughts or experiences on this subject? Is anyone aware of a more rigorous or academic study on the relation between rationality and positive psychology?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "izp6eeJJEg9v5zcur": 1, "3QnDqGSdRMA5mdMM6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w6QGyqyoL8k5GwuPu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 4, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "11", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-05T15:56:45.958Z", "modifiedAt": null, "url": null, "title": "Posting now enabled", "slug": "posting-now-enabled", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:01.527Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sr7n7WpiisSJ8oJk2/posting-now-enabled", "pageUrlRelative": "/posts/sr7n7WpiisSJ8oJk2/posting-now-enabled", "linkUrl": "https://www.lesswrong.com/posts/sr7n7WpiisSJ8oJk2/posting-now-enabled", "postedAtFormatted": "Thursday, March 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Posting%20now%20enabled&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APosting%20now%20enabled%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsr7n7WpiisSJ8oJk2%2Fposting-now-enabled%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Posting%20now%20enabled%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsr7n7WpiisSJ8oJk2%2Fposting-now-enabled", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsr7n7WpiisSJ8oJk2%2Fposting-now-enabled", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 61, "htmlBody": "<p>Posting is now enabled with a minimum karma required of 20 - that is, you must have gotten at least 20 upvotes on your comments in order to publish a post.&nbsp; Or an adminstrator such as myself or Robin (by default you should bother me) can temporarily bless you with posting ability - in the long run this shouldn't happen much.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sr7n7WpiisSJ8oJk2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 5, "extendedScore": null, "score": 4.793904749170497e-07, "legacy": true, "legacyId": "32", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-05T16:15:00.000Z", "modifiedAt": null, "url": null, "title": "Posting now enabled on Less Wrong", "slug": "posting-now-enabled-on-less-wrong", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:47.193Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LD5kLJEwzaPrc9cik/posting-now-enabled-on-less-wrong", "pageUrlRelative": "/posts/LD5kLJEwzaPrc9cik/posting-now-enabled-on-less-wrong", "linkUrl": "https://www.lesswrong.com/posts/LD5kLJEwzaPrc9cik/posting-now-enabled-on-less-wrong", "postedAtFormatted": "Thursday, March 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Posting%20now%20enabled%20on%20Less%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APosting%20now%20enabled%20on%20Less%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLD5kLJEwzaPrc9cik%2Fposting-now-enabled-on-less-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Posting%20now%20enabled%20on%20Less%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLD5kLJEwzaPrc9cik%2Fposting-now-enabled-on-less-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLD5kLJEwzaPrc9cik%2Fposting-now-enabled-on-less-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 333, "htmlBody": "<p>Posting is now enabled on <a href=\"http://lesswrong.com/new/\">Less Wrong</a>, with a minimum karma required of 20 - that is, you must have gotten at least 20 upvotes on your comments in order to publish a post.&#0160; Or an adminstrator such as myself or Robin (by default you should bother me) can temporarily bless you with posting ability - in the long run this shouldn&#39;t happen much.</p><p>For those of you who haven&#39;t yet subscribed to / gotten in the habit of checking Less Wrong:</p><ul>\n<li><em><a href=\"http://lesswrong.com/lw/h/test_your_rationality/\">Test Your Rationality</a> by Robin Hanson.</em>&#0160; It&#39;s easy to find reasons to believe yourself more rational than others, but most people do this; what real ways can be found to test your rationality?</li>\n<li><em><a href=\"http://lesswrong.com/lw/m/unteachable_excellence/\">Unteachable Excellence</a> and <a href=\"http://lesswrong.com/lw/l/teaching_the_unteachable/\">Teaching the Unteachable</a> by Eliezer Yudkowsky.</em>&#0160; The rare superstars are rare because their skills are currently hard to transfer.&#0160; A large number of Nobel laureates are students of other Nobel laureates.&#0160; How do you teach skills you can&#39;t put into words?</li>\n<li><em><a href=\"http://lesswrong.com/lw/j/the_costs_of_rationality/\">The Costs of Rationality</a> by Robin Hanson.</em>&#0160; Rationality can be useful for many things, but humans aren&#39;t really designed for it, and a true effort to believe truly can get in the way of many aspects of ordinary life.&#0160; Are you willing to pay the real costs of ratonality?</li>\n<li><em><a href=\"http://lesswrong.com/lw/r/no_really_ive_deceived_myself/\">No, Really, I&#39;ve Deceived Myself</a> and <a href=\"http://lesswrong.com/lw/s/belief_in_selfdeception/\">Belief in Self-Deception</a> by Eliezer Yudkowsky.</em>&#0160; A woman I met who didn&#39;t seem to believe in God at all, while honestly believing that she had deceived herself successfully - which may bring most of the same placebo benefits.</li>\n<li><em><a href=\"http://lesswrong.com/lw/u/the_ethic_of_handwashing_and_community_epistemic/\">The ethic of hand-washing and commuity epistemic practice</a> by Steve Rayhawk and Anna Salamon.&#0160;</em> Diseases become more virulent in the presence of poor hygiene, since they can jump hosts more easily.&#0160; Are there analogous effects for ideas?&#0160; What is the equivalent of washing our hands?</li>\n</ul>\n<p>The five most recent LW posts now appear in OB&#39;s sidebar (and vice versa), but aside from this you shouldn&#39;t expect further regular summaries of LW on OB.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LD5kLJEwzaPrc9cik", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "1259", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Kn6H8Tk6EPT4Atq4k", "34Tu4SCK5r5Asdrn3", "9SaAyq7F7MAuzAWNN", "9Z3pezjiWLfNANg9P", "rZX4WuufAPbN6wQTv", "wP2ymm44kZZwaFPYh", "ZP2om2oWHPhvWP2Q3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-05T16:50:02.621Z", "modifiedAt": null, "url": null, "title": "Kinnaird's truels", "slug": "kinnaird-s-truels", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:01.277Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Johnicholas", "createdAt": "2009-02-27T15:01:52.708Z", "isAdmin": false, "displayName": "Johnicholas"}, "userId": "kBvTXutfPytNtzPyD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HahzBTjKLFLv4iQE8/kinnaird-s-truels", "pageUrlRelative": "/posts/HahzBTjKLFLv4iQE8/kinnaird-s-truels", "linkUrl": "https://www.lesswrong.com/posts/HahzBTjKLFLv4iQE8/kinnaird-s-truels", "postedAtFormatted": "Thursday, March 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Kinnaird's%20truels&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKinnaird's%20truels%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHahzBTjKLFLv4iQE8%2Fkinnaird-s-truels%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Kinnaird's%20truels%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHahzBTjKLFLv4iQE8%2Fkinnaird-s-truels", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHahzBTjKLFLv4iQE8%2Fkinnaird-s-truels", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<p>A \"truel\" is something like a duel, but among three gunmen. Martin Gardner popularized a puzzle based on this scenario, and there are many variants of the puzzle which mathematicians and game theorists have <a href=\"http://rutherglen.ics.mq.edu.au/math106S206/ex/threewayduel.pdf\">analyzed</a>.</p>\n<p>The optimal strategy varies with the details of the scenario, of course. One take-away from the analyses is that it is often disadvantageous to be very skillful. A very skillful gunman is a high-priority target.</p>\n<p>The environment of evolutionary adaptedness undoubtedly contained multiplayer social games. If some of these games had a truel-like structure, they may have rewarded mediocrity. This might be an explanation of psychological phenomena like \"fear of success\" and \"choking under pressure\".</p>\n<p>Robin Hanson has mentioned that there are costs to \"truth-seeking\". One of the example costs might be convincingly declaring \"I believe in God\" in order to be accepted into a religious community. I think truels are a game-theoretic structure that suggests that there are costs to (short-sighted) \"winning\", just as there are costs to \"truth-seeking\".</p>\n<p>How can you identify truel-like situations? What should you (a rationalist) do if you might be in a truel-like situation?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 2, "Ng8Gice9KNkncxqcj": 2, "fkABsGCJZ6y9qConW": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HahzBTjKLFLv4iQE8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 28, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "7", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-05T18:25:06.240Z", "modifiedAt": null, "url": null, "title": "Define Rationality", "slug": "define-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:50.779Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Marshall", "createdAt": "2009-02-28T07:15:06.496Z", "isAdmin": false, "displayName": "Marshall"}, "userId": "KkerWJMGQiGW2NFNQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5zkntzzStbYsSaDza/define-rationality", "pageUrlRelative": "/posts/5zkntzzStbYsSaDza/define-rationality", "linkUrl": "https://www.lesswrong.com/posts/5zkntzzStbYsSaDza/define-rationality", "postedAtFormatted": "Thursday, March 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Define%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADefine%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5zkntzzStbYsSaDza%2Fdefine-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Define%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5zkntzzStbYsSaDza%2Fdefine-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5zkntzzStbYsSaDza%2Fdefine-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 197, "htmlBody": "<p>I would like to suggest that we try to come up with several defintions of rationality. I don't feel we have exhausted this search area by any means. Robin has suggested, \"M<span style=\"font-family: Arial; line-height: 19px;\">ore \"rational\" means better believing what is true, given one's limited info and analysis resources\". Other commenters have emphasised goal-directed behaviour as a necessary ingredience of rationality. I think these defintions miss out on several important ingrediences - such as the social nature of rationality. There is also a subtext which argues - that rationality only gives one (correct) answer even if we only can approximate it. I feel strongly that rationality can give several correct answers and thus imagination is an ingredience of rationality. So without in any way believing that I have found the one correct defintion, I propose the following: When two or more brains try to be sensible about things and expand their agency. I believe that \"sensible\" in this context does not need to be defined as it is a primitive and each player willl submit their own meaning.</span></p>\n<p><span style=\"font-family: Arial; line-height: 19px;\">Maybe this is a can of worms - but are there other suggestions or defintions for rationality we can apply in our lives?</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "aa3Qg7Qrp9LM7QMaz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5zkntzzStbYsSaDza", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "33", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-05T20:23:09.098Z", "modifiedAt": null, "url": null, "title": "Recommended Rationalist Resources", "slug": "recommended-rationalist-resources", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:31.914Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelHoward", "createdAt": "2009-03-01T12:12:44.105Z", "isAdmin": false, "displayName": "MichaelHoward"}, "userId": "TCDxnyqmz6ZTCYuQQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ijSZW27bd8dCqBwCC/recommended-rationalist-resources", "pageUrlRelative": "/posts/ijSZW27bd8dCqBwCC/recommended-rationalist-resources", "linkUrl": "https://www.lesswrong.com/posts/ijSZW27bd8dCqBwCC/recommended-rationalist-resources", "postedAtFormatted": "Thursday, March 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Recommended%20Rationalist%20Resources&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARecommended%20Rationalist%20Resources%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FijSZW27bd8dCqBwCC%2Frecommended-rationalist-resources%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Recommended%20Rationalist%20Resources%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FijSZW27bd8dCqBwCC%2Frecommended-rationalist-resources", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FijSZW27bd8dCqBwCC%2Frecommended-rationalist-resources", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 226, "htmlBody": "<p>I thought <a title=\"Recommended Rationalist Reading\" href=\"http://www.overcomingbias.com/2007/10/recommended-rat.html\">Recommended Rationalist Reading</a> was very useful and interesting. Now we have voting and threading it seems a good time to comprehensively gather opinions on <em>online </em>material.</p>\n<p>Please suggest high-quality links related to or useful for improving rationality. It could be a blog, a forum, a great essay, a reference site, an e-book, anything clickable. Anyone interested can then check out what looks promising and report back.</p>\n<p>[edit]<br />There seems to be confusion... <em><strong>The post's for online material, not physical books.</strong></em> We already have <a title=\"Recommended Rationalist Reading\" href=\"http://www.overcomingbias.com/2007/10/recommended-rat.html\">Recommended Rationalist Reading</a>, but as that hasn't got threading and voting, <em>if</em> people think it's a good idea I (or someone else) can do a seperate post for books <em>[metaedit] ...not happening, is against blog guidelines. [/metaedit]</em></p>\n<p>Looks like we're getting lots of suggestions, so <strong>please don't forget to vote on them</strong> so busier readers have an idea which ones are worth more investigating!<br />[/edit]</p>\n<hr />\n<p><strong>Contributors</strong> - if making multiple suggestions, please give each their own comment so we can vote on them separately. Click 'Help' for how to do links.</p>\n<p><strong>Voters</strong> - for top level comments containing suggestions (as oppose to comments replying to suggestions) please vote on the quality of the resource, not anything else in the comment. If you feel strongly about the comment quality, just post a sub-comment.</p>\n<p>Here's 3 to get started:</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"tdt83ChxnEgwwKxi6": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ijSZW27bd8dCqBwCC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 6, "extendedScore": null, "score": 4.794285827880726e-07, "legacy": true, "legacyId": "34", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-06T04:08:04.882Z", "modifiedAt": null, "url": null, "title": "Information cascades", "slug": "information-cascades", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:09.258Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Johnicholas", "createdAt": "2009-02-27T15:01:52.708Z", "isAdmin": false, "displayName": "Johnicholas"}, "userId": "kBvTXutfPytNtzPyD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DNQw596nPCX4x7xT9/information-cascades", "pageUrlRelative": "/posts/DNQw596nPCX4x7xT9/information-cascades", "linkUrl": "https://www.lesswrong.com/posts/DNQw596nPCX4x7xT9/information-cascades", "postedAtFormatted": "Friday, March 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Information%20cascades&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInformation%20cascades%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDNQw596nPCX4x7xT9%2Finformation-cascades%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Information%20cascades%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDNQw596nPCX4x7xT9%2Finformation-cascades", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDNQw596nPCX4x7xT9%2Finformation-cascades", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 472, "htmlBody": "<p>An <a href=\"http://en.wikipedia.org/wiki/Information_cascade\">information cascade</a> is a problem in group rationality. Wikipedia has excellent introductions and links about the phenomenon, but here is a meta-ish example using likelihood ratios.</p>\n<p>Suppose in some future version of this site, there are several well-known facts:</p>\n<ul>\n<li>All posts come in two kinds, high quality (insightful and relevant) and low quality (old ideas rehashed, long hypotheticals).</li>\n<li>There is a well-known prior 60% chance of anything being high quality, rather than low quality. (We're doing well!)</li>\n<li>Readers get a private signal, either \"high\" or \"low\", their personal judgement of quality, which is wrong 20% of the time.</li>\n<li>The number of up <em>and down</em> votes is displayed next to each post. (Note the difference from the present system, which only displays <em>up minus down</em>. This hypothesis makes the math easier.)</li>\n<li>Readers are competent in Bayesian statistics and strive to vote the true quality of the post.</li>\n</ul>\n<p>Let's talk about how the very first reader would vote. If they judged the post high quality, then they would multiply the prior likelihood ratio (6:4) times the bayes factor for a high private signal (4:1), get (6*4:4*1) = (6:1) and vote the post up. If they judged the post low quality then they would instead multiply by the bayes factor for a low private signal (1:4), get (6*1:4*4) = (3:8) and vote the post down.<a id=\"more\"></a></p>\n<p>There were two scenarios for the first reader (private information high or low). If we speculate that the first reader did in fact vote up, then there are two scenarios for the second scenario: There are two scenarios for the second reader:</p>\n<ol>\n<li>Personal judgement high: (6:4)*(4:1)*(4:1) = (24:1), vote up.</li>\n<li>Personal judgement low: (6:4)*(1:4)*(4:1) = (6:4), vote up <em>against personal judgement</em>.</li>\n</ol>\n<p>Note that now there are two explanations for ending up two votes up. It could be that the second reader actually agreed, or it could be that the second reader was following the first reader and the prior against their personal judgement. That means that the third reader gets zero information from the second reader's personal judgement! The two scenarios for the third reader, and every future reader, are exactly analogous to the two scenarios for the second reader.</p>\n<ol>\n<li>Personal judgement high: (6:4)*(4:1)*(4:1) = (24:1), vote up.</li>\n<li>Personal judgement low: (6:4)*(1:4)*(4:1) = (6:4), vote up <em>against personal judgement</em>.</li>\n</ol>\n<p>This has been a nightmare scenario of groupthink afflicting even diligent bayesians. Possible conclusions:</p>\n<ul>\n<li>Don't strive to vote the true quality of the post, strive to vote your personal judgement.</li>\n<li>Try to avoid even noticing the score. (Maybe scores could even be occluded, like spoiler-text?)</li>\n<li>Information cascades are dangerous and interesting. We should develop <a href=\"/lw/u/the_ethic_of_handwashing_and_community_epistemic/\">good cognitive citizenship</a> techniques.</li>\n<li>Broadcast novel evidence, not conclusions.</li>\n</ul>\n<p>Note: <a href=\"/user/Olle/\">Olle</a> found an error that necessitated a rewrite. I apologize.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 4, "ALwRRZqvhaop8gxkT": 2, "sHbKQDqrSinRPcnBv": 16}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DNQw596nPCX4x7xT9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 61, "baseScore": 59, "extendedScore": null, "score": 9.2e-05, "legacy": true, "legacyId": "35", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 59, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZP2om2oWHPhvWP2Q3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-06T04:44:05.349Z", "modifiedAt": null, "url": null, "title": "Is it rational to take psilocybin?", "slug": "is-it-rational-to-take-psilocybin", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:04.703Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pwno", "createdAt": "2009-02-27T06:17:31.584Z", "isAdmin": false, "displayName": "pwno"}, "userId": "SCgoHNxqc2agmDWEg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6QqfAirjEQLiwcosH/is-it-rational-to-take-psilocybin", "pageUrlRelative": "/posts/6QqfAirjEQLiwcosH/is-it-rational-to-take-psilocybin", "linkUrl": "https://www.lesswrong.com/posts/6QqfAirjEQLiwcosH/is-it-rational-to-take-psilocybin", "postedAtFormatted": "Friday, March 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20it%20rational%20to%20take%20psilocybin%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20it%20rational%20to%20take%20psilocybin%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6QqfAirjEQLiwcosH%2Fis-it-rational-to-take-psilocybin%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20it%20rational%20to%20take%20psilocybin%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6QqfAirjEQLiwcosH%2Fis-it-rational-to-take-psilocybin", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6QqfAirjEQLiwcosH%2Fis-it-rational-to-take-psilocybin", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 221, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\" \" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\"   DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\"   LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\"    UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--  /* Font Definitions */  @font-face \t{font-family:\"Cambria Math\"; \tpanose-1:2 4 5 3 5 4 6 3 2 4; \tmso-font-charset:0; \tmso-generic-font-family:roman; \tmso-font-pitch:variable; \tmso-font-signature:-1610611985 1107304683 0 0 159 0;} @font-face \t{font-family:Calibri; \tpanose-1:2 15 5 2 2 2 4 3 2 4; \tmso-font-charset:0; \tmso-generic-font-family:swiss; \tmso-font-pitch:variable; \tmso-font-signature:-1610611985 1073750139 0 0 159 0;}  /* Style Definitions */  p.MsoNormal, li.MsoNormal, div.MsoNormal \t{mso-style-unhide:no; \tmso-style-qformat:yes; \tmso-style-parent:\"\"; \tmargin-top:0in; \tmargin-right:0in; \tmargin-bottom:10.0pt; \tmargin-left:0in; \tline-height:115%; \tmso-pagination:widow-orphan; \tfont-size:11.0pt; \tfont-family:\"Calibri\",\"sans-serif\"; \tmso-ascii-font-family:Calibri; \tmso-ascii-theme-font:minor-latin; \tmso-fareast-font-family:Calibri; \tmso-fareast-theme-font:minor-latin; \tmso-hansi-font-family:Calibri; \tmso-hansi-theme-font:minor-latin; \tmso-bidi-font-family:\"Times New Roman\"; \tmso-bidi-theme-font:minor-bidi;} p \t{mso-style-noshow:yes; \tmso-style-priority:99; \tmso-margin-top-alt:auto; \tmargin-right:0in; \tmso-margin-bottom-alt:auto; \tmargin-left:0in; \tmso-pagination:widow-orphan; \tfont-size:12.0pt; \tfont-family:\"Times New Roman\",\"serif\"; \tmso-fareast-font-family:\"Times New Roman\";} .MsoChpDefault \t{mso-style-type:export-only; \tmso-default-props:yes; \tmso-ascii-font-family:Calibri; \tmso-ascii-theme-font:minor-latin; \tmso-fareast-font-family:Calibri; \tmso-fareast-theme-font:minor-latin; \tmso-hansi-font-family:Calibri; \tmso-hansi-theme-font:minor-latin; \tmso-bidi-font-family:\"Times New Roman\"; \tmso-bidi-theme-font:minor-bidi;} .MsoPapDefault \t{mso-style-type:export-only; \tmargin-bottom:10.0pt; \tline-height:115%;} @page Section1 \t{size:8.5in 11.0in; \tmargin:1.0in 1.0in 1.0in 1.0in; \tmso-header-margin:.5in; \tmso-footer-margin:.5in; \tmso-paper-source:0;} div.Section1 \t{page:Section1;} --><!--[if gte mso 10]> <mce:style><!   /* Style Definitions */  table.MsoNormalTable \t{mso-style-name:\"Table Normal\"; \tmso-tstyle-rowband-size:0; \tmso-tstyle-colband-size:0; \tmso-style-noshow:yes; \tmso-style-priority:99; \tmso-style-qformat:yes; \tmso-style-parent:\"\"; \tmso-padding-alt:0in 5.4pt 0in 5.4pt; \tmso-para-margin-top:0in; \tmso-para-margin-right:0in; \tmso-para-margin-bottom:10.0pt; \tmso-para-margin-left:0in; \tline-height:115%; \tmso-pagination:widow-orphan; \tfont-size:11.0pt; \tfont-family:\"Calibri\",\"sans-serif\"; \tmso-ascii-font-family:Calibri; \tmso-ascii-theme-font:minor-latin; \tmso-fareast-font-family:\"Times New Roman\"; \tmso-fareast-theme-font:minor-fareast; \tmso-hansi-font-family:Calibri; \tmso-hansi-theme-font:minor-latin;} --> <!--[endif]--></p>\n<p>Is it rational to take psilocybin?</p>\n<p>Just to make my definition of rational clear:</p>\n<p>Rationality is only intelligible when in the context of a goal (whether that goal be rational or irrational). Now, if one acts rationally, given their information set, will chose the best plan-of-action towards succeeding their goal. Part of being rational is knowing which goals will maximize one&rsquo;s utility function.</p>\n<p>According to <a href=\"http://dsc.discovery.com/news/2008/07/01/shroom-health-drug.html\">Discovery</a>:</p>\n<p>&ldquo;Scientists released their findings on a recent survey of volunteer psilocybin users 14 months after they took the drug.&rdquo;</p>\n<p>&ldquo;Sixty-four percent of the volunteers said they still felt at least a moderate increase in well-being or life satisfaction<a href=\"http://www.findingdulcinea.com/news/science/July-August/Study-Shrooms-Confer-Lasting-Positive-Effects.html#1\"></a>, in terms of things like feeling more creative, self-confident, flexible and optimistic. And 61 percent reported at least a moderate behavior change in what they considered positive ways.&rdquo;</p>\n<p>Assuming you won&rsquo;t get a bad trip, is it rational to take the drug?</p>\n<p>I doubt a psychedelic experience can help me optimize my current utility function better than my sober self. How can I be more rational from a drug? Therefore, I conclude that it must, in fact, change my preference ordering&mdash;make me care about things more than I would have otherwise. I prefer my preferences and therefore would rather keep my preferences the way they are now.</p>\n<p>If you were guaranteed to have all these positive results from taking the drug, would you take it?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FwM9CYSSXgjX6fJvG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6QqfAirjEQLiwcosH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 13, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "37", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-06T12:35:34.304Z", "modifiedAt": null, "url": null, "title": "Does blind review slow down science?", "slug": "does-blind-review-slow-down-science", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:01.956Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fsSoAMsntpsmrEC6a/does-blind-review-slow-down-science", "pageUrlRelative": "/posts/fsSoAMsntpsmrEC6a/does-blind-review-slow-down-science", "linkUrl": "https://www.lesswrong.com/posts/fsSoAMsntpsmrEC6a/does-blind-review-slow-down-science", "postedAtFormatted": "Friday, March 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20blind%20review%20slow%20down%20science%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20blind%20review%20slow%20down%20science%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfsSoAMsntpsmrEC6a%2Fdoes-blind-review-slow-down-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20blind%20review%20slow%20down%20science%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfsSoAMsntpsmrEC6a%2Fdoes-blind-review-slow-down-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfsSoAMsntpsmrEC6a%2Fdoes-blind-review-slow-down-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1100, "htmlBody": "<p>Previously, Robin Hanson <a href=\"http://www.overcomingbias.com/2007/08/anonymous-revie.html\">pointed out</a> that even if implementing anonymous peer review has an effect on the acceptance rate of different papers, this doesn't necessarily tell us the previous practice was biased. Yesterday, I ran across an interesting passage suggesting one way that anonymous review might actually be harmful:</p>\r\n<blockquote>\r\n<p>&nbsp;Second, fame may confer license. If a person has done valuable work in the past, this increases the probability that his current work is also valuable and induces the audience to suspend its disbelief. He can therefore afford to thumb his nose at the crowd. This is merely the obverse of the \"shamelessness\" of the old, which Aristotle discussed. Peter Messeri argues in this vein that \"senior scientists are better situated than younger scientists to withstand adverse consequences of public advocacy of unpopular positions,\" and that this factor may explain why the tendency for older scientists to resist new theories is, in fact, weak. And remember Kenneth Dover's negative verdict on old age (chapter 5)? He offered one qualification: \"There just aren't any [aspects of old age which compensate for its ills] - except, maybe, a complacent indifference to fashion, because people no longer seeking employment or promotion have less to fear.\"</p>\r\n<p>This point suggests that the use by scholarly journals of blind refereeing is a mistaken policy. It may cause them to turn down unconventional work to which they would rightly have given the benefit of doubt had they known that the author was not a neophyte or eccentric.</p>\r\n</blockquote>\r\n<p>&nbsp;(From Richard A. Posner, \"<a href=\"http://www.amazon.com/Aging-Old-Age-Richard-Posner/dp/0226675688/\">Aging and Old Age</a>\")</p>\r\n<p>&nbsp;If this hypothesis holds (and Posner admits it hasn't been tested, at least at the time of writing), then blind review may actually slow down the acceptance of theories which are radical but true. Looking up the Peter Messeri reference gave me the article \"<a href=\"http://www.jstor.org/stable/285378\">Age Differences in the Reception of New Scientific Theories: The Case of Plate Tectonics Theory</a>\". It notes:</p>\r\n<p><a id=\"more\"></a></p>\r\n<blockquote>\r\n<p>Young scientists may adopt a new theory before their elders in part because they are better informed about current research in a broader range of fields. As Zuckerman and Merton observe with respect to life-course differences in opportunities for discovery, young scientists - closer in time to their formal training with an 'aggregate of specialists at work on [many] research front[s]' - are likely to be more up to date 'in a wider variety of fields than their older and more specialized' colleagues. Moreover, defects in existing knowledge may be more apparent to young scientists burdened with fewer preconceptions reinforced by prior practice. &nbsp;Conversly, older scientists may take longer to be won over to a new theoretical perspective because of greater familiarity with past successes of established theory in overcoming previous theoretical or empirical challenges. Commitment to existing knowledge may also strenghten with age because older scientists have a greater social and cognitive investment in its perpetuation, and have less to gain from adopting new ideas. This may be particularly important when adoption of a new theory requires substantial effort to master new research skills and concepts.</p>\r\n</blockquote>\r\n<p>In this light, an older scientist's acceptance of a new, radical hypothesis should tell us to give the new hypothesis extra weight. It might even be appropriate to apply a heavier \"reputation weighting\" on controversial theories than established ones - we'd expect the established scientist to write papers supporting the established theories, but not controversial ones. However, blind review makes this impossible. The reviewers may even mistake the established scientist as an <a href=\"http://www.overcomingbias.com/2007/06/against_free_th.html\">undiscriminating free thinker</a>, who endorses any controversial theories simply because they're unpopular. This will slow down the acceptance of  hypotheses which are, in fact, correct.</p>\r\n<p>A possible objection to this could be that old scientists are unlikely to change their minds, and therefore old scientists not getting enough credit for their achievements won't have much of an effect in the spread of new hypotheses. (After all, if no old scientist endorses controversial hypotheses, then it doesn't matter if those endorsements aren't properly weighted in review.) Not so. Messeri finds that science does actually progress a lot faster than \"one funeral at a time\" (as Max Planck put it), and old scientists <em>are</em> ready to adopt new theories given sufficient evidence. While the last holdouts for outdated theories do tend to be the old, their increased security also makes them the first who are willing to publicly support new theories:</p>\r\n<blockquote>\r\n<p>&nbsp;...the episode which promted Planck's 'observation' ... seems like a poor illustration of the 'fact' Planck claims to have learned. ... Wilhelm Ostwald, one of the leaders of the opposition of 'Energetics' school prominently mentioned by Planck, was only five years older than Planck, whereas Ludwig Boltzmann, whose theoretical work on entropy, in no small measure ... helped bring the scientific community around to Planck's view, was fourteen years Planck's senior. ...</p>\r\n<p>In a study of the Chemical Revolution, McCann reports a negative correlation between author's age and the use of the oxygen paradigm in scientific papers written between 1760 and 1795. On closer inspection of the data, he finds the earliest group of converts to the oxygen paradigm (between 1772 and 1777) were middle-aged men with close ties to Lavoisier; the inverse age effect became manifest only after 1785, during the ten-year period of 'major conversion and consolidation'. ...</p>\r\n<p>As for evolutionary theory, Hull and his colleagues find weak support for 'Planck's Principle' among nineteenth-century British scientists. The small minority of scientists who held out against the theory after 1869 were, on average, almost ten years older than earlier adopters. Age in 1859 (the year the Origin of Species was published) was unrelated, however, to speed of acceptance for the great majority of those converting to evolutionary theory by 1869. ...<br /><br />I examined the publications of ninety-six North American earth scientists actively engaged in pertinent research during the 1960s and 1970s. ... The dependent variable for this study is the year in which a scientist decided to adopt the mobilist programme of research rather than to continue working within a stabilist programme. ... Before 1966, when prevailing scientific opinion still ran strongly against the mobilist perspective, the small number of scientists adopting the programme were considerably older (in terms of career age) than other scientists active during this early period. Thus, scientists adopting the programme through 1963 were on average nineteen years 'older' than non-adopters. ... Adopters in 1964 were twenty-three years older than non-adopters. ... Only with the shift in scientific opinion favourable to mobilist concepts beginning in 1966, do we start to see a progressive narrowing, and then reversal, in the age differentials between adopters and non-adopters.</p>\r\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZpG9rheyAkgCoEQea": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fsSoAMsntpsmrEC6a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 21, "extendedScore": null, "score": 4.795675924083732e-07, "legacy": true, "legacyId": "40", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-06T20:22:39.032Z", "modifiedAt": null, "url": null, "title": "Formalization is a rationality technique", "slug": "formalization-is-a-rationality-technique", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:12.758Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Johnicholas", "createdAt": "2009-02-27T15:01:52.708Z", "isAdmin": false, "displayName": "Johnicholas"}, "userId": "kBvTXutfPytNtzPyD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gM4XxTNe7ChfZgpkS/formalization-is-a-rationality-technique", "pageUrlRelative": "/posts/gM4XxTNe7ChfZgpkS/formalization-is-a-rationality-technique", "linkUrl": "https://www.lesswrong.com/posts/gM4XxTNe7ChfZgpkS/formalization-is-a-rationality-technique", "postedAtFormatted": "Friday, March 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Formalization%20is%20a%20rationality%20technique&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFormalization%20is%20a%20rationality%20technique%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgM4XxTNe7ChfZgpkS%2Fformalization-is-a-rationality-technique%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Formalization%20is%20a%20rationality%20technique%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgM4XxTNe7ChfZgpkS%2Fformalization-is-a-rationality-technique", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgM4XxTNe7ChfZgpkS%2Fformalization-is-a-rationality-technique", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 533, "htmlBody": "<p>We are interested in developing practical techniques of rationality. One practical technique, used widely and successfully in science and technology is formalization, transforming a less-formal argument into a more-formal one. Despite its successes, formalization isn't trivial to learn, and schools rarely try to teach <em>general</em> techniques of thinking and deciding. Instead, schools generally only teach domain-specific reasoning. We end up with graduates who can apply formalization skillfully inside of specific domains (e.g. electrical engineering or biology), but fail to apply, or misapply, their skills to other domains (e.g. politics or religion).</p>\n<p><a id=\"more\"></a>A side excursion, to be used as an example:</p>\n<blockquote>\n<p>Is it true that a real-world decision can be perfectly justified via a mathematical proof? No.</p>\n<p>Here is one reason: Any real-world \"proof\", whether it is a publication in a math journal or the output of a computer proof-checker, might contain a mistake. Human mathematicians routinely make mistakes in publications. The computer running the proof-checker is physical, and might be struck by a cosmic ray. Even if the computer were perfect, the proof-checker might have a bug in it.</p>\n<p>Here is another reason: Even if you had a proof known to be mistake free, mathematical proof always reasons from assumptions to conclusions. To apply the proof to the real world, you must make an informal argument that the assumptions apply in this real-world situation, and a second informal argument that the conclusion of the proof corresponds to the real-world decision. These first and last links in the chain will still be there, no matter how rigorous and trusted the proof is. The problem with duplicating real-world spheres with <a href=\"http://en.wikipedia.org/wiki/Banach%E2%80%93Tarski_paradox\">Banach-Tarski</a> isn't that there's a mistake in the proof, it's that the real-world spheres don't fit the assumptions.</p>\n</blockquote>\n<p>If you were fitting this argument into your beliefs, you might produce a number, a \"gut\" estimate of how likely this informal argument is wrong. Can we improve on that using the technique of formalization? What would a formalization of this argument look like? One possible starting point might be to rename everything. We're confident (via philosophy of logic) that renaming won't increase or decrease the quality of the argument. We will reason better about the correctness of the <em>form</em> if we hide the <em>subjects</em> of the argument.</p>\n<blockquote>\n<p>Is it true that A? No.</p>\n<p>Here is one reason: B is true (and B implies not A). Intuition pump. Intuition pump. Intuition pump.</p>\n<p>Here is another reason: Even assuming B were false, C is true (and C implies not A). Intuition pump.</p>\n</blockquote>\n<p>Note: there are many choices in this renaming process. It's not a trivial, thought-free operation at all. Someone else might get a completely different \"underlying structure\" from the same starting point. This particular structure suggests an equation, something like:</p>\n<blockquote>\n<p>P( whole argument is wrong ) = P( first subargument is wrong ) * P( second subargument is wrong | first subargument is wrong )</p>\n</blockquote>\n<p>The equation allows you to estimate the probability of the whole argument being wrong, using two \"gut\" estimates instead of one. This is probably an improved, lower-variance estimate.</p>\n<p>The point is:</p>\n<ul>\n<li>Formalization is a rationality technique.</li>\n<li>Renaming primitive notions to meaningless symbols is a reasonable first step in formalization.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "6nS8oYmSMuFMaiowF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gM4XxTNe7ChfZgpkS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 3, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "41", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-07T01:34:03.994Z", "modifiedAt": null, "url": null, "title": "Slow down a little... maybe?", "slug": "slow-down-a-little-maybe", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:01.516Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GTzBTtkZH8KxNNfxA/slow-down-a-little-maybe", "pageUrlRelative": "/posts/GTzBTtkZH8KxNNfxA/slow-down-a-little-maybe", "linkUrl": "https://www.lesswrong.com/posts/GTzBTtkZH8KxNNfxA/slow-down-a-little-maybe", "postedAtFormatted": "Saturday, March 7th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Slow%20down%20a%20little...%20maybe%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASlow%20down%20a%20little...%20maybe%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGTzBTtkZH8KxNNfxA%2Fslow-down-a-little-maybe%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Slow%20down%20a%20little...%20maybe%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGTzBTtkZH8KxNNfxA%2Fslow-down-a-little-maybe", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGTzBTtkZH8KxNNfxA%2Fslow-down-a-little-maybe", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<p>I think that three posts a day over and above Yudkowsky and/or Hanson posts might be enough.&nbsp; Where anything that gets voted to 0 or below doesn't count, nor do quick links.</p>\n<p>Say you differently, readers?&nbsp; I'm just trying to space things out so we don't get overloaded with everything, all at once... if it turns out that people just have more to say than this, sustainably in the long term, then we can raise the posting speed.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GTzBTtkZH8KxNNfxA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": -4, "extendedScore": null, "score": -4e-06, "legacy": true, "legacyId": "44", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-07T15:47:14.673Z", "modifiedAt": null, "url": null, "title": "Checklists", "slug": "checklists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:55.564Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Johnicholas", "createdAt": "2009-02-27T15:01:52.708Z", "isAdmin": false, "displayName": "Johnicholas"}, "userId": "kBvTXutfPytNtzPyD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HnzB46zsL8ehdpLcd/checklists", "pageUrlRelative": "/posts/HnzB46zsL8ehdpLcd/checklists", "linkUrl": "https://www.lesswrong.com/posts/HnzB46zsL8ehdpLcd/checklists", "postedAtFormatted": "Saturday, March 7th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Checklists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChecklists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHnzB46zsL8ehdpLcd%2Fchecklists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Checklists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHnzB46zsL8ehdpLcd%2Fchecklists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHnzB46zsL8ehdpLcd%2Fchecklists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 413, "htmlBody": "<p><a href=\"http://www.newyorker.com/reporting/2007/12/10/071210fa_fact_gawande\">Checklists</a> are a rationality technique, <a href=\"http://www.overcomingbias.com/2009/02/the-intervention-and-the-checklist-two-paradigms-for-improvement.html\">mentioned previously on OB</a>. Everyone knows this, but we don't hear about them as often as we should, possibly because they seem prosaic and boring.</p>\n<p>In the context of doing something over and over, there is a checklist improvement cycle.</p>\n<ul>\n<li>You try to make the thing (e.g. the blog post, the rational decision, the mathematical proof)</li>\n<li>For each kind of error in your checklist, you search the thing for that kind of error, and fix it if it occurs.</li>\n<li>When something that passed your checklist turns out to have had an error, you add that kind of error to your checklist.</li>\n</ul>\n<p>There are many caveats to this description: Some checklists are not primarily lists of errors, but primarily ordered procedures. You may want to complicate the cycle to track the cost and benefit of the items on the checklist. We're assuming that errors are eventually discovered. I want to pass over those caveats and claim that this kind of checklist-of-errors is very successful. If you agree, my question is: What feature or features of our minds are checklists compensating for? If we understood that, then we would be able to use checklists even more effectively.<a id=\"more\"></a></p>\n<p>The act of considering the current checklist item and the post/decision/proof simultaneously reminds me of \"Forced Association\", a creativity technique. So one idea is that by putting one's mind into several different states via forced association with the items on the checklist, we gain more independent chances to detect an error.</p>\n<p>Even if you haven't made any errors yet, and so your checklist is empty, conducting several searches for errors while wearing <a href=\"http://en.wikipedia.org/wiki/De_Bono_Hats\">De Bono's hats</a> (or another forced association list) might be a way to make fewer errors.</p>\n<p>If you're consciously looking for a red minivan, then you will notice more red minivans. This \"noticing\" seems surprisingly spontaneous, unlike deliberately scanning a scene and considering \"Is that a red minivan? Is that a red minivan?\". Possibly this is because the noticing is being done by unconscious modules of our minds. A checklist breaks our search for errors into a sequence of searches for more specific kinds of errors. Possibly checklists are effective because the general concept \"error\" is too vague for those modules. By breaking it into easier chunks (e.g. \"ad hominem fallacy\", \"missing semicolon\"), we can start using those modules to get a more thorough search.</p>\n<p>I admit, I'm not sure how to use this \"modules\" idea to use checklists more effectively.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2oWPnnnzMbiAxWfbs": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HnzB46zsL8ehdpLcd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 17, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "45", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-07T18:32:13.264Z", "modifiedAt": null, "url": null, "title": "The Golem", "slug": "the-golem", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:02.243Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d4pyYJ8Xdi5o6GJYT/the-golem", "pageUrlRelative": "/posts/d4pyYJ8Xdi5o6GJYT/the-golem", "linkUrl": "https://www.lesswrong.com/posts/d4pyYJ8Xdi5o6GJYT/the-golem", "postedAtFormatted": "Saturday, March 7th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Golem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Golem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd4pyYJ8Xdi5o6GJYT%2Fthe-golem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Golem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd4pyYJ8Xdi5o6GJYT%2Fthe-golem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd4pyYJ8Xdi5o6GJYT%2Fthe-golem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 381, "htmlBody": "<p>Anthony Ravenscroft <a href=\"http://www.amazon.com/Polyamory-Roadmaps-Clueless-Anthony-Ravenscroft/dp/1890109533/\">writing on</a> why it is important, in a relationship, to honestly communicate your grievances to the other person:</p>\r\n<blockquote>\r\n<p>If you don't present your gripes to the responsible party, you <em>cannot</em> humanly bury those complaints - it's just not possible to \"forget\" about something that has hurt or stung you. Actually, you are probably \"testing\" these complaints against your experience of the person, trying to figure out what they would say, how they would react. You create a simulacrum in order to argue this all out in your head, and thus to avoid unpleasantness. Certain conclusions are made, which you file away. When another problem comes up, you then test this against your estimates of the person, which have been expanded by your previous guesswork.<br /><br />Eventually, you will have created this huge guesswork of assumptions, which are so far removed from the actual person that they likely have no bearing on the reality. I call this \"a golem made of boxes\", a warehouse-sized beast that has nothing to do with the simple small human being from which it is supposedly modeled.<br /><br />When I have had such a golem used against me, I was told by my lover that she had kept a rather ugly situation from me \"because I know how you'd react.\" I described to her exactly what the situation was, as I'd pieced it together very accurately (you <em>can</em> do this with the <em>actions</em> of humans, not the humans themselves). She was stunned. When I described for her how the root assumptions she had made were very largely off the mark, she actually became very angry with me, defending the golem as though it represented the truth, and therefore I must be lying! In the end, she could have better determined my reaction from writing down the possibilities on slips of paper and choosing one out of a hat. ...<br /><br />The golem is handy, but almost entirely dishonest. It begins from faulty (incomplete, biased) data, and runs rapidly downhill from there.</p>\r\n</blockquote>\r\n<p>The map and the territory. How have you had the golem used against you? When have you, yourselves, made the mistake of resorting to a golem and had it blow in your face?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"mip7tdAN87Jarkcew": 1, "wMPYFGmhcFg4bSb4Z": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d4pyYJ8Xdi5o6GJYT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 17, "extendedScore": null, "score": 4.798248845523149e-07, "legacy": true, "legacyId": "47", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-07T22:55:33.476Z", "modifiedAt": null, "url": null, "title": "Simultaneously Right and Wrong", "slug": "simultaneously-right-and-wrong", "viewCount": null, "lastCommentedAt": "2022-05-25T21:35:33.986Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/P3uavjFmZD5RopJKk/simultaneously-right-and-wrong", "pageUrlRelative": "/posts/P3uavjFmZD5RopJKk/simultaneously-right-and-wrong", "linkUrl": "https://www.lesswrong.com/posts/P3uavjFmZD5RopJKk/simultaneously-right-and-wrong", "postedAtFormatted": "Saturday, March 7th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Simultaneously%20Right%20and%20Wrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASimultaneously%20Right%20and%20Wrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP3uavjFmZD5RopJKk%2Fsimultaneously-right-and-wrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Simultaneously%20Right%20and%20Wrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP3uavjFmZD5RopJKk%2Fsimultaneously-right-and-wrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP3uavjFmZD5RopJKk%2Fsimultaneously-right-and-wrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1006, "htmlBody": "<p><strong>Related to:</strong> <a href=\"http://www.overcomingbias.com/2007/07/belief-in-belie.html\">Belief in Belief</a>, <a href=\"http://www.overcomingbias.com/2008/11/convenient-over.html\">Convenient Overconfidence</a></p>\n<p><em>&nbsp;&nbsp;&nbsp;&nbsp; \"You've no idea of what a poor opinion I have of myself, and how little I deserve it.\"</em></p>\n<p>&nbsp; &nbsp; &nbsp; -- W.S. Gilbert <br /><br />In 1978, Steven Berglas and Edward Jones performed a <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/650387\">study</a> on voluntary use of performance inhibiting drugs. They asked subjects to solve certain problems. The control group received simple problems, the experimental group impossible problems. The researchers then told all subjects they'd solved the problems successfully, leaving the controls confident in their own abilities and the experimental group privately aware they'd just made a very lucky guess.<br /><br />Then they offered the subjects a choice of two drugs to test. One drug supposedly enhanced performance, the other supposedly handicapped it.</p>\n<p>There's a cut here in case you want to predict what happened.</p>\n<p><a id=\"more\"></a></p>\n<p>Males in the lucky guesser group chose the performance-inhibiting drug significantly more than those in the control group<sup>1</sup>.<br /><br />The researchers conjectured that the lucky guessers felt good about acing the first test. They anticipated failing the second whether they chose the helpful or the harmful drug. So they chose the harmful drug as an excuse: \"Oh, I would have passed the test, only the drug was making me stupid.\" As the study points out, this is a win-win situation: if they fail, the drug excuses their failure, and if they succeed it's doubly impressive that they passed even with a handicap.<br /><br />Since this study, psychologists have applied self-handicapping to interpret common phenomena like <a href=\"http://blogs.psychologytoday.com/blog/dont-delay/200805/delay-self-handicapping-strategy-i-can-protect-my-self-image-procrastinating\">procrastination</a>, <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/6842365\">hypochondria</a>, and <a href=\"http://psp.sagepub.com/cgi/content/abstract/28/12/1667\">minority underachievement</a>; they've also discovered factors that increase or decrease self-handicapping tendencies (for example, you can increase it by making the test more relevant to your self-esteem: just say \"this is a proven test of intellectual ability in general\").<br /><br />But some especially interesting studies investigated the effect of privacy on self-handicapping. For example, Hobden conducted an <a href=\"https://tspace.library.utoronto.ca/bitstream/1807/10783/1/NQ27951.pdf\">experiment</a> similar to Berglas and Jones', albeit with tapes of performance-enhancing or -handicapping music instead of drugs. The twist: half the subjects' choice of tapes and test scores were public, and half the subjects believed their choice of tapes and test scores were anonymous. What happens when no one but the subject himself will ever know his test score? He self-handicaps just as often as everyone else. And it seems to *work*. The same set of studies showed that subjects who self-handicap on a test are less likely to attribute their failure on the test to their own incompetence.<br /><br />In order to handicap, subjects must have an inaccurate assessment of their own abilities. Otherwise, there's no self-esteem to protect. If I believe my IQ is 80, and I get 80 on an IQ test, I have no incentive to make excuses to myself, or to try to explain away the results. The only time I would want to explain away the results as based on some external factor was if I'd been going around thinking my real IQ was 100.<br /><br />But subjects also must have an accurate assessment of their own abilities. Subjects who take an easy pre-test and expect an easy test do not self-handicap. Only subjects who understand their low chances of success can think \"I will probably fail this test, so I will need an excuse<sup>2</sup>. <br /><br />If this sounds familiar, it's because it's another form of the dragon problem from <a href=\"http://www.overcomingbias.com/2007/07/belief-in-belie.html\">Belief in Belief</a>. The believer says there is a dragon in his garage, but expects all attempts to detect the dragon's presence to fail. Eliezer writes: \"The claimant must have an accurate model of the situation somewhere in his mind, because he can anticipate, in advance, exactly which experimental results he'll need to excuse.\" <br /><br />Should we say that the subject believes he will get an 80, but believes in believing that he will get a 100? This doesn't quite capture the spirit of the situation. Classic belief in belief seems to involve value judgments and complex belief systems, but self-handicapping seems more like simple overconfidence bias<sup>3</sup>. Is there any other evidence that overconfidence has a belief-in-belief aspect to it?<br /><br />Last November, <a href=\"http://www.overcomingbias.com/2008/11/convenient-over.html\">Robin described</a> a study where subjects were less overconfident if asked to predict their performance on tasks they will actually be expected to complete. He ended by noting that \"It is almost as if we at some level realize that our overconfidence is unrealistic.\"<br /><br />Belief in belief in religious faith and self-confidence seem to be two areas in which we can be simultaneously right and wrong: expressing a biased position on a superficial level while holding an accurate position on a deeper level. The specifics are different in each case, but perhaps the same general mechanism may underlie both. How many other biases use this same mechanism?<br /><br /><strong>Footnotes</strong><br /><br />1: In most studies on this effect, it's most commonly observed among males. The reasons are too complicated and controversial to be discussed in this post, but are left as an exercise for the reader with a background in evolutionary psychology.<br /><br />2: Compare the ideal Bayesian, for whom expected future expectation is always the same as the current expectation, and investors in an ideal stock market, who must always expect a stock's price tomorrow to be on average the same as its price today - to this poor creature, who accurately predicts that he will lower his estimate of his intelligence after taking the test, but who doesn't use that prediction to change his pre-test estimates.<br /><br />3: I have seen \"overconfidence bias\" used in two different ways: to mean poor calibration on guesses (ie predictions made with 99% certainty that are only right 70% of the time) and to mean the tendency to overestimate one's own good qualities and chance of success. I am using the latter definition here to remain consistent with the common usage on Overcoming Bias; other people may call this same error \"optimism bias\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YTCrHWYHAsAD74EHo": 1, "LDTSbmXtokYAsEq8e": 1, "KWFhr6A2dHEb6wmWJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "P3uavjFmZD5RopJKk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 103, "baseScore": 108, "extendedScore": null, "score": 0.000164, "legacy": true, "legacyId": "49", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 108, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 63, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-08T02:27:39.732Z", "modifiedAt": null, "url": null, "title": "Moore's Paradox", "slug": "moore-s-paradox", "viewCount": null, "lastCommentedAt": "2020-04-13T01:56:26.514Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ERRk4thxxYNcScqR4/moore-s-paradox", "pageUrlRelative": "/posts/ERRk4thxxYNcScqR4/moore-s-paradox", "linkUrl": "https://www.lesswrong.com/posts/ERRk4thxxYNcScqR4/moore-s-paradox", "postedAtFormatted": "Sunday, March 8th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Moore's%20Paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMoore's%20Paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FERRk4thxxYNcScqR4%2Fmoore-s-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Moore's%20Paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FERRk4thxxYNcScqR4%2Fmoore-s-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FERRk4thxxYNcScqR4%2Fmoore-s-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 672, "htmlBody": "<p>I think I understand Moore's Paradox a bit better now, after reading some of the comments on Less Wrong.&nbsp; <a href=\"/lw/r/no_really_ive_deceived_myself/#ga\">Jimrandomh</a> suggests:</p>\n<blockquote>\n<p>Many people cannot distinguish between levels of indirection. To them, \"I believe X\" and \"X\" are the same thing, and therefore, reasons why it is beneficial to believe X are also reasons why X is true.</p>\n</blockquote>\n<p>I don't think this is correct&mdash;relatively young children can understand the concept of having a false belief, which requires separate mental buckets for the map and the territory.&nbsp; But it points in the direction of a similar idea:</p>\n<p>Many people may not consciously distinguish between <em>believing </em>something and <em>endorsing</em> it.</p>\n<p>After all&mdash;\"I believe in democracy\" means, colloquially, that you endorse the concept of democracy, not that you believe democracy exists.&nbsp; The word \"belief\", then, has more than one meaning.&nbsp; We could be looking at a <a href=\"http://www.overcomingbias.com/2008/02/compress-fallac.html\">confused word</a> that causes confused thinking (or maybe it just reflects pre-existing confusion).</p>\n<p>So: in the <a href=\"/lw/s/belief_in_selfdeception/\">original example</a>, \"I believe people are nicer than they are\", she came up with some reasons why it would be good to believe people are nice&mdash;health benefits and such&mdash;and since she now had some warm affect on \"believing people are nice\", she introspected on this warm affect and concluded, \"I believe people are nice\".&nbsp; That is, she mistook the <em>positive affect</em> attached to the quoted belief, as signaling <em>her belief in the proposition.</em>&nbsp; At the same time, the world itself seemed like people weren't so nice.&nbsp; So she said, \"I believe people are nicer than they are.\"<em><a id=\"more\"></a></em></p>\n<p>And that verges on being an honest mistake&mdash;sort of&mdash;since people are not taught explicitly how to know when they believe something.&nbsp; As in the parable of <a href=\"http://www.overcomingbias.com/2007/07/belief-in-belie.html\">the dragon in the garage</a>; the one who says \"There is a dragon in my garage&mdash;but it's invisible\", does not recognize his <em>anticipation</em> of seeing no dragon, as indicating that he possesses an (accurate) model with no dragon in it.</p>\n<p>It's not as if people are <em>trained </em>to recognize when they believe something.&nbsp; It's not like they're ever taught in high school:&nbsp; \"What it feels like to actually believe something&mdash;to have that statement in your belief pool&mdash;is that it just seems like the way the world <em>is.</em>&nbsp; You should recognize this feeling, which is actual (unquoted) belief, and distinguish it from having good feelings about a belief that you recognize as a belief (which means that it's in quote marks).\"</p>\n<p>This goes a long way toward making this real-life case of Moore's Paradox <a href=\"/lw/s/belief_in_selfdeception/\">seem less alien</a>, and providing another mechanism whereby people can be <a href=\"/lw/1d/simultaneously_right_and_wrong/\">simultaneously right and wrong</a>.</p>\n<p>Likewise <a href=\"/lw/r/no_really_ive_deceived_myself/#gk\">Kurige</a> who wrote:</p>\n<blockquote>\n<p>I believe that there is a God&mdash;and that He has instilled a sense of right and wrong in us by which we are able to evaluate the world around us.&nbsp; I also believe a sense of morality has been evolutionarily programmed into us&mdash;a sense of morality that is most likely a result of the formation of meta-political coalitions in Bonobo communities a very, very long time ago.&nbsp; These two beliefs are not contradictory, but the complexity lies in reconciling the two.</p>\n</blockquote>\n<p>I suspect, Kurige, you have decided that you have <em>reasons to endorse</em> the quoted belief that God has instilled a sense of right and wrong in us.&nbsp; And also that you have reasons to endorse the verdict of science.&nbsp; They both seem like good communities to join, right?&nbsp; There are benefits to both sets of beliefs?&nbsp; You introspect and find that you feel good about both beliefs?</p>\n<p>But you did <em>not </em>say:</p>\n<p>\"God instilled a sense of right and wrong in us, and also a sense of morality has been evolutionarily programmed into us.&nbsp; The two states of reality are not inconsistent, but the complexity lies in reconciling the two.\"</p>\n<p>If you're reading this, Kurige, you should very quickly say the above out loud, so you can notice that it seems at least slightly harder to swallow&mdash;notice the <em>subjective difference</em>&mdash;before you go to the trouble of rerationalizing.</p>\n<p>This is the subjective difference between having reasons to endorse two different beliefs, and your mental model of a single world, a single way-things-are.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"L3NcKBNTvQaFXwv9u": 3, "Ng8Gice9KNkncxqcj": 2, "br65ewZbmvPA6GKPi": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ERRk4thxxYNcScqR4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 66, "baseScore": 71, "extendedScore": null, "score": 0.000108, "legacy": true, "legacyId": "51", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "qqFS6Kw5fmPyzkLby", "canonicalCollectionSlug": "rationality", "canonicalBookId": "coGq3LC5Yn4vZiu6k", "canonicalNextPostSlug": "don-t-believe-you-ll-self-deceive", "canonicalPrevPostSlug": "belief-in-self-deception", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 71, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wP2ymm44kZZwaFPYh", "P3uavjFmZD5RopJKk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-08T07:23:41.621Z", "modifiedAt": null, "url": null, "title": "It's the Same Five Dollars!", "slug": "it-s-the-same-five-dollars", "viewCount": null, "lastCommentedAt": "2013-11-19T21:40:33.700Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Z_M_Davis", "createdAt": "2009-02-27T04:57:37.811Z", "isAdmin": false, "displayName": "Z_M_Davis"}, "userId": "YEWv9mcjBb7Z7Cgw3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JZRtfjG48xNR3GKeo/it-s-the-same-five-dollars", "pageUrlRelative": "/posts/JZRtfjG48xNR3GKeo/it-s-the-same-five-dollars", "linkUrl": "https://www.lesswrong.com/posts/JZRtfjG48xNR3GKeo/it-s-the-same-five-dollars", "postedAtFormatted": "Sunday, March 8th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20It's%20the%20Same%20Five%20Dollars!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIt's%20the%20Same%20Five%20Dollars!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJZRtfjG48xNR3GKeo%2Fit-s-the-same-five-dollars%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=It's%20the%20Same%20Five%20Dollars!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJZRtfjG48xNR3GKeo%2Fit-s-the-same-five-dollars", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJZRtfjG48xNR3GKeo%2Fit-s-the-same-five-dollars", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 420, "htmlBody": "<p><span style=\"font-family: Times; font-size: 16px; \">\n<div style=\"color: #000000; font-family: Verdana, Arial, Helvetica, sans-serif; margin-top: 8px; margin-right: 8px; margin-bottom: 8px; margin-left: 8px; background-image: initial; background-repeat: initial; background-attachment: initial; -webkit-background-clip: initial; -webkit-background-origin: initial; font-size: small; background-color: white; padding-top: 0.5em; padding-right: 0.5em; padding-bottom: 0.5em; padding-left: 0.5em; background-position: initial initial; \">\n<p>From Tversky and Khaneman's \"The Framing of Decisions and the Psychology of Choice\" (<em>Science</em>, Vol. 211, No. 4481, 1981):</p>\n<blockquote style=\"border-left-width: 2px; border-left-style: solid; border-left-color: #336699; padding-left: 4px; margin-top: 5px; margin-bottom: 5px; margin-left: 5px; margin-right: 15px; \">\n<p>The following problem [...] illustrates the effect of embedding an option in different accounts. Two versions of this problem were presented to different groups of subjects. One group (N = 93) was given the values that appear in parentheses, and the other group (N = 88) the values shown in brackets.</p>\n<p style=\"padding-left: 30px;\">[...] Imagine that you are about to purchase a jacket for ($125) [$15], and a calculator for ($15) [$125]. The calculator salesman informs you that the calculator you wish to buy is on sale for ($10) [$120] at the other branch of the store, located 20 minutes drive away. Would you make the trip to the other store?<a id=\"more\"></a></p>\n<p>The response to the two versions of [the problem] were markedly different: 68 percent of the respondents were willing to make an extra trip to save $5 on a $15 calculator; only 29 percent were willing to exert the same effort when the price of the calculator was $125. [...]&nbsp;A closely related observation has been reported [...] that the variability of the prices at which a given product is sold by different stores is roughly proportional to the mean price of that product. The same pattern was observed for both frequently and infrequently purchased items. Overall, a ratio of 2:1 in the mean price of two products is associated with a ratio of 1.86:1 in the standard deviation of the respective quoted prices. If the effort that consumers exert to save each dollar on a purchase [...] were independent of price, the dispersion of quoted prices should be about the same for all products.</p>\n</blockquote>\n<p>This one's a killer. Money is supposed to be fungible, but these observations really highlight how difficult it is to really behave as if you believed that. So, aspiring rationalists, how might we combat this in ourselves? Maybe it would help to consciously convert between money and time: if you value your time at 25 $/hr, then the cost of a twenty-minute drive is 25 $/hr * (1/3) hr = $8.33 &gt; $5, so you buy the calculator in front of you in either case. So this heuristic at least takes care of the calculator problem, although I would guess it fails miserably in other contexts, I currently know not which.</p>\n<p>Another takeaway lesson is to ignore advertisements boasting that a product is currently such-and-such percent off. We don't care about the percentage! How many <em>minutes</em>&nbsp;are you saving?&nbsp;</p>\n</div>\n</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PDJ6KqJBRzvKPfuS3": 1, "4R8JYu4QF2FqzJxE5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JZRtfjG48xNR3GKeo", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 26, "extendedScore": null, "score": 4.799354395230169e-07, "legacy": true, "legacyId": "54", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-03-08T07:23:41.621Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-08T14:43:22.152Z", "modifiedAt": null, "url": null, "title": "Lies and Secrets", "slug": "lies-and-secrets", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:01.856Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "steven0461", "createdAt": "2009-02-27T16:16:38.980Z", "isAdmin": false, "displayName": "steven0461"}, "userId": "cn4SiEmqWbu7K9em5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uXJQekmFRKDGCfXph/lies-and-secrets", "pageUrlRelative": "/posts/uXJQekmFRKDGCfXph/lies-and-secrets", "linkUrl": "https://www.lesswrong.com/posts/uXJQekmFRKDGCfXph/lies-and-secrets", "postedAtFormatted": "Sunday, March 8th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lies%20and%20Secrets&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALies%20and%20Secrets%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuXJQekmFRKDGCfXph%2Flies-and-secrets%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lies%20and%20Secrets%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuXJQekmFRKDGCfXph%2Flies-and-secrets", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuXJQekmFRKDGCfXph%2Flies-and-secrets", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 593, "htmlBody": "<p>My intuition says presenting bad facts or pieces of reasoning is wrong, but withholding good facts or pieces of reasoning is less wrong. I assume most of you agree.<br /><br />This is a puzzle, because on the face of it, the effect is the same.<br /><br />Suppose the Walrus and the Carpenter are talking of whether pigs have wings.<br /><br />Scenario 1: The Carpenter is 80% sure that pigs have wings, but the Walrus wants him to believe that they don't. So the Walrus claims that it's a deep principle of evolution theory that no animal can have wings, and the Carpenter updates to 60%.<br /><br />Scenario 2: The Carpenter is 60% sure that pigs have wings, and the Walrus wants him to believe that they don't. So the Walrus neglects to mention that he once saw a picture of a winged pig in a book. Learning this would cause the Carpenter to update to 80%, but he doesn't learn this, so he stays at 60%.<br /><br />In both scenarios, the Walrus chose for the Carpenter's probability to be 60% when he could have chosen for it to be 80%. So what's the difference?<br /><a id=\"more\"></a><br />If there isn't any, then we're forced to claim bias (maybe <a href=\"http://en.wikipedia.org/wiki/Omission_bias\">omission bias</a>), which we can then try to overcome.<br /><br />But in this post I want to try rationalizing the asymmetry. I don't feel that my thinking here is clear, so this is very tentative.<br /><br />If a man is starving, not giving him a loaf of bread is as deadly as giving him cyanide. But if there are a lot of random objects lying around in the neighborhood, the former deed is less deadly: it's far more likely that one of the random objects is a loaf of bread than that it is an antidote to cyanide.<br /><br />I believe that, likewise, it is more probable that you'll randomly find a good argument duplicated (conditioning on it makes some future evidence redundant), than that you'll randomly find a bad argument debunked (conditioning on it makes some future counter-evidence relevant). In other words, whether you're uninformed or misinformed, you're equally mistaken; but in an environment where evidence is not independent, it's normally easier to recover from being uninformed than from being misinformed.<br /><br />The case becomes stronger when you think of it in terms of boundedly rational agents fishing from a common meme pool. If agents can remember or hold in mind fewer pieces of information than they are likely to encounter, pieces of disinformation floating in the pool not only do damage by themselves, but do further damage by displacing pieces of good information.<br /><br />These are not the only asymmetries. A banal one is that misinforming takes effort and not informing saves effort. And if you're caught misinforming, that makes you look far worse than if you're caught not informing. (But the question is why this should be so. Part of it is that, usually, there are plausible explanations other than bad faith for why one might not inform -- if not, it's called \"lying by omission\" -- but no such explanations for why one might misinform.) And no doubt there are yet others.</p>\n<p>But I think a major part of it has to be that ignorance heals better than confusion when placed in a bigger pool of evidence. Do you agree? Do you think \"lies\" are worse than \"secrets\", and if so, why?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nANxo5C4sPG9HQHzr": 1, "3uE2pXvbcnS9nnZRE": 1, "nSHiKwWyMZFdZg5qt": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uXJQekmFRKDGCfXph", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 18, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "55", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-08T20:39:39.139Z", "modifiedAt": null, "url": null, "title": "The Mystery of the Haunted Rationalist", "slug": "the-mystery-of-the-haunted-rationalist", "viewCount": null, "lastCommentedAt": "2022-02-04T23:47:04.912Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mja6jZ6k9gAwki9Nu/the-mystery-of-the-haunted-rationalist", "pageUrlRelative": "/posts/mja6jZ6k9gAwki9Nu/the-mystery-of-the-haunted-rationalist", "linkUrl": "https://www.lesswrong.com/posts/mja6jZ6k9gAwki9Nu/the-mystery-of-the-haunted-rationalist", "postedAtFormatted": "Sunday, March 8th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Mystery%20of%20the%20Haunted%20Rationalist&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Mystery%20of%20the%20Haunted%20Rationalist%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmja6jZ6k9gAwki9Nu%2Fthe-mystery-of-the-haunted-rationalist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Mystery%20of%20the%20Haunted%20Rationalist%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmja6jZ6k9gAwki9Nu%2Fthe-mystery-of-the-haunted-rationalist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmja6jZ6k9gAwki9Nu%2Fthe-mystery-of-the-haunted-rationalist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 860, "htmlBody": "<p><strong>Followup to: </strong><a href=\"/lw/1d/simultaneously_right_and_wrong/\">Simultaneously Right and Wrong</a></p>\n<p>&nbsp;&nbsp;&nbsp; <em>\"The most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents.\"</em></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - H.P. Lovecraft, <span style=\"text-decoration: underline;\">The Call of Cthulhu</span><br /><br />There is an old yarn about two skeptics who stayed overnight in a supposedly haunted mansion, just to prove they weren't superstitious. At first, they laughed and joked with each other in the well-lit master bedroom. But around eleven, there was a thunderstorm - hardly a rare event in those parts - and all the lights went off. As it got later and later, the skeptics grew more and more nervous, until finally around midnight, the stairs leading up to their room started to creak. The two of them shot out of there and didn't stop running until they were in their car and driving away. <br /><br />So the skeptics' emotions overwhelmed their rationality. That happens all the time. Is there any reason to think this story proves anything more interesting than that some skeptics are cowards?</p>\n<p><a id=\"more\"></a></p>\n<p>The Koreans have a superstition called \"<a href=\"http://en.wikipedia.org/wiki/Fan_death\">fan death</a>\": if you sleep in a closed room with a fan on all night, you will die. Something about the fan blades shredding the oxygen molecules or something. It all sounds pretty far-fetched, but in Korea it's endorsed by everyone from doctors to the government's official consumer safety board.<br /><br />I don't believe in ghosts, and I don't believe in fan death. But my reactions to spending the night in a haunted mansion and spending the night with a fan are completely different. Put me in a haunted mansion, and I'll probably run out screaming the first time something goes bump in the night<sup>1</sup>. Put me in a closed room with a fan and I'll shrug and sleep like a baby. Not because my superior rationality has conquered my fear. Because fans just plain don't kill people by chopping up oxygen, and to think otherwise is just <em>stupid</em>.</p>\n<p>So although it's correct to say that the skeptics' emotions overwhelmed their rationality, they wouldn't have those emotions unless they thought on <em>some</em> level that ghosts were worth getting scared about.<br /><br />A psychologist armed with the theory of belief-profession versus <a href=\"http://www.overcomingbias.com/2007/07/making-beliefs-.html\">anticipation-control</a> would conclude that I profess disbelief in ghosts to fit in with my rationalist friends, but that I anticipate being killed by a ghost if I remain in the haunted mansion. He'd dismiss my skepticism about ghosts as exactly the same sort of belief in belief afflicting the man who thinks his dragon is <a href=\"http://www.overcomingbias.com/2007/07/belief-in-belie.html\">permeable to flour</a>.<br /><br />If this psychologist were really interested in investigating my beliefs, he might offer me X dollars to stay in the haunted mansion. This is all a thought experiment, so I can't say for certain what I would do. But when I imagine the scenario, I visualize myself still running away when X = 10, but fighting my fear and staying around when X = 1000000.<br /><br />This looks suspiciously like I'm making an expected utility calculation. Probability of being killed by ghost * value of my life, compared to a million dollars. It also looks like I'm using a rather high number for (probability of being killed by ghost): certainly still less than .5, but much greater than the &lt;.001 I would consciously assign it. Is my mind haunted by an invisible probability of ghosts, ready to jump out and terrify me into making irrational decisions?<br /><br />How can I defend myself against the psychologist's accusation that I merely profess a disbelief in ghosts? Well, while I am running in terror out of the mansion, a bookie runs up beside me. He offers me a bet: he will go back in and check to see if there is a ghost. If there isn't, he owes me $100. If there is, I owe him $10,000 (payable to his next of kin). Do I take the bet? <br /><br />Thought experiments don't always work, but I imagine myself taking the bet. I assign a less than 1/100 chance to the existence of ghosts, so it's probably a good deal. The fact that I am running away from a ghost as I do the calculation changes the odds not at all.<br /><br />But if that's true, we're now up to three different levels of belief. The one I profess to my friends, the one that controls my anticipation, and the one that influences my emotions.<br /><br />There are no ghosts, profess skepticism.<br />There are no ghosts, take the bet.<br />There are ghosts, run for your life!</p>\n<p><strong>Footnote</strong></p>\n<p>1: I worry when writing this that I may be alone among Less Wrong community members, and that the rest of the community would remain in the mansion with minimal discomfort. If \"run screaming out of the mansion\" is too dramatic for you, will you agree that you might, after the floorboards get especially creaky, feel a tiny urge to light a candle or turn on a flashlight? Even that is enough to preserve the point I am trying to make here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HXA9WxPpzZCCEwXHT": 12, "Ng8Gice9KNkncxqcj": 2, "KWFhr6A2dHEb6wmWJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mja6jZ6k9gAwki9Nu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 89, "baseScore": 92, "extendedScore": null, "score": 0.000139, "legacy": true, "legacyId": "57", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 92, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 69, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["P3uavjFmZD5RopJKk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-09T08:03:20.329Z", "modifiedAt": null, "url": null, "title": "Don't Believe You'll Self-Deceive", "slug": "don-t-believe-you-ll-self-deceive", "viewCount": null, "lastCommentedAt": "2022-05-12T16:47:33.312Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W7LcN9gmdnaAk9K52/don-t-believe-you-ll-self-deceive", "pageUrlRelative": "/posts/W7LcN9gmdnaAk9K52/don-t-believe-you-ll-self-deceive", "linkUrl": "https://www.lesswrong.com/posts/W7LcN9gmdnaAk9K52/don-t-believe-you-ll-self-deceive", "postedAtFormatted": "Monday, March 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Don't%20Believe%20You'll%20Self-Deceive&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADon't%20Believe%20You'll%20Self-Deceive%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW7LcN9gmdnaAk9K52%2Fdon-t-believe-you-ll-self-deceive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Don't%20Believe%20You'll%20Self-Deceive%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW7LcN9gmdnaAk9K52%2Fdon-t-believe-you-ll-self-deceive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW7LcN9gmdnaAk9K52%2Fdon-t-believe-you-ll-self-deceive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 592, "htmlBody": "<p>I don't mean to seem like I'm picking on Kurige, but I think you have to expect a certain amount of questioning if you show up on Less Wrong and <a href=\"/lw/r/no_really_ive_deceived_myself/#gk\">say</a>:</p>\n<blockquote>\n<p>One thing I've come to realize that helps to explain the disparity I feel when I talk with most other Christians is the fact that somewhere along the way my world-view took a major shift away from blind faith and landed somewhere in the vicinity of Orwellian double-think.</p>\n</blockquote>\n<p>\"If you <em>know</em> it's <a href=\"http://www.overcomingbias.com/2007/09/doublethink-cho.html\">double-think</a>...</p>\n<p>...how can you still <em>believe </em>it?\" I helplessly want to say.</p>\n<p><a href=\"/lw/1f/moores_paradox/#u3\">Or</a>:</p>\n<blockquote>\n<p>I chose to believe in the existence of God&mdash;deliberately and consciously. This decision, however, has absolutely zero effect on the actual existence of God.</p>\n</blockquote>\n<p>If you <em>know </em>your belief isn't correlated to reality, how can you still believe it?</p>\n<p>Shouldn't the <em>gut-level</em> realization, \"Oh, wait, the sky really <em>isn't</em> green\" follow from the realization \"My map that says 'the sky is green' has no reason to be correlated with the territory\"?</p>\n<p>Well... apparently not.</p>\n<p>One part of this puzzle may be my explanation of <a href=\"/lw/1f/moores_paradox/\">Moore's Paradox</a> (\"It's raining, but I don't believe it is\")&mdash;that people introspectively mistake positive affect attached to a quoted belief, for actual credulity.</p>\n<p>But another part of it may just be that&mdash;contrary to the indignation I initially wanted to put forward&mdash;it's actually quite <em>easy</em> not to make the jump from \"The map that reflects the territory would say 'X'\" to actually believing \"X\".&nbsp; It takes some work to <em>explain</em> the ideas of <a href=\"http://www.overcomingbias.com/2007/09/what-is-evidenc.html\">minds as map-territory correspondence builders</a>, and even then, it may take more work to get the implications on a <em>gut level</em>.<a id=\"more\"></a></p>\n<p>I realize now that when I <a href=\"/lw/1f/moores_paradox/\">wrote</a> \"You cannot make yourself believe the sky is green by an act of will\", I wasn't just a dispassionate reporter of the existing facts.&nbsp; I was also trying to instill a self-fulfilling prophecy.</p>\n<p>It may be wise to go around deliberately repeating \"I can't get away with double-thinking!&nbsp; Deep down, I'll know it's not true!&nbsp; If I know my map has no reason to be correlated with the territory, that means I don't believe it!\"</p>\n<p>Because that way&mdash;if you're ever tempted to try&mdash;the thoughts \"But I know this isn't really true!\" and \"I can't fool myself!\" will always rise readily to mind; and that way, you will indeed be less likely to fool yourself successfully.&nbsp; You're more likely to get, on a gut level, that telling yourself X doesn't make X true: and therefore, really truly not-X.</p>\n<p>If you keep telling yourself that you <em>can't</em> just deliberately choose to believe the sky is green&mdash;then you're less likely to succeed in fooling yourself on one level or another; either in the sense of really believing it, or of falling into <a href=\"/lw/1f/moores_paradox/\">Moore's Paradox</a>, <a href=\"http://www.overcomingbias.com/2007/07/belief-in-belie.html\">belief in belief</a>, or <a href=\"/lw/s/belief_in_selfdeception/\">belief in self-deception</a>.</p>\n<p>If you keep telling yourself that deep down you'll know&mdash;</p>\n<p>If you keep telling yourself that you'd just look at your elaborately constructed false map, and just know that it was a false map without any expected correlation to the territory, and therefore, despite all its elaborate construction, you wouldn't be able to invest any credulity in it&mdash;</p>\n<p>If you keep telling yourself that reflective consistency will take over and make you stop believing on the object level, once you come to the meta-level realization that the map is not reflecting&mdash;</p>\n<p>Then when push comes to shove&mdash;you may, indeed, fail.</p>\n<p>When it comes to deliberate self-deception, you must <em>believe in your own inability!</em></p>\n<p>Tell yourself the effort is doomed&mdash;<em>and it will be!</em></p>\n<p>Is that the power of positive thinking, or the power of negative thinking?&nbsp; Either way, it seems like a wise precaution.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "YTCrHWYHAsAD74EHo": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W7LcN9gmdnaAk9K52", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": 27, "extendedScore": null, "score": 4.1e-05, "legacy": true, "legacyId": "60", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": "Rationality: A-Z", "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "qqFS6Kw5fmPyzkLby", "canonicalCollectionSlug": "rationality", "canonicalBookId": "coGq3LC5Yn4vZiu6k", "canonicalNextPostSlug": "anchoring-and-adjustment", "canonicalPrevPostSlug": "moore-s-paradox", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 70, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ERRk4thxxYNcScqR4", "wP2ymm44kZZwaFPYh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-09T12:52:41.695Z", "modifiedAt": null, "url": null, "title": "The Wrath of Kahneman", "slug": "the-wrath-of-kahneman", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:02.737Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "steven0461", "createdAt": "2009-02-27T16:16:38.980Z", "isAdmin": false, "displayName": "steven0461"}, "userId": "cn4SiEmqWbu7K9em5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YGPzzqqpYcAoyzF4d/the-wrath-of-kahneman", "pageUrlRelative": "/posts/YGPzzqqpYcAoyzF4d/the-wrath-of-kahneman", "linkUrl": "https://www.lesswrong.com/posts/YGPzzqqpYcAoyzF4d/the-wrath-of-kahneman", "postedAtFormatted": "Monday, March 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Wrath%20of%20Kahneman&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Wrath%20of%20Kahneman%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYGPzzqqpYcAoyzF4d%2Fthe-wrath-of-kahneman%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Wrath%20of%20Kahneman%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYGPzzqqpYcAoyzF4d%2Fthe-wrath-of-kahneman", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYGPzzqqpYcAoyzF4d%2Fthe-wrath-of-kahneman", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 216, "htmlBody": "<p>Cass Sunstein, David Schkade, and Daniel Kahneman, in a 1999 paper named <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=166168\">Do People Want Optimal Deterrence</a>, write:</p>\n<blockquote>\n<p>Previous research suggests that people&rsquo;s judgments about punitive damage awards are a reflection of outrage at the defendant&rsquo;s actions rather than of deterrence. This is not to say that people do not care about deterrence; of course they do. Our hypothesis here is that they do not attempt to promote optimal deterrence; for this reason they do not make the kinds of distinctions that are obvious, even secondnature, for those who study deterrence questions. Above all, they may not believe that in order to ensure optimal deterrence, the amount that a given defendant is required to pay should be increased or decreased depending on the probability of detection, a central claim in the economic analysis of law.</p>\n</blockquote>\n<p>If we're after <a href=\"http://users.ugent.be/~gdegeest/3700book.pdf\">optimal deterrence</a>, we should punish potentially harmful actions more if they're hard to detect, or else the expected disutility of the punishment is too small. But apparently this does not accord with people's sense of justice.</p>\n<p>Does this mean we should change our sense of justice? And should we apply optimal deterrence theory to informal social rewards and punishments, such as by getting angrier at antisocial behaviors that we learned of by (what the wrongdoer thought was) a freak coincidence?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YGPzzqqpYcAoyzF4d", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 29, "extendedScore": null, "score": 4.6e-05, "legacy": true, "legacyId": "61", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-09T17:35:42.390Z", "modifiedAt": null, "url": null, "title": "The Mistake Script", "slug": "the-mistake-script", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:02.215Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GDKX7S9mqL46qWct2/the-mistake-script", "pageUrlRelative": "/posts/GDKX7S9mqL46qWct2/the-mistake-script", "linkUrl": "https://www.lesswrong.com/posts/GDKX7S9mqL46qWct2/the-mistake-script", "postedAtFormatted": "Monday, March 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Mistake%20Script&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Mistake%20Script%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGDKX7S9mqL46qWct2%2Fthe-mistake-script%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Mistake%20Script%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGDKX7S9mqL46qWct2%2Fthe-mistake-script", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGDKX7S9mqL46qWct2%2Fthe-mistake-script", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 848, "htmlBody": "<p>Here on Less Wrong, we have hopefully developed our ability to spot mistaken arguments. Suppose you're reading an article and you encounter a fallacy. What do you do? Consider the following script:</p>\n<ol>\n<li>Reread the argument to determine whether it's really an error. (If not, resume reading.)</li>\n<li>Verify that the error is relevant to the point of the article. (If not, resume reading.)</li>\n<li>Decide whether the remainder of the article is worth reading despite the error. Resume reading or don't.</li>\n</ol>\n<p>This script seems intuitively correct, and many people follow a close approximation of it. However, following this script is very bad, because the judgement in step (3) is tainted: you are more likely to continue reading the article if you agree with its conclusion than if you don't. If you disagreed with the article, then you were also more likely to have spotted the mistake in the first place. These two biases can cause you to unknowingly avoid reading anything you disagree with, which makes you strongly resist changing your beliefs. Long articles almost always include some bad arguments, even when their conclusion is correct. We can greatly improve this script with an explicit countermeasure:</p>\n<ol>\n<li>Reread the argument to determine whether it's really an error. (If not, resume reading.)</li>\n<li>Verify that the error is relevant to the point of the article. (If not, resume reading.)</li>\n<li><strong>Decide whether you agree with the article's conclusion. If you are sure you do, stop reading. If you aren't sure what the conclusion is or aren't sure you agree with it, continue.<br /></strong></li>\n<li>Decide whether the remainder of the article is worth reading despite the error. Resume reading or don't.</li>\n</ol>\n<p>This extra step protects us from confirmation bias and the \"echo chamber\" effect. We might try adding more steps, to reduce bias even further:<a id=\"more\"></a></p>\n<ol>\n<li>Reread the argument to determine whether it's really an error. (If not, resume reading.)</li>\n<li>Verify that the error is relevant to the point of the article. (If not, resume reading.)</li>\n<li><strong>Attempt to generate other arguments which could substitute for the faulty one. If you produce a valid one, resume reading.</strong> </li>\n<li>Decide whether you agree with the article's conclusion. (If you are sure you do, stop reading.  If you aren't sure what the conclusion is or aren't sure you agree with it, continue.)</li>\n<li>Decide whether the remainder of the article is worth reading despite the error. Resume reading or don't.</li>\n</ol>\n<p>While seemingly valid, this extra step would be bad, because the associated cost is too high. Generating arguments takes much more time and mental effort than evaluating someone else's, so you will always be tempted to skip this step. If you were to use any means to force yourself to include it when you invoke the script, then you would instead bias yourself against invoking the script in the first place, and let errors slide.</p>\n<p>Finding an error in someone else's argument shouldn't cause you to spend much time. Dealing with a mistake in an argument you wrote yourself, on the other hand, is more involved. Suppose you catch yourself writing, saying, or thinking an argument that you know is invalid. What do you do about it? Here is my script:</p>\n<ol> </ol> <ol>\n<li>If you caught the problem immediately when you first generated the bad argument, things are working as they should, so skip this script.</li>\n<li>Check your emotional reaction to the conclusion of the bad argument. If you want it to be true, then you have caught yourself rationalizing. Run a script for that.</li>\n<li>Give yourself an imaginary gold star for having recognized your mistake. If you feel bad about having made the mistake in the first place, give yourself enough additional gold stars to counter this feeling.</li>\n<li>Name the heuristic or fallacy you used (Surface similarity, overgeneralization, ad hominem, non sequitur, etc.)</li>\n<li>Estimate how often the named heuristic or fallacy has lead you astray. If the answer is more often than you think is acceptable, note it, so you can think about how to counter that bias later.</li>\n<li>Generate other conclusions which you have used this same argument to support in the past, if any. Note them, to reevaluate later.</li>\n</ol>\n<p>A good script provides a checklist of things to think about, plus guidance on how long to think about each, and what state to be in while doing so. When evaluating our own mistakes, emotional state is important; if acknowledging that we've made a mistake causes us to feel bad, then we simply won't acknowledge our mistakes, hence step (3) in this procedure.</p>\n<p>Thinking accurately is more complicated than just following scripts, but script-following is a major part of how the mind works. If left alone, the mind will generate its own scripts for common occurances, but they probably won't be optimal. The scripts we use for error handling filter the information we receive and regulate all other beliefs; they are too important to leave to chance. What other anti-bias countermeasures could we add? What other scripts do we follow that could be improved?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "ZzxvopS4BwLuQy42n": 1, "wzgcQCrwKfETcBpR9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GDKX7S9mqL46qWct2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 12, "extendedScore": null, "score": 4.802297202549616e-07, "legacy": true, "legacyId": "52", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-09T19:18:44.923Z", "modifiedAt": "2020-05-09T01:52:51.651Z", "url": null, "title": "LessWrong anti-kibitzer (hides comment authors and vote counts)", "slug": "lesswrong-anti-kibitzer-hides-comment-authors-and-vote", "viewCount": null, "lastCommentedAt": "2020-05-09T01:45:34.412Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Marcello", "createdAt": "2009-02-27T04:18:25.253Z", "isAdmin": false, "displayName": "Marcello"}, "userId": "dLzgizPS3R7upB8YZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XbfdLQrAWTRfpggRM/lesswrong-anti-kibitzer-hides-comment-authors-and-vote", "pageUrlRelative": "/posts/XbfdLQrAWTRfpggRM/lesswrong-anti-kibitzer-hides-comment-authors-and-vote", "linkUrl": "https://www.lesswrong.com/posts/XbfdLQrAWTRfpggRM/lesswrong-anti-kibitzer-hides-comment-authors-and-vote", "postedAtFormatted": "Monday, March 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20anti-kibitzer%20(hides%20comment%20authors%20and%20vote%20counts)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20anti-kibitzer%20(hides%20comment%20authors%20and%20vote%20counts)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXbfdLQrAWTRfpggRM%2Flesswrong-anti-kibitzer-hides-comment-authors-and-vote%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20anti-kibitzer%20(hides%20comment%20authors%20and%20vote%20counts)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXbfdLQrAWTRfpggRM%2Flesswrong-anti-kibitzer-hides-comment-authors-and-vote", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXbfdLQrAWTRfpggRM%2Flesswrong-anti-kibitzer-hides-comment-authors-and-vote", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 280, "htmlBody": "<p><strong>Related to</strong> <a href=\"/lw/z/information_cascades/\">Information Cascades</a></p><p><a href=\"/lw/z/information_cascades/\">Information Cascades</a> has implied that people&apos;s votes are being biased by the number of votes already cast.  Similarly, some commenters express a <a href=\"/lw/1d/simultaneously_right_and_wrong/v6\">perception that higher status posters are being upvoted too much.</a></p><p>EDIT: the UserScript below no longer works because it is a very old version of the site.  <a href=\"https://github.com/mherreshoff/lw-antikibitzer\">LessWrong v2.0 Anti-Kibitzer</a> is for the new version of the site (working as of May 2020).  It has the added feature that each user is assigned a color and style of censor-bar to represent their username, which makes threaded conversations easier to follow.</p><p><br>If, like me, you suspect that you might be prone to these biases, you can correct for them by installing <a href=\"http://stanford.edu/~marce110/greasemonkey/LWantikibitzer.user.js\">LessWrong anti-kibitzer</a> which I hacked together yesterday morning.  You will need Firefox with the <a href=\"https://addons.mozilla.org/en-US/firefox/addon/748\">greasemonkey extention</a> installed.  Once you have greasemonkey installed, clicking on the link to the script will pop up a dialog box asking if you want to enable the script.  Once you enable it, a button which you can use to toggle the visibility of author and point count information should appear in the upper right corner of any page on LessWrong.  (On any page you load, the authors and pointcounts are automatically hidden until you show them.)  Let me know if it doesn&apos;t work for any of you.<br><br>Already, I&apos;ve had some interesting experiences.  There were a few comments that I thought were written by Eliezer that turned out not to be (<a href=\"/lw/1d/simultaneously_right_and_wrong/sr\">though perhaps people are copying his writing style</a>.)  There were also comments that I thought contained good arguments which were written by people I was apparently too quick to dismiss as trolls.  What are your experiences?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sHbKQDqrSinRPcnBv": 2, "MfpEPj6kJneT9gWT6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XbfdLQrAWTRfpggRM", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 68, "baseScore": 67, "extendedScore": null, "score": 0.000101, "legacy": true, "legacyId": "64", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": "2019-08-18T21:30:12.356Z", "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": true, "hideFrontpageComments": false, "maxBaseScore": 67, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["DNQw596nPCX4x7xT9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2009-03-09T19:18:44.923Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-09T23:18:35.876Z", "modifiedAt": null, "url": null, "title": "You May Already Be A Sinner", "slug": "you-may-already-be-a-sinner", "viewCount": null, "lastCommentedAt": "2017-06-17T04:37:03.843Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Cq45AuedYnzekp3LX/you-may-already-be-a-sinner", "pageUrlRelative": "/posts/Cq45AuedYnzekp3LX/you-may-already-be-a-sinner", "linkUrl": "https://www.lesswrong.com/posts/Cq45AuedYnzekp3LX/you-may-already-be-a-sinner", "postedAtFormatted": "Monday, March 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20You%20May%20Already%20Be%20A%20Sinner&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYou%20May%20Already%20Be%20A%20Sinner%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCq45AuedYnzekp3LX%2Fyou-may-already-be-a-sinner%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=You%20May%20Already%20Be%20A%20Sinner%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCq45AuedYnzekp3LX%2Fyou-may-already-be-a-sinner", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCq45AuedYnzekp3LX%2Fyou-may-already-be-a-sinner", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1027, "htmlBody": "<p><strong>Followup to: </strong><a href=\"/lw/1d/simultaneously_right_and_wrong/\">Simultaneously Right and Wrong</a></p>\n<p><strong>Related to: </strong><a href=\"http://www.overcomingbias.com/2009/03/augustines-paradox.html\">Augustine's Paradox of Optimal Repentance</a><em></em></p>\n<p style=\"padding-left: 30px;\"><em>\"When they inquire into predestination, they are penetrating the sacred precincts of divine wisdom. If anyone with carefree </em><em>assurance breaks into this place, he will not succeed in satisfying his curiosity and he will enter a labyrinth from which he </em><em>can find no exit.\"</em></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- John Calvin<br /><br />John Calvin preached the doctrine of predestination: that God irreversibly decreed each man's eternal fate at the moment of Creation. Calvinists separate mankind into two groups: the elect, whom God predestined for Heaven, and the reprobate, whom God predestined for eternal punishment in Hell.<br /><br />If you had the bad luck to be born a sinner, there is nothing you can do about it. You are too corrupted by original sin to even have the slightest urge to seek out the true faith. Conversely, if you were born one of the elect, you've got it pretty good; no matter what your actions on Earth, it is impossible for God to revoke your birthright to eternal bliss.<br /><br />However, it is believed that the elect always live pious, virtuous lives full of faith and hard work. Also, the reprobate always commit heinous sins like greed and sloth and commenting on anti-theist blogs. This isn't what causes God to damn them. It's just what happens to them after they've been damned: their soul has no connection with God and so it tends in the opposite direction.<br /><br />Consider two Calvinists, Aaron and Zachary, both interested only in maximizing his own happiness. Aaron thinks to himself \"Whether or not I go to Heaven has already been decided, regardless of my actions on Earth. Therefore, I might as well try to have as much fun as possible, knowing it won't effect the afterlife either way.\" He spends his days in sex, debauchery, and anti-theist blog comments.<br /><br />Zachary sees Aaron and thinks \"That sinful man is thus proven one of the reprobate, and damned to Hell. I will avoid his fate by living a pious life.\" Zachary becomes a great minister, famous for his virtue, and when he dies his entire congregation concludes he must have been one of the elect.<br /><br />Before the cut: If you were a Calvinist, which path would you take?<a id=\"more\"></a></p>\n<p>Amos Tversky, Stanford psychology professor by day, <a href=\"http://en.wikipedia.org/wiki/Amos_Tversky#Notable_contributions\">bias-fighting superhero by night</a>, thinks you should live a life of sin. He bases <a href=\"http://books.google.ie/books?id=OCXAgA3sigIC&amp;pg=PA827&amp;lpg=PA827&amp;dq=Tversky+calvinism&amp;source=bl&amp;ots=P7pBg_AOrl&amp;sig=xLRMx6P2rsMe3cWELhNR6j1Dfb4&amp;hl=en&amp;ei=Hpu1Se_7CuLBjAeApeCvCQ&amp;sa=X&amp;oi=book_result&amp;resnum=1&amp;ct=result\">his analysis</a> of the issue on the famous maxim that <a href=\"http://www.xkcd.com/552/\">correlation is not causation</a>. Your virtue during life is correlated to your eternal reward, but only because they're both correlated to a hidden <a href=\"http://en.wikipedia.org/wiki/Confounding_variable\">third variable</a>, your status as one of the elect, which causes both.<br /><br />Just to make that more concrete: people who own more cars live longer. Why? Rich people buy more cars, and rich people have higher life expectancies. Both cars and long life are caused by a hidden third variable, wealth. Trying to increase your chances of getting into Heaven by being virtuous is as futile as trying to increase your life expectancy by buying another car.<br /><br />Some people would stop there, but not Amos Tversky, bias-fighting superhero. He and George Quattrone conducted <a href=\"http://cat.inist.fr/?aModele=afficheN&amp;cpsidt=9728808\">a study</a> that both illuminated a flaw in human reasoning about causation and demonstrated yet another way people can be simultaneously right and wrong.<br /><br />Subjects came in thinking it was a study on cardiovascular health. First, experimenters tested their pain tolerance by making them stick their hands in a bucket of freezing water until they couldn't bear it any longer. However long they kept it there was their baseline pain tolerance score.<br /><br />Then experimenters described two supposed types of human heart: Type I hearts, which work poorly and are prone to heart attack and will kill you at a young age, and Type II hearts, which work well and will bless you with a long life. You can tell a Type I heart from a Type II heart because...and here the subjects split into two groups. Group A learned that people with Type II hearts, the good hearts, had higher pain tolerance after exercise. Group B learned that Type II hearts had lower pain tolerance after exercise.<br /><br />Then the subjects exercised for a while and stuck their hands in the bucket of ice water again. Sure enough, the subjects who thought increased pain tolerance meant a healthier heart kept their hands in longer. And then when the researchers went and asked them, they said they must have a Type II heart because the ice water test went so well!<br /><br />The subjects seem to have believed <em>on some level</em> that keeping their hand in the water longer could give them a different kind of heart. Dr. Tversky declared that people have a cognitive blind spot to \"hidden variable\" causation, and this explains the Calvinists who made such an effort to live virtuously.<br /><br />But this study is also interesting as an example of self-deception. One level of the mind made the (irrational) choice to leave the hand in the ice water longer. Another level of the mind that wasn't consciously aware of this choice interpreted it as evidence for the Type II heart. There are two cognitive flaws here: the subject's choice to try harder on the ice water test, and his lack of realization that he'd done so.<br /><br />I don't know of any literature explicitly connecting this study to <a href=\"/lw/1d/simultaneously_right_and_wrong/\">self-handicapping</a>, but the surface similarities are striking. In both, a person takes an action intended to protect his self-image that will work if and only if he doesn't realize this intent. In both, the action is apparently successful, self-image is protected, and the conscious mind remains unaware of the true motives.<br /><br />Despite all this, and with all due respect to Dr. Tversky I think he might be wrong about the whole predestination issue. If I were a Calvinist, I'd live a life of sin if and only if I would two-box on <a href=\"http://www.overcomingbias.com/2008/01/newcombs-proble.html\">Newcomb's Problem</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1, "fihKHQuS5WZBJgkRm": 2, "NSMKfa8emSbGNXRKD": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Cq45AuedYnzekp3LX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 50, "extendedScore": null, "score": 8.1e-05, "legacy": true, "legacyId": "66", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 50, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["P3uavjFmZD5RopJKk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-09T23:29:46.758Z", "modifiedAt": null, "url": null, "title": "Striving to Accept", "slug": "striving-to-accept", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:09.305Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Cxcormwz6jb98gGzW/striving-to-accept", "pageUrlRelative": "/posts/Cxcormwz6jb98gGzW/striving-to-accept", "linkUrl": "https://www.lesswrong.com/posts/Cxcormwz6jb98gGzW/striving-to-accept", "postedAtFormatted": "Monday, March 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Striving%20to%20Accept&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStriving%20to%20Accept%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCxcormwz6jb98gGzW%2Fstriving-to-accept%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Striving%20to%20Accept%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCxcormwz6jb98gGzW%2Fstriving-to-accept", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCxcormwz6jb98gGzW%2Fstriving-to-accept", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1260, "htmlBody": "<p><strong>Reply to</strong>:&nbsp; <a href=\"/lw/1l/the_mystery_of_the_haunted_rationalist/\">The Mystery of the Haunted Rationalist</a><br /><strong>Followup to</strong>:&nbsp; <a href=\"/lw/1o/dont_believe_youll_selfdeceive/\">Don't Believe You'll Self-Deceive</a></p>\n<p>Should a rationalist ever find themselves trying hard to believe something?</p>\n<p>You may be tempted to answer \"No\", because \"trying to believe\" sounds so stereotypical of <a href=\"http://www.overcomingbias.com/2008/10/the-dark-side.html\">Dark Side Epistemology</a>.&nbsp; You may be tempted to reply, \"Surely, if you have to try hard to believe something, it isn't worth believing.\"</p>\n<p>But Yvain <a href=\"/lw/1l/the_mystery_of_the_haunted_rationalist/\">tells</a> us that - even though he knows damn well, on one level, that spirits and other supernatural things are not to be found in the causal closure we name \"reality\" - and even though he'd bet $100 against $10,000 that an examination would find no spirits in a haunted house - he's pretty sure he's still scared of haunted houses.</p>\n<p>Maybe it's okay for Yvain to try a little harder to <em>accept </em>that there are no ghosts, since he already <em>knows </em>that there are no ghosts?</p>\n<p>In my <em>very </em>early childhood I was lucky enough to read a book from the children's section of a branch library, called \"The Mystery of Something Hill\" or something.&nbsp; In which one of the characters says, roughly:&nbsp; \"There are two ways to believe in ghosts.&nbsp; One way is to fully believe in ghosts, to look for them and talk about them.&nbsp; But the other way is to <em>half-believe</em> - to make fun of the idea of ghosts, and talk scornfully of ghosts; but to break into a cold sweat when you hear a bump in the night, or be afraid to enter a graveyard.\"<a id=\"more\"></a></p>\n<p>I wish I remembered the book's name, or the exact quote, because this was one of those statements that sinks in during childhood and remains a part of you for the rest of the life.&nbsp; But all I remember was that the solution to the mystery had to do with hoofbeats echoing from a nearby road.</p>\n<p>So whenever I found something that I knew I shouldn't believe, I also tried to avoid <em>half-believing</em>; and I soon noticed that this was the harder part of the problem.&nbsp; In my childhood, I cured myself of the fear of the dark by thinking:&nbsp; <em>If I'm going to think magically anyway, then I'll pretend that all the black and shapeless, dark and shadowy things are my friends.</em>&nbsp; Not <em>quite </em>the way I would do it nowadays, but it worked to counteract the half-belief, and in not much time I wasn't thinking about it at all.</p>\n<p>Considerably later in my life, I realized that I was having a problem with half-believing in magical thinking - that I would sometimes try to avoid visualizing unpleasant things, from half-fear that they would happen.&nbsp; If, before walking through a door, I visualized that a maniac had chosen that exact moment to sneak into the room on the other side, and was armed and waiting with a knife - then I would be that little bit more scared, and look around more nervously, when entering the room.</p>\n<p>So - being, at this point, a bit more sophisticated - I visualized a spread of probable worlds, in which - in some tiny fraction - a knife-wielding maniac had indeed chosen that moment to lurk behind the door; and I visualized the fact that my visualizing the knife-wielding maniac did not make him the tiniest bit more likely to be there - did not increase the total number of maniacs across the worlds.&nbsp; And that did cure me, and it was done; along with a good deal of other half-superstitions of the same pattern, like not thinking too loudly about other people in case they heard me.</p>\n<p><em>Enforcing reflectivity</em> - making ourselves accept what we already know - is, in general, an ongoing challenge for rationalists.&nbsp; I cite the example above because it's a very direct illustration of the genre:&nbsp; I actually went so far as to visualize the (non-)correlation of map to territory across possible worlds, in order to get my object-level map to realize that the maniac really really <em>wasn't</em> there.</p>\n<p>It wouldn't be unusual for a rationalist to find themselves struggling to rid themselves of attachment to an unwanted belief.&nbsp; If we can get out of sync in that direction, why not the other direction?&nbsp; If it's okay to make ourselves try to disbelieve, why not make ourselves try to believe?</p>\n<p>Well, because it really <em>is</em> one of the classic warning signs of Dark Side Epistemology that you have to struggle to make yourself believe something.</p>\n<p>So let us then delimit, and draw sharp boundaries around the particular and rationalist version of striving for acceptance, as follows:</p>\n<p>First, you should only find yourself doing this when you find yourself thinking, \"Wait a minute, that really is actually true - why can't I get my mind to accept that?\"&nbsp; Not Gloriously and Everlastingly True, mind you, but plain old mundanely true.&nbsp; This will be gameable in verbal arguments between people - \"Wait, but I do believe it's really actually true!\" - but if you're honestly trying, you should be able to tell the difference <em>internally</em>.&nbsp; If you can't find that feeling of frustration at your own inability to accept the <em>obvious,</em> then you should back up and ask whether or not it really is obvious, before trying to <em>make</em> your mind do anything.&nbsp; Can the fool say, \"But I do think it's completely true and obvious\" about random silly beliefs?&nbsp; Yes, they can.&nbsp; But as for you, just don't do that.&nbsp; This is to be understood as a technique for not shooting off your own foot, not as a way of proving anything to anyone.</p>\n<p>Second, I call it \"striving to accept\", not \"striving to believe\", following <a href=\"/lw/1o/dont_believe_youll_selfdeceive/xc#comments\">billswift's suggestion</a>.&nbsp; Why?&nbsp; Consider the difference between \"<a href=\"/lw/s/belief_in_selfdeception/\">I believe people are nicer than they are</a>\" and \"I accept people are nicer than they are\".&nbsp; You shouldn't be trying to raise desperate enthusiasm for a belief - if it doesn't seem like a plain old reality that you need to accept, then you're using the wrong technique.</p>\n<p>Third and I think most importantly - you should always be striving to accept some <em>particular argument</em> that you feel isn't sinking in.&nbsp; Strive to accept \"X implies Y\", not just \"Y\".&nbsp; Strive to accept that there are no ghosts <em>because </em><a href=\"http://www.overcomingbias.com/2008/04/brain-breakthro.html\">spirits are only made of material neurons</a>, or because <a href=\"http://www.overcomingbias.com/2008/09/excluding-the-s.html\">the supernatural is incoherent</a>.&nbsp; Strive to accept that there's no maniac behind the door because your thoughts don't change reality.&nbsp; Strive to accept that you won't win the lottery because you could make one distinct statement every second for a year with every one of them wrong, and not be so wrong as you would be by saying \"I will win the lottery.\"</p>\n<p>So there is my attempt to draw a line between the Dark Side and the Light Side versions of \"trying to believe\".&nbsp; Of course the Light Side also tends to be aimed a bit more heavily at accepting negative beliefs than positive beliefs, as is seen from the three examples.&nbsp; Trying to think of a positive belief-to-accept was difficult; the best I could come up with offhand was, \"Strive to accept that your personal identity is preserved by diassembly and reassembly, because <a href=\"http://www.overcomingbias.com/2008/06/qm-and-identity.html\">deep down, there just aren't any billiard balls down there</a>.\"&nbsp; And even that, I suspect, is more a negative belief, that identity is <em>not disrupted</em>.</p>\n<p>But to summarize - there should always be some particular argument, that has the feeling of being plain old actually true, and that you are only trying to <em>accept</em>, and are frustrated at your own trouble in accepting.&nbsp; Not a belief that you feel obligated to believe in more strongly and enthusiastically, apart from any particular argument.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XYHzLjwYiqpeqaf4c": 1, "HXA9WxPpzZCCEwXHT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Cxcormwz6jb98gGzW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 40, "extendedScore": null, "score": 6.6e-05, "legacy": true, "legacyId": "63", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mja6jZ6k9gAwki9Nu", "W7LcN9gmdnaAk9K52", "wP2ymm44kZZwaFPYh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-10T13:20:32.081Z", "modifiedAt": null, "url": null, "title": "Software tools for community truth-seeking", "slug": "software-tools-for-community-truth-seeking", "viewCount": null, "lastCommentedAt": "2017-06-17T03:55:53.047Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Johnicholas", "createdAt": "2009-02-27T15:01:52.708Z", "isAdmin": false, "displayName": "Johnicholas"}, "userId": "kBvTXutfPytNtzPyD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MYXrsBzXNPqTNarqQ/software-tools-for-community-truth-seeking", "pageUrlRelative": "/posts/MYXrsBzXNPqTNarqQ/software-tools-for-community-truth-seeking", "linkUrl": "https://www.lesswrong.com/posts/MYXrsBzXNPqTNarqQ/software-tools-for-community-truth-seeking", "postedAtFormatted": "Tuesday, March 10th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Software%20tools%20for%20community%20truth-seeking&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASoftware%20tools%20for%20community%20truth-seeking%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMYXrsBzXNPqTNarqQ%2Fsoftware-tools-for-community-truth-seeking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Software%20tools%20for%20community%20truth-seeking%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMYXrsBzXNPqTNarqQ%2Fsoftware-tools-for-community-truth-seeking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMYXrsBzXNPqTNarqQ%2Fsoftware-tools-for-community-truth-seeking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 30, "htmlBody": "<p><strong>In reply to: </strong><a href=\"/lw/u/the_ethic_of_handwashing_and_community_epistemic/\">Community Epistemic Practice</a></p>\n<p>There are software tools, possibly helpful for community truth-seeking. For example, <a href=\"http://www.truthmapping.com/\">truthmapping.com</a> is described very well <a href=\"http://www.youtube.com/watch?v=T8XgPDs_pHc\">here</a>. Also, <a href=\"http://debategraph.org/\">debategraph.org</a>, and I'm sure there are others.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TkZ7MFwCi4D63LJ5n": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MYXrsBzXNPqTNarqQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 2, "extendedScore": null, "score": 4.803997827768302e-07, "legacy": true, "legacyId": "70", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZP2om2oWHPhvWP2Q3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-11T04:59:40.459Z", "modifiedAt": null, "url": null, "title": "Wanted: Python open source volunteers", "slug": "wanted-python-open-source-volunteers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:27.623Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J2sr4tdC4ThrbJRR3/wanted-python-open-source-volunteers", "pageUrlRelative": "/posts/J2sr4tdC4ThrbJRR3/wanted-python-open-source-volunteers", "linkUrl": "https://www.lesswrong.com/posts/J2sr4tdC4ThrbJRR3/wanted-python-open-source-volunteers", "postedAtFormatted": "Wednesday, March 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Wanted%3A%20Python%20open%20source%20volunteers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWanted%3A%20Python%20open%20source%20volunteers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ2sr4tdC4ThrbJRR3%2Fwanted-python-open-source-volunteers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Wanted%3A%20Python%20open%20source%20volunteers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ2sr4tdC4ThrbJRR3%2Fwanted-python-open-source-volunteers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ2sr4tdC4ThrbJRR3%2Fwanted-python-open-source-volunteers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 401, "htmlBody": "<p>Less Wrong is a fork of the open Reddit codebase, written in Python.&nbsp; <a href=\"http://github.com/tricycle/lesswrong/tree/master\">Less Wrong's code is online at Github</a> (look in r2/r2 for the meat), <a href=\"http://code.google.com/p/lesswrong/issues/list\">the issues tracker is at code.google.com</a>.&nbsp; See <strong><a href=\"http://wiki.github.com/tricycle/lesswrong/contributing-to-less-wrong\">Contributing to Less Wrong</a></strong> for a gentle introduction to getting started.</p>\n<p>According to Reddit's blog, we are the <a href=\"http://blog.reddit.com/2009/03/lesswrong-coolest-use-of-reddit-source.html\">coolest use of reddit source code</a> they've seen!&nbsp; But we've still got a long way to go before we're as cool as we <em>want </em>to be.</p>\n<p>If anyone out there is fluent in Python and willing to donate a noticeable amount of time to a good cause, an extra hand or two might help us implement many Less Wrong features a lot sooner.</p>\n<p>The Reddit codebase does not have unit tests or a whole lot of documentation.&nbsp; Contributors need to be able to wade through Reddit's code to grok it, and write unit tests for what they do (or better yet, write unit tests for existing code).</p>\n<p><a href=\"http://code.google.com/p/lesswrong/issues/list\">Items on the issues tracker</a> marked \"Contributions-Welcome\" are those that look relatively easy to contribute.&nbsp; Items marked \"Contributions-LeaveItToUs\" are those that look big and complicated, or that Tricycle (the main developers) have strong opinions about how to design and implement.&nbsp; You could hack the big ones, but the developers might need to spend time talking to you - so please don't step up unless you're fluent in Python, have the necessary time, and are serious about it.<a id=\"more\"></a></p>\n<p>An example of a Welcome contribution would be having the <a href=\"http://code.google.com/p/lesswrong/issues/detail?id=112\">registration page explain</a> what is a valid username, or making sure that any HTML generated automatically in comments is also <a href=\"http://code.google.com/p/lesswrong/issues/detail?id=84\">legal to enter directly</a> (like &lt;a href&gt;&lt;/a&gt;).</p>\n<p>An example of a LeaveItToUs contribution would be the future <a href=\"/lw/5/issues_bugs_and_requested_features/10a#comments\">Tag and Sequence system</a> - a next/prev tracking system for tags (so that you can navigate through via \"next in self_deception\" / \"prev in self_deception\" arrows); a system that lets authors create sequences which behave like tags but are owned by that author; and RSS feeds for tags and sequences that feed a specified number of posts per day to new users trying to catch up.&nbsp; An experienced volunteer Python dev with enough block hours free to run out and do this would be <em>very</em> welcome.</p>\n<p>Items like making the site <a href=\"http://code.google.com/p/lesswrong/issues/detail?id=99\">fluid-width</a> or <a href=\"http://code.google.com/p/lesswrong/issues/detail?id=7\">importing</a> old posts from Overcoming Bias are probably best left to the current site designers.</p>\n<p>Hopefully this gives you an idea of what kind of work needs doing.&nbsp; See the <a href=\"http://code.google.com/p/lesswrong/issues/list\">issues tracker</a> for more.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J2sr4tdC4ThrbJRR3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 16, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "65", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-11T08:17:12.726Z", "modifiedAt": null, "url": null, "title": "Selective processes bring tag-alongs (but not always!)", "slug": "selective-processes-bring-tag-alongs-but-not-always", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:02.554Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HThGHPeLhe7uy8FEx/selective-processes-bring-tag-alongs-but-not-always", "pageUrlRelative": "/posts/HThGHPeLhe7uy8FEx/selective-processes-bring-tag-alongs-but-not-always", "linkUrl": "https://www.lesswrong.com/posts/HThGHPeLhe7uy8FEx/selective-processes-bring-tag-alongs-but-not-always", "postedAtFormatted": "Wednesday, March 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Selective%20processes%20bring%20tag-alongs%20(but%20not%20always!)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelective%20processes%20bring%20tag-alongs%20(but%20not%20always!)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHThGHPeLhe7uy8FEx%2Fselective-processes-bring-tag-alongs-but-not-always%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Selective%20processes%20bring%20tag-alongs%20(but%20not%20always!)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHThGHPeLhe7uy8FEx%2Fselective-processes-bring-tag-alongs-but-not-always", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHThGHPeLhe7uy8FEx%2Fselective-processes-bring-tag-alongs-but-not-always", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1616, "htmlBody": "<p>by Anna Salamon and Steve Rayhawk (joint authorship)</p>\n<p><strong>Related to:</strong> <a href=\"http://www.overcomingbias.com/2007/11/conjuring-an-ev.html\">Conjuring An Evolution To Serve You</a>, <a href=\"http://www.overcomingbias.com/2008/02/disguised-queri.html\">Disguised Queries</a>&nbsp;</p>\n<p>Let&rsquo;s say you have a bucket full of &ldquo;instances&rdquo; (e.g., genes, hypotheses, students, foods), and you want to choose a good one.&nbsp; You fish around in the bucket, draw out the first 10 instances you find, and pick the instance that scores highest on some selection criterion.</p>\n<p>For example, perhaps your selection criterion is &ldquo;number of polka dots&rdquo;, and you reach into the bucket pictured below, and you draw out 10 instances.&nbsp; What do you get?&nbsp; Assuming some instances have more polka dots than others, you get hypotheses with an above average number of expected polka dots.&nbsp; The point I want to dwell on, though -- which is obvious when you think about it, but which sheds significant light on everyday phenomena -- is that you don&rsquo;t get instances that are <em>just</em> high in polka dots.&nbsp; You get instances are <em>also </em>high in every trait that <em>correlates</em> with having the most polka dots.</p>\n<p><img src=\"http://images.lesswrong.com/t3_22_3.png?v=12e887ce3145479b2126ec9e1449f1e2\" alt=\"\" width=\"200\" height=\"260\" /></p>\n<p>For example, in the bucket above, selecting for instances that have many polka dots implies inadvertently selecting for instances that are red.&nbsp; Selective processes bring tag-alongs, and the specific tag-alongs that you get (redness, in this case) depend on both the trait you&rsquo;re selecting for, and the bucket from which you&rsquo;re selecting.</p>\n<p>Nearly all cases of useful selection (e.g., evolution, science) would be unable to produce the cool properties they produce (complex order in organisms, truth in theories) if they didn&rsquo;t have particular, selection-friendly types of buckets, in addition to good selection criteria.&nbsp; Zoom in carefully enough, and nearly <em>all </em>of the traits one gets by selection can be considered tag-alongs.<em>&nbsp; </em>Conversely, if you are consciously selecting entities from buckets with a particular aim in view, you may want to consciously safeguard the &ldquo;selection-friendliness&rdquo; of the buckets you are using.<a id=\"more\"></a></p>\n<p>Some examples:</p>\n<h3>Algebra test:</h3>\n<p>Let&rsquo;s say you&rsquo;re trying to select for students who know algebra.&nbsp; So you write out an algebra exam, E, find ten students at random from your pool, and see which one does best on exam E.&nbsp; For <em>many</em> sorts of buckets of students, this procedure should work: selecting for the criterion &ldquo;does well on algebra exam E&rdquo; will give you more than just that criterion.&nbsp; It&rsquo;ll give you the tag-along property &ldquo;does well on other algebra problems&rdquo; or &ldquo;understands algebra&rdquo;.</p>\n<p>But not for all buckets.&nbsp; If, for example, you release the test questions ahead of time, you&rsquo;re liable to end up with a bucket of students for which the tag-along property which &ldquo;does well on algebra exam E&rdquo; gives you is only &ldquo;memorized the answers to exam E&rdquo;, and not &ldquo;understands algebra&rdquo;.</p>\n<h3>Taste and nutrition:</h3>\n<p>Or, again, perhaps you&rsquo;re designing a test for healthy foods.&nbsp; In a hunter-gatherer environment, human tastes are perhaps not a bad indicator.&nbsp; Grab 10 foodstuffs at random from your bucket, select for &ldquo;best tasting&rdquo;, and you&rsquo;re liable to get &ldquo;above average nutrition&rdquo; as a tag-along.</p>\n<p>But for the buckets of food-choices created by modern manufacturing (now that we know chemistry, and we can create compounds on purpose that trigger&nbsp;<em>just those sensory mechanisms that signal &ldquo;good taste&rdquo;</em>), selecting for taste no longer selects for nutrition (at least, not as much).</p>\n<h3>Defensive mimicry:</h3>\n<p>Selecting against prey items that have warning coloration (particular patterns of bright color, as on poison arrow frogs) will select against poisonous prey items, among some buckets of possible prey.&nbsp; But as other species evolve to mimic the warning coloration pattern, the bucket of prey items changes, and &ldquo;poisonous&rdquo; becomes less tied to the indicator trait &ldquo;such-and-such a coloration pattern&rdquo;.</p>\n<h3>Choosing coins that come up heads:</h3>\n<p>If you have a bucket of fair coins, and you flip 10 of them several times and choose the one that come up heads the most, this one will be no likelier than average to come up heads in later rounds.&nbsp; If you have a bucket where half the coins are two-headed and the other half are two-tailed, then selecting for even a single head is enough to give you a guarantee of heads on every future coin-toss.&nbsp; And if you have a bucket mixed fair and unfair coins, then selecting for heads helps, but more weakly.</p>\n<p>Nassim Taleb argues that the financial success of particular traders is similar to the success of particular coins from the fair coins bucket -- selecting for &ldquo;traders who were successful in the past&rdquo; doesn&rsquo;t give you traders who are particularly likely to be successful in the future.&nbsp; Most traders who obtain above-market returns in a particular timespan are, on Taleb&rsquo;s analysis, traders with ordinary judgment who put their money on a short string of lucky strategies (&ldquo;buy US real estate&rdquo;).&nbsp; The bucket of traders doesn&rsquo;t have the type of structure that allows &ldquo;future success&rdquo; to be predicted from &ldquo;past success&rdquo;.</p>\n<p>In the &ldquo;<a href=\"http://www.investorhome.com/scam.htm\">Perfect Prediction Scam</a>&rdquo;, crooks send out all possible prediction-sequences for a small number of e.g. sports outcomes, with each prediction-sequence going to a different set of people.&nbsp; <em>Some</em> string of predictions inevitably does well, allowing the crooks to make money off the proven success of their &ldquo;psychic powers&rdquo; or &ldquo;sophisticated computer models&rdquo; -- but of course, with a bucket like this, the recipients of the correct prediction-sequence are no likelier than average to receive accurate predictions in the next iteration.</p>\n<h3>Selecting alleles, within biological evolution:</h3>\n<p>Alleles are selected based on their differential reproductive impact on one generation -- whether they help a given animal flee from <em>this particular</em> tiger.&nbsp; But it turns out that alleles that help animals flee from <em>this</em> tiger often also help animals flee from <em>other, future</em> tigers.&nbsp; One can <em>imagine</em> a (weird, physically implausible) biology in which alleles that help one flee this particular tiger are <em>not</em> more likely to help one flee other tigers.&nbsp; Technically speaking, the ability to &ldquo;flee from other, future tigers&rdquo; is a tag-along, given to animals by the trait-correlations in evolution&rsquo;s buckets.</p>\n<p>With traits as closely linked as &ldquo;fleeing this tiger&rdquo; and &ldquo;fleeing other, future tigers&rdquo;, it may seem odd to give credit to the bucket of alleles for the fact that selecting for the one trait often also gives you the other trait.&nbsp; One may be tempted to explain our modern tendency to flee tigers by talking solely about &ldquo;selection&rdquo;, with no mention of&nbsp; what sort of buckets evolution was pulling traits from.&nbsp; But there are plenty of recurring biological traits that one <em>does</em> intuitively regard as due to co-incidences in evolution&rsquo;s buckets.&nbsp; Whiteness of bones does not in itself much impact fitness, but it so happens that when evolution selected for bones with advantageous structural properties, it ended up also selecting for bones with a particular, fairly uniform, color.</p>\n<p>Moreover, the traits that tag along with particular selection criteria vary from species to species.&nbsp; Depending on evolution&rsquo;s starting-point, selecting for particular properties will <em>sometimes</em> simultaneously select for particular other traits, and sometimes not.&nbsp; If you take a group of migratory moths, and select for &ldquo;ability to find the egg-laying site&rdquo;, you will improve their ability to find a <em>single, particular</em> location.&nbsp; If you take a group of hominids and select for &ldquo;ability to find their way home&rdquo;, you may instead improve their <em>general</em> navigational ability.&nbsp; (Similarly, selective pressures for bees to communicate with other bees produced a fixed, species-universal communication system, while selective pressures on hominids to communicate with other hominids produced a system for <em>learning</em> any language that fits a particular, species-universal language template.&nbsp; Species such as hominids that have large brains, and that can therefore have their brains tweaked in different directions by particular chance alleles, create different types of variation-buckets for evolution to select within -- perhaps buckets whose tag-alongs more often include &ldquo;general&rdquo; abilities.)</p>\n<p>So... it is naively tempting to view traits as &ldquo;due to selection&rdquo; or &ldquo;due to chance correlations in evolution&rsquo;s buckets&rdquo;.&nbsp; One might imagine &ldquo;running from tigers&rdquo; as a single, unified property that <em>of course</em> you should get &ldquo;from selection&rdquo;, when you select for running from past tigers.&nbsp; One might imagine bones&rsquo; whiteness as a &ldquo;mere chance&rdquo; property that you get &ldquo;from the correlations in the bucket&rdquo;, based on the accident that candidate-bones with good structural properties also happen to have a certain color.&nbsp; But I suspect that a person who solidly understood evolution would see <em>all</em> biological traits (white bones; navigational ability in moths and hominids; tiger fleeing) as due to a sort of tagging-along process that depends on <em>both</em> the traits being selected for, <em>and</em> the buckets of alleles selected among.&nbsp; The effective categories available to evolution (like &ldquo;tiger-fleeing&rdquo;, or &ldquo;finding your way to the nest site&rdquo; vs. &ldquo;finding your way anywhere&rdquo;) are somehow <em>in</em> the buckets... so how did they get in there?&nbsp; And what kind of categories land in naturally occurring buckets?&nbsp; More on this in later posts.</p>\n<h3><a href=\"http://www.overcomingbias.com/2007/09/the-bottom-line.html\">Reason vs. rationalization</a>:</h3>\n<p>If you search out the best arguments for each position and exert equal strength on each search, then selecting for the position for which you found the strongest arguments will often also select for positions that are true, as a tag-along.</p>\n<p>If you instead find the best arguments you can for your initial opinions, and only allow yourself to notice weak evidence for your opponents&rsquo; positions -- if you <a href=\"http://www.overcomingbias.com/2007/10/avoiding-your-b.html\">avoid your beliefs' real weak points</a>&nbsp;-- then selecting for the positions for which you found the strongest arguments will no longer much select for positions that are true.</p>\n<p>Reasoning produces more &ldquo;selection-friendly&rdquo; buckets of arguments than does rationalization (for a person seeking truth).</p>\n<p>&nbsp;</p>\n<p>(The above might not seem as though it has that much to do with rationality.&nbsp; But it lays groundwork for a couple other posts I want to do, that help explain where categories come from and what kind of use we do and can make of categories, and of generalization from past to future data-sets, in science.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MAp6Ft8b3s7kJdrQ9": 4, "3uE2pXvbcnS9nnZRE": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HThGHPeLhe7uy8FEx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 34, "extendedScore": null, "score": 4.805630342469616e-07, "legacy": true, "legacyId": "74", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>by Anna Salamon and Steve Rayhawk (joint authorship)</p>\n<p><strong>Related to:</strong> <a href=\"http://www.overcomingbias.com/2007/11/conjuring-an-ev.html\">Conjuring An Evolution To Serve You</a>, <a href=\"http://www.overcomingbias.com/2008/02/disguised-queri.html\">Disguised Queries</a>&nbsp;</p>\n<p>Let\u2019s say you have a bucket full of \u201cinstances\u201d (e.g., genes, hypotheses, students, foods), and you want to choose a good one.&nbsp; You fish around in the bucket, draw out the first 10 instances you find, and pick the instance that scores highest on some selection criterion.</p>\n<p>For example, perhaps your selection criterion is \u201cnumber of polka dots\u201d, and you reach into the bucket pictured below, and you draw out 10 instances.&nbsp; What do you get?&nbsp; Assuming some instances have more polka dots than others, you get hypotheses with an above average number of expected polka dots.&nbsp; The point I want to dwell on, though -- which is obvious when you think about it, but which sheds significant light on everyday phenomena -- is that you don\u2019t get instances that are <em>just</em> high in polka dots.&nbsp; You get instances are <em>also </em>high in every trait that <em>correlates</em> with having the most polka dots.</p>\n<p><img src=\"http://images.lesswrong.com/t3_22_3.png?v=12e887ce3145479b2126ec9e1449f1e2\" alt=\"\" width=\"200\" height=\"260\"></p>\n<p>For example, in the bucket above, selecting for instances that have many polka dots implies inadvertently selecting for instances that are red.&nbsp; Selective processes bring tag-alongs, and the specific tag-alongs that you get (redness, in this case) depend on both the trait you\u2019re selecting for, and the bucket from which you\u2019re selecting.</p>\n<p>Nearly all cases of useful selection (e.g., evolution, science) would be unable to produce the cool properties they produce (complex order in organisms, truth in theories) if they didn\u2019t have particular, selection-friendly types of buckets, in addition to good selection criteria.&nbsp; Zoom in carefully enough, and nearly <em>all </em>of the traits one gets by selection can be considered tag-alongs.<em>&nbsp; </em>Conversely, if you are consciously selecting entities from buckets with a particular aim in view, you may want to consciously safeguard the \u201cselection-friendliness\u201d of the buckets you are using.<a id=\"more\"></a></p>\n<p>Some examples:</p>\n<h3 id=\"Algebra_test_\">Algebra test:</h3>\n<p>Let\u2019s say you\u2019re trying to select for students who know algebra.&nbsp; So you write out an algebra exam, E, find ten students at random from your pool, and see which one does best on exam E.&nbsp; For <em>many</em> sorts of buckets of students, this procedure should work: selecting for the criterion \u201cdoes well on algebra exam E\u201d will give you more than just that criterion.&nbsp; It\u2019ll give you the tag-along property \u201cdoes well on other algebra problems\u201d or \u201cunderstands algebra\u201d.</p>\n<p>But not for all buckets.&nbsp; If, for example, you release the test questions ahead of time, you\u2019re liable to end up with a bucket of students for which the tag-along property which \u201cdoes well on algebra exam E\u201d gives you is only \u201cmemorized the answers to exam E\u201d, and not \u201cunderstands algebra\u201d.</p>\n<h3 id=\"Taste_and_nutrition_\">Taste and nutrition:</h3>\n<p>Or, again, perhaps you\u2019re designing a test for healthy foods.&nbsp; In a hunter-gatherer environment, human tastes are perhaps not a bad indicator.&nbsp; Grab 10 foodstuffs at random from your bucket, select for \u201cbest tasting\u201d, and you\u2019re liable to get \u201cabove average nutrition\u201d as a tag-along.</p>\n<p>But for the buckets of food-choices created by modern manufacturing (now that we know chemistry, and we can create compounds on purpose that trigger&nbsp;<em>just those sensory mechanisms that signal \u201cgood taste\u201d</em>), selecting for taste no longer selects for nutrition (at least, not as much).</p>\n<h3 id=\"Defensive_mimicry_\">Defensive mimicry:</h3>\n<p>Selecting against prey items that have warning coloration (particular patterns of bright color, as on poison arrow frogs) will select against poisonous prey items, among some buckets of possible prey.&nbsp; But as other species evolve to mimic the warning coloration pattern, the bucket of prey items changes, and \u201cpoisonous\u201d becomes less tied to the indicator trait \u201csuch-and-such a coloration pattern\u201d.</p>\n<h3 id=\"Choosing_coins_that_come_up_heads_\">Choosing coins that come up heads:</h3>\n<p>If you have a bucket of fair coins, and you flip 10 of them several times and choose the one that come up heads the most, this one will be no likelier than average to come up heads in later rounds.&nbsp; If you have a bucket where half the coins are two-headed and the other half are two-tailed, then selecting for even a single head is enough to give you a guarantee of heads on every future coin-toss.&nbsp; And if you have a bucket mixed fair and unfair coins, then selecting for heads helps, but more weakly.</p>\n<p>Nassim Taleb argues that the financial success of particular traders is similar to the success of particular coins from the fair coins bucket -- selecting for \u201ctraders who were successful in the past\u201d doesn\u2019t give you traders who are particularly likely to be successful in the future.&nbsp; Most traders who obtain above-market returns in a particular timespan are, on Taleb\u2019s analysis, traders with ordinary judgment who put their money on a short string of lucky strategies (\u201cbuy US real estate\u201d).&nbsp; The bucket of traders doesn\u2019t have the type of structure that allows \u201cfuture success\u201d to be predicted from \u201cpast success\u201d.</p>\n<p>In the \u201c<a href=\"http://www.investorhome.com/scam.htm\">Perfect Prediction Scam</a>\u201d, crooks send out all possible prediction-sequences for a small number of e.g. sports outcomes, with each prediction-sequence going to a different set of people.&nbsp; <em>Some</em> string of predictions inevitably does well, allowing the crooks to make money off the proven success of their \u201cpsychic powers\u201d or \u201csophisticated computer models\u201d -- but of course, with a bucket like this, the recipients of the correct prediction-sequence are no likelier than average to receive accurate predictions in the next iteration.</p>\n<h3 id=\"Selecting_alleles__within_biological_evolution_\">Selecting alleles, within biological evolution:</h3>\n<p>Alleles are selected based on their differential reproductive impact on one generation -- whether they help a given animal flee from <em>this particular</em> tiger.&nbsp; But it turns out that alleles that help animals flee from <em>this</em> tiger often also help animals flee from <em>other, future</em> tigers.&nbsp; One can <em>imagine</em> a (weird, physically implausible) biology in which alleles that help one flee this particular tiger are <em>not</em> more likely to help one flee other tigers.&nbsp; Technically speaking, the ability to \u201cflee from other, future tigers\u201d is a tag-along, given to animals by the trait-correlations in evolution\u2019s buckets.</p>\n<p>With traits as closely linked as \u201cfleeing this tiger\u201d and \u201cfleeing other, future tigers\u201d, it may seem odd to give credit to the bucket of alleles for the fact that selecting for the one trait often also gives you the other trait.&nbsp; One may be tempted to explain our modern tendency to flee tigers by talking solely about \u201cselection\u201d, with no mention of&nbsp; what sort of buckets evolution was pulling traits from.&nbsp; But there are plenty of recurring biological traits that one <em>does</em> intuitively regard as due to co-incidences in evolution\u2019s buckets.&nbsp; Whiteness of bones does not in itself much impact fitness, but it so happens that when evolution selected for bones with advantageous structural properties, it ended up also selecting for bones with a particular, fairly uniform, color.</p>\n<p>Moreover, the traits that tag along with particular selection criteria vary from species to species.&nbsp; Depending on evolution\u2019s starting-point, selecting for particular properties will <em>sometimes</em> simultaneously select for particular other traits, and sometimes not.&nbsp; If you take a group of migratory moths, and select for \u201cability to find the egg-laying site\u201d, you will improve their ability to find a <em>single, particular</em> location.&nbsp; If you take a group of hominids and select for \u201cability to find their way home\u201d, you may instead improve their <em>general</em> navigational ability.&nbsp; (Similarly, selective pressures for bees to communicate with other bees produced a fixed, species-universal communication system, while selective pressures on hominids to communicate with other hominids produced a system for <em>learning</em> any language that fits a particular, species-universal language template.&nbsp; Species such as hominids that have large brains, and that can therefore have their brains tweaked in different directions by particular chance alleles, create different types of variation-buckets for evolution to select within -- perhaps buckets whose tag-alongs more often include \u201cgeneral\u201d abilities.)</p>\n<p>So... it is naively tempting to view traits as \u201cdue to selection\u201d or \u201cdue to chance correlations in evolution\u2019s buckets\u201d.&nbsp; One might imagine \u201crunning from tigers\u201d as a single, unified property that <em>of course</em> you should get \u201cfrom selection\u201d, when you select for running from past tigers.&nbsp; One might imagine bones\u2019 whiteness as a \u201cmere chance\u201d property that you get \u201cfrom the correlations in the bucket\u201d, based on the accident that candidate-bones with good structural properties also happen to have a certain color.&nbsp; But I suspect that a person who solidly understood evolution would see <em>all</em> biological traits (white bones; navigational ability in moths and hominids; tiger fleeing) as due to a sort of tagging-along process that depends on <em>both</em> the traits being selected for, <em>and</em> the buckets of alleles selected among.&nbsp; The effective categories available to evolution (like \u201ctiger-fleeing\u201d, or \u201cfinding your way to the nest site\u201d vs. \u201cfinding your way anywhere\u201d) are somehow <em>in</em> the buckets... so how did they get in there?&nbsp; And what kind of categories land in naturally occurring buckets?&nbsp; More on this in later posts.</p>\n<h3 id=\"Reason_vs__rationalization_\"><a href=\"http://www.overcomingbias.com/2007/09/the-bottom-line.html\">Reason vs. rationalization</a>:</h3>\n<p>If you search out the best arguments for each position and exert equal strength on each search, then selecting for the position for which you found the strongest arguments will often also select for positions that are true, as a tag-along.</p>\n<p>If you instead find the best arguments you can for your initial opinions, and only allow yourself to notice weak evidence for your opponents\u2019 positions -- if you <a href=\"http://www.overcomingbias.com/2007/10/avoiding-your-b.html\">avoid your beliefs' real weak points</a>&nbsp;-- then selecting for the positions for which you found the strongest arguments will no longer much select for positions that are true.</p>\n<p>Reasoning produces more \u201cselection-friendly\u201d buckets of arguments than does rationalization (for a person seeking truth).</p>\n<p>&nbsp;</p>\n<p>(The above might not seem as though it has that much to do with rationality.&nbsp; But it lays groundwork for a couple other posts I want to do, that help explain where categories come from and what kind of use we do and can make of categories, and of generalization from past to future data-sets, in science.)</p>", "sections": [{"title": "Algebra test:", "anchor": "Algebra_test_", "level": 1}, {"title": "Taste and nutrition:", "anchor": "Taste_and_nutrition_", "level": 1}, {"title": "Defensive mimicry:", "anchor": "Defensive_mimicry_", "level": 1}, {"title": "Choosing coins that come up heads:", "anchor": "Choosing_coins_that_come_up_heads_", "level": 1}, {"title": "Selecting alleles, within biological evolution:", "anchor": "Selecting_alleles__within_biological_evolution_", "level": 1}, {"title": "Reason vs. rationalization:", "anchor": "Reason_vs__rationalization_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-11T16:56:05.745Z", "modifiedAt": null, "url": null, "title": "Adversarial System Hats", "slug": "adversarial-system-hats", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:56.222Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Johnicholas", "createdAt": "2009-02-27T15:01:52.708Z", "isAdmin": false, "displayName": "Johnicholas"}, "userId": "kBvTXutfPytNtzPyD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qmufiasd6cevHRcr3/adversarial-system-hats", "pageUrlRelative": "/posts/qmufiasd6cevHRcr3/adversarial-system-hats", "linkUrl": "https://www.lesswrong.com/posts/qmufiasd6cevHRcr3/adversarial-system-hats", "postedAtFormatted": "Wednesday, March 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Adversarial%20System%20Hats&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdversarial%20System%20Hats%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqmufiasd6cevHRcr3%2Fadversarial-system-hats%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Adversarial%20System%20Hats%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqmufiasd6cevHRcr3%2Fadversarial-system-hats", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqmufiasd6cevHRcr3%2Fadversarial-system-hats", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 320, "htmlBody": "<p><strong>In Reply to:</strong> <a href=\"http://www.overcomingbias.com/2007/09/rationalization.html\">Rationalization</a>, <a href=\"/lw/u/the_ethic_of_handwashing_and_community_epistemic/\">Epistemic Handwashing</a>, <a href=\"/lw/22/selective_processes_bring_tagalongs_but_not_always/\">Selective Processes</a></p>\n<p><a href=\"/user/Eliezer_Yudkowsky\">Eliezer Yudkowsky</a> wrote about scientists defending pet hypotheses, and prosecutors and defenders as examples of clever <a href=\"http://www.overcomingbias.com/2007/09/rationalization.html\">rationalization</a>. His primary focus was advice to the well-intentioned <em>individual</em> rationalist, which is excellent as far as it goes. But <a href=\"/user/AnnaSalamon/\">Anna Salamon</a> and Steve Rayhawk <a href=\"/lw/u/the_ethic_of_handwashing_and_community_epistemic/\">ask</a> how a social system should be structured for <em>group</em> rationality.</p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Adversarial_system\">adversarial system</a> is widely used in criminal justice. In the legal world, roles such as Prosecution, Defense, and Judge are all guaranteed to be filled, with roughly the same amount of human effort applied to each side. Suppose individuals chose their own roles. It is possible that one role turns out more popular. Because different effort is applied to different sides, <a href=\"/lw/22/selective_processes_bring_tagalongs_but_not_always/\">selecting</a> for the positions with the strongest arguments will no longer much select for positions that are true.</p>\n<p><a id=\"more\"></a></p>\n<p>One role might be more popular because of an <a href=\"/lw/z/information_cascades/\">information cascade</a>: individuals read the extant arguments and then choose a role, striving to align themselves with the truth, and create arguments for that position. Alternately, a role may be popular due to <a href=\"http://www.overcomingbias.com/2009/03/status-affiliations.html\">status-based affiliation</a>, or striving to be on the \"winning\" side.</p>\n<p>I'm well aware that there are <a href=\"http://www.overcomingbias.com/2007/05/policy_tugowar.html\">vastly more than two sides</a> to most questions. Imagine a list of rationalist roles something like IDEO's \"<a href=\"http://www.tenfacesofinnovation.com/tenfaces/index.htm\">Ten Faces</a>\".</p>\n<p>Example rationalist roles, leaving the obvious ones for last:</p>\n<ul>\n<li>The Mediator, who strives for common understanding and combining evidence.</li>\n<li>The Wise, who may not take a stand, but only criticize internal consistency of arguments.</li>\n<li>The Perpendicularist, who strives to break up polarization by \"pulling the rope sideways\". </li>\n<li>The Advocate, who champions a controversial claim or proposed action.</li>\n<li>The Detractor, who points out flaws in the controversial claim or proposed action.</li>\n</ul>\n<p>Due to natural group phenomena (cascades, affiliation), in order to achieve group rationality, there need to be social structures that strive to prevent those natural phenomena. Roles might help.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZzxvopS4BwLuQy42n": 1, "izp6eeJJEg9v5zcur": 1, "zv7v2ziqexSn5iS9v": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qmufiasd6cevHRcr3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 8, "extendedScore": null, "score": 4.806375897965694e-07, "legacy": true, "legacyId": "75", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZP2om2oWHPhvWP2Q3", "HThGHPeLhe7uy8FEx", "DNQw596nPCX4x7xT9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-11T19:23:23.651Z", "modifiedAt": null, "url": null, "title": "Beginning at the Beginning", "slug": "beginning-at-the-beginning", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:12.464Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Annoyance", "createdAt": "2009-02-28T04:06:40.600Z", "isAdmin": false, "displayName": "Annoyance"}, "userId": "yEm6LWatswJYGwBFq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wPbxcq6MdexECaXvM/beginning-at-the-beginning", "pageUrlRelative": "/posts/wPbxcq6MdexECaXvM/beginning-at-the-beginning", "linkUrl": "https://www.lesswrong.com/posts/wPbxcq6MdexECaXvM/beginning-at-the-beginning", "postedAtFormatted": "Wednesday, March 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Beginning%20at%20the%20Beginning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABeginning%20at%20the%20Beginning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwPbxcq6MdexECaXvM%2Fbeginning-at-the-beginning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Beginning%20at%20the%20Beginning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwPbxcq6MdexECaXvM%2Fbeginning-at-the-beginning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwPbxcq6MdexECaXvM%2Fbeginning-at-the-beginning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1216, "htmlBody": "<p>I can't help but notice that some people are utilizing some very peculiar and idiosyncratic meanings for the word 'rational' in their posts and comments.&nbsp; In many instances, the correctness of rationality is taken for granted; in others, the process of being rational is not only ignored, but dispensed with alltogether, and rational is defined as 'that which makes you win'.</p>\n<p>That's not a very useful definition.&nbsp; If I went to someone looking for helping selecting between options, and was told to choose \"the best one\", or \"the right one\", or \"the one that gives you the greatest chance of winning\", what help would I have received?&nbsp; If I had clear ideas about how to determine the best, the right, or the one that would win, I wouldn't have come looking for help in the first place.&nbsp; The responses provide no operational assistance.</p>\n<p>There is a definite lack of understanding here of what rationality is, much less why it is correct, and this general incomprehension can only cripple attempts to discuss its nature or how to apply it. We might think that this site would try to dispel the fog surrounding the concept.&nbsp; Remarkably, a blog established to help \"refining the art of human rationality\" neither explains nor defines rationality.</p>\n<p>Those are absolutely critical goals if lesswrong is to accomplish what it advertises itself as attempting.&nbsp; So let's try to reach them.</p>\n<hr />\n<p>The human mind is at the same time both extremely sophisticated and shockingly primitive.&nbsp; Most of its operations take place beneath the level of explicit awareness; we don't know how we reach conclusions and make decisions, we're merely presented with the results along with an emotional sense of rightness or confidence.</p>\n<p>Despite these emotional assurances, we sometimes suspect that such feelings are unfounded.&nbsp; Careful examination shows that to be precisely the case.&nbsp; We can and do develop confidence in results, not because they are reliable, but for a host of other reasons.&nbsp;</p>\n<p>Our approval or disapproval of some properties can cross over into our evaluation of others.&nbsp; We can fall prey to shortcuts while believing that we've been thorough.&nbsp; We tend to interpret evidence in terms of our preferences, perceiving what we want to perceive and screening out evidence we find inconvenient or uncomfortable.&nbsp; Sometimes, we even construct evidence out of whole cloth to support something we want to be true.</p>\n<p>It's very difficult to detect these flaws in ourselves as we make them.&nbsp; It is somewhat easier to detect them in others, or in hindsight while reflecting upon past decisions which we are no longer strongly emotionally involved in.&nbsp; Without knowing how our decisions are reached, though, we're helpless in the face of impulses and feelings of the moment, even while we're ultimately skeptical about how our judgment functions.</p>\n<p>So how can we try to improve our judgment if we don't even know what it's doing?</p>\n<p>How did Aristotle establish the earliest-known examination of the principles of justification?&nbsp; If he originated the foundation of the systems we know as *logic*, how could that be accomplished without the use of logic?</p>\n<p>As Aristotle noted, the principles he made into a set of formal rules already existed.&nbsp; He observed the arguments of others, noting how people defended positions and attacked the positions of others, and how certain arguments had flaws that could be pointed out while others seemed to possess no counters.&nbsp; His attempts to organize people's implicit understandings of the validity of arguments led to an explicit, formal system.&nbsp; The principles of logic were implicit before they were understood explicitly.</p>\n<p>The brain is capable of performing astounding feats of computation, but our conscious grasp of mathematics is emulated and lacks the power of the system that creates it.&nbsp; We can intuitively comprehend how a projectile will move from just a glimpse of its trajectory, although solving the explicit partial differential equation that describes that motion is terrifically difficult, and virtually impossible to accomplish in real-time.&nbsp; Yet our explicit grasp of mathematics makes it possible for us to solve problems and comprehend ideas completely beyond the capacity of our hunter-gatherer ancestors, even though the processing power of our brains does not appear to have changed from those early days.</p>\n<p>In the same way, our models of what proper thought means give us options and opportunities far beyond what our intuitive, unconscious reasoning makes possible, even though the conscious understanding works with much fewer resources than the unconscious.</p>\n<p>When we consciously and deliberately model the evolution of one statement into another according to elementary rules that make up the foundation of logical consistency, something new and exciting happens.&nbsp; The self-referential aspects of that modeling permit us to compare the decisions presented to us by the parts of our minds beneath the threshold of our awareness and override them.&nbsp; We can evaluate our own evaluations, reaching conclusions that our emotions don't lead us to and rejecting some of those that they do.</p>\n<p>That's what rationality is:&nbsp; having explicit and conscious standards of validity, and applying them in a systematic way.&nbsp; It doesn't matter if we possess an inner conviction that something is true - if we can't demonstrate that it can be generated from basic principles according to well-defined rules, it's not valid.</p>\n<p>What makes this so interesting is that it's self-correcting.&nbsp; If we observe an empirical relationship that our understanding doesn't predict, we can treat it as a new fact.&nbsp; For example, let's say that we find that certain manipulations of tarot decks permit us to predict the weather, even though we have no idea of why the two should be correlated at all.&nbsp; With rationality, we don't need to know why.&nbsp; Once we've recognized that the relationship exists, it becomes rational for us to use it.&nbsp; Likewise, if a previously-useful relationship suddenly ceases to be, even though we have no theoretical grounds for expecting that to happen, we simply acknowledge the fact.&nbsp; Once we've done so, we can justify ignoring that which we previously considered to be evidence.</p>\n<p>Human reasoning is especially plagued by superstitions, because it's easy for us to accept contradictory principles without acknowledging the inconsistency.&nbsp; But when we're forced to construct step-by-step justifications for our beliefs, contradiction is thrown into sharp relief, and can't be ignored.</p>\n<p>Arguments that are not made explicitly, with conscious awareness of how each point is derived from fundamental principles and empirical observations, may or may not be correct.&nbsp; But they're never rational.&nbsp; Rational reasoning does not guarantee correctness; rational choice does not guarantee victory.&nbsp; What rationality offers is self-knowledge of validity.&nbsp; If rational standards are maintained when thinking, the best choice as defined by the knowledge we possess will be made.&nbsp; Whether it will be best when we gain new knowledge, or in some absolute sense, is unknown and unknowable until that moment comes.</p>\n<p>Yet those who speak here often of the value of human rationality frequently don't do so by rational means.&nbsp; They make implicit arguments with hidden assumptions and do not acknowledge or clarify them.&nbsp; They emphasize the potential for rationality to bootstrap itself to greater and greater levels of understanding, yet don't concern themselves with demonstrating that their arguments arise from the most basic elements of reason.&nbsp; Rationality starts when we make a conscious attempt to understand and apply those basic elements, to emulate in our minds the principles that make the existence of our minds possible.</p>\n<p>Are we doing so?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "aa3Qg7Qrp9LM7QMaz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wPbxcq6MdexECaXvM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 5, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "76", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 60, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-11T21:39:47.614Z", "modifiedAt": "2020-12-02T12:18:19.332Z", "url": null, "title": "The Apologist and the Revolutionary", "slug": "the-apologist-and-the-revolutionary", "viewCount": null, "lastCommentedAt": "2020-12-02T12:17:31.327Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZiQqsgGX6a42Sfpii/the-apologist-and-the-revolutionary", "pageUrlRelative": "/posts/ZiQqsgGX6a42Sfpii/the-apologist-and-the-revolutionary", "linkUrl": "https://www.lesswrong.com/posts/ZiQqsgGX6a42Sfpii/the-apologist-and-the-revolutionary", "postedAtFormatted": "Wednesday, March 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Apologist%20and%20the%20Revolutionary&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Apologist%20and%20the%20Revolutionary%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZiQqsgGX6a42Sfpii%2Fthe-apologist-and-the-revolutionary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Apologist%20and%20the%20Revolutionary%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZiQqsgGX6a42Sfpii%2Fthe-apologist-and-the-revolutionary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZiQqsgGX6a42Sfpii%2Fthe-apologist-and-the-revolutionary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1438, "htmlBody": "<p>Rationalists complain that most people are too willing to <a href=\"http://www.overcomingbias.com/2007/10/fake-justificat.html\">make excuses</a> for their positions, and too unwilling to abandon those positions for ones that better fit the evidence. And most people really <em>are</em> pretty bad at this. But certain stroke victims called anosognosiacs are much, much worse.<br /><br /><a href=\"http://en.wikipedia.org/wiki/Anosognosia\">Anosognosia</a> is the condition of not being aware of your own disabilities. To be clear, we're not talking minor disabilities here, the sort that only show up during a comprehensive clinical exam. We're talking paralysis or even blindness<sup>1</sup>. Things that should be pretty hard to miss.<br /><br />Take the example of the woman discussed in Lishman's <a href=\"http://books.google.ie/books?id=Jsq-O12Ydo8C&amp;pg=PA69&amp;lpg=PA69&amp;dq=anosognosia+shoulder+ring&amp;source=bl&amp;ots=_JDmzV0NJe&amp;sig=_8rVlu85GNnsSkYEDRmAaUhWL1Q&amp;hl=en&amp;ei=Utu2SfjDL5SIjAfQpeSlCQ&amp;sa=X&amp;oi=book_result&amp;resnum=1&amp;ct=result\">Organic Psychiatry</a>. After a right-hemisphere stroke, she lost movement in her left arm but continuously denied it. When the doctor asked her to move her arm, and she observed it not moving, she claimed that it wasn't actually her arm, it was her daughter's. Why was her daughter's arm attached to her shoulder? The patient claimed her daughter had been there in the bed with her all week. Why was her wedding ring on her daughter's hand? The patient said her daughter had borrowed it. Where was the patient's arm? The patient \"turned her head and searched in a bemused way over her left shoulder\".<br /><br />Why won't these patients admit they're paralyzed, and what are the implications for neurotypical humans? Dr. <a href=\"http://en.wikipedia.org/wiki/Vilayanur_S._Ramachandran#Scientific_career\">Vilayanur Ramachandran</a>, leading neuroscientist and current holder of the world land-speed record for hypothesis generation, has a theory.</p>\n<p><a id=\"more\"></a></p>\n<p>One immediately plausible hypothesis: the patient is unable to cope psychologically with the possibility of being paralyzed, so he responds with denial. Plausible, but according to Dr. Ramachandran, wrong. He notes that patients with left-side strokes almost never suffer anosognosia, even though the left side controls the right half of the body in about the same way the right side controls the left half. There must be something special about the right hemisphere.<br /><br />Another plausible hypothesis: the part of the brain responsible for thinking about the affected area was damaged in the stroke. Therefore, the patient has lost access to the area, so to speak. Dr. Ramachandran doesn't like this idea either. The lack of right-sided anosognosia in left-hemisphere stroke victims argues against it as well. But how can we disconfirm it?<br /><br />Dr. Ramachandran performed an experiment<sup>2</sup> where he \"paralyzed\" an anosognosiac's good right arm. He placed it in a clever system of mirrors that caused a research assistant's arm to look as if it was attached to the patient's shoulder. Ramachandran told the patient to move his own right arm, and the false arm didn't move. What happened? The patient claimed he could see the arm moving - a classic anosognosiac response. This suggests that the anosognosia is not specifically a deficit of the brain's left-arm monitoring system, but rather some sort of failure of rationality.</p>\n<p>Says Dr. Ramachandran:</p>\n<blockquote>\n<p>The reason anosognosia is so puzzling is that we have come to regard the 'intellect' as primarily propositional in character and one ordinarily expects propositional logic to be internally consistent. To listen to a patient deny ownership of her arm and yet, in the same breath, admit that it is attached to her shoulder is one of the most perplexing phenomena that one can encounter as a neurologist.</p>\n</blockquote>\n<p>So what's Dr. Ramachandran's solution? He <a href=\"http://psych.utoronto.ca/~peterson/psy430s2001/Ramachandran%20VS%20Evolution%20of%20self-deception%20Med%20Hypoth%201996.pdf\">posits two different reasoning modules</a> located in the two different hemispheres. The left brain tries to fit the data to the theory to preserve a coherent internal narrative and prevent a person from jumping back and forth between conclusions upon each new data point. It is primarily an apologist, there to explain why any experience is exactly what its own theory would have predicted. The right brain is the seat of the <a href=\"http://yudkowsky.net/rational/virtues\">second virtue</a>. When it's had enough of the left-brain's confabulating, it initiates a Kuhnian paradigm shift to a completely new narrative. Ramachandran describes it as \"a left-wing revolutionary\".<br /><br />Normally these two systems work in balance. But if a stroke takes the revolutionary offline, the brain loses its ability to change its mind about anything significant. If your left arm was working before your stroke, the little voice that ought to tell you it might be time to reject the \"left arm works fine\" theory goes silent. The only one left is the poor apologist, who must tirelessly invent stranger and stranger excuses for why all the facts really fit the \"left arm works fine\" theory perfectly well.<br /><br />It gets weirder. For some reason, <a href=\"http://www.neurology.org/cgi/content/abstract/65/8/1278\">squirting cold water into the left ear canal</a> wakes up the revolutionary. Maybe the intense sensory input from an unexpected source makes the right hemisphere unusually aroused. Maybe distoring the balance sense causes the eyes to move rapidly, activating a latent system for inter-hemisphere co-ordination usually restricted to REM sleep<sup>3</sup>. In any case, a patient who has been denying paralysis for weeks or months will, upon having cold water placed in the ear, admit to paralysis, admit to having been paralyzed the past few weeks or months, and express bewilderment at having ever denied such an obvious fact. And then the effect wears off, and the patient not only denies the paralysis but denies ever having admitted to it.<br /><br />This divorce between the apologist and the revolutionary might also explain some of the odd behavior of <a href=\"http://en.wikipedia.org/wiki/Split-brain\">split-brain</a> patients. Consider <a href=\"http://books.google.ie/books?id=_rkKxbevFZEC&amp;pg=PA10&amp;lpg=PA10&amp;dq=split-brain+chicken+shovel&amp;source=bl&amp;ots=9gVX7xBkJq&amp;sig=yKnpOKg1jdzifungp7VgIXMLMcA&amp;hl=en&amp;ei=muu2SZKXA-LBjAeupOCvCQ&amp;sa=X&amp;oi=book_result&amp;resnum=8&amp;ct=result\">the following experiment</a>: a split-brain patient was shown two images, one in each visual field. The left hemisphere received the image of a chicken claw, and the right hemisphere received the image of a snowed-in house. The patient was asked verbally to describe what he saw, activating the left (more verbal) hemisphere. The patient said he saw a chicken claw, as expected. Then the patient was asked to point with his left hand (controlled by the right hemisphere) to a picture related to the scene. Among the pictures available were a shovel and a chicken. He pointed to the shovel. So far, no crazier than what we've come to expect from neuroscience.<br /><br />Now the doctor verbally asked the patient to describe why he just pointed to the shovel. The patient verbally (left hemisphere!) answered that he saw a chicken claw, and of course shovels are necessary to clean out chicken sheds, so he pointed to the shovel to indicate chickens. The apologist in the left-brain is helpless to do anything besides explain why the data fits its own theory, and its own theory is that whatever happened had something to do with chickens, dammit!<br /><br />The logical follow-up experiment would be to ask the right hemisphere to explain the left hemisphere's actions. Unfortunately, the right hemisphere is either non-linguistic or as close as to make no difference. Whatever its thoughts, it's keeping them to itself.<br /><br />...you know, my mouth is <em>still </em>agape at that whole cold-water-in-the-ear trick. I have this fantasy of gathering all the leading creationists together and squirting ice cold water in each of their left ears. All of a sudden, one and all, they admit their mistakes, and express bafflement at ever having believed such nonsense. And then ten minutes later the effect wears off, and they're all back to talking about irreducible complexity or whatever. I don't mind. I've already run off to upload the video to YouTube.<br /><br />This is surely so great an exaggeration of Dr. Ramachandran's theory as to be a parody of it. And in any case I don't know how much to believe all this about different reasoning modules, or how closely the intuitive understanding of it I take from his paper matches the way a neuroscientist would think of it. Are the apologist and the revolutionary active in normal thought? Do anosognosiacs demonstrate the same pathological inability to change their mind on issues other than their disabilities? What of the argument that <a href=\"http://books.google.ie/books?id=_rkKxbevFZEC&amp;dq=brain+fiction+confabulation&amp;printsec=frontcover&amp;source=bl&amp;ots=9gVX7xFiMo&amp;sig=ecS9mLduiZePctwU8lly9DejYjo&amp;hl=en&amp;ei=jvq2SYigB9nHjAefq62dCQ&amp;sa=X&amp;oi=book_result&amp;resnum=1&amp;ct=result#PPP11,M1\">confabulation</a> is a rather common failure mode of the brain, shared by some conditions that have little to do with right-hemisphere failure? Why does the effect of the cold water wear off so quickly? I've yet to see any really satisfying answers to any of these questions.</p>\n<p>But whether Ramachandran is right or wrong, I give him enormous credit for doing serious research into the neural correlates of human rationality. I can think of few other fields that offer so many potential benefits.</p>\n<p>&nbsp;</p>\n<p><strong>Footnotes</strong></p>\n<p>1: See <a href=\"http://en.wikipedia.org/wiki/Anton%27s_syndrome\">Anton-Babinski syndrome</a></p>\n<p>2: See Ramachandran's \"The Evolutionary Biology of Self-Deception\", the link from \"posits two different reasoning modules\" in this article.</p>\n<p>3: For Ramachandran's thoughts on REM, again see \"The Evolutionary Biology of Self Deception\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZzxvopS4BwLuQy42n": 9, "Ng8Gice9KNkncxqcj": 11, "gsv9XWbZDcnZmKuqM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZiQqsgGX6a42Sfpii", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 190, "baseScore": 221, "extendedScore": null, "score": 0.000334, "legacy": true, "legacyId": "72", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 221, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 100, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-03-11T21:39:47.614Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-12T04:28:49.168Z", "modifiedAt": null, "url": null, "title": "Raising the Sanity Waterline", "slug": "raising-the-sanity-waterline", "viewCount": null, "lastCommentedAt": "2021-08-26T09:06:07.996Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XqmjdBKa4ZaXJtNmf/raising-the-sanity-waterline", "pageUrlRelative": "/posts/XqmjdBKa4ZaXJtNmf/raising-the-sanity-waterline", "linkUrl": "https://www.lesswrong.com/posts/XqmjdBKa4ZaXJtNmf/raising-the-sanity-waterline", "postedAtFormatted": "Thursday, March 12th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Raising%20the%20Sanity%20Waterline&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARaising%20the%20Sanity%20Waterline%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXqmjdBKa4ZaXJtNmf%2Fraising-the-sanity-waterline%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Raising%20the%20Sanity%20Waterline%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXqmjdBKa4ZaXJtNmf%2Fraising-the-sanity-waterline", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXqmjdBKa4ZaXJtNmf%2Fraising-the-sanity-waterline", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 871, "htmlBody": "<p>To <a href=\"http://www.acceleratingfuture.com/steven/?p=3\">paraphrase</a> the Black Belt Bayesian:&nbsp; Behind every exciting, dramatic failure, there is a more important story about a larger and less dramatic failure that made the first failure possible.</p>\n<p>If every trace of religion was magically eliminated from the world tomorrow, then&mdash;however much improved the lives of many people would be&mdash;we would not even have come close to solving the larger failures of sanity that made religion possible in the first place.</p>\n<p>We have good cause to spend some of our efforts on trying to eliminate religion directly, because it <em>is</em> a direct problem.&nbsp; But religion also serves the function of an asphyxiated canary in a coal mine&mdash;religion is a sign, a symptom, of larger problems that don't go away just because someone loses their religion.</p>\n<p>Consider this thought experiment&mdash;what could you teach people that is not <em>directly</em> about religion, which is true and useful as a <em>general </em>method of rationality, which would cause them to lose their religions?&nbsp; In fact&mdash;imagine that we're going to go and survey all your students five years later, and see how many of them have lost their religions compared to a control group; if you make the slightest move at fighting religion <em>directly,</em> you will invalidate the experiment.&nbsp; You may not make a single mention of religion or any religious belief in your classroom, you may not even hint at it in any obvious way.&nbsp; All your examples must center about real-world cases that have nothing to do with religion.</p>\n<p>If you can't fight religion <em>directly,</em> what do you teach that raises the <em>general waterline of sanity</em> to the point that religion goes underwater?<a id=\"more\"></a></p>\n<p>Here are some such topics I've already covered&mdash;<em>not</em> avoiding all mention of religion, but it could be done:</p>\n<ul>\n<li><a href=\"http://www.overcomingbias.com/2007/12/affective-death.html\">Affective Death Spirals</a>&mdash;plenty of <a href=\"http://www.overcomingbias.com/2007/12/ayn-rand.html\">non-supernaturalist</a> examples.</li>\n<li>How to avoid <a href=\"http://www.overcomingbias.com/2007/10/cached-thoughts.html\">cached thoughts</a> and <a href=\"http://www.overcomingbias.com/2007/10/how-to-seem-and.html\">fake wisdom</a>; the pressure of <a href=\"http://www.overcomingbias.com/2007/12/aschs-conformit.html\">conformity</a>.</li>\n<li><a href=\"http://www.overcomingbias.com/2007/09/what-is-evidenc.html\">Evidence</a> and <a href=\"http://www.overcomingbias.com/2007/09/occams-razor.html\">Occam's Razor</a>&mdash;the rules of probability.</li>\n<li><a href=\"http://www.overcomingbias.com/2007/09/the-bottom-line.html\">The Bottom Line</a> / <a href=\"http://www.overcomingbias.com/2008/02/second-law.html\">Engines of Cognition</a>&mdash;the causal reasons why Reason works.</li>\n<li><a href=\"http://www.overcomingbias.com/2007/08/mysterious-answ.html\">Mysterious Answers to Mysterious Questions</a>&mdash;and the whole associated sequence, like <a href=\"http://www.overcomingbias.com/2007/07/making-beliefs-.html\">making beliefs pay rent</a> and <a href=\"http://www.overcomingbias.com/2007/08/semantic-stopsi.html\">curiosity-stoppers</a>&mdash;have excellent historical examples in vitalism and <a href=\"http://www.overcomingbias.com/2007/08/fake-causality.html\">phlogiston</a>.</li>\n<li><a href=\"http://www.overcomingbias.com/2008/09/excluding-the-s.html\">Non-existence of ontologically fundamental mental things</a>&mdash;apply the <a href=\"http://www.overcomingbias.com/2008/03/mind-projection.html\">Mind Projection Fallacy</a> to <a href=\"http://www.overcomingbias.com/2008/03/mind-probabilit.html\">probability</a>, move on to <a href=\"http://www.overcomingbias.com/2008/03/reductionism.html\">reductionism</a> versus <a href=\"http://www.overcomingbias.com/2007/08/the-futility-of.html\">holism</a>, then <a href=\"http://www.overcomingbias.com/2008/04/brain-breakthro.html\">brains</a> and <a href=\"http://www.overcomingbias.com/2008/03/angry-atoms.html\">cognitive science</a>.</li>\n<li>The many sub-arts of <a href=\"http://www.overcomingbias.com/2008/10/got-crisis.html\">Crisis of Faith</a>&mdash;though you'd better find something else to call this ultimate high master-level technique of <em>actually updating on evidence</em>.</li>\n<li><a href=\"http://www.overcomingbias.com/2008/10/the-dark-side.html\">Dark Side Epistemology</a>&mdash;teaching this with no mention of religion would be hard, but perhaps you could videotape the interrogation of some snake-oil sales agent as your real-world example.</li>\n<li><a href=\"http://www.overcomingbias.com/2009/01/fun-theory-sequence.html\">Fun Theory</a>&mdash;teach as a literary theory of utopian fiction, without the direct application to <a href=\"http://www.overcomingbias.com/2009/01/the-uses-of-fun-theory.html\">theodicy</a>.</li>\n<li><a href=\"http://www.overcomingbias.com/2008/03/joy-in-the-real.html\">Joy in the Merely Real</a>, <a href=\"http://www.overcomingbias.com/2008/08/rightness-redux.html\">naturalistic metaethics</a>, etcetera etcetera etcetera and so on.</li>\n</ul>\n<p>But to look at it another way&mdash;</p>\n<p>Suppose we have <a href=\"http://www.overcomingbias.com/2007/01/outside_the_lab.html\">a scientist who's still religious</a>, either full-blown scriptural-religion, or in the sense of tossing around vague casual endorsements of \"spirituality\".</p>\n<p>We now know this person is not applying any <em>technical, explicit</em> understanding of...</p>\n<ul>\n<li>...what constitutes evidence and why;</li>\n<li>...Occam's Razor;</li>\n<li>...how the above two rules derive from the lawful and causal operation of minds as mapping engines, and do not switch off when you talk about tooth fairies;</li>\n<li>...how to tell the difference between a real answer and a curiosity-stopper;</li>\n<li>...how to rethink matters for themselves instead of just repeating things they heard;</li>\n<li>...certain general trends of science over the last three thousand years;</li>\n<li>...the difficult arts of actually updating on new evidence and relinquishing old beliefs;</li>\n<li>...epistemology 101;</li>\n<li>...self-honesty 201;</li>\n<li>...etcetera etcetera etcetera and so on.</li>\n</ul>\n<p>When you consider it&mdash;these are all rather <em>basic</em> matters of study, as such things go.&nbsp; A quick introduction to <em>all</em> of them (well, except <a href=\"http://www.overcomingbias.com/2009/01/fragile-value.html\">naturalistic metaethics</a>) would be... a four-credit undergraduate course with no prerequisites?</p>\n<p>But there are Nobel laureates who haven't taken that course!&nbsp; <a href=\"http://www.overcomingbias.com/2007/02/what_evidence_i.html?cid=61577404#comment-61577404\">Richard Smalley</a> if you're looking for a cheap shot, or <a href=\"http://www.overcomingbias.com/2008/05/changing-the-de.html\">Robert Aumann</a> if you're looking for a scary shot.</p>\n<p>And they can't be isolated exceptions.&nbsp; If all of their professional compatriots had taken that course, then Smalley or Aumann would either have been corrected (as their colleagues kindly took them aside and explained the bare fundamentals) or else regarded with too much pity and concern to win a Nobel Prize.&nbsp; Could you&mdash;<em>realistically </em>speaking, regardless of fairness&mdash;win a Nobel while advocating the existence of Santa Claus?</p>\n<p>That's what the dead canary, religion, is telling us: that the general sanity waterline is currently <em>really ridiculously low.</em>&nbsp; Even in the highest halls of science.</p>\n<p>If we throw out that dead and rotting canary, then our mine may stink a bit less, but the sanity waterline may not rise much higher.</p>\n<p>This is not to criticize the neo-atheist movement.&nbsp; The harm done by religion is clear and present danger, or rather, current and ongoing disaster.&nbsp; Fighting religion's directly harmful effects takes precedence over its use as a canary or experimental indicator.&nbsp; But even if Dawkins, and Dennett, and Harris, and Hitchens should somehow win utterly and absolutely to the last corner of the human sphere, the real work of rationalists will be only just beginning.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "izp6eeJJEg9v5zcur": 9, "MXcpQvaPGtXpB6vkM": 5, "gHCNhqxuJq2bZ2akb": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XqmjdBKa4ZaXJtNmf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 182, "baseScore": 211, "extendedScore": null, "score": 0.000317, "legacy": true, "legacyId": "50", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "pvim9PZJ6qHRTMqD3", "canonicalCollectionSlug": "rationality", "canonicalBookId": "e6WsPsivzBifrWHeA", "canonicalNextPostSlug": "a-sense-that-more-is-possible", "canonicalPrevPostSlug": "final-words", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 211, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 229, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 22, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-12T18:48:28.967Z", "modifiedAt": null, "url": null, "title": "Rational Defense of Irrational Beliefs", "slug": "rational-defense-of-irrational-beliefs", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:06.448Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cleonid", "createdAt": "2009-03-05T01:26:41.015Z", "isAdmin": false, "displayName": "cleonid"}, "userId": "Dh7Ax8Qp8bzp4xZBP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bjxo24nCFByCzkjdP/rational-defense-of-irrational-beliefs", "pageUrlRelative": "/posts/bjxo24nCFByCzkjdP/rational-defense-of-irrational-beliefs", "linkUrl": "https://www.lesswrong.com/posts/bjxo24nCFByCzkjdP/rational-defense-of-irrational-beliefs", "postedAtFormatted": "Thursday, March 12th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20Defense%20of%20Irrational%20Beliefs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20Defense%20of%20Irrational%20Beliefs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbjxo24nCFByCzkjdP%2Frational-defense-of-irrational-beliefs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20Defense%20of%20Irrational%20Beliefs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbjxo24nCFByCzkjdP%2Frational-defense-of-irrational-beliefs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbjxo24nCFByCzkjdP%2Frational-defense-of-irrational-beliefs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 263, "htmlBody": "<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: Times New Roman;\"><span>&ldquo;Everyone complains of his <span style=\"font-weight: normal;\">memory</span>, but nobody of his judgment.\" This <span style=\"font-weight: normal;\">maxim</span> of <span style=\"font-weight: normal;\">La Rochefoucauld rings as true today as it did back in the XVIIth century. </span></span>People tend overestimate their reasoning abilities even when this overconfidence has a direct monetary cost. For instance, multiple studies have shown that investors who are more confident of their ability to beat the market receive lower returns on their investments. This overconfidence penalty applies even to the supposed experts, such as fund managers.</span><span style=\"font-family: Times New Roman;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: Times New Roman;\">So what an expert rationalist should do to avoid this overconfidence trap? The seeming answer is that we should rely less on our own reasoning and more on the &ldquo;wisdom of the crowds&rdquo;. To a certain extent this is already achieved by the society pressure to conform, which acts as an internal policeman in our minds. Yet those of us who deem themselves not very susceptible to such pressures (overconfidence, here we go again) might need to shift their views even further.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: Times New Roman;\">I invite you now to experiment on how this will work in practice. Quite a few of the recent posts and comments were speaking with derision about religion and the supernatural phenomena in general. Did the authors of these comments fully consider the fact that the existence of God is firmly believed by the majority? <span style=\"mso-spacerun: yes;\">&nbsp;</span>Or that this belief is not restricted to the uneducated but shared by many famous scientists, including Newton and Einstein? <span style=\"mso-spacerun: yes;\">&nbsp;</span>Would they be willing to shift their views to accommodate the chance that their own reasoning powers are insufficient to get the right answer? </span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: Times New Roman;\">Let the stone throwing begin.</span></p>\r\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "ZzxvopS4BwLuQy42n": 1, "5f5c37ee1b5cdee568cfb153": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bjxo24nCFByCzkjdP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 5, "extendedScore": null, "score": 4.80860766032571e-07, "legacy": true, "legacyId": "78", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-12T22:15:59.935Z", "modifiedAt": null, "url": null, "title": "So you say you're an altruist...", "slug": "so-you-say-you-re-an-altruist", "viewCount": null, "lastCommentedAt": "2019-09-06T05:54:03.503Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Dc3bwCjM9HzZcq9M8/so-you-say-you-re-an-altruist", "pageUrlRelative": "/posts/Dc3bwCjM9HzZcq9M8/so-you-say-you-re-an-altruist", "linkUrl": "https://www.lesswrong.com/posts/Dc3bwCjM9HzZcq9M8/so-you-say-you-re-an-altruist", "postedAtFormatted": "Thursday, March 12th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20So%20you%20say%20you're%20an%20altruist...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASo%20you%20say%20you're%20an%20altruist...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDc3bwCjM9HzZcq9M8%2Fso-you-say-you-re-an-altruist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=So%20you%20say%20you're%20an%20altruist...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDc3bwCjM9HzZcq9M8%2Fso-you-say-you-re-an-altruist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDc3bwCjM9HzZcq9M8%2Fso-you-say-you-re-an-altruist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 22, "htmlBody": "<p>I'd be really interested to hear what the Less Wrong community thinks of <a href=\"http://www.aaronsw.com/weblog/handwritingwall\" target=\"_blank\">this</a>.&nbsp; Don't spoil it by reading the comments first.</p>\n<p>.</p>\n<p>.</p>\n<p>.</p>\n<p>.</p>\n<p>.</p>\n<p>.</p>\n<p>.</p>\n<p>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JsJPrdgRGRqnci8cZ": 2, "qAvbtzdG2A2RBn7in": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Dc3bwCjM9HzZcq9M8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 12, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "83", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 107, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-13T01:15:30.208Z", "modifiedAt": null, "url": null, "title": "A Sense That More Is Possible", "slug": "a-sense-that-more-is-possible", "viewCount": null, "lastCommentedAt": "2020-07-08T19:01:07.614Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Nu3wa6npK4Ry66vFp/a-sense-that-more-is-possible", "pageUrlRelative": "/posts/Nu3wa6npK4Ry66vFp/a-sense-that-more-is-possible", "linkUrl": "https://www.lesswrong.com/posts/Nu3wa6npK4Ry66vFp/a-sense-that-more-is-possible", "postedAtFormatted": "Friday, March 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Sense%20That%20More%20Is%20Possible&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Sense%20That%20More%20Is%20Possible%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNu3wa6npK4Ry66vFp%2Fa-sense-that-more-is-possible%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Sense%20That%20More%20Is%20Possible%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNu3wa6npK4Ry66vFp%2Fa-sense-that-more-is-possible", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNu3wa6npK4Ry66vFp%2Fa-sense-that-more-is-possible", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1345, "htmlBody": "<p>To teach people about a topic you've labeled \"rationality\", it helps for them to be interested in \"rationality\".&nbsp; (There are less direct ways to teach people how to attain <a href=\"http://www.overcomingbias.com/2006/11/why_truth_and.html\">the map that reflects the territory</a>, or optimize reality according to their values; but the explicit method <em>is</em> the course I tend to take.)</p>\n<p>And when people explain why they're <em>not</em> interested in rationality, one of the most commonly proffered reasons tends to be like:&nbsp; \"Oh, I've known a couple of rational people and they didn't seem any happier.\"</p>\n<p>Who are they thinking of?&nbsp; Probably an <a href=\"http://www.overcomingbias.com/2007/12/ayn-rand.html\">Objectivist</a> or some such.&nbsp; Maybe someone they know who's an <a href=\"http://www.overcomingbias.com/2008/05/do-scientists-a.html\">ordinary scientist</a>.&nbsp; Or an <a href=\"/lw/1e/raising_the_sanity_waterline/\">ordinary atheist</a>.</p>\n<p>That's really <em>not </em>a whole lot of rationality, as I have previously said.</p>\n<p>Even if you limit yourself to people who can derive Bayes's Theorem&mdash;which is going to eliminate, what, 98% of the above personnel?&mdash;that's <em>still</em> not a whole lot of rationality.&nbsp; I mean, it's a pretty basic theorem.</p>\n<p>Since the beginning I've had a sense that there ought to be some discipline of cognition, some art of thinking, the studying of which would make its students visibly more competent, more formidable: the equivalent of <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/TookALevelInBadass\">Taking a Level in Awesome</a>.</p>\n<p>But when I look around me in the real world, I don't see that.&nbsp; Sometimes I see a hint, an echo, of what I think should be possible, when I read the writings of folks like Robyn Dawes, Daniel Gilbert, Tooby &amp; Cosmides.&nbsp; A few very rare and very senior researchers in psychological sciences, who visibly care a <em>lot</em> about rationality&mdash;to the point, I suspect, of making their colleagues feel uncomfortable, because it's not cool to care that much.&nbsp; I can see that they've found a rhythm, a unity that begins to pervade their arguments&mdash;</p>\n<p>Yet even that... isn't really a whole lot of rationality either.<a id=\"more\"></a></p>\n<p>Even among those whose few who impress me with a hint of dawning formidability&mdash;I don't think that their mastery of rationality could compare to, say, John Conway's mastery of math.&nbsp; The base knowledge that we drew upon to build our understanding&mdash;if you extracted only the parts we used, and not everything we had to study to find it&mdash;it's probably not comparable to what a professional nuclear engineer knows about nuclear engineering.&nbsp; It may not even be comparable to what a construction engineer knows about bridges.&nbsp; We practice our skills, we do, in the ad-hoc ways we taught ourselves; but that practice probably doesn't compare to the training regimen an Olympic runner goes through, or maybe even an ordinary professional tennis player.</p>\n<p>And the root of <em>this</em> problem, I do suspect, is that we haven't really gotten together and systematized our skills.&nbsp; We've had to create all of this for ourselves, ad-hoc, and there's a limit to how much one mind can do, even if it can manage to draw upon work done in outside fields.</p>\n<p>The chief obstacle to doing this the way it <em>really</em> should be done, is the difficulty of testing the <em>results </em>of rationality training programs, so you can have evidence-based training methods.&nbsp; I will write more about this, because I think that recognizing successful training and distinguishing it from failure is the essential, blocking obstacle.</p>\n<p>There are experiments done now and again on debiasing interventions for particular biases, but it tends to be something like, \"Make the students practice this for an hour, then test them two weeks later.\"&nbsp; Not, \"Run half the signups through version A of the three-month summer training program, and half through version B, and survey them five years later.\"&nbsp; You can see, here, the implied amount of effort that I think would go into a training program for people who were Really Serious about rationality, as opposed to the attitude of taking Casual Potshots That Require Like An Hour Of Effort Or Something.</p>\n<p>Daniel Burfoot brilliantly <a href=\"http://www.overcomingbias.com/2008/10/isshokenmei.html#comment-133847467\">suggests</a> that this is why intelligence seems to be such a big factor in rationality&mdash;that when you're improvising everything ad-hoc with very little training or systematic practice, intelligence ends up being the most important factor in what's left.</p>\n<p>Why aren't \"rationalists\" surrounded by a visible aura of formidability?&nbsp; Why aren't they found at the top level of every elite selected on any basis that has anything to do with thought?&nbsp; Why do most \"rationalists\" just seem like ordinary people, perhaps of moderately above-average intelligence, with one more hobbyhorse to ride?</p>\n<p>Of this there are several answers; but one of them, surely, is that they have received less systematic training of rationality in a less systematic context than a first-dan black belt gets in hitting people.</p>\n<p>I do not except myself from this criticism.&nbsp; I am no <a href=\"http://www.overcomingbias.com/2008/05/eld-science.html\">beisutsukai</a>, because there are limits to how much Art you can create on your own, and how well you can guess without evidence-based statistics on the results.&nbsp; I know about a <em>single </em>use of rationality, which might be termed \"reduction of confusing cognitions\".&nbsp; This I asked of my brain, this it has given me.&nbsp; There are other arts, I think, that a mature rationality training program would not neglect to teach, which would make me stronger and happier and more effective&mdash;if I could just go through a standardized training program using the cream of teaching methods experimentally demonstrated to be effective.&nbsp; But the kind of tremendous, focused effort that I put into creating my single <em>sub-art</em> of rationality from scratch&mdash;my life doesn't have room for more than one of those.</p>\n<p>I consider myself something more than a first-dan black belt, and less.&nbsp; I can <em>punch </em>through brick and I'm working on steel along my way to adamantine, but I have a mere casual street-fighter's grasp of how to kick or throw or block.</p>\n<p>Why are there schools of martial arts, but not <a href=\"http://www.overcomingbias.com/2006/11/the_martial_art.html\">rationality dojos</a>?&nbsp; (This was the first question I asked in my <a href=\"http://www.overcomingbias.com/2006/11/the_martial_art.html\">first blog post</a>.)&nbsp; Is it more important to hit people than to think?</p>\n<p>No, but it's easier to verify when you <em>have</em> hit someone.&nbsp; That's part of it, a highly central part.</p>\n<p>But maybe even more importantly&mdash;there are people out there who <em>want</em> to hit, and who have the idea that there ought to be a systematic art of hitting that makes you into a visibly more formidable fighter, with a speed and grace and strength beyond the struggles of the unpracticed.&nbsp; So they go to a school that promises to teach that.&nbsp; And that school exists because, long ago, some people had the sense that more was possible.&nbsp; And they got together and shared their techniques and practiced and formalized and practiced and developed the Systematic Art of Hitting.&nbsp; They pushed themselves that far because <em>they thought they should be awesome</em> and they were willing to put some <em>back</em> into it.</p>\n<p>Now&mdash;they <em>got</em> somewhere with that aspiration, unlike a thousand other aspirations of awesomeness that failed, because they could <em>tell </em>when they had hit someone; and the schools competed against each other regularly in realistic contests with clearly-defined winners.</p>\n<p>But before even that&mdash;there was first the aspiration, the <a href=\"http://www.overcomingbias.com/2007/03/tsuyoku_naritai.html\">wish to become stronger</a>, a sense that more was possible.&nbsp; A vision of a speed and grace and strength that they did not already possess, but <em>could</em> possess, <em>if</em> they were willing to put in a lot of work, that drove them to systematize and train and test.</p>\n<p>Why don't we have an Art of Rationality?</p>\n<p>Third, because current \"rationalists\" have trouble working in groups: of this I shall speak more.</p>\n<p>Second, because it is hard to verify success in training, or which of two schools is the stronger.</p>\n<p>But first, because people lack the sense that rationality is something that <em>should</em> be systematized and trained and tested like a martial art, that should have as much knowledge behind it as nuclear engineering, whose superstars should practice as hard as chess grandmasters, whose successful practitioners should be surrounded by an evident aura of awesome.</p>\n<p>And conversely they don't look at the <em>lack </em>of visibly greater formidability, and say, \"We must be doing something wrong.\"</p>\n<p>\"Rationality\" just seems like one more hobby or hobbyhorse, that people talk about at parties; an adopted mode of conversational <a href=\"http://www.overcomingbias.com/2007/08/belief-as-attir.html\">attire</a> with few or no real consequences; and it doesn't seem like there's anything wrong about that, either.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "5f5c37ee1b5cdee568cfb163": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Nu3wa6npK4Ry66vFp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 102, "baseScore": 121, "extendedScore": null, "score": 0.000182, "legacy": true, "legacyId": "84", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "pvim9PZJ6qHRTMqD3", "canonicalCollectionSlug": "rationality", "canonicalBookId": "e6WsPsivzBifrWHeA", "canonicalNextPostSlug": "epistemic-viciousness", "canonicalPrevPostSlug": "raising-the-sanity-waterline", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 121, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 218, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XqmjdBKa4ZaXJtNmf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 13, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-13T01:41:28.925Z", "modifiedAt": null, "url": null, "title": "Talking Snakes: A Cautionary Tale", "slug": "talking-snakes-a-cautionary-tale", "viewCount": null, "lastCommentedAt": "2021-06-12T19:25:25.144Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/atcJqdhCxTZiJSxo2/talking-snakes-a-cautionary-tale", "pageUrlRelative": "/posts/atcJqdhCxTZiJSxo2/talking-snakes-a-cautionary-tale", "linkUrl": "https://www.lesswrong.com/posts/atcJqdhCxTZiJSxo2/talking-snakes-a-cautionary-tale", "postedAtFormatted": "Friday, March 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Talking%20Snakes%3A%20A%20Cautionary%20Tale&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATalking%20Snakes%3A%20A%20Cautionary%20Tale%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FatcJqdhCxTZiJSxo2%2Ftalking-snakes-a-cautionary-tale%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Talking%20Snakes%3A%20A%20Cautionary%20Tale%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FatcJqdhCxTZiJSxo2%2Ftalking-snakes-a-cautionary-tale", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FatcJqdhCxTZiJSxo2%2Ftalking-snakes-a-cautionary-tale", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 754, "htmlBody": "<p>I particularly remember one scene from Bill Maher's \"<a href=\"http://www.religulousmovie.net/\">Religulous</a>\". I can't find the exact quote, but I will try to sum up his argument as best I remember.</p>\n<blockquote>\n<p>Christians believe that sin is caused by a talking snake. They may have billions of believers, thousands of years of tradition behind them, and a vast literature of apologetics justifying their faith - but when all is said and done, they're adults who believe in a talking snake.</p>\n</blockquote>\n<p>I have read of the absurdity heuristic. I know that it is not carte blanche to go around rejecting beliefs that seem silly. But I was still sympathetic to the talking snake argument. After all...a<em> talking snake</em>?</p>\n<p><a id=\"more\"></a></p>\n<p>I changed my mind in a Cairo cafe, talking to a young Muslim woman. I let it slip during the conversation that I was an atheist, and she seemed genuinely curious why. You've all probably been in such a situation, and you probably know how hard it is to choose just one reason, but I'd been reading about Biblical contradictions at the time and I mentioned the myriad errors and atrocities and contradictions in all the Holy Books.<br /><br />Her response? \"Oh, thank goodness it's that. I was afraid you were one of those crazies who believed that monkeys transformed into humans.\"<br /><br />I admitted that <a href=\"http://www.overcomingbias.com/2008/09/say-it-loud.html\">um, well, maybe I sorta kinda</a> might in fact believe that.<br /><br />It is hard for me to describe exactly the look of shock on her face, but I have no doubt that her horror was genuine. I may have been the first flesh-and-blood evolutionist she ever met. \"But...\" she looked at me as if I was an idiot. \"Monkeys don't change into humans. What on Earth makes you think monkeys can change into humans?\"<br /><br />I admitted that the whole process was rather complicated. I suggested that it wasn't exactly a Optimus Prime-style transformation so much as a gradual change over eons and eons. I recommended a few books on evolution that might explain it better than I could.<br /><br />She said that she respected me as a person but that quite frankly I could save my breath because there was no way any book could possibly convince her that monkeys have human babies or whatever sort of balderdash I was preaching. She accused me and other evolution believers of being too willing to accept absurdities, motivated by our atheism and our fear of the self-esteem hit we'd take by accepting Allah was greater than ourselves.<br /><br />It is not clear to me that this woman did anything differently than Bill Maher. Both heard statements that sounded so crazy as to not even merit further argument. Both recognized that there was a large group of people who found these statements plausible and had written extensive literature justifying them. Both decided that the statements were so absurd as to not merit examining that literature more closely. Both came up with reasons why they could discount the large number of believers because those believers must be biased.<br /><br />I post this as a cautionary tale as we <a href=\"/lw/26/rational_defense_of_irrational_beliefs/\">discuss the logic</a> or <a href=\"/lw/1e/raising_the_sanity_waterline/\">illogic</a> of theism. I propose taking from it the following lessons:<br /><br />- The <a href=\"http://www.overcomingbias.com/2007/09/absurdity-heuri.html\">absurdity heuristic</a> doesn't work very well.</p>\n<p>- Even on <a href=\"http://www.overcomingbias.com/2008/12/we-agree-get-froze.html\">things</a> <a href=\"http://www.overcomingbias.com/2008/05/timeless-physic.html\">that</a> <a href=\"http://www.overcomingbias.com/2008/06/the-quantum-phy.html\">sound</a> <a href=\"http://www.overcomingbias.com/2008/05/many-worlds-one.html\">really</a>, <a href=\"http://intelligence.org/\">really</a> <a href=\"/lw/20/the_apologist_and_the_revolutionary/14w#comments\">absurd</a>.</p>\n<p>- If a large number of intelligent people believe something, it deserves your attention. After you've studied it on its own terms, then you have a right to reject it. You could still be wrong, though.</p>\n<p>- Even if you can think of a good reason why people might be biased towards the silly idea, thus explaining it away, your good reason may still be false.</p>\n<p>- If someone cannot explain why something is not stupid to you over twenty minutes at a cafe, that doesn't mean it's stupid. It just means it's complicated, or they're not very good at explaining things.</p>\n<p>- There is no royal road.</p>\n<p><em>(special note to those prone to <a href=\"http://en.wikipedia.org/wiki/Fundamental_Attribution_Error\">fundamental attribution errors</a>: I do not accept theism. I think theism is wrong. I think it can be demonstrated to be wrong on logical grounds. I think the nonexistence of talking snakes is evidence against theism and can be worked into a general argument against theism. I just don't think it's as easy as saying \"talking snakes are silly, therefore theism is false.\" And I find it embarrassing when atheists say things like that, and then get called on it by intelligent religious people.)</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3RnEKrsNgNEDxuNnw": 2, "dJ6eJxJrCEget7Wb6": 2, "NSMKfa8emSbGNXRKD": 3, "5f5c37ee1b5cdee568cfb19d": 5}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "atcJqdhCxTZiJSxo2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 143, "baseScore": 163, "extendedScore": null, "score": 0.000247, "legacy": true, "legacyId": "85", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 165, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 232, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bjxo24nCFByCzkjdP", "XqmjdBKa4ZaXJtNmf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 14, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-13T06:36:54.862Z", "modifiedAt": null, "url": null, "title": "Boxxy and Reagan", "slug": "boxxy-and-reagan", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:03.438Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "sZ43ey6Lm3gzxvdSG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/96EhxPsNfr5FQYKsf/boxxy-and-reagan", "pageUrlRelative": "/posts/96EhxPsNfr5FQYKsf/boxxy-and-reagan", "linkUrl": "https://www.lesswrong.com/posts/96EhxPsNfr5FQYKsf/boxxy-and-reagan", "postedAtFormatted": "Friday, March 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Boxxy%20and%20Reagan&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABoxxy%20and%20Reagan%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F96EhxPsNfr5FQYKsf%2Fboxxy-and-reagan%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Boxxy%20and%20Reagan%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F96EhxPsNfr5FQYKsf%2Fboxxy-and-reagan", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F96EhxPsNfr5FQYKsf%2Fboxxy-and-reagan", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 12, "htmlBody": "<p><a href=\"http://forgetomori.com/2009/science/boxxy-for-president/\">An interesting article on forgetomori about Boxxy and Reagan and mirror neurons.</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"csMv9MvvjYJyeHqoo": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "96EhxPsNfr5FQYKsf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -5, "extendedScore": null, "score": 4.809627474811822e-07, "legacy": true, "legacyId": "86", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-13T15:17:19.701Z", "modifiedAt": null, "url": null, "title": "Reality vs Virtual Reality", "slug": "reality-vs-virtual-reality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:03.508Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cleonid", "createdAt": "2009-03-05T01:26:41.015Z", "isAdmin": false, "displayName": "cleonid"}, "userId": "Dh7Ax8Qp8bzp4xZBP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NcFFzMExY5DDJHRAg/reality-vs-virtual-reality", "pageUrlRelative": "/posts/NcFFzMExY5DDJHRAg/reality-vs-virtual-reality", "linkUrl": "https://www.lesswrong.com/posts/NcFFzMExY5DDJHRAg/reality-vs-virtual-reality", "postedAtFormatted": "Friday, March 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reality%20vs%20Virtual%20Reality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReality%20vs%20Virtual%20Reality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNcFFzMExY5DDJHRAg%2Freality-vs-virtual-reality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reality%20vs%20Virtual%20Reality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNcFFzMExY5DDJHRAg%2Freality-vs-virtual-reality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNcFFzMExY5DDJHRAg%2Freality-vs-virtual-reality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 402, "htmlBody": "<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: Times New Roman;\">Since the first days of civilization, humans have been known to entertain themselves with virtual reality games. The 6<sup>th</sup> century saw the birth of chess, a game where a few carved figures placed on a checkered board are supposed to mimic human social hierarchy. <span style=\"mso-spacerun: yes;\">&nbsp;</span>Later, the technological breakthroughs of the 20<sup>th</sup> century allowed&nbsp;creation of games which were significantly more sophisticated. For instance,&nbsp;the highly addictive &ldquo;Civilization&rdquo; allowed players to create a history for an entire nation, guiding it from the initial troubles of wheel invention and to the headaches of global warming. Here is a quick summary of the virtual reality games features.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; text-indent: -0.25in; mso-list: l0 level1 lfo1; tab-stops: list .5in;\"><span style=\"font-family: Times New Roman;\"><span style=\"mso-list: Ignore;\">1)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span>The &ldquo;reality&rdquo; of the game, while being superficially similar to the reality of the player, must at the same time be much simpler. Hence three-dimensional humans play in the two-dimensional world.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; text-indent: -0.25in; mso-list: l0 level1 lfo1; tab-stops: list .5in;\"><span style=\"font-family: Times New Roman;\"><span style=\"mso-list: Ignore;\">2)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span>The laws of the game must be largely deterministic to allow a meaningful intervention by the player. Yet, in order not to make it too predictable and hence boring, an element of chance must be introduced.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; text-indent: -0.25in; mso-list: l0 level1 lfo1; tab-stops: list .5in;\"><span style=\"font-family: Times New Roman;\"><span style=\"mso-list: Ignore;\">3)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span>The game protagonists must appear to have freedom of movement and yet be limited to the borders of the screen/allocated memory size. The limits of this virtual freedom are usually low at the early stages of the game, but grow as the scenario develops.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; text-indent: -0.25in; mso-list: l0 level1 lfo1; tab-stops: list .5in;\"><span style=\"font-family: Times New Roman;\"><span style=\"mso-list: Ignore;\">4)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span>The game scenario must end before it reaches the limit of the allocated resources.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: Times New Roman;\">I now propose a little Gedanken experiment.<strong><span style=\"mso-ansi-language: EN;\"> </span></strong>Imagine the existence of a four-dimensional world hosting a civilization whose technology is way ahead of ours. Is there a strong reason to think that such civilization is impossible or that members of this civilization would not play virtual reality games?<span style=\"mso-spacerun: yes;\">&nbsp; </span>If the answer is no, how may these games look like? Using the analogy with our own games, we might expect the following.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; text-indent: -0.25in; mso-list: l1 level1 lfo2; tab-stops: list .5in;\"><span style=\"font-family: Times New Roman;\"><span style=\"mso-list: Ignore;\">1)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span>The game protagonists would resemble the players. Yet, the need for simplification would require them to be three-dimensional.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; text-indent: -0.25in; mso-list: l1 level1 lfo2; tab-stops: list .5in;\"><span style=\"font-family: Times New Roman;\"><span style=\"mso-list: Ignore;\">2)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span>To satisfy the second rule we need to combine determinism and chance. In the three-dimensional universe quantum mechanics is known to do the trick.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; text-indent: -0.25in; mso-list: l1 level1 lfo2; tab-stops: list .5in;\"><span style=\"font-family: Times New Roman;\"><span style=\"mso-list: Ignore;\">3)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span>At the early stages of the game, protagonists&rsquo; freedom of movement is constrained by low technological development. At later stages a physical limit may be required (speed of light?).</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; text-indent: -0.25in; mso-list: l1 level1 lfo2; tab-stops: list .5in;\"><span style=\"font-family: Times New Roman;\"><span style=\"mso-list: Ignore;\">4)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp; </span></span>This point may have something to do with the Fermi Paradox.</span></p>\r\n<p><span style=\"font-family: Times New Roman;\">\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\">I&rsquo;m interested in other possible analogies. If somebody can suggest a way to rule out the whole idea that would be even greater.</p>\r\n</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"22z6XpWKqw3bNv4oR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NcFFzMExY5DDJHRAg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": -12, "extendedScore": null, "score": -1.6e-05, "legacy": true, "legacyId": "87", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-13T17:10:20.436Z", "modifiedAt": null, "url": null, "title": "Dialectical Bootstrapping", "slug": "dialectical-bootstrapping", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:03.536Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Johnicholas", "createdAt": "2009-02-27T15:01:52.708Z", "isAdmin": false, "displayName": "Johnicholas"}, "userId": "kBvTXutfPytNtzPyD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jeenaghE46m7Tttch/dialectical-bootstrapping", "pageUrlRelative": "/posts/jeenaghE46m7Tttch/dialectical-bootstrapping", "linkUrl": "https://www.lesswrong.com/posts/jeenaghE46m7Tttch/dialectical-bootstrapping", "postedAtFormatted": "Friday, March 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dialectical%20Bootstrapping&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADialectical%20Bootstrapping%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjeenaghE46m7Tttch%2Fdialectical-bootstrapping%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dialectical%20Bootstrapping%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjeenaghE46m7Tttch%2Fdialectical-bootstrapping", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjeenaghE46m7Tttch%2Fdialectical-bootstrapping", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 134, "htmlBody": "<p>\"Dialectical Bootstrapping\" is a simple procedure that may improve your estimates. This is how it works:</p>\n<ol>\n<li>Estimate the number in whatever manner you usually would estimate. Write that down.</li>\n<li>Assume your first estimate is off the mark.</li>\n<li>Think about a few reasons why that could be. Which assumptions and considerations could have been wrong?</li>\n<li>What do these new considerations imply? Was the first estimate rather too high or too low? </li>\n<li>Based on this new perspective, make a second, alternative estimate.</li>\n</ol>\n<p><a href=\"http://www.psycho.unibas.ch/fakultaet/angewandt/articles/HerzogHertwig_DialecticalBootstrapping.pdf\">Herzog and Hertwig</a> find that average of the two estimates (in a historical-date estimating task) is more accurate than the first estimate, (<strong>Edit</strong>: or the average of two estimates without the \"assume you're wrong\" manipulation). To put the finding in a OB/LW-centric manner, this procedure (sometimes, partially) avoids <a href=\"http://www.overcomingbias.com/2007/10/cached-thoughts.html\">Cached Thoughts</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AeqCtS3BaY3cwzKAs": 1, "8daMDi9NEShyLqxth": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jeenaghE46m7Tttch", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 22, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "88", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-13T20:45:41.691Z", "modifiedAt": null, "url": null, "title": "Is Santa Real?", "slug": "is-santa-real", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:43.955Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "thomblake", "createdAt": "2009-02-27T15:35:08.282Z", "isAdmin": false, "displayName": "thomblake"}, "userId": "zCHE6bXWKB6kfJsJS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iQRA6mMrxrs3xPhGz/is-santa-real", "pageUrlRelative": "/posts/iQRA6mMrxrs3xPhGz/is-santa-real", "linkUrl": "https://www.lesswrong.com/posts/iQRA6mMrxrs3xPhGz/is-santa-real", "postedAtFormatted": "Friday, March 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20Santa%20Real%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20Santa%20Real%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiQRA6mMrxrs3xPhGz%2Fis-santa-real%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20Santa%20Real%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiQRA6mMrxrs3xPhGz%2Fis-santa-real", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiQRA6mMrxrs3xPhGz%2Fis-santa-real", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 259, "htmlBody": "<p><strong style=\"font-weight: bold;\">Related on OB</strong>: <a title=\"OB: Lying to Kids\" href=\"http://www.overcomingbias.com/2008/05/lying-to-kids.html\">Lying to Kids</a> <a title=\"The Third Alternative\" href=\"http://www.overcomingbias.com/2007/05/the_third_alter.html\">The Third Alternative</a></p>\n<p>My wife and I are planning to have kids, so of course we've been going through the usual sorts of debates regarding upbringing. We wondered briefly, will we raise our children as atheists? It's kindof a cruel experiment, as folks tend to use their own experiences to guide raising children, and both of us were raised Catholic. Nonetheless, it was fairly well settled after about 5 minutes of dialogue that atheist was the way to go.</p>\n<p>Then we had the related discussion of whether to teach our children about Santa Claus. After hours of debate, we decided we'd both have to think on the question some more. It's still been an open question for years now.</p>\n<p>Should we teach kids that Santa Claus exists? This isn't a new question, by any means. But it's now motivated by <a title=\"Tell your Rationalist Origin Story\" href=\"/lw/2/tell_your_rationalist_origin_story/\">this thread</a> about rationalist origin stories. Note that many of the posters mark the 'rationalist awakening' as the time they realized God doesn't exist. The shock that everybody, including their parents, were wrong and/or lying to them was enough to motivate them to pursue rationality and truth.</p>\n<p>If those same children were never taught about God, Santa Claus, and other falsehoods, would they have become rationalists, or would they have contented themselves with playing better video games? &nbsp;If the child never realized there's no Santa Claus, would we have a reason to say, <a title=\"Mommy, Is Santa Real?\" href=\"http://www.quazen.com/Kids-and-Teens/Mommy,-Is-Santa-Real.10507\">\"You're growing up and I'm proud of you\"</a>?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q55STnFh6gbSezRuR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iQRA6mMrxrs3xPhGz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 19, "extendedScore": null, "score": 4.810848201808006e-07, "legacy": true, "legacyId": "89", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 75, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BHMBBFupzb4s8utts"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-13T23:33:48.785Z", "modifiedAt": "2020-06-02T18:28:34.889Z", "url": null, "title": "Epistemic Viciousness", "slug": "epistemic-viciousness", "viewCount": null, "lastCommentedAt": "2020-12-14T10:58:03.896Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/T8ddXNtmNSHexhQh8/epistemic-viciousness", "pageUrlRelative": "/posts/T8ddXNtmNSHexhQh8/epistemic-viciousness", "linkUrl": "https://www.lesswrong.com/posts/T8ddXNtmNSHexhQh8/epistemic-viciousness", "postedAtFormatted": "Friday, March 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Epistemic%20Viciousness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEpistemic%20Viciousness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT8ddXNtmNSHexhQh8%2Fepistemic-viciousness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Epistemic%20Viciousness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT8ddXNtmNSHexhQh8%2Fepistemic-viciousness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT8ddXNtmNSHexhQh8%2Fepistemic-viciousness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 765, "htmlBody": "<p>Someone deserves a large hattip for this, but I'm having trouble remembering who; my records don't seem to show any email or OB comment which told me of this 12-page essay, \"<a href=\"http://www.artsci.wustl.edu/~grussell/epistemicviciousness.pdf \">Epistemic Viciousness in the Martial Arts</a>\" by Gillian Russell.&nbsp; Maybe Anna Salamon?</p>\n<blockquote>\n<p style=\"padding-left: 30px;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We all lined up in our ties and sensible shoes (this was England) and copied him&mdash;left, right, left, right&mdash;and afterwards he told us that if we practised in the air with sufficient devotion for three years, then we would be able to use our punches to kill a bull with one blow.<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I worshipped Mr Howard (though I would sooner have died than told him that) and so, as a skinny, eleven-year-old girl, I came to believe that if I practised, I would be able to kill a bull with one blow by the time I was fourteen.<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This essay is about epistemic viciousness in the martial arts, and this story illustrates just that. Though the word &lsquo;viciousness&rsquo; normally suggests deliberate cruelty and violence, I will be using it here with the more old-fashioned meaning, possessing of vices.</p>\n</blockquote>\n<p>It all generalizes <em>amazingly.</em>&nbsp; To summarize some of the key observations for how epistemic viciousness arises:</p>\n<ul>\n<li>The art, the dojo, and the sensei are seen as sacred.&nbsp; \"Having red toe-nails in the dojo is like going to church in a mini-skirt and halter-top...&nbsp; The students of other martial arts are talked about like they are practicing the wrong religion.\"</li>\n<li>If your teacher takes you aside and teaches you a special move and you practice it for 20 years, you have a large emotional investment in it, and you'll want to discard any incoming evidence against the move.</li>\n<li>Incoming students don't have much choice: a martial art can't be learned from a book, so they have to trust the teacher.</li>\n<li>Deference to famous historical masters.&nbsp; \"Runners think that the contemporary staff of Runner's World know more about running than than all the ancient Greeks put together.&nbsp; And it's not just running, or other physical activities, where history is kept in its place; the same is true in any well-developed area of study.&nbsp; It is not considered disrespectful for a physicist to say that Isaac Newton's theories are false...\"&nbsp; (Sound familiar?)</li>\n<li>\"We martial artists struggle with a kind of poverty&mdash;data-poverty&mdash;which makes our beliefs hard to test... Unless you're unfortunate enough to be fighting a hand-to-hand war you cannot <em>check </em>to see how much force and exactly which angle a neck-break requires...\"<a id=\"more\"></a></li>\n<li>\"If you can't test the effectiveness of a technique, then it is hard to test methods for improving the technique.&nbsp; Should you practice your nukite in the air, or will that just encourage you to overextend? ... Our inability to test our fighting methods restricts our ability to test our training methods.\"</li>\n<li>\"But the real problem isn&rsquo;t just that we live in data poverty&mdash;I think that&rsquo;s true for some perfectly respectable disciplines, including theoretical physics&mdash;the problem is that we live in poverty but continue to act as though we live in luxury, as though we can safely afford to believe whatever we&rsquo;re told...\"&nbsp; (<strong>+10!</strong>)</li>\n</ul>\n<p>One thing that I remembered being in this essay, but, on a second reading, wasn't actually there, was the degeneration of martial arts after the decline of real fights&mdash;by which I mean, fights where people were really trying to hurt each other and someone occasionally got killed.</p>\n<p>In those days, you had some idea of who the real masters were, and which school could defeat others.</p>\n<p>And then things got all <em>civilized</em>.&nbsp; And so things went downhill to the point that we have videos on Youtube of supposed Nth-dan black belts being pounded into the ground by someone with real fighting experience.</p>\n<p>I had one case of this bookmarked somewhere (but now I can't find the bookmark) that was <em>really</em> sad; it was a master of a school who was convinced he could use <em>ki</em> techniques.&nbsp; His students would actually fall over when he used ki attacks, a strange and remarkable and frightening case of self-hypnosis or <em>something...</em> and the master goes up against a skeptic and of course gets pounded completely into the floor.&nbsp; Feel free to comment this link if you know where it is.</p>\n<p>Truly is it said that \"how to not lose\" is more broadly applicable information than \"how to win\".&nbsp; Every single one of these risk factors transfers straight over to any attempt to start a \"rationality dojo\".&nbsp; I put to you the question:&nbsp; What can be done about it?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xgpBASEThXPuKRhbS": 1, "fH8jPjHF2R27sRTTG": 1, "NSMKfa8emSbGNXRKD": 1, "ALwRRZqvhaop8gxkT": 2, "5f5c37ee1b5cdee568cfb163": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "T8ddXNtmNSHexhQh8", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 77, "baseScore": 96, "extendedScore": null, "score": 0.000145, "legacy": true, "legacyId": "90", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "pvim9PZJ6qHRTMqD3", "canonicalCollectionSlug": "rationality", "canonicalBookId": "e6WsPsivzBifrWHeA", "canonicalNextPostSlug": "schools-proliferating-without-evidence", "canonicalPrevPostSlug": "a-sense-that-more-is-possible", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 96, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 93, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-03-13T23:33:48.785Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-13T23:48:50.617Z", "modifiedAt": null, "url": null, "title": "On the Care and Feeding of Young Rationalists", "slug": "on-the-care-and-feeding-of-young-rationalists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:34.867Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SWraogEDJ6gocpvwa/on-the-care-and-feeding-of-young-rationalists", "pageUrlRelative": "/posts/SWraogEDJ6gocpvwa/on-the-care-and-feeding-of-young-rationalists", "linkUrl": "https://www.lesswrong.com/posts/SWraogEDJ6gocpvwa/on-the-care-and-feeding-of-young-rationalists", "postedAtFormatted": "Friday, March 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20the%20Care%20and%20Feeding%20of%20Young%20Rationalists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20the%20Care%20and%20Feeding%20of%20Young%20Rationalists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSWraogEDJ6gocpvwa%2Fon-the-care-and-feeding-of-young-rationalists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20the%20Care%20and%20Feeding%20of%20Young%20Rationalists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSWraogEDJ6gocpvwa%2Fon-the-care-and-feeding-of-young-rationalists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSWraogEDJ6gocpvwa%2Fon-the-care-and-feeding-of-young-rationalists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 539, "htmlBody": "<p><strong>Related to:&nbsp;<span style=\"font-weight: normal;\"><a href=\"/lw/2h/is_santa_real/\">Is Santa Real?</a>,&nbsp;<a href=\"/lw/1e/raising_the_sanity_waterline/\">Raising the Sanity Waterline</a></span></strong></p>\n<p><strong>Related on OB:&nbsp;<span style=\"font-weight: normal;\"><a href=\"http://www.overcomingbias.com/2009/02/formative-youth.html\">Formative Youth</a></span></strong></p>\n<p><span style=\"font-family: Arial;\">JulianMorrison&nbsp;<span style=\"font-family: Verdana;\"><a href=\"/lw/1e/raising_the_sanity_waterline/16e#comments\">writes</a>:</span></span></p>\n<blockquote>\n<p style=\"margin: 0.0px 0.0px 13.0px 0.0px; text-align: justify; line-height: 19.0px; font: 13.0px Arial; background-color: #f7f7f8;\">If you want people to repeat this back, write it in a test, <em>maybe</em> even apply it in an academic context, a four-credit undergrad course will work.</p>\n<p style=\"margin: 0.0px 0.0px 13.0px 0.0px; text-align: justify; line-height: 19.0px; font: 13.0px Arial; background-color: #f7f7f8;\">If you want them to have it as the ground state of their mind in everyday life, you probably need to have taught them songs about it in kindergarten.</p>\n</blockquote>\n<p>There's been some discussion here on the formation of rationalist&nbsp;communities.</p>\n<p>If you look at any large, well-established religious community, you will see <a href=\"http://www.lds.org/hf/display/0,16783,4209-1,00.html\">an extraordinary amount of attention</a> being paid to the way children are raised. This is hardly surprising, since upbringing is <em>how new members enter the community</em>. Far more people become Mormons because they were raised by Mormon parents than because they had a long talk with two guys in white shirts.</p>\n<p>This doesn't seem to be the case in the rationalist community. Looking at&nbsp;<a href=\"/lw/2/tell_your_rationalist_origin_story/\">how we all got here</a>, I don't see all that many who were simply raised by rationalist parents, and had a leg up. Maybe this is a sampling bias -- a straightforward, enlightened upbringing is not as dramatic as a break from fundamentalist religion, and perhaps people don't think it's a story worth telling.</p>\n<p>But I don't think we can count out the possibility that we're just not doing the job of passing on our memes. I can even see a few reasons why this might be the case. In mixed marriages, it is usually the more devout parent, and especially the parent from the <em>more demanding</em>&nbsp;religion, who has the say in raising the children. Thus, we see spouses convert to Catholicism, to Orthodox&nbsp;Judaism, to Mormonism, but rarely to Congregationalism, or Unitarianism. On this totem pole, atheism seems to be pretty much at the bottom.</p>\n<p>And perhaps this also has something to do with the shyness of the atheist parent. Even in unmixed marriages, the unspoken thought seems to be \"so, our children will become atheists just because their parents were atheists, just as Baptist children become Baptist because their parents were Baptist -- are we really any better?\"&nbsp; We resent the idea of the religionists forcing their foolish memes on their children, and want ours to choose their own paths. We think if we just get out of the way, our children will do whatever is right for them, as though by doing this we could make them the&nbsp;<a href=\"http://www.overcomingbias.com/2008/06/the-ultimate-so.html\">ultimate source</a>&nbsp;of their lives.</p>\n<p>We have to make choices -- <a href=\"http://www.overcomingbias.com/2008/12/not-taking-over.html\" target=\"_self\">doing nothing is still a choice</a>. There is nothing wrong with attempting to light our children's way to wisdom which took us time and effort to locate on our own.</p>\n<p>So, best practices, resources, websites, books, songs, nusery rhymes; anything that will help us to raise rationalist children.<a id=\"more\"></a></p>\n<p>(As always, please post one suggestion per comment so that voting can represent an individual judgment of that suggestion.)</p>\n<p>(Disclaimer: I am not a parent, and am not remotely ready to become one. Nonetheless, I feel this could be a fruitful topic for discussion)</p>\n<p>ETA: I hope it doesn't look like I'm simply asking how to raise atheists -- I'm setting the bar a bit higher than that. I speak of atheist parents in the latter part of the post simply because I have no data on avowedly rationalist parents.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q55STnFh6gbSezRuR": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SWraogEDJ6gocpvwa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 30, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "77", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Related_to__Is_Santa_Real___Raising_the_Sanity_Waterline\">Related to:&nbsp;<span style=\"font-weight: normal;\"><a href=\"/lw/2h/is_santa_real/\">Is Santa Real?</a>,&nbsp;<a href=\"/lw/1e/raising_the_sanity_waterline/\">Raising the Sanity Waterline</a></span></strong></p>\n<p><strong id=\"Related_on_OB__Formative_Youth\">Related on OB:&nbsp;<span style=\"font-weight: normal;\"><a href=\"http://www.overcomingbias.com/2009/02/formative-youth.html\">Formative Youth</a></span></strong></p>\n<p><span style=\"font-family: Arial;\">JulianMorrison&nbsp;<span style=\"font-family: Verdana;\"><a href=\"/lw/1e/raising_the_sanity_waterline/16e#comments\">writes</a>:</span></span></p>\n<blockquote>\n<p style=\"margin: 0.0px 0.0px 13.0px 0.0px; text-align: justify; line-height: 19.0px; font: 13.0px Arial; background-color: #f7f7f8;\">If you want people to repeat this back, write it in a test, <em>maybe</em> even apply it in an academic context, a four-credit undergrad course will work.</p>\n<p style=\"margin: 0.0px 0.0px 13.0px 0.0px; text-align: justify; line-height: 19.0px; font: 13.0px Arial; background-color: #f7f7f8;\">If you want them to have it as the ground state of their mind in everyday life, you probably need to have taught them songs about it in kindergarten.</p>\n</blockquote>\n<p>There's been some discussion here on the formation of rationalist&nbsp;communities.</p>\n<p>If you look at any large, well-established religious community, you will see <a href=\"http://www.lds.org/hf/display/0,16783,4209-1,00.html\">an extraordinary amount of attention</a> being paid to the way children are raised. This is hardly surprising, since upbringing is <em>how new members enter the community</em>. Far more people become Mormons because they were raised by Mormon parents than because they had a long talk with two guys in white shirts.</p>\n<p>This doesn't seem to be the case in the rationalist community. Looking at&nbsp;<a href=\"/lw/2/tell_your_rationalist_origin_story/\">how we all got here</a>, I don't see all that many who were simply raised by rationalist parents, and had a leg up. Maybe this is a sampling bias -- a straightforward, enlightened upbringing is not as dramatic as a break from fundamentalist religion, and perhaps people don't think it's a story worth telling.</p>\n<p>But I don't think we can count out the possibility that we're just not doing the job of passing on our memes. I can even see a few reasons why this might be the case. In mixed marriages, it is usually the more devout parent, and especially the parent from the <em>more demanding</em>&nbsp;religion, who has the say in raising the children. Thus, we see spouses convert to Catholicism, to Orthodox&nbsp;Judaism, to Mormonism, but rarely to Congregationalism, or Unitarianism. On this totem pole, atheism seems to be pretty much at the bottom.</p>\n<p>And perhaps this also has something to do with the shyness of the atheist parent. Even in unmixed marriages, the unspoken thought seems to be \"so, our children will become atheists just because their parents were atheists, just as Baptist children become Baptist because their parents were Baptist -- are we really any better?\"&nbsp; We resent the idea of the religionists forcing their foolish memes on their children, and want ours to choose their own paths. We think if we just get out of the way, our children will do whatever is right for them, as though by doing this we could make them the&nbsp;<a href=\"http://www.overcomingbias.com/2008/06/the-ultimate-so.html\">ultimate source</a>&nbsp;of their lives.</p>\n<p>We have to make choices -- <a href=\"http://www.overcomingbias.com/2008/12/not-taking-over.html\" target=\"_self\">doing nothing is still a choice</a>. There is nothing wrong with attempting to light our children's way to wisdom which took us time and effort to locate on our own.</p>\n<p>So, best practices, resources, websites, books, songs, nusery rhymes; anything that will help us to raise rationalist children.<a id=\"more\"></a></p>\n<p>(As always, please post one suggestion per comment so that voting can represent an individual judgment of that suggestion.)</p>\n<p>(Disclaimer: I am not a parent, and am not remotely ready to become one. Nonetheless, I feel this could be a fruitful topic for discussion)</p>\n<p>ETA: I hope it doesn't look like I'm simply asking how to raise atheists -- I'm setting the bar a bit higher than that. I speak of atheist parents in the latter part of the post simply because I have no data on avowedly rationalist parents.&nbsp;</p>", "sections": [{"title": "Related to:\u00a0Is Santa Real?,\u00a0Raising the Sanity Waterline", "anchor": "Related_to__Is_Santa_Real___Raising_the_Sanity_Waterline", "level": 1}, {"title": "Related on OB:\u00a0Formative Youth", "anchor": "Related_on_OB__Formative_Youth", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "22 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iQRA6mMrxrs3xPhGz", "XqmjdBKa4ZaXJtNmf", "BHMBBFupzb4s8utts"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-14T02:11:15.177Z", "modifiedAt": null, "url": null, "title": "The Least Convenient Possible World", "slug": "the-least-convenient-possible-world", "viewCount": null, "lastCommentedAt": "2021-11-10T21:05:56.920Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/neQ7eXuaXpiYw7SBy/the-least-convenient-possible-world", "pageUrlRelative": "/posts/neQ7eXuaXpiYw7SBy/the-least-convenient-possible-world", "linkUrl": "https://www.lesswrong.com/posts/neQ7eXuaXpiYw7SBy/the-least-convenient-possible-world", "postedAtFormatted": "Saturday, March 14th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Least%20Convenient%20Possible%20World&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Least%20Convenient%20Possible%20World%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FneQ7eXuaXpiYw7SBy%2Fthe-least-convenient-possible-world%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Least%20Convenient%20Possible%20World%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FneQ7eXuaXpiYw7SBy%2Fthe-least-convenient-possible-world", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FneQ7eXuaXpiYw7SBy%2Fthe-least-convenient-possible-world", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1381, "htmlBody": "<p><strong>Related to: </strong><a href=\"http://www.overcomingbias.com/2008/12/your-true-rejec.html\">Is That Your True Rejection?</a></p>\n<p style=\"padding-left: 30px;\"><em>\"If you&rsquo;re interested in being on the right side of disputes, you will refute your opponents&rsquo; arguments.&nbsp; But if you&rsquo;re interested in producing truth, you will fix your opponents&rsquo; arguments for them.&nbsp; To win, you must fight not only the creature you encounter; you must fight the most horrible thing that can be constructed from its corpse.\"</em></p>\n<p style=\"padding-left: 30px;\"><em>&nbsp;&nbsp; --</em> <a href=\"http://www.acceleratingfuture.com/steven/?p=155\">Black Belt Bayesian</a>, via <a href=\"http://www.overcomingbias.com/2008/09/rationality-quo.html\">Rationality Quotes 13</a></p>\n<p>Yesterday <a href=\"/lw/2b/so_you_say_youre_an_altruist/\">John Maxwell's post</a> wondered <a href=\"http://www.aaronsw.com/weblog/handwritingwall\">how much the average person would do</a> to save ten people from a ruthless tyrant. I remember asking some of my friends a vaguely related question as part of an investigation of the <a href=\"http://en.wikipedia.org/wiki/Trolley_Problem\">Trolley Problems</a>:</p>\n<blockquote>\n<p>You are a doctor in a small rural hospital. You have ten patients, each of whom is dying for the lack of a separate organ; that is, one person needs a heart transplant, another needs a lung transplant, another needs a kidney transplant, and so on. A traveller walks into the hospital, mentioning how he has no family and no one knows that he's there. All of his organs seem healthy. You realize that by killing this traveller and distributing his organs among your patients, you could save ten lives. Would this be moral or not?</p>\n</blockquote>\n<p>I don't want to discuss the answer to this problem today. I want to discuss the answer one of my friends gave, because I think it illuminates a very interesting kind of defense mechanism that rationalists need to be watching for. My friend said:</p>\n<blockquote>\n<p>It wouldn't be moral. After all, people often reject organs from random donors. The traveller would probably be a genetic mismatch for your patients, and the transplantees would have to spend the rest of their lives on immunosuppressants, only to die within a few years when the drugs failed.</p>\n</blockquote>\n<p>On the one hand, I have to give my friend credit: his answer is biologically accurate, and beyond a doubt the technically correct answer to the question I asked. On the other hand, I don't have to give him very <em>much</em> credit: he completely missed the point and lost a valuable effort to examine the nature of morality.<br /><br />So I asked him, \"In the least convenient possible world, the one where everyone was genetically compatible with everyone else and this objection was invalid, what would you do?\"<br /><br />He mumbled something about counterfactuals and refused to answer. But I learned something very important from him, and that is to always ask this question of <em>myself</em>. Sometimes the least convenient possible world is the only place where I can figure out my true motivations, or which step to take next. I offer three examples:</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<p><strong>1:&nbsp; Pascal's Wager</strong>. Upon being presented with Pascal's Wager, one of the first things most atheists think of is this:</p>\n<p>Perhaps God values intellectual integrity so highly that He is prepared to reward honest atheists, but will punish anyone who practices a religion he does not truly believe simply for personal gain. Or perhaps, as the Discordians claim, \"Hell is reserved for people who believe in it, and the hottest levels of Hell are reserved for people who believe in it on the principle that they'll go there if they don't.\"<br /><br />This is a good argument against Pascal's Wager, but it isn't the least convenient possible world. The least convenient possible world is the one where Omega, the completely trustworthy superintelligence who is always right, informs you that God definitely doesn't value intellectual integrity that much. In fact (Omega tells you) either God does not exist or the Catholics are right about absolutely everything.<br /><br />Would you become a Catholic in this world? Or are you willing to admit that maybe your rejection of Pascal's Wager has less to do with a hypothesized pro-atheism God, and more to do with a belief that it's wrong to abandon your intellectual integrity on the off chance that a crazy deity is playing a perverted game of blind poker with your eternal soul?</p>\n<p><strong>2:</strong> <strong>The God-Shaped Hole. </strong>Christians claim there is one in every atheist, keeping him from spiritual fulfillment.<strong> <br /></strong></p>\n<p>Some commenters on <a href=\"/lw/1e/raising_the_sanity_waterline/\">Raising the Sanity Waterline</a> don't deny the existence of such a hole, if it is intepreted as a desire for purpose or connection to something greater than one's self. But, some commenters say, science and rationality can fill this hole even better than God can.<br /><br />What luck! Evolution has by a wild coincidence created us with a big rationality-shaped hole in our brains! Good thing we happen to be rationalists, so we can fill this hole in the best possible way! I don't know - despite my sarcasm this may even be true. But in the least convenient possible world, Omega comes along and tells you that sorry, the hole is exactly God-shaped, and anyone without a religion will lead a less-than-optimally-happy life. Do you head down to the nearest church for a baptism? Or do you admit that even if believing something makes you happier, you still don't want to believe it unless it's true?<br /><strong></strong></p>\n<p><strong>3</strong>: <strong>Extreme Altruism.</strong> John Maxwell mentions the utilitarian argument for donating almost everything to charity.</p>\n<p>Some commenters object that many forms of charity, especially the classic \"give to starving African orphans,\" are counterproductive, either because they enable dictators or thwart the free market. This is quite true.<br /><br />But in the least convenient possible world, here comes Omega again and tells you that Charity X has been proven to do exactly what it claims: help the poor without any counterproductive effects. So is your real objection the corruption, or do you just not believe that you're morally obligated to give everything you own to starving Africans?</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p>You may argue that this citing of convenient facts is at worst a venial sin. If you still get to the correct answer, and you do it by a correct method, what does it matter if this method isn't really the one that's convinced you personally?<br /><br />One easy answer is that it saves you from embarrassment later. If some scientist does a study and finds that people really do have a god-shaped hole that can't be filled by anything else, no one can come up to you and say \"Hey, didn't you say the reason you didn't convert to religion was because rationality filled the god-shaped hole better than God did? Well, I have some bad news for you...\"<br /><br />Another easy answer is that your real answer teaches you something about yourself. My friend may have successfully avoiding making a distasteful moral judgment, but he didn't learn anything about morality. My refusal to take the easy way out on the transplant question helped me develop the form of precedent-utilitarianism I use today.<br /><br />But more than either of these, it matters because it seriously influences where you go next.<br /><br />Say \"I accept the argument that I need to donate almost all my money to poor African countries, but my only objection is that corrupt warlords might get it instead\", and the <em>obvious</em> next step is to see if there's a poor African country without corrupt warlords (see: Ghana, Botswana, etc.) and donate almost all your money to them. Another acceptable answer would be to donate to another warlord-free charitable cause like the Singularity Institute.<br /><br />If you <em>just</em> say \"Nope, corrupt dictators might get it,\" you may go off and spend the money on a new TV. Which is fine, <em>if</em> a new TV is what you really want. But if you're the sort of person who <em>would have</em> been convinced by John Maxwell's argument, but you dismissed it by saying \"Nope, corrupt dictators,\" then you've lost an opportunity to change your mind.<br /><br />So I recommend: limit yourself to responses of the form \"I completely reject the entire basis of your argument\" or \"I accept the basis of your argument, but it doesn't apply to the real world because of contingent fact X.\" If you just say \"Yeah, well, contigent fact X!\" and walk away, you've left yourself too much wiggle room.<br /><br />In other words: always have a plan for what you would do in the least convenient possible world.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RE6h98Ziwcfh4EP9T": 10, "Ng8Gice9KNkncxqcj": 2, "rjEZWSbSffhaWYRvo": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "neQ7eXuaXpiYw7SBy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 229, "baseScore": 257, "extendedScore": null, "score": 0.000387, "legacy": true, "legacyId": "92", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 257, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 202, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Dc3bwCjM9HzZcq9M8", "XqmjdBKa4ZaXJtNmf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 14, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-14T07:51:04.675Z", "modifiedAt": null, "url": null, "title": "Closet survey #1", "slug": "closet-survey-1", "viewCount": null, "lastCommentedAt": "2018-10-21T18:09:21.997Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "sZ43ey6Lm3gzxvdSG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gPdM553fhdZuNmDxn/closet-survey-1", "pageUrlRelative": "/posts/gPdM553fhdZuNmDxn/closet-survey-1", "linkUrl": "https://www.lesswrong.com/posts/gPdM553fhdZuNmDxn/closet-survey-1", "postedAtFormatted": "Saturday, March 14th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Closet%20survey%20%231&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACloset%20survey%20%231%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgPdM553fhdZuNmDxn%2Fcloset-survey-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Closet%20survey%20%231%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgPdM553fhdZuNmDxn%2Fcloset-survey-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgPdM553fhdZuNmDxn%2Fcloset-survey-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 137, "htmlBody": "<p>What do you believe that most people on this site don't?</p>\n<p>I'm especially looking for things that you wouldn't even mention if someone wasn't explicitly asking for them. Stuff you're not even comfortable writing under your own name. Making a one-shot account here is very easy, go ahead and do that if you don't want to tarnish your image.</p>\n<p>I think a big problem with a \"community\" dedicated to being less wrong is that it will make people more concerned about APPEARING less wrong. The biggest part of my intellectual journey so far has been the acquisition of new and startling knowledge, and that knowledge doesn't seem likely to turn up here in the conditions that currently exist.</p>\n<p>So please, tell me the crazy things you're otherwise afraid to say. I want to know them, because they might be true.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "6Qic6PwwBycopJFNN": 1, "ABG8vt87eW4FFA6gD": 1, "kJrjorSx3hXa7q7CJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gPdM553fhdZuNmDxn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 58, "baseScore": 71, "extendedScore": null, "score": 0.000114, "legacy": true, "legacyId": "93", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 66, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 676, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-14T21:48:49.463Z", "modifiedAt": null, "url": null, "title": "Soulless morality", "slug": "soulless-morality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:45.454Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E5QXf3tCeE7fZGq4t/soulless-morality", "pageUrlRelative": "/posts/E5QXf3tCeE7fZGq4t/soulless-morality", "linkUrl": "https://www.lesswrong.com/posts/E5QXf3tCeE7fZGq4t/soulless-morality", "postedAtFormatted": "Saturday, March 14th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Soulless%20morality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASoulless%20morality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE5QXf3tCeE7fZGq4t%2Fsoulless-morality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Soulless%20morality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE5QXf3tCeE7fZGq4t%2Fsoulless-morality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE5QXf3tCeE7fZGq4t%2Fsoulless-morality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 904, "htmlBody": "<p>Follow-up to: <a href=\"/lw/2b/so_you_say_youre_an_altruist/\">So you say you're an altruist</a></p>\n<p>The responses to <a href=\"/lw/2b/so_you_say_youre_an_altruist/\">So you say you're an altruist</a> indicate that people have split their values into two categories:</p>\n<ol>\n<li>values they use to decide what they want</li>\n<li>values that are admissible for moral reasoning</li>\n</ol>\n<p>(where 2 is probably a subset of 1 for atheists, and probably nearly disjoint from 1 for Presbyterians).</p>\n<p>You're reading Less Wrong.&nbsp; You're a rationalist.&nbsp; You've put a lot of effort into education, and learning the truth about the world.&nbsp; You value knowledge and rationality and truth a lot.</p>\n<p>Someone says you should send all your money to Africa, because this will result in more human lives.</p>\n<p>What happened to the value you placed on knowledge and rationality?</p>\n<p>There is little chance that any of the people you save in Africa will get a good post-graduate education and then follow that up by rejecting religion, embracing rationality, and writing Less Wrong posts.</p>\n<p>Here you are, spending a part of your precious life reading Less Wrong.&nbsp; If you spend 10% of your life on the Web, you are saying that that activity is worth <em>at least</em> 1/10th of a life, and that lives with no access to the Web are worth less than lives with access.&nbsp; If you value rationality, then lives lived rationally are more valuable than lives lived irrationally.&nbsp; If you think something has a value, you have to give it the same value in every equation.&nbsp; Not doing so is immoral.&nbsp; You can't use different value scales for everyday and moral reasoning.</p>\n<p>Society tells you to work to make yourself more valuable.&nbsp; Then it tells you that when you reason morally, you must assume that all lives are equally valuable.&nbsp; You can't have it both ways.&nbsp; If all lives have equal value, we shouldn't criticize someone who decides to become a drug addict on welfare.&nbsp; Value is value, regardless of which equation it's in at the moment.<a id=\"more\"></a></p>\n<p>How do you weigh rationality, and your other qualities and activities, relative to life itself?&nbsp; I would say that life itself has zero value; the value of a life is the sum of the values of things done and experienced during that life.&nbsp; But society teaches the opposite: that mere life has a tremendous value, and anything you do with your life has negligible additional value.&nbsp; That's why it's controversial to execute criminals, but not controversial to lock them up in a bare room for 20 years.&nbsp; We have a death-penalty debate in the US, which has consequences for less than 100 people per year.&nbsp; We have a few hundred thousand people serving sentences of 20 years and up, but no debate about it.&nbsp; That shows that most Americans place a huge value on life itself, and almost no value on what happens to that life.</p>\n<p>I think this comes from believing in the soul, and binary thought in general.&nbsp; People want a simple moral system that classifies things as good or bad, allowable or not allowable, valuable or not valuable.&nbsp; We use real values in deciding what to do on Saturday, but we discretize them on Sunday.&nbsp; Killing people is not allowable; locking them up forever is.&nbsp; Killing enemy soldiers is allowable; killing enemy civilians is not.&nbsp; Killing enemy soldiers is allowable; torturing them is not.&nbsp; Losing a pilot is not acceptable; losing a $360,000,000 plane is.&nbsp; The results of this binarized thought include millions of lives wasted in prison; and hundreds of thousands of lives lost or ruined, and economies wrecked, because we fight wars in a way intended to avoid violating boundary constraints of a binarized value system rather than in a way intended to maximize our values.</p>\n<p>The idea of the soul is the ultimate discretizer.&nbsp; Saving souls is good.&nbsp; Losing souls is bad.&nbsp; That is the sum total of Christian pragmatic morality.</p>\n<p>The religious conception is that personal values that you use for deciding what to do on Saturday are selfish, whereas moral values are unselfish.&nbsp; It teaches that people need religion to be moral, because their natural inclination is to be selfish.&nbsp; Rather than having a single set of values that you can plug into your equations, you have two completely different systems of logic which counterbalance each other.&nbsp; No wonder people act schizophrenic on moral questions.</p>\n<p>What that worldview is really saying is that people are the wrong level of rationality.&nbsp; Rationality is a win for the rational agent.&nbsp; But in many prisoners-dilemma and tragedy-of-the-commons scenarios, having rational agents is not a win for society.&nbsp; Religion teaches people to replace rational morality with an irrational dual-system morality under the (hidden) theory that rational morality leads to worse outcomes.</p>\n<p>That teaching isn't obviously wrong.&nbsp; It isn't obviously irrational.&nbsp; But it is opposed to rationalism, the dogma that rationality always wins.&nbsp; I use the term \"rationalism\" to mean not just the reasonable assertion that rationality is the best policy for an agent, but also the dogmatic belief that rational agents are the best thing for society.&nbsp; And I think this blog is about giving fanatical rationalism a chance.</p>\n<p>So, if you really want to be rational, you should throw away your specialized moral logic, and use just one logic and one set of values for all decisions.&nbsp; If you decide to be a fanatic, you should tell other people to do so, too.</p>\n<p>EDIT: This is not an argument for or against aid to Africa.&nbsp; It's an observation on an error that I think people made in reasoning about aid to Africa.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E5QXf3tCeE7fZGq4t", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 25, "extendedScore": null, "score": 4.5e-05, "legacy": true, "legacyId": "96", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Dc3bwCjM9HzZcq9M8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-15T00:12:23.184Z", "modifiedAt": null, "url": null, "title": "The Skeptic's Trilemma", "slug": "the-skeptic-s-trilemma", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:28.480Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M7rwT264CSYY6EdR3/the-skeptic-s-trilemma", "pageUrlRelative": "/posts/M7rwT264CSYY6EdR3/the-skeptic-s-trilemma", "linkUrl": "https://www.lesswrong.com/posts/M7rwT264CSYY6EdR3/the-skeptic-s-trilemma", "postedAtFormatted": "Sunday, March 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Skeptic's%20Trilemma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Skeptic's%20Trilemma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM7rwT264CSYY6EdR3%2Fthe-skeptic-s-trilemma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Skeptic's%20Trilemma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM7rwT264CSYY6EdR3%2Fthe-skeptic-s-trilemma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM7rwT264CSYY6EdR3%2Fthe-skeptic-s-trilemma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1528, "htmlBody": "<p><strong>Followup to:</strong> <a href=\"/lw/2d/talking_snakes_a_cautionary_tale/\">Talking Snakes: A Cautionary Tale</a></p>\n<p><strong>Related to:</strong> <a href=\"http://www.overcomingbias.com/2007/09/explainworshipi.html\"><span style=\"text-decoration: underline;\">E</span>xplain, <span style=\"text-decoration: underline;\">W</span>orship, <span style=\"text-decoration: underline;\">I</span>gnore</a></p>\n<p>Skepticism is like sex and pizza: when it's good, it's very very good, and when it's bad, it's still pretty good.<br /><br />It really is hard to dislike skeptics. Whether or not their rational justifications are perfect, they are doing society a service by raising the social cost of holding false beliefs. But there is a failure mode for skepticism. It's the same as the failure mode for so many other things: it becomes a <a href=\"http://www.overcomingbias.com/2006/12/a_fable_of_scie.html\">blue vs. green</a> style tribe, <a href=\"http://www.overcomingbias.com/2007/02/politics_is_the.html\">demands support</a> of all 'friendly' arguments, enters an <a href=\"http://www.overcomingbias.com/2007/12/supercritical-u.html\">affective death spiral</a>, and collapses into a cult.<br /><br />What does it look like when skepticism becomes a cult? Skeptics become more interested in supporting their \"team\" and insulting the \"enemy\" than in finding the truth or convincing others. They begin to think \"If a assigning .001% probability to Atlantis and not accepting its existence without extraordinarily compelling evidence is good, then assigning 0% probability to Atlantis and refusing to even consider any evidence for its existence must be <em>great</em>!\" They begin to deny any evidence that seems pro-Atlantis, and cast aspersions on the character of anyone who produces it. They become anti-Atlantis fanatics.</p>\n<p>Wait a second. There is no lost continent of Atlantis. How do I know what a skeptic would do when confronted with evidence for it? For that matter, why do I care?<br /><a id=\"more\"></a></p>\n<p>Way back in 2007, Eliezer described the rationalist equivalent of Abort, Retry, Fail: the trilemma of <a href=\"http://www.overcomingbias.com/2007/09/explainworshipi.html\">Explain, Worship, Ignore</a>. Don't understand where rain comes from? You can try to <span style=\"text-decoration: underline;\">e</span>xplain it as part of the water cycle, although it might take a while. You can <span style=\"text-decoration: underline;\">w</span>orship it as the sacred mystery of the rain god. Or you can <span style=\"text-decoration: underline;\">i</span>gnore it and go on with your everyday life.<br /><br />So someone tells you that Plato, normally a pretty smart guy, wrote <a href=\"http://en.wikipedia.org/wiki/Critias_(dialogue)\">a long account</a> of a lost continent called Atlantis complete with a bunch of really specific geographic details that seem a bit excessive for a meaningless allegory. Plato claims to have gotten most of the details from a guy called <a href=\"http://en.wikipedia.org/wiki/Solon\">Solon</a>, legendary for his honesty, who got them from the Egyptians, who are known for their obsessive record-keeping. This seems interesting. But there's no evidence for a lost continent anywhere near the Atlantic Ocean, and geology tells us continents can't just go missing.<br /><br />One option is to hit <span style=\"text-decoration: underline;\">W</span>orship. Between the Theosophists, Edgar Cayce, the Nazis, and a bunch of well-intentioned but crazy amateurs including <a href=\"http://en.wikipedia.org/wiki/Ignatius_Donnelly\">a U.S. Congressman</a>, we get a supercontinent with technology far beyond our wildest dreams, littered with glowing crystal pyramids and powered by the peaceful and eco-friendly mystical wisdom of the ancients, source of all modern civilization and destined to rise again to herald the dawning of the Age of Aquarius.<br /><br />Or you could hit <span style=\"text-decoration: underline;\">I</span>gnore. I accuse the less pleasnt variety of skeptic of taking this option. Atlantis is stupid. Anyone who believes it is stupid. Plato was a dirty rotten liar. Any scientist who finds anomalous historical evidence suggesting a missing piece to the early history of the Mediterranean region is also a dirty rotten liar, motivated by crazy New Age beliefs, and should be fired. Anyone who talks about Atlantis is the Enemy, and anyone who denies Atlantis gains immediate access to our in-group and official Good Rational Scientific Person status.<br /><br /><a href=\"http://en.wikipedia.org/wiki/Spyridon_Marinatos\">Spyridon Marinatos</a>, a Greek archaeologist who really deserves more fame than he received, was a man who hit <span style=\"text-decoration: underline;\">E</span>xplain. The geography of Plato's Atlantis, a series of concentric circles of land and sea, had been derided as fanciful; Marinatos noted<sup>1</sup> that it matched the geography of the Mediterranean island of Santorini quite closely. He also noted that Santorini had a big volcano right in the middle and seemed somehow linked to the Minoan civilization, a glorious race of seafarers who had mysteriously collapsed a thousand years before Plato. So he decided to go digging in Santorini. And he found...<br /><br />...the lost city of Atlantis. Well, I'm making an assumption here. But <a href=\"http://en.wikipedia.org/wiki/Akrotiri_(Santorini)\">the city he found</a> was over four thousand years old, had a population of over ten thousand people at its peak, boasted three-story buildings and astounding works of art, and had hot and cold running water - an unheard-of convenience that it shared with the city in Plato's story. For the Early Bronze Age, that's <em>darn</em>ed impressive. And like Plato's Atlantis, it was destroyed in a single day. The volcano that loomed only a few miles from its center <a href=\"http://en.wikipedia.org/wiki/Minoan_eruption\">went off around 1600 BC</a>, utterly burying it and destroying its associated civilization. No one knows what happened to the survivors, but the most popular theory is that some fled to Egypt<sup>2</sup>, with which the city had flourishing trade routes at its peak.<br /><br />The Atlantis = Santorini equivalence is still controversial, and the point of this post isn't to advocate for it. But just look at the difference between Joe Q. Skeptic and Dr. Marinatos. Both were rightly skeptical of the crystal pyramid story erected by the Atlantis-worshippers. But Joe Q. Skeptic considered the whole issue a nuisance, or at best a way of proving his intellectual superiority over the believers. Dr. Marinatos saw an honest mystery, developed a theory that made testable predictions, then went out and started digging.<br /><br />The fanatical skeptic, when confronted with some evidence for a seemingly paranormal claim, says \"Wow, that's stupid.\" It's a soldier on the opposing side, and the only thing to be done with it is kill it as quickly as possible. The wise skeptic, when confronted with the same evidence, says \"Hmmm, that's interesting.\"<br /><br />Did people at Roswell discovered the debris of a strange craft made of seemingly otherworldly material lying in a field, only to be silenced by the government later? You can <span style=\"text-decoration: underline;\">w</span>orship the mighty aliens who are cosmic bringers of peace. You can <span style=\"text-decoration: underline;\">i</span>gnore it, because UFOs don't exist so the people are clearly lying. Or you can search for an <span style=\"text-decoration: underline;\">e</span>xplanation until you find that the government was conducting tests of <a href=\"http://en.wikipedia.org/wiki/Air_Force_reports_on_the_Roswell_UFO_incident\">Project Mogul</a> in that very spot.<br /><br />Do thousands of people claim that therapies with no scientific basis are working? You can <span style=\"text-decoration: underline;\">w</span>orship alternative medicine as a natural and holistic alternative to stupid evil materialism. You can <span style=\"text-decoration: underline;\">i</span>gnore all the evidence for their effectiveness. Or you can shut up and discover the placebo effect, <span style=\"text-decoration: underline;\">e</span>xplaining the lot of them in one fell swoop.<br /><br />Does someone claim to see tiny people, perhaps elves, running around and doing elvish things? You can call them lares and <span style=\"text-decoration: underline;\">w</span>orship them as household deities. You can <span style=\"text-decoration: underline;\">i</span>gnore the person because he's an obvious crank. Or you can go to a neurologist, and he'll <span style=\"text-decoration: underline;\">e</span>xplain that the person's probably suffering from <a href=\"http://en.wikipedia.org/wiki/Charles_Bonnet_syndrome\">Charles Bonnet</a> Syndrome.<br /><br />All unexplained phenomena are real. That is, they're real unexplained phenomena. The explanation may be prosaic, like that people are gullible. Or it may be an entire four thousand year old lost city of astounding sophistication. But even \"people are gullible\" can be an interesting explanation if you're smart enough to make it one. There's a big difference between \"people are gullible, so they believe in stupid things like religion, let's move on\" and a complete list of the cognitive biases that make explanations involving agency and intention more attractive than naturalistic explanations to a naive human mind. A <a href=\"http://www.overcomingbias.com/2008/05/faster-than-ein.html\">sufficiently intelligent</a> thinker could probably reason from the mere existence of religion all the way back to the fundamentals of evolutionary psychology.<br /><br />This I consider a specific application of a more general rationalist technique: not prematurely dismissing things that go against your worldview. There's a big difference between dismissing that whole Lost Continent of Atlantis story, and <em>prematurely</em> dismissing it. It's the difference between discovering an ancient city and resting smugly satisfied that you don't have to.</p>\n<p>&nbsp;</p>\n<p><strong>Footnotes</strong></p>\n<p><strong>1</strong>: I may be unintentionally sexing up the story here. I read a book on Dr. Marinatos a few years ago, and I know he did make the Santorini-Atlantis connection, but I don't remember whether he made it before starting his excavation, or whether it only clicked during the dig (and the Internet is silent on the matter). If it was the latter, all of my moralizing about how wonderful it was that he made a testable prediction falls a bit flat. I should have used another example where I knew for sure, but this story was too perfect. Mea culpa.</p>\n<p><strong>2: </strong>I don't include it in the main article because it is highly controversial and you have to fudge some dates for it to really work out, but here is a Special Bonus Scientific Explanation of a Paranormal Claim: the eruption of this same supervolcano in 1600 BC <a href=\"http://en.wikipedia.org/wiki/Minoan_eruption#Biblical_traditions\">caused</a> the series of geologic and climatological catastrophes recorded in the Bible as the Ten Plagues of Egypt. However, I specify that I'm including this because it's fun to think about rather than because there's an especially large amount of evidence for it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZpG9rheyAkgCoEQea": 1, "bY5MaF2EATwDkomvu": 1, "xgpBASEThXPuKRhbS": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M7rwT264CSYY6EdR3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 81, "baseScore": 80, "extendedScore": null, "score": 0.000136, "legacy": true, "legacyId": "97", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 80, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["atcJqdhCxTZiJSxo2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-15T06:43:28.509Z", "modifiedAt": null, "url": null, "title": "Schools Proliferating Without Evidence", "slug": "schools-proliferating-without-evidence", "viewCount": null, "lastCommentedAt": "2020-12-12T15:15:20.978Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JnKCaGcgZL4Rsep8m/schools-proliferating-without-evidence", "pageUrlRelative": "/posts/JnKCaGcgZL4Rsep8m/schools-proliferating-without-evidence", "linkUrl": "https://www.lesswrong.com/posts/JnKCaGcgZL4Rsep8m/schools-proliferating-without-evidence", "postedAtFormatted": "Sunday, March 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Schools%20Proliferating%20Without%20Evidence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASchools%20Proliferating%20Without%20Evidence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJnKCaGcgZL4Rsep8m%2Fschools-proliferating-without-evidence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Schools%20Proliferating%20Without%20Evidence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJnKCaGcgZL4Rsep8m%2Fschools-proliferating-without-evidence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJnKCaGcgZL4Rsep8m%2Fschools-proliferating-without-evidence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 769, "htmlBody": "<p>Robyn Dawes, author of one of the original papers from <em>Judgment Under Uncertainty</em> and of the book <em>Rational Choice in an Uncertain World</em>&mdash;one of the few who tries really hard to import the results to real life&mdash;is also the author of <em>House of Cards: Psychology and Psychotherapy Built on Myth.</em></p>\n<p>From <em>House of Cards, </em>chapter 1:</p>\n<blockquote>\n<p style=\"padding-left: 30px;\">The ability of these professionals has been subjected to empirical scrutiny&mdash;for example, their effectiveness as therapists (Chapter 2), their insight about people (Chapter 3), and the relationship between how well they function and the amount of experience they have had in their field (Chapter 4).&nbsp; Virtually all the research&mdash;and this book will reference more than three hundred empirical investigations and summaries of investigations&mdash;has found that these professionals' claims to superior intuitive insight, understanding, and skill as therapists are simply invalid...</p>\n</blockquote>\n<p>Remember Rorschach ink-blot tests?&nbsp; It's such an appealing argument: the patient looks at the ink-blot and says what he sees, the psychotherapist interprets their psychological state based on this.&nbsp; There've been hundreds of experiments looking for some evidence that it actually works.&nbsp; Since you're reading this, you can guess the answer is simply \"No.\"&nbsp; Yet the Rorschach is still in use.&nbsp; It's just such a <em>good story</em> that psychotherapists just can't bring themselves to believe the vast mounds of experimental evidence saying it doesn't work&mdash;</p>\n<p>&mdash;which tells you what sort of field we're dealing with here.</p>\n<p>And the experimental results on the field as a whole are commensurate.&nbsp; Yes, patients who see psychotherapists have been known to get better faster than patients who simply do nothing.&nbsp; But there is no statistically discernible difference between the many schools of psychotherapy.&nbsp; There is no discernible gain from years of expertise.</p>\n<p>And there's also no discernible difference between seeing a psychotherapist and spending the same amount of time talking to a randomly selected college professor from another field.&nbsp; It's just talking to <em>anyone </em>that helps you get better, apparently.</p>\n<p><em>In the entire absence of the slightest experimental evidence for their effectiveness,</em> psychotherapists became licensed by states, their testimony accepted in court, their teaching schools accredited, and their bills paid by health insurance.</p>\n<p>And there was also a huge proliferation of \"schools\", of traditions of practice, in psychotherapy; despite&mdash;or perhaps <em>because</em> of&mdash;the lack of any experiments showing that one school was better than another...<a id=\"more\"></a></p>\n<p>I should really post more some other time on all the sad things this says about our world; about how <em>the essence of medicine,</em> as recognized by society and the courts, is not a repertoire of procedures with statistical evidence for their healing effectiveness; but, rather, the right air of authority.</p>\n<p>But the subject today is the proliferation of traditions in psychotherapy.&nbsp; So far as I can discern, this was the way you picked up prestige in the field&mdash;not by discovering an amazing new technique whose effectiveness could be experimentally verified and adopted by all; but, rather, by splitting off your own \"school\", supported by your charisma as founder, and by the good stories you told about all the reasons your techniques <em>should</em> work.</p>\n<p>This was probably, to no small extent, responsible for the existence and continuation of psychotherapy in the first place&mdash;the promise of making yourself a Master, like Freud who'd done it first (also without the slightest scrap of experimental evidence).&nbsp; That's the brass ring of success to chase&mdash;the prospect of being a guru and having your own adherents.&nbsp; It's the struggle for adherents that <a href=\"http://www.overcomingbias.com/2009/03/loving-cranks-to-death.html\">keeps the clergy vital</a>.</p>\n<p>That's what happens to a field when it unbinds itself from the experimental evidence&mdash;though there were other factors that also placed psychotherapists at risk, such as the deference shown them by their patients, the wish of society to believe that mental healing was possible, and, of course, <a href=\"http://www.overcomingbias.com/2007/12/cultish-counter.html\">the general dangers of telling people how to think</a>.</p>\n<p>The field of hedonic psychology (happiness studies) began, to some extent, with the realization that you could <em>measure</em> happiness&mdash;that there was a family of measures that by golly did validate well against each other.</p>\n<p>The act of creating a new measurement creates new science; if it's a <em>good </em>measurement, you get good science.</p>\n<p>If you're going to create an organized practice of anything, you really do need some way of telling how well you're doing, and a practice of doing serious testing&mdash;that means a control group, an experimental group, and statistics&mdash;on plausible-sounding techniques that people come up with.&nbsp; You <em>really</em> need it.</p>\n<p><strong>Added</strong>:&nbsp; Dawes wrote in the 80s and I know that the Rorschach was still in use as recently as the 90s, but it's possible matters have improved since then (as one commenter states).&nbsp; I do remember hearing that there was positive evidence for the greater effectiveness of cognitive-behavioral therapy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "5f5c37ee1b5cdee568cfb163": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JnKCaGcgZL4Rsep8m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 63, "baseScore": 56, "extendedScore": null, "score": 8.7e-05, "legacy": true, "legacyId": "91", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "pvim9PZJ6qHRTMqD3", "canonicalCollectionSlug": "rationality", "canonicalBookId": "e6WsPsivzBifrWHeA", "canonicalNextPostSlug": "3-levels-of-rationality-verification", "canonicalPrevPostSlug": "epistemic-viciousness", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 56, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 60, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-15T06:51:34.773Z", "modifiedAt": null, "url": null, "title": "Really Extreme Altruism", "slug": "really-extreme-altruism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:34.515Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CronoDAS", "createdAt": "2009-02-27T04:42:19.587Z", "isAdmin": false, "displayName": "CronoDAS"}, "userId": "Q2oaNonArzibx5cQN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W3XpQDTEkaPQAuvHz/really-extreme-altruism", "pageUrlRelative": "/posts/W3XpQDTEkaPQAuvHz/really-extreme-altruism", "linkUrl": "https://www.lesswrong.com/posts/W3XpQDTEkaPQAuvHz/really-extreme-altruism", "postedAtFormatted": "Sunday, March 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Really%20Extreme%20Altruism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReally%20Extreme%20Altruism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW3XpQDTEkaPQAuvHz%2Freally-extreme-altruism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Really%20Extreme%20Altruism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW3XpQDTEkaPQAuvHz%2Freally-extreme-altruism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW3XpQDTEkaPQAuvHz%2Freally-extreme-altruism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<p>In secret, an unemployed man with poor job prospects uses his savings to buy a large term life insurance policy, and designates <a href=\"http://www.givewell.org/charities/top-charities\">a charity</a> as the beneficiary. Two years after the policy is purchased, it will pay out in the event of suicide. The man waits the required two years, and then kills himself, much to the dismay of his surviving relatives. The charity receives the money and saves the lives of many people who would otherwise have died.</p>\n<p>Are the actions of this man admirable or shameful?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JsJPrdgRGRqnci8cZ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W3XpQDTEkaPQAuvHz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 15, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "99", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 98, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-15T14:48:29.060Z", "modifiedAt": null, "url": null, "title": "Storm by Tim Minchin", "slug": "storm-by-tim-minchin", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:02.572Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Z8vv4F2vQbXsQiXpW/storm-by-tim-minchin", "pageUrlRelative": "/posts/Z8vv4F2vQbXsQiXpW/storm-by-tim-minchin", "linkUrl": "https://www.lesswrong.com/posts/Z8vv4F2vQbXsQiXpW/storm-by-tim-minchin", "postedAtFormatted": "Sunday, March 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Storm%20by%20Tim%20Minchin&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStorm%20by%20Tim%20Minchin%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ8vv4F2vQbXsQiXpW%2Fstorm-by-tim-minchin%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Storm%20by%20Tim%20Minchin%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ8vv4F2vQbXsQiXpW%2Fstorm-by-tim-minchin", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZ8vv4F2vQbXsQiXpW%2Fstorm-by-tim-minchin", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 41, "htmlBody": "<p>I'm sure many of you have already seen this performance. <a href=\"http://www.youtube.com/watch?v=UB_htqDCP-s\">Tim Minchin's beat poem \"Storm\"</a> is about the sceptical, secular understanding of the world, stupidity of quackery and supernatural, weight of dishonesty, and joy in the merely real. Contains strong language.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AXhEhCkTrHZbjXXu3": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Z8vv4F2vQbXsQiXpW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 17, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "101", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-15T17:19:14.736Z", "modifiedAt": "2020-07-13T03:56:08.048Z", "url": null, "title": "3 Levels of Rationality Verification", "slug": "3-levels-of-rationality-verification", "viewCount": null, "lastCommentedAt": "2022-02-23T23:30:19.918Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5K7CMa6dEL7TN7sae/3-levels-of-rationality-verification", "pageUrlRelative": "/posts/5K7CMa6dEL7TN7sae/3-levels-of-rationality-verification", "linkUrl": "https://www.lesswrong.com/posts/5K7CMa6dEL7TN7sae/3-levels-of-rationality-verification", "postedAtFormatted": "Sunday, March 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%203%20Levels%20of%20Rationality%20Verification&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A3%20Levels%20of%20Rationality%20Verification%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5K7CMa6dEL7TN7sae%2F3-levels-of-rationality-verification%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=3%20Levels%20of%20Rationality%20Verification%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5K7CMa6dEL7TN7sae%2F3-levels-of-rationality-verification", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5K7CMa6dEL7TN7sae%2F3-levels-of-rationality-verification", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 884, "htmlBody": "<p>I strongly suspect that there is a possible art of rationality (attaining the map that reflects the territory, choosing so as to direct reality into regions high in your preference ordering) which goes beyond the skills that are standard, and beyond what any single practitioner singly knows.&nbsp; I have <a href=\"/lw/2c/a_sense_that_more_is_possible/\">a sense that more is possible</a>.</p>\n<p>The degree to which a <em>group </em>of people can do anything useful about this, will depend <em>overwhelmingly </em>on what methods we can devise to <em>verify</em> our many amazing good ideas.</p>\n<p>I suggest stratifying verification methods into 3 levels of usefulness:</p>\n<ul>\n<li>Reputational</li>\n<li>Experimental</li>\n<li>Organizational</li>\n</ul>\n<p>If your martial arts master occasionally fights realistic duels (ideally, <em>real </em>duels) against the masters of other schools, and wins or at least doesn't lose too often, then you know that the master's <em>reputation </em>is <em>grounded in reality;</em> you know that your master is not a complete poseur.&nbsp; The same would go if your school regularly competed against other schools.&nbsp; You'd be <em>keepin' it real.</em></p>\n<p>Some martial arts fail to compete realistically enough, and their students go down in seconds against real streetfighters.&nbsp; Other martial arts schools fail to compete at <em>all</em>&mdash;except based on charisma and good stories&mdash;and their masters decide they have chi powers.&nbsp; In this latter class we can also place the <a href=\"/lw/2j/schools_proliferating_without_evidence/\">splintered schools of psychoanalysis</a>.</p>\n<p>So even just the basic step of trying to <em>ground reputations</em> in some realistic trial other than charisma and good stories, has tremendous positive effects on a whole field of endeavor.</p>\n<p>But that doesn't yet get you a science.&nbsp; A science requires that you be able to test 100 applications of method A against 100 applications of method B and run statistics on the results.&nbsp; <em>Experiments </em>have to be <a href=\"http://www.overcomingbias.com/2007/08/scientific-evid.html\">replicable</a> and replicated<em>.</em>&nbsp; This requires <em>standard measurements </em>that can be run on students who've been taught using randomly-assigned alternative methods, not just <em>realistic duels</em> fought between masters using all of their accumulated techniques and strength.<a id=\"more\"></a></p>\n<p>The field of happiness studies was created, more or less, by realizing that asking people \"On a scale of 1 to 10, how good do you feel right now?\" was a measure that statistically validated well against other ideas for measuring happiness.&nbsp; And this, despite all skepticism, looks like it's actually a pretty useful measure of some things, if you ask 100 people and average the results.</p>\n<p>But suppose you wanted to put happier people in positions of power&mdash;pay happy people to train other people to be happier, or employ the happiest at a hedge fund?&nbsp; Then you're going to need some test that's <em>harder to game </em>than just asking someone \"How happy are you?\"</p>\n<p>This question of verification methods good enough to build <em>organizations,</em> is a huge problem at all levels of modern human society.&nbsp; If you're going to use the SAT to control admissions to elite colleges, then can the SAT be defeated by studying <em>just</em> for the SAT in a way that ends up not correlating to other scholastic potential?&nbsp; If you give colleges the power to grant degrees, then do they have an incentive not to fail people?&nbsp; (I consider it drop-dead obvious that the task of verifying acquired skills and hence the power to grant degrees should be separated from the institutions that do the teaching, but let's not go into that.)&nbsp; If a hedge fund posts 20% returns, are they really that much better than the indices, or are they selling puts that will blow up in a down market?</p>\n<p>If you have a verification method that can be gamed, the whole field adapts to game it, and <a href=\"https://lesswrong.com/posts/sP2Hg6uPwpfp3jZJN/lost-purposes\">loses its purpose</a>.&nbsp; Colleges turn into tests of whether you can endure the classes.&nbsp; High schools do nothing but teach to statewide tests.&nbsp; Hedge funds sell puts to boost their returns.</p>\n<p>On the other hand&mdash;we still manage to teach engineers, even though our organizational verification methods aren't perfect.&nbsp; So what perfect or imperfect methods could you use for verifying rationality skills, that would be at least a <em>little</em> resistant to gaming?</p>\n<p>(Added:&nbsp; Measurements with high noise can still be used <em>experimentally, </em>if you randomly assign enough subjects to have an expectation of washing out the variance.&nbsp; But for the <em>organizational </em>purpose of verifying particular individuals, you need low-noise measurements.)</p>\n<p>So I now put to you the question&mdash;how do you verify rationality skills?&nbsp; At any of the three levels?&nbsp; Brainstorm, I beg you; even a difficult and expensive measurement can become a gold standard to verify other metrics.&nbsp; Feel free to email me at sentience@pobox.com to suggest any measurements that are better off not being publicly known (though this is of course a major disadvantage of that method).&nbsp; Stupid ideas can suggest good ideas, so if you can't come up with a good idea, <em>come up with a stupid one.</em></p>\n<p>Reputational, experimental, organizational:</p>\n<ul>\n<li>Something the masters and schools can do to keep it real (realistically real);</li>\n<li>Something you can do to measure each of a hundred students;</li>\n<li>Something you could use as a test even if people have an incentive to game it.</li>\n</ul>\n<p>Finding good solutions at each level determines what a whole field of study can be useful for&mdash;how much it can hope to accomplish.&nbsp; This is one of the Big Important Foundational Questions, so&mdash;</p>\n<p><em>Think!</em></p>\n<p>(<strong>PS</strong>:&nbsp; And ponder on your own before you look at the other comments; we need breadth of coverage here.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ipJwbLxhR83ZksN6Z": 1, "32DdRimdM7sB5wmKu": 1, "tRPnS4FoZeWjRfBxN": 2, "fR7QfYx4JA3BnptT9": 2, "5f5c37ee1b5cdee568cfb163": 7}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5K7CMa6dEL7TN7sae", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 58, "baseScore": 68, "extendedScore": null, "score": 0.000102, "legacy": true, "legacyId": "100", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "pvim9PZJ6qHRTMqD3", "canonicalCollectionSlug": "rationality", "canonicalBookId": "e6WsPsivzBifrWHeA", "canonicalNextPostSlug": "why-our-kind-can-t-cooperate", "canonicalPrevPostSlug": "schools-proliferating-without-evidence", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 69, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 240, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["Nu3wa6npK4Ry66vFp", "JnKCaGcgZL4Rsep8m", "sP2Hg6uPwpfp3jZJN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-15T17:32:43.186Z", "modifiedAt": null, "url": null, "title": "The Tragedy of the Anticommons", "slug": "the-tragedy-of-the-anticommons", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:45.785Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RAftfkp3NqDDR2o79/the-tragedy-of-the-anticommons", "pageUrlRelative": "/posts/RAftfkp3NqDDR2o79/the-tragedy-of-the-anticommons", "linkUrl": "https://www.lesswrong.com/posts/RAftfkp3NqDDR2o79/the-tragedy-of-the-anticommons", "postedAtFormatted": "Sunday, March 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Tragedy%20of%20the%20Anticommons&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Tragedy%20of%20the%20Anticommons%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRAftfkp3NqDDR2o79%2Fthe-tragedy-of-the-anticommons%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Tragedy%20of%20the%20Anticommons%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRAftfkp3NqDDR2o79%2Fthe-tragedy-of-the-anticommons", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRAftfkp3NqDDR2o79%2Fthe-tragedy-of-the-anticommons", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1396, "htmlBody": "<p>I assume that most of you are familiar with the concept of the <a href=\"http://en.wikipedia.org/wiki/Tragedy_of_the_commons\">Tragedy of the Commons</a>. If you aren't, well, that was a Wikipedia link right there.<br /><br />However, fewer are familiar with the <a href=\"http://en.wikipedia.org/wiki/Tragedy_of_the_anticommons\">Tragedy of the Anticommons</a>, a term coined by Michael Heller. Where the Tragedy of the Commons is created by too little ownership, the Tragedy of the Anticommons is created by too <em>much</em>.</p>\n<p>For instance, the classical solution to the TotC is to divide up the commons between the herders using it, giving each of them ownership for a particular part. This gives each owner an incentive to enforce its sustainability. But what would happen if the commons were divided up to thousands of miniature pieces, say one square inch each? In order to herd your cattle, you'd have to acquire permission from hundreds of different owners. Not only would this be a massive undertaking by itself, any one of them could say no, potentially ruining your entire attempt.<br /><br />This isn't just a theoretical issue. In <a href=\"http://www.amazon.com/Gridlock-Economy-Ownership-Markets-Innovation/dp/0465029167/\">his book</a>, Heller offers numerous examples, such as this one:</p>\n<blockquote>\n<p>&nbsp;...gridlock prevents a promising treatment for Alzheimer's diseases being tested. The head of research at a \"Big Pharma\" drugmaker told me that his lab scientists developed the potential cure (call it Compound X) years ago, but biotech competitors blocked its development. ... the company developing Compound X needed to pay every owner of a patent relevant to its testing. Ignoring even one would invite an expensive and crippling lawsuit. Each patent holder viewed its own discovery as the crucial one and demanded a corresponding fee, until the demands exceeded the drug's expected profits. None of the patent owners would yield first. ...<br />This story does not have a happy ending. No valiant patent bundler came along. Because the head of research could not figure out how to pay off all the patent owners and still have a good chance of earning a profit, he shifted his priorities to less ambitious options. Funding went to spin-offs of existing drugs for which his firm already controlled the underlying patents. His lab reluctantly shelved Compound X even though he was certain the science was solid, the market huge, and the potential for easing human suffering beyond measure.</p>\n</blockquote>\n<p><a id=\"more\"></a>Patents aren't the only field affected by this tragedy. America's airports are unnecessarily congested because land owners block all attempts to build new airports. 90% of the US broadcast spectrum goes unused, because in order to build national coverage, you'd need to apply for permission in 734 separate areas. Re-releasing an important documentary required a 600 000 dollar donation and negotiations that stretched over 20 years, because there were so many different copyright owners for all the pictures and music used in the documentary.<br /><br />So, what does all of this have to do with rationality, and why am I bringing it up here?</p>\n<p>The interesting thing about the tragedy of the anticommons is that most people will be entirely blind to it. Patent owners block drug development, and the only people who'll know are the owners in question, as well as the people who tried to develop the drug. A documentary doesn't get re-released? A few people might wonder why the documentary they saw 20 years ago isn't available on DVD anywhere, but aside for that, nobody'll know. If you're not even aware that a problem exists, then you can't fix it.<br /><br />In general, something <em>not</em> happening is much harder to spot than something <em>happening</em>. Heller remarks that even the term \"underuse\" hasn't existed for very long:</p>\n<blockquote>\n<p>According to the <em>OED</em>, <em>underuse</em> is a recent coinage. In its first recorded appearance, in 1960, the word was hedged about with an anxious hyphen and scare quotes: \"There might, in some places, be considerable 'under-use' of [parking] meters.\" By 1970, copy editors felt sufficiently comfortable to cast aside the quotes: \"A country can never recover by persistently under-using its resources, as Britain has done for too long.\" The hyphen began to disappear around 1975.</p>\n</blockquote>\n<p>This gives an interesting case study for rationality. If 'underuse' didn't become a word until 1960, that implies that people have been blind to its damages until then. Heller speculates:</p>\n<blockquote>\n<p>In the OED, this new word means \"to use something below the optimum\" and \"insufficient use\". The reference to an \"optimum\" suggests to me how <em>underuse</em> entered English. It was, I think, an unintended consequence of the increasing role of cost-benefit analysis in public policy debates. ...In the old world of overuse versus ordinary use, our choices were binary and clear-cut: injury or health, waste or efficiency, bad or good. In the new world, we are looking for something more subtle - an \"optimum\" along a continuum. Looking for an optimal level of use has a surprising twist: it requires a concept of underuse and surreptitiously changes the long-standing meaning of overuse. Like Goldilocks, we are looking for something not too hot, not too cold, not too much or too little - just right. ...<br />How can we know whether we are overusing, underusing, or optimally using resources? It's not easy, and not just a matter of economic analysis. Consider, for example, the public health push to icnrease the use of \"statins\", drugs such as Liptor that help lower cholesterol. Underuse of statins may mean too many heart attacks and strokes. But no one suggests that everyone should take statins: putting the drug int he water supply would be overuse. So what is the optimal level of use? ... We estimate the cost of the drugs, we assign a dollar value to death and disease averted, and quantify the negative effects of increased use.<br />Driving faster gets you home sooner but increases your chance of crashing. Is the trade-off worthwhile? To answer that question, you need to know how to value life. If life were beyond value, we would require perfect auto safety, cars would be infinitely expensive, and car use would drop to nothing. But if there is too little safety regulation, too many will die. With auto safety, society faces aother Goldilocks' quest: we strive to ensure that, all things considered, cars kill the optimal amount of people. It sounds callous, but that's what an optimum is all about.<br />... The possibility of underuse reorients policymaking from relatively simple either-or choices to the more contentious trade-offs that make up modern regulation of risk.</p>\n</blockquote>\n<p>There are several lessons one could draw here.<br /><br />One is that we're biased to be blind to underuse, whereas overuse is much easier to spot.<br /><br />Another is the more general case: we should be careful to look for hidden effects in any policies we institute or actions we take. Even if there seems to be no damage, there may actually be. How can we detect such hidden effects? Cost-benefit calculations looking for the optimal level of use are one way, though that's time-consuming and will only help detect under/overuse.<br /><br />Not to mention that it's hard, requiring us to <a href=\"/lw/2o/soulless_morality/\">set a value on lives</a> and other moral questions. That brings us to the second lesson. Heller's analysis implies that for a long time, people were actually blind to the possibility of underuse because they were reluctant to really tackle the hard problems. Refuse to assign a value on life? Then you can't engage in cost-benefit analysis... and as a result, you'll stay blind to the whole concept of underuse. If you'd looked at things objectively, you'd have seen that we <em>need</em> to give lives a value in order to make decisions in society. By refusing to do so, you'll stay blind to a huge class of problems ever after, simply because you didn't want to objectively ponder hard questions.</p>\n<p>That reinforces a message Eliezer's <a href=\"http://www.overcomingbias.com/2008/10/the-dark-side.html\">been talking about</a>. I doubt anybody could have foreseen that by refusing to put a value on life, they'd fail to discover the concept of underuse, and thereby have difficulty noticing the risk of patents blocking the development of new drugs. If you let your beliefs get in the way of your rationality in even one thing, it may end up hurting you in entirely unexpected ways. Don't do it.</p>\n<p>(There are also some other lessons, which I realized after typing out this post... can you come up with them yourself?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PDJ6KqJBRzvKPfuS3": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RAftfkp3NqDDR2o79", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 43, "extendedScore": null, "score": 0.0001, "legacy": true, "legacyId": "103", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["E5QXf3tCeE7fZGq4t"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-15T19:30:09.265Z", "modifiedAt": null, "url": null, "title": "Are You a Solar Deity?", "slug": "are-you-a-solar-deity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:37.867Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/T5McDuWDeCvDZKeSj/are-you-a-solar-deity", "pageUrlRelative": "/posts/T5McDuWDeCvDZKeSj/are-you-a-solar-deity", "linkUrl": "https://www.lesswrong.com/posts/T5McDuWDeCvDZKeSj/are-you-a-solar-deity", "postedAtFormatted": "Sunday, March 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20You%20a%20Solar%20Deity%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20You%20a%20Solar%20Deity%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT5McDuWDeCvDZKeSj%2Fare-you-a-solar-deity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20You%20a%20Solar%20Deity%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT5McDuWDeCvDZKeSj%2Fare-you-a-solar-deity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT5McDuWDeCvDZKeSj%2Fare-you-a-solar-deity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1164, "htmlBody": "<p>Max Muller was one of the greatest religious scholars of the 19th century. Born in Germany, he became fascinated with Eastern religion, and moved to England to be closer to the center of Indian scholarship in Europe. There he mastered English and Sanskrit alike to come out with the first English translation of the Rig Veda, the holiest book of Hinduism.<br /><br />One of Muller's most controversial projects was his attempt to interpret all pagan mythologies as linked to one another, deriving from a common ur-mythology and ultimately from the celestial cycle. His tools were exhaustive knowledge of the myths of all European cultures combined with a belief in the interpretive power of linguistics. <br /><br />What the <a href=\"http://books.google.ie/books?id=iTwyFzs2PQ8C&amp;pg=PA67&amp;lpg=PA67&amp;dq=Max+Muller+%22celestial+mythology%22&amp;source=bl&amp;ots=eDR6DwF_WZ&amp;sig=18BXTiCP-aCaEpO1YETj_jJd7xM&amp;hl=en&amp;ei=c4S5SfvlOsTj-AaRrfm0BA&amp;sa=X&amp;oi=book_result&amp;resnum=5&amp;ct=result#PPA88,M1\">significance</a> of Orpheus' descent into the underworld to reclaim his wife's soul? The sun sets beneath the Earth each evening, and returns with renewed brightness. Why does Apollo love Daphne? Daphne is cognate with Sanskrit Dahana, the maiden of the dawn. The death of Hercules? It occurs after he's completed twelve labors (cf. twelve signs of zodiac) when he's travelling west (like the sun), he is killed by Deianeira (compare Sanskrit dasya-nari, a demon of darkness) and his body is cremated (fire = the sun).&nbsp; His followers extended the method to Jesus - who was clearly based on a lunar deity, since he spent three days dead and then returned to life, just as the new moon goes dark for three days and then reappears.<br /><br />Muller's work was massively influential during his time, and many 19th century mythographers tried to critique his paradigm and poke holes in it. Some accused him of trying to destroy the mystery of religion, and others accused him of shoddy scholarship.<br /><br />R.F. Littledale, an Anglican apologist, took a completely different route. He claimed that there was, in fact, no such person as Professor Max Muller, holder of the Taylorian Chair in Modern European Languages. All these stories about \"Max Muller\" were nothing but a thinly disguised solar myth.</p>\n<p><a id=\"more\"></a>Littledale begins his argument by noting Muller's heritage. He was supposedly born in Germany, only to travel to England when he came of age. This looks suspiciously like the classic Journey of the Sun, which is born in the east but travels to the west. Muller's origin in Germany is a clear reference to Germanus Apollo, one of the old appelations of the Greek sun god. <br /><br />His Christian name must be related to Latin \"maximus\" or Sanskrit \"maha\", meaning great, a suitable description of the King of Gods, and his surname is cognate with Mjolnir, the mighty hammer of the sky god Thor. His claim to fame is bringing the ancient wisdom of the East to the people of the West - that is, illuminating them with eastern light.<br /><br />Muller teaches at Oxford for the same reason that Genesis describes the sky as \"the waters above\" and the Egyptians gave Ra a solar barge: ancient people interpreted the sky as a river, and the sun as crossing that river upon his chariot (perhaps an <strong>ox</strong>-drawn chariot, <strong>ford</strong>ing the river?). His chair at Oxford is the throne of the sky, his status as Taylorian Professor because \"he cuts away with his glittering shears the ragged edges of cloud; he allows the...cuttings from his workshop, to descend in fertilizing showers upon the earth.\"<br /><br />I could go on; instead I recommend you read <a href=\"http://www.elfinspell.com/Kottabos.html\">the original essay</a>. The take-home lesson is that any technique powerful enough to prove that Hercules is a solar myth is also powerful enough to prove that <em>anyone</em> is a solar myth. Muller lacked the <a href=\"http://www.overcomingbias.com/2007/08/your-strength-a.html\">strength of a rationalist</a>: the ability to be more confused by fiction than by reality. This makes the Hercules theory useless, but that is not immediately apparent on a first or even a second reading of Muller's work. When reading Muller's work, the primary impression one gets is \"Wow, this man has gathered a <em>lot</em> of supporting evidence.\"<br /><br />This is a problem encountered in many fields of scholarship, especially \"comparative\" anything. In comparative linguistics, for example, it's usually possible to make a <a href=\"http://www.zompist.com/proto.html\">case that two languages are related</a> good enough to convince a layman, no matter which two languages or how distant they may be. In comparative religion, we get cases like this blog's recent discussion over the <a href=\"/lw/r/no_really_ive_deceived_myself/go#comments\">possible derivation of Esther and Mordechai defeating Haman</a> from Ishtar and Marduk defeating Humbaba. The less said about comparative literature, the better, although I can't help but quote humor writer Dave Barry:</p>\n<blockquote>\n<p>Suppose you are studying Moby-Dick. Anybody with any common sense would say that Moby-Dick is a big white whale, since the characters in the book refer to it as a big white whale roughly eleven thousand times. So in <em>your</em> paper, <em>you</em> say Moby-Dick is actually the Republic of Ireland. Your professor, who is sick to death of reading papers and never liked Moby-Dick anyway, will think you are enormously creative. If you can regularly come up with lunatic interpretations of simple stories, you should major in English.</p>\n</blockquote>\n<p>The worst (but most fun to read!) are in pseudoscience, where plausible sounding comparisons can prove almost anything. Did you know the Mayans believed in a lost homeland called Atzlan, the Indonesians believed in a lost island called Atala, and the Greeks believed in a lost continent called Atlantis? Likewise, did you know that Nostradamus predicted a great battle involving Germany and \"Hister\", which sounds almost like \"Hitler\"?<br /><br />Yet it would be a mistake to reject all such comparisons. In fact, I have thus far been enormously unfair to Professor Muller, whose work established several correspondences still viewed as valid today. Virtually all modern mythologists accept that the Hindu Varuna is the Greek Uranus, and that the Greek sky god Zeus equals the Hindu sky god Dyaus Pita and the Roman Jupiter (compare to Latin <em>deus pater</em>, meaning God the Father). Likewise, comparative linguists are quite certain that all modern European languages and Sanskrit derive from a common Indo-European root, and in my opinion even the Nostratic project - an ambitious attempt to link Semitic, Indo-European, Uralic. and a bunch of other languages - is at least worth consideration.<br /><br />We need a test to distinguish between true and false correspondences. But the standard method, making and testing predictions, is useless here. A good mythologist already knows the stories of Varuna and Uranus. The chances of discovering a new fact that either confirms or overturns the Varuna-Uranus correspondence is not even worth considering.<br /><br />Mark Rosenfelder has <a href=\"http://www.zompist.com/chance.htm\">an excellent article on chance resemblances between languages</a> which offers a semi-formal model for spotting dubious comparisons. But such precision may not be possible when comparing two deities. <br /><br />I have what might be a general strategy for approaching this sort of problem, which I will present tomorrow. But how would you go about it?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NSMKfa8emSbGNXRKD": 1, "bY5MaF2EATwDkomvu": 1, "pszEEb3ctztv3rozd": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "T5McDuWDeCvDZKeSj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 50, "baseScore": 47, "extendedScore": null, "score": 7.2e-05, "legacy": true, "legacyId": "104", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 47, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-15T20:44:47.697Z", "modifiedAt": null, "url": null, "title": "In What Ways Have You Become Stronger?", "slug": "in-what-ways-have-you-become-stronger", "viewCount": null, "lastCommentedAt": "2020-03-06T13:40:21.051Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EsGuKGNC9gerguLv9/in-what-ways-have-you-become-stronger", "pageUrlRelative": "/posts/EsGuKGNC9gerguLv9/in-what-ways-have-you-become-stronger", "linkUrl": "https://www.lesswrong.com/posts/EsGuKGNC9gerguLv9/in-what-ways-have-you-become-stronger", "postedAtFormatted": "Sunday, March 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20What%20Ways%20Have%20You%20Become%20Stronger%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20What%20Ways%20Have%20You%20Become%20Stronger%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEsGuKGNC9gerguLv9%2Fin-what-ways-have-you-become-stronger%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20What%20Ways%20Have%20You%20Become%20Stronger%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEsGuKGNC9gerguLv9%2Fin-what-ways-have-you-become-stronger", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEsGuKGNC9gerguLv9%2Fin-what-ways-have-you-become-stronger", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 297, "htmlBody": "<p><strong>Related to</strong>: <a href=\"http://www.overcomingbias.com/2007/03/tsuyoku_naritai.html\">Tsuyoku Naritai! (I Want To Become Stronger)</a>, <a href=\"/lw/h/test_your_rationality/\">Test Your Rationality</a>, <a href=\"/lw/2s/3_levels_of_rationality_verification/\">3 Levels of Rationality Verification</a>.</p>\n<p><a href=\"/lw/h/test_your_rationality/\">Robin</a> and <a href=\"/lw/2s/3_levels_of_rationality_verification/\">Eliezer</a> ask about the ways to test rationality skills, for each of the many important purposes such testing might have. Depending on what's possible, you may want to test yourself to learn how well you are doing at your studies, at least to some extent check the sanity of the teaching that you follow, estimate the effectiveness of specific techniques, or even force a rationality test on a person whose position depends on the outcome.</p>\n<p>Verification procedures have various weaknesses, making them admissible for one purpose and not for another. But however rigorous the verification methods are, one must first find the specific properties to test for. These properties or skills may come naturally with the art, or they may be cultivated specifically for the testing, in which case they need to be good signals, hard to demonstrate without also becoming more rational.</p>\n<p><a id=\"more\"></a></p>\n<p>So, my question is this - what have you become reliably <a href=\"http://www.overcomingbias.com/2007/03/tsuyoku_naritai.html\">stronger</a> at, after you walked the path of an aspiring rationalist for considerable time? Maybe you have noticeably improved at something, or maybe you haven't learned a certain skill yet, but you are reasonably sure that because of your study of rationality you'll be able to do that considerably better than other people.</p>\n<p>This is a significantly different question from the ones Eliezer and Robin ask. Some of the skills you obtained may be virtually unverifiable, some of them may be easy to fake, some of them may be easy to learn without becoming sufficiently rational, and some of them may be standard in other disciplines. But I think it's useful to step back, and write a list of skills before selecting ones more suitable for the testing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"irYLXtT9hkPXoZqhH": 2, "fR7QfYx4JA3BnptT9": 2, "fkABsGCJZ6y9qConW": 2, "Ng8Gice9KNkncxqcj": 2, "5f5c37ee1b5cdee568cfb117": 4, "5f5c37ee1b5cdee568cfb163": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EsGuKGNC9gerguLv9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 28, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "105", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Kn6H8Tk6EPT4Atq4k", "5K7CMa6dEL7TN7sae"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-15T22:44:13.162Z", "modifiedAt": null, "url": null, "title": "Taboo \"rationality,\" please.", "slug": "taboo-rationality-please", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:06.390Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9cEKk7naZaCggiXsg/taboo-rationality-please", "pageUrlRelative": "/posts/9cEKk7naZaCggiXsg/taboo-rationality-please", "linkUrl": "https://www.lesswrong.com/posts/9cEKk7naZaCggiXsg/taboo-rationality-please", "postedAtFormatted": "Sunday, March 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Taboo%20%22rationality%2C%22%20please.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATaboo%20%22rationality%2C%22%20please.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9cEKk7naZaCggiXsg%2Ftaboo-rationality-please%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Taboo%20%22rationality%2C%22%20please.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9cEKk7naZaCggiXsg%2Ftaboo-rationality-please", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9cEKk7naZaCggiXsg%2Ftaboo-rationality-please", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<p><strong>Related on OB: </strong><a href=\"http://www.overcomingbias.com/2008/02/taboo-words.html\">Taboo Your Words</a></p>\n<p>I realize this seems odd on a blog <em>about</em> rationality, but I'd like to strongly suggest that commenters make an effort to avoid using the words \"rational,\" \"rationality,\" or \"rationalist\" when other phrases will do.&nbsp; I think we've been stretching the words to cover too much meaning, and it's starting to show.<br /><br />Here are some suggested substitutions to start you off.<br /><br />Rationality:</p>\n<ul>\n<li>truth-seeking</li>\n<li>probability updates under bayes rule</li>\n<li>the \"winning way\"</li>\n</ul>\n<p>Rationalist:</p>\n<ul>\n<li>one who reliably wins</li>\n<li>one who can reliably be expected to speak truth</li>\n</ul>\n<p>Are there any others?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb12a": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9cEKk7naZaCggiXsg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 26, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": "107", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-16T15:48:21.121Z", "modifiedAt": null, "url": null, "title": "Science vs. art", "slug": "science-vs-art", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:59.052Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BnoFcEkKE9syNCqWw/science-vs-art", "pageUrlRelative": "/posts/BnoFcEkKE9syNCqWw/science-vs-art", "linkUrl": "https://www.lesswrong.com/posts/BnoFcEkKE9syNCqWw/science-vs-art", "postedAtFormatted": "Monday, March 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Science%20vs.%20art&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScience%20vs.%20art%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBnoFcEkKE9syNCqWw%2Fscience-vs-art%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Science%20vs.%20art%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBnoFcEkKE9syNCqWw%2Fscience-vs-art", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBnoFcEkKE9syNCqWw%2Fscience-vs-art", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 653, "htmlBody": "<p>In the comments on <a href=\"/lw/2o/soulless_morality/\" target=\"_self\">Soulless Morality</a>, a few people mentioned contributing to humanity's knowledge as an ultimate value.&nbsp; I used to place a high value on this myself.</p>\n<p>Now, though, I doubt whether making scientific advances would give me satisfaction on my deathbed.&nbsp; All you can do in science is discover something before someone else discovers it.&nbsp; (It's a lot like the race to the north pole, which struck me as stupid when I was a child; yet I never transferred that judgement to scientific races.)&nbsp; The short-term effects of your discovering something sooner might be good, and might not.&nbsp; The long-term effects are likely to be to bring about apocalypse a little sooner.</p>\n<p>Art is different.&nbsp; There's not much downside to art.&nbsp; There are some exceptions - romance novels perpetuate destructive views of love; 20th-century developments in orchestral music killed orchestral music; and Ender's Game has <a href=\"http://plover.net/~bonds/ender.html\" target=\"_blank\">warped the psyches of many intelligent people</a>.&nbsp; But artists seldom worry that their art might destroy the world.&nbsp; And if you write a great song, you've really contributed, because no one else would have written that song.</p>\n<p>EDIT: What is above is instrumental talk.&nbsp; I find that, as I get older, science fails to satisfy me as much.&nbsp; I don't assign it the high intrinsic value I used to.&nbsp; But it's hard for me to tell whether this is really an intrinsic valuation, or the result of diminishing faith in its instrumental value.</p>\n<p>I think that people who value rationality tend to place an unusually high value on knowledge.&nbsp; Rationality requires knowledge; but that gives knowledge only instrumental value.&nbsp; It doesn't (can't, by definition) justify giving knowledge intrinsic value.</p>\n<p>What do the rest of you think?&nbsp; Is there a strong correlation between rationalism, giving knowledge high intrinsic value, and giving art low intrinsic value?&nbsp; If so, why?&nbsp; And which would you rather be - a great scientist, or a great artist of some type?&nbsp; (Pretend that great scientists and great artists are equally well-paid and sexually attractive.)</p>\n<p>(I originally wrote this as over-valuing knowledge and under-valuing art, but Roko pointed out that that's incoherent.)</p>\n<p>Under a theory that intrinsic and instrumental values are separate things, there's no reason why giving science a high instrumental value should correlate with giving it a high intrinsic value, or vice-versa.&nbsp; Yet the people here seem to be doing one of those things.</p>\n<p>My theory is that we can't keep intrinsic and instrumental values separate from each other.&nbsp; We attach positive valences to both, and then operate on the positive valences.&nbsp; Or, we can't distinguish our intrinsic values from our instrumental values by introspection.&nbsp; (You may have noticed that I started using examples that refer to both intrinsic and instrumental values.&nbsp; I don't think I can separate them, except retrospectively; and with about as much accuracy as a courtroom witness asked to testify about an event that took place 20 years ago.)</p>\n<p>It's tempting to mention friends and family in here too, as another competing fundamental value.&nbsp; But that would demand solving the relationship between personal values that you yourself take, and the valuations you would want a society or a singleton AI to make.&nbsp; That's too much to take on here.&nbsp; I want to talk just about intrinsic value given to science vs. art.</p>\n<p>Oh, and saying science is an art is a dodge.&nbsp; You then have to say whether you value the knowledge, or the artistic endeavor.&nbsp; Also, ignore the possibility that your scientific work can make a safe Singularity.&nbsp; That would be science as instrumental value.&nbsp; I'm asking about science vs. art as intrinsic values.</p>\n<p>EDIT:&nbsp; An obvious explanation:&nbsp; I was assuming that people here want to be rational as an instrumental value, and that we should find the distribution of intrinsic values to be the same as in the general populace.&nbsp; But of course some people are drawn here because rationality is an intrinsic value to them, and this heavily biases the distribution of intrinsic values found here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZpG9rheyAkgCoEQea": 1, "KDpqtN3MxHSmD4vcB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BnoFcEkKE9syNCqWw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 7, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "108", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["E5QXf3tCeE7fZGq4t"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-16T22:33:55.765Z", "modifiedAt": "2021-03-31T21:42:39.790Z", "url": null, "title": "What Do We Mean By \"Rationality\"?", "slug": "what-do-we-mean-by-rationality-1", "viewCount": null, "lastCommentedAt": "2022-06-02T16:54:41.869Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RcZCwxFiZzE6X7nsv/what-do-we-mean-by-rationality-1", "pageUrlRelative": "/posts/RcZCwxFiZzE6X7nsv/what-do-we-mean-by-rationality-1", "linkUrl": "https://www.lesswrong.com/posts/RcZCwxFiZzE6X7nsv/what-do-we-mean-by-rationality-1", "postedAtFormatted": "Monday, March 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Do%20We%20Mean%20By%20%22Rationality%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Do%20We%20Mean%20By%20%22Rationality%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcZCwxFiZzE6X7nsv%2Fwhat-do-we-mean-by-rationality-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Do%20We%20Mean%20By%20%22Rationality%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcZCwxFiZzE6X7nsv%2Fwhat-do-we-mean-by-rationality-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcZCwxFiZzE6X7nsv%2Fwhat-do-we-mean-by-rationality-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1722, "htmlBody": "<p>I mean two things:</p><p>1. <strong>Epistemic rationality</strong>: systematically improving the accuracy of your beliefs.</p><p>2. <strong>Instrumental rationality</strong>: systematically achieving your values.</p><p>The first concept is simple enough. When you open your eyes and look at the room around you, you\u2019ll locate your laptop in relation to the table, and you\u2019ll locate a bookcase in relation to the wall. If something goes wrong with your eyes, or your brain, then your mental model might say there\u2019s a bookcase where no bookcase exists, and when you go over to get a book, you\u2019ll be disappointed.</p><p>This is what it\u2019s like to have a false belief, a map of the world that doesn\u2019t correspond to the territory. Epistemic rationality is about building accurate maps instead. This correspondence between belief and reality is commonly called \u201ctruth,\u201d and I\u2019m happy to call it that.<sup>1</sup></p><p>Instrumental rationality, on the other hand, is about <i>steering</i> reality\u2014sending the future where you want it to go. It\u2019s the art of choosing actions that lead to outcomes ranked higher in your preferences. I sometimes call this \u201cwinning.\u201d</p><p>So rationality is about forming true beliefs and making decisions that help you win.</p><p>(Where truth doesn't mean \u201ccertainty,\u201d since we can do plenty to increase the <i>probability</i> that our beliefs are accurate even though we're uncertain; and winning doesn't mean \u201cwinning at others' expense,\u201d since our values include <i>everything</i> we care about, including other people.)</p><p>When people say \u201cX is rational!\u201d it\u2019s usually just a more strident way of saying \u201cI think X is true\u201d or \u201cI think X is good.\u201d So why have an additional word for \u201crational\u201d as well as \u201ctrue\u201d and \u201cgood\u201d?</p><p>An analogous argument can be given against using \u201ctrue.\u201d There is no need to say \u201cit is true that snow is white\u201d when you could just say \u201csnow is white.\u201d What makes the idea of truth useful is that it allows us to talk about the general features of map-territory correspondence. \u201cTrue models usually produce better experimental predictions than false models\u201d is a useful generalization, and it\u2019s not one you can make without using a concept like \u201ctrue\u201d or \u201caccurate.\u201d</p><p>Similarly, \u201cRational agents make decisions that maximize the probabilistic expectation of a coherent utility function\u201d is the kind of thought that depends on a concept of (instrumental) rationality, whereas \u201cIt\u2019s rational to eat vegetables\u201d can probably be replaced with \u201cIt\u2019s useful to eat vegetables\u201d or \u201cIt\u2019s in your interest to eat vegetables.\u201d We need a concept like \u201crational\u201d in order to note&nbsp;general facts about those ways of thinking that systematically produce truth or value\u2014and the systematic ways in which we fall short of those standards.</p><p>As we\u2019ve observed in the previous essays, experimental psychologists sometimes uncover human reasoning that seems very strange. For example, someone rates the probability \u201cBill plays jazz\u201d as <i>less</i> than the probability \u201cBill is an accountant who plays jazz.\u201d This seems like an odd judgment, since any particular jazz-playing accountant is obviously a jazz player. But to what higher vantage point do we appeal in saying that the judgment is <i>wrong</i> ?</p><p>Experimental psychologists use two gold standards: <i>probability theory</i>, and <i>decision theory</i>.</p><p>Probability theory is the set of laws underlying rational belief. The mathematics of probability applies equally to \u201cfiguring out where your bookcase is\u201d and \u201cestimating how many hairs were on Julius Caesars head,\u201d even though our evidence for the claim \u201cJulius Caesar was bald\u201d is likely to be more complicated and indirect than our evidence for the claim \u201ctheres a bookcase in my room.\u201d It\u2019s all the same problem of how to process the evidence and observations to update one\u2019s beliefs. Similarly, decision theory is the set of laws underlying rational action, and is equally applicable regardless of what one\u2019s goals and available options are.</p><p>Let \u201cP(such-and-such)\u201d stand for \u201cthe probability that such-and-such happens,\u201d and \u201cP(A,B)\u201d for \u201cthe probability that both A and B happen.\u201d Since it is a universal law of probability theory that P(A) \u2265 P(A,B), the judgment that P(Bill plays jazz) is less than P(Bill plays jazz, Bill is an accountant) is labeled incorrect.</p><p>To keep it technical, you would say that this probability judgment is&nbsp;<i>non-Bayesian</i>. Beliefs that conform to a coherent probability distribution, and decisions that maximize the probabilistic expectation of a coherent utility function, are called \u201cBayesian.\u201d</p><p>I should emphasize that this&nbsp;<i>isn't&nbsp;</i>the notion of rationality thats common in popular culture. People may use the same string of sounds, \u201cra-tio-nal,\u201d to refer to \u201cacting like Mr. Spock of&nbsp;<i>Star Trek</i>\u201d and \u201cacting like a Bayesian\u201d; but this doesn't mean that acting Spock-like helps one hair with epistemic or instrumental rationality.<sup>2</sup></p><p>All of this does not quite exhaust the problem of what is meant in practice by \u201crationality,\u201d for two major reasons:</p><p>First, the Bayesian formalisms in their full form are computationally intractable on most real-world problems. No one can&nbsp;<i>actually&nbsp;</i>calculate and obey the math, any more than you can predict the stock market by calculating the movements of quarks.</p><p>This is why there is a whole site called \u201cLess Wrong,\u201d rather than a single page that simply states the formal axioms and calls it a day. There\u2019s a whole further art to finding the truth and accomplishing value&nbsp;<i>from inside a human mind</i>: we have to learn our own flaws, overcome our biases, prevent ourselves from self-deceiving, get&nbsp;ourselves into good emotional shape to confront the truth and do what needs doing, et cetera, et cetera.</p><p>Second, sometimes the meaning of the math itself is called into question. The exact rules of probability theory are called into question by, e.g.,&nbsp;<a href=\"http://www.anthropic-principle.com/?q=anthropic_principle/primer\">anthropic problems</a> in which the number of observers is uncertain. The exact rules of decision theory are called into question by, e.g., Newcomblike problems in which other agents may predict your decision before it happens.<sup>3</sup></p><p>In cases where our best formalizations still come up short, we can return to simpler ideas like \u201ctruth\u201d and \u201cwinning.\u201d If you are a scientist just beginning to investigate fire, it might be a lot wiser to point to a campfire and say \u201cFire is that orangey-bright hot stuff over there,\u201d rather than saying \u201cI define fire as an alchemical transmutation of substances which releases phlogiston.\u201d You certainly shouldn\u2019t ignore something just because you can\u2019t define it. I can't quote the equations of General Relativity from memory, but nonetheless if I walk off a cliff, I'll fall. And we can say the same of cognitive biases and other obstacles to truth\u2014they wont hit any less hard if it turns out we can't define compactly what \u201cirrationality\u201d is.</p><p>In cases like these, it is futile to try to settle the problem by coming up with some new definition of the word \u201crational\u201d and saying, \u201cTherefore my preferred answer,&nbsp;<i>by definition</i>, is what is meant by the word \u2018rational.\u2019 \u201d This simply raises the question of why anyone should pay attention to your definition. I\u2019m not interested in probability theory because it is the holy word handed down from Laplace. I\u2019m interested in Bayesian-style belief-updating (with Occam priors) because I expect that this style of thinking gets us systematically closer to, you know,&nbsp;<i>accuracy</i>, the map that reflects the territory.</p><p>And then there are questions of how to think that seem not quite answered by either probability theory or decision theory\u2014like the question of how to feel about the truth once you have it. Here, again, trying to define \u201crationality\u201d a particular way doesn\u2019t support an answer, but merely presumes one.</p><p>I am not here to argue the meaning of a word, not even if that word is \u201crationality.\u201d The point of attaching sequences of letters to particular concepts is to let two people <i>communicate</i>\u2014to help transport thoughts from one mind to another. You cannot change reality, or prove the thought, by manipulating which meanings go with which words.</p><p>So if you understand what concept I am&nbsp;<i>generally getting at&nbsp;</i>with this word \u201crationality,\u201d and with the sub-terms \u201cepistemic rationality\u201d and \u201cinstrumental rationality,\u201d we&nbsp;<i>have communicated</i>: we have accomplished everything there is to accomplish by talking about how to define \u201crationality.\u201d What\u2019s left to discuss is not&nbsp;<i>what meaning&nbsp;</i>to attach to the syllables \u201cra-tio-na-li-ty\u201d; what\u2019s left to discuss is&nbsp;<i>what is a good way to think</i>.</p><p>If you say, \u201cIt\u2019s (epistemically) rational for me to believe X, but the truth is Y,\u201d then you are probably using the word \u201crational\u201d to mean something other than what I have in mind. (E.g., \u201crationality\u201d should be <i>consistent under reflection</i>\u2014\u201crationally\u201d looking at the evidence, and \u201crationally\u201d&nbsp;considering how your mind processes the evidence, shouldn\u2019t lead to two different conclusions.)</p><p>Similarly, if you find yourself saying, \u201cThe (instrumentally) rational thing for me to do is X, but the right thing for me to do is Y,\u201d then you are almost certainly using some other meaning for the word \u201crational\u201d or the word \u201cright.\u201d I use the term \u201crationality\u201d&nbsp;<i>normatively</i>, to pick out desirable patterns of thought.</p><p>In this case\u2014or in any other case where people disagree about word meanings\u2014you should substitute more specific language in place of \u201crational\u201d: \u201cThe self-benefiting thing to do is to run away, but I hope I would at least try to drag the child off the railroad tracks,\u201d or \u201cCausal decision theory as usually formulated says you should two-box on Newcomb\u2019s Problem, but I\u2019d rather have a million dollars.\u201d</p><p>In fact, I recommend reading back through this essay, replacing every instance of&nbsp;\u201crational\u201d with \u201cfoozal,\u201d and seeing if that changes the connotations of what I\u2019m saying any. If so, I say: strive not for rationality, but for foozality.</p><p>The word \u201crational\u201d has potential pitfalls, but there are plenty of&nbsp;<i>non</i>-borderline cases where \u201crational\u201d works fine to communicate what I\u2019m getting at. Likewise \u201cirrational.\u201d In these cases I\u2019m not afraid to use it.</p><p>Yet one should be careful not to&nbsp;<i>overuse&nbsp;</i>that word. One receives no points merely for pronouncing it loudly. If you speak overmuch of the Way, you will not attain it.</p><hr><p><sup>1</sup> For a longer discussion of truth, see \u201c<a href=\"https://www.lesswrong.com/rationality/the-simple-truth\">The Simple Truth</a>\u201d at the very end of this volume.</p><p><sup>2</sup> The idea that rationality is about strictly privileging verbal reasoning over feelings is a case in point. Bayesian rationality applies to urges, hunches, perceptions, and wordless intuitions, not just to assertions.</p><p>I gave the example of opening your eyes, looking around you, and building a mental model of a room containing a bookcase against the wall. The modern idea of rationality is general enough to include your eyes and your brains visual areas as things-that-map, and to include instincts and emotions in the belief-and-goal calculus.</p><p><sup>3</sup> For an informal statement of Newcomb\u2019s Problem, see Jim Holt, \u201cThinking Inside the Boxes,\u201d <i>Slate</i>, 2002, <a href=\"http://www.slate.com/articles/arts/egghead/2002/02/thinkinginside_the_boxes.single.html\">http://www.slate.com/articles/arts/egghead/2002/02/thinkinginside_the_boxes.single.html</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 16, "aa3Qg7Qrp9LM7QMaz": 2, "AHK82ypfxF45rqh9D": 6, "9DmA84e4ZvYoYu6q8": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RcZCwxFiZzE6X7nsv", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 256, "baseScore": 252, "extendedScore": null, "score": 0.000382, "legacy": true, "legacyId": "109", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": "5g5TkQTe9rmPS5vvM", "canonicalCollectionSlug": "rationality", "canonicalBookId": "PYQ2izdfDPTe5uJTG", "canonicalNextPostSlug": "planning-fallacy", "canonicalPrevPostSlug": "burdensome-details", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 253, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "2.4.0", "pingbacks": {"Posts": ["X3HpE8tMXz4m4w6Rz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-16T22:34:51.045Z", "modifiedAt": null, "url": null, "title": "Comments for \"Rationality\"", "slug": "comments-for-rationality", "viewCount": null, "lastCommentedAt": "2019-01-15T13:26:11.909Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8XjwhnM9Guvyaf9Cj/comments-for-rationality", "pageUrlRelative": "/posts/8XjwhnM9Guvyaf9Cj/comments-for-rationality", "linkUrl": "https://www.lesswrong.com/posts/8XjwhnM9Guvyaf9Cj/comments-for-rationality", "postedAtFormatted": "Monday, March 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Comments%20for%20%22Rationality%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComments%20for%20%22Rationality%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8XjwhnM9Guvyaf9Cj%2Fcomments-for-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Comments%20for%20%22Rationality%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8XjwhnM9Guvyaf9Cj%2Fcomments-for-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8XjwhnM9Guvyaf9Cj%2Fcomments-for-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 26, "htmlBody": "<p>I wrote an Admin page for \"<a href=\"/lw/31/what_do_we_mean_by_rationality/\">What do we mean by 'Rationality'?</a>\" since this has risen to the status of a FAQ.&nbsp; Comments can go here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8XjwhnM9Guvyaf9Cj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 2, "extendedScore": null, "score": 4.817232521391514e-07, "legacy": true, "legacyId": "111", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RcZCwxFiZzE6X7nsv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-17T00:52:54.095Z", "modifiedAt": null, "url": null, "title": "The \"Spot the Fakes\" Test", "slug": "the-spot-the-fakes-test", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:26.568Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KLjQedNYNEP4tW73W/the-spot-the-fakes-test", "pageUrlRelative": "/posts/KLjQedNYNEP4tW73W/the-spot-the-fakes-test", "linkUrl": "https://www.lesswrong.com/posts/KLjQedNYNEP4tW73W/the-spot-the-fakes-test", "postedAtFormatted": "Tuesday, March 17th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20%22Spot%20the%20Fakes%22%20Test&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20%22Spot%20the%20Fakes%22%20Test%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKLjQedNYNEP4tW73W%2Fthe-spot-the-fakes-test%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20%22Spot%20the%20Fakes%22%20Test%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKLjQedNYNEP4tW73W%2Fthe-spot-the-fakes-test", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKLjQedNYNEP4tW73W%2Fthe-spot-the-fakes-test", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 955, "htmlBody": "<p><strong>Followup to: </strong><a href=\"/lw/2w/are_you_a_solar_deity/\">Are You a Solar Deity?</a></p>\n<p>James McAuley and Harold Stewart were mid-20th century Australian poets, and they were not happy. After having society ignore their poetry in favor of \"experimental\" styles they considered fashionable nonsense, they wanted to show everyone what they already knew: the Australian literary world was full of empty poseurs.<br /><br />They began by selecting random phrases from random books. Then they linked them together into something sort of like poetry. Then they invented the most fashionable possible story: Ern Malley, a loner working a thankless job as an insurance salesman, writing sad poetry in his spare time and hiding it away until his death at an early age. Posing as Malley's sister, who had recently discovered the hidden collection, they sent the works to Angry Penguins, one of Australia's top experimental poetry magazines.<br /><br />You wouldn't be reading this if the magazine hadn't rushed a special issue to print in honor of \"a poet in the same class as W.H. Auden or Dylan Thomas\".<br /><br /><a href=\"http://en.wikipedia.org/wiki/Ern_Malley\">The hoax</a> was later revealed<sup>1</sup>, everyone involved ended up with egg on their faces, and modernism in Australia received a serious blow. But as I am reminded every time I look through a modern poetry anthology, one Ern Malley every fifty years just isn't enough. I daydream about an alternate dimension where people are genuinely interested in keeping literary criticism honest. In this universe, any would-be literary critic would have to distinguish between ten poems generally recognized as brilliant that he'd never seen before, and ten pieces of nonsense invented on the spot by drunk college students, in order to keep his critic's license.<br /><br />Can we refine this test? And could it help Max Muller with his solar deity problem?</p>\n<p><a id=\"more\"></a></p>\n<p>In the Malley hoax, McAuley and Steward suspected that a certain school of modernist poetry was without value. Because its supporters were too biased to admit this directly, they submitted a control poem they knew was without value, and found the modernists couldn't tell the difference. This suggests a powerful technique for determining when something otherwise untestable might be, as Neal Stephenson calls it, bulshytte.<br /><br />Perhaps Max Muller thinks Hercules is a solar deity. He will write up a argument for this proposition, and submit it for consideration before all the great mythologists of the world. Even if these mythologists want to be unbiased, they will have a difficult time of it: Muller has a prestigious reputation, and they may not have any set conception of what does and doesn't qualify as a solar deity.<br /><br />What if, instead of submitting one argument, Muller submitted ten? One sincere argument for why Hercules is a solar deity, and other bogus arguments for why Perseus, Bellerophon, Theseus, et cetera are solar myths (which he has nevertheless constructs to the best of his ability). Then he instructs the mythologists \"Please independently determine which of these arguments is true, and which ones I have just come up with by writing 'X is a solar deity' as my bottom line and then inventing fake justifications for the fact?\" If every mythologist finds the Hercules argument most convincing, then that doesn't prove anything about Hercules but it at least shows Muller has a strong case. On the other hand, if they're all convinced by different arguments, or find none of the arguments convincing, or worst of all they all settle on Bellerophon, then Dr. Muller knows his beliefs about Hercules are quite probably wishful thinking.<br /><br />This method hinges on Dr. Muller's personal honesty: a dishonest man could simply do a bad job arguing for Theseus and Bellerophon. What if we thought Dr. Muller was dishonest? We might find another mythologist whom independent observers rate as equally persuasive as Dr. Muller, and ask her to come up with the bogus arguments. <br /><br />The rationalists I know sometimes take a dim view of the humanities as academic disciplines. Part of the problem is the seeming untestability of their conclusions through good, blinded experimental methods. I don't think most humanities professors are really looking all that hard for such methods. But for those who are, I consider this technique a little better than nothing<sup>2</sup>.</p>\n<p><strong>Footnotes</strong></p>\n<p><strong><a name=\"footnote1\"></a>1: </strong>The <a href=\"http://en.wikipedia.org/wiki/Sokal_Hoax\">Sokal Affair</a> is another related hoax. Wikipedia's Sokal Hoax page has <a href=\"http://en.wikipedia.org/wiki/Sokal_Hoax#Similar_affairs\">some other excellent examples</a> of this sort of test.</p>\n<p><strong><a name=\"footnote2\"></a>2: </strong>One more example where this method could prove useful. I remember debating a very smart Christian on the subject of Biblical atrocities. You know, stuff about death by stoning for minor crimes, or God ordering the Israelites to murder women and enslave children - that sort of thing. My friend, who was quite smart, was always able to come up with a superficially plausible excuse, and it was getting on my nerves. But having just read <a href=\"http://www.overcomingbias.com/2007/08/your-strength-a.html\">Your Strength as a Rationalist</a>, I knew that being able to explain anything wasn't always a virtue. I proposed the following experiment: I'd give my friend ten atrocities commanded by random Bronze Age kings generally agreed by historical consensus to be jerks, and ten commanded by God in the Bible. His job would be to determine which ten, for whatever reason, really weren't all that bad. If he identified the ten Bible passages, that would be strong evidence that Biblical commandments only seemed atrocious when misunderstood. But if he couldn't tell the difference between God and Ashurbanipal, that would prove God wasn't really that great. To my disgust, my friend knew his Bible so well that I couldn't find any atrocities he wasn't already familiar with. So much for that technique. I offer it to anyone who debates theists with less comprehensive knowledge of Scripture.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vg4LDxjdwHLotCm8w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KLjQedNYNEP4tW73W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 59, "baseScore": 61, "extendedScore": null, "score": 9.3e-05, "legacy": true, "legacyId": "112", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 61, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["T5McDuWDeCvDZKeSj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-17T08:53:06.300Z", "modifiedAt": null, "url": null, "title": "On Juvenile Fiction", "slug": "on-juvenile-fiction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:36.281Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4GeE83592epCErQse/on-juvenile-fiction", "pageUrlRelative": "/posts/4GeE83592epCErQse/on-juvenile-fiction", "linkUrl": "https://www.lesswrong.com/posts/4GeE83592epCErQse/on-juvenile-fiction", "postedAtFormatted": "Tuesday, March 17th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20Juvenile%20Fiction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20Juvenile%20Fiction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4GeE83592epCErQse%2Fon-juvenile-fiction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20Juvenile%20Fiction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4GeE83592epCErQse%2Fon-juvenile-fiction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4GeE83592epCErQse%2Fon-juvenile-fiction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 339, "htmlBody": "<p><strong>Follow-up To:&nbsp;<span style=\"font-weight: normal;\"><a href=\"/lw/25/on_the_care_and_feeding_of_young_rationalists/\">On the Care and Feeding of Young Rationalists</a></span></strong></p>\n<p><strong>Related on OB:&nbsp;<span style=\"font-weight: normal;\"><a href=\"http://www.overcomingbias.com/2009/02/formative-youth.html\">Formative Youth</a></span></strong></p>\n<p>Eliezer suspects he may have chosen an altruistic life <a href=\"http://www.overcomingbias.com/2009/02/formative-youth.html\">because of Thundercats</a>.</p>\n<p>Nominull thinks his path to truth-seeking might have been lit <a href=\"/lw/2/tell_your_rationalist_origin_story/1io#comments\">by Asimov's Robot stories</a>.</p>\n<p>PhilGoetz&nbsp;<a href=\"/lw/30/science_vs_art/\">suggests</a>&nbsp;that Ender's Game has&nbsp;<a href=\"http://plover.net/~bonds/ender.html\">warped the psyches of many intelligent people</a>.</p>\n<p>For good or ill, we seem to agree that fiction strongly influences the way we grow up, and the people we come to be.</p>\n<p>So for those of us with the tremendous task of <a href=\"http://www.overcomingbias.com/2008/12/no-unbirthing.html\">bringing new sentience into the world</a>, it seems sensible to spend some time thinking about what fictions our charges will be exposed to.</p>\n<p><a id=\"more\"></a></p>\n<p>The natural counter-part to this question is, of course, are there any particular fictions, or types of fiction, to which we should <em>avoid</em> exposing our children?</p>\n<p>Again, this is a pattern we see more commonly in the <a href=\"http://www.christianitytoday.com/movies/\">religious community</a> -- and the rest of us tend to look on and laugh at the prudery on display. Still, the general idea doesn't seem to be something we can reject out of hand. So far as we can tell, all (currently existing) minds are vulnerable to being hacked, young minds more than others. If we determine that a particular piece of fiction, or a particular kind of fiction, tends to reliably and destructively hack vulnerable minds, that seems a <a href=\"http://www.overcomingbias.com/2007/03/policy_debates_.html\">disproportionate consequence</a> for pulling the wrong book off the shelf.</p>\n<p>So, what books, what films, what stories would you say affected your childhood for the better? &nbsp;What stories do you wish you had encountered earlier? If there are any members of the Bardic Conspiracy present, what sorts of stories should we <em>start</em>&nbsp;telling? Finally, what stories (if any) should young minds not encounter until they have developed some additional robustness?</p>\n<p>ETA: If there are particular stories which you think the (adult) members of the community would benefit from, please feel free to share these as well.</p>\n<p>ETA2: My wildly optimistic best-case scenario for this post would be someone actually writing a rationalist children's story in the comments thread.</p>\n<p>ETA3: On second thought, this edit has become <a href=\"/lw/3c/rationalist_storybooks_a_challenge/\">its own post</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4GeE83592epCErQse", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 31, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "98", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Follow_up_To__On_the_Care_and_Feeding_of_Young_Rationalists\">Follow-up To:&nbsp;<span style=\"font-weight: normal;\"><a href=\"/lw/25/on_the_care_and_feeding_of_young_rationalists/\">On the Care and Feeding of Young Rationalists</a></span></strong></p>\n<p><strong id=\"Related_on_OB__Formative_Youth\">Related on OB:&nbsp;<span style=\"font-weight: normal;\"><a href=\"http://www.overcomingbias.com/2009/02/formative-youth.html\">Formative Youth</a></span></strong></p>\n<p>Eliezer suspects he may have chosen an altruistic life <a href=\"http://www.overcomingbias.com/2009/02/formative-youth.html\">because of Thundercats</a>.</p>\n<p>Nominull thinks his path to truth-seeking might have been lit <a href=\"/lw/2/tell_your_rationalist_origin_story/1io#comments\">by Asimov's Robot stories</a>.</p>\n<p>PhilGoetz&nbsp;<a href=\"/lw/30/science_vs_art/\">suggests</a>&nbsp;that Ender's Game has&nbsp;<a href=\"http://plover.net/~bonds/ender.html\">warped the psyches of many intelligent people</a>.</p>\n<p>For good or ill, we seem to agree that fiction strongly influences the way we grow up, and the people we come to be.</p>\n<p>So for those of us with the tremendous task of <a href=\"http://www.overcomingbias.com/2008/12/no-unbirthing.html\">bringing new sentience into the world</a>, it seems sensible to spend some time thinking about what fictions our charges will be exposed to.</p>\n<p><a id=\"more\"></a></p>\n<p>The natural counter-part to this question is, of course, are there any particular fictions, or types of fiction, to which we should <em>avoid</em> exposing our children?</p>\n<p>Again, this is a pattern we see more commonly in the <a href=\"http://www.christianitytoday.com/movies/\">religious community</a> -- and the rest of us tend to look on and laugh at the prudery on display. Still, the general idea doesn't seem to be something we can reject out of hand. So far as we can tell, all (currently existing) minds are vulnerable to being hacked, young minds more than others. If we determine that a particular piece of fiction, or a particular kind of fiction, tends to reliably and destructively hack vulnerable minds, that seems a <a href=\"http://www.overcomingbias.com/2007/03/policy_debates_.html\">disproportionate consequence</a> for pulling the wrong book off the shelf.</p>\n<p>So, what books, what films, what stories would you say affected your childhood for the better? &nbsp;What stories do you wish you had encountered earlier? If there are any members of the Bardic Conspiracy present, what sorts of stories should we <em>start</em>&nbsp;telling? Finally, what stories (if any) should young minds not encounter until they have developed some additional robustness?</p>\n<p>ETA: If there are particular stories which you think the (adult) members of the community would benefit from, please feel free to share these as well.</p>\n<p>ETA2: My wildly optimistic best-case scenario for this post would be someone actually writing a rationalist children's story in the comments thread.</p>\n<p>ETA3: On second thought, this edit has become <a href=\"/lw/3c/rationalist_storybooks_a_challenge/\">its own post</a>.</p>", "sections": [{"title": "Follow-up To:\u00a0On the Care and Feeding of Young Rationalists", "anchor": "Follow_up_To__On_the_Care_and_Feeding_of_Young_Rationalists", "level": 1}, {"title": "Related on OB:\u00a0Formative Youth", "anchor": "Related_on_OB__Formative_Youth", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "135 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 135, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SWraogEDJ6gocpvwa", "BnoFcEkKE9syNCqWw", "tHJ43CyPdEkQ9Dfup"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-17T13:39:29.073Z", "modifiedAt": null, "url": null, "title": "Rational Me or We?", "slug": "rational-me-or-we", "viewCount": null, "lastCommentedAt": "2021-08-03T10:59:27.646Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinHanson", "createdAt": "2009-02-26T13:46:26.443Z", "isAdmin": false, "displayName": "RobinHanson"}, "userId": "P4HT9AG3PuXjZv5Mw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w9kwayt5SWqBQe8Nx/rational-me-or-we", "pageUrlRelative": "/posts/w9kwayt5SWqBQe8Nx/rational-me-or-we", "linkUrl": "https://www.lesswrong.com/posts/w9kwayt5SWqBQe8Nx/rational-me-or-we", "postedAtFormatted": "Tuesday, March 17th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20Me%20or%20We%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20Me%20or%20We%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw9kwayt5SWqBQe8Nx%2Frational-me-or-we%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20Me%20or%20We%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw9kwayt5SWqBQe8Nx%2Frational-me-or-we", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw9kwayt5SWqBQe8Nx%2Frational-me-or-we", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 345, "htmlBody": "<p>Martial arts can be a good training to ensure your personal security, <strong>if</strong> you assume the worst about your tools and environment.&nbsp; If you expect to find yourself unarmed in a dark alley, or fighting hand to hand in a war, it makes sense.&nbsp; But most people do a lot better at ensuring their personal security by coordinating to live in peaceful societies and neighborhoods; they pay someone else to learn martial arts.&nbsp; Similarly, while \"survivalists\" plan and train to stay warm, dry, and fed given worst case assumptions about the world around them, most people achieve these goals by participating in a modern economy.<br /><br />The martial arts metaphor for rationality training seems popular at this website, and most discussions here about how to believe the truth seem to assume an environmental worst case: how to figure out everything for yourself given fixed info and assuming the worst about other folks.&nbsp; In this context, a good rationality test is a publicly-visible personal test, applied to your personal beliefs when you are isolated from others' assistance and info.&nbsp;&nbsp; <br /><br />I'm much more interested in how <strong>we</strong> can can join together to believe truth, and it actually seems easier to design institutions which achieve this end than to design institutions to test individual isolated general tendencies to discern truth.&nbsp; For example, with subsidized prediction markets, we can each specialize on the topics where we contribute best, relying on market consensus on all other topics.&nbsp; We don't each need to train to identify and fix each possible kind of bias; each bias can instead have specialists who look for where that bias appears and then correct it.&nbsp; <br /><br />Perhaps martial-art-style rationality makes sense for isolated survivalist Einsteins forced by humanity's vast stunning cluelessness to single-handedly block the coming robot rampage.&nbsp; But for those of us who respect the opinions of enough others to want to work with them to find truth, it makes more sense to design and field institutions which give each person better incentives to update a common consensus.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "izp6eeJJEg9v5zcur": 2, "ipJwbLxhR83ZksN6Z": 2, "zv7v2ziqexSn5iS9v": 5}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w9kwayt5SWqBQe8Nx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 153, "baseScore": 157, "extendedScore": null, "score": 0.000238, "legacy": true, "legacyId": "114", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 158, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 157, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 15, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-17T14:51:11.304Z", "modifiedAt": null, "url": null, "title": "Dead Aid", "slug": "dead-aid", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:59.153Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z2EDCcQuknWTpc5Fi/dead-aid", "pageUrlRelative": "/posts/z2EDCcQuknWTpc5Fi/dead-aid", "linkUrl": "https://www.lesswrong.com/posts/z2EDCcQuknWTpc5Fi/dead-aid", "postedAtFormatted": "Tuesday, March 17th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dead%20Aid&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADead%20Aid%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz2EDCcQuknWTpc5Fi%2Fdead-aid%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dead%20Aid%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz2EDCcQuknWTpc5Fi%2Fdead-aid", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz2EDCcQuknWTpc5Fi%2Fdead-aid", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 324, "htmlBody": "<p>Followup to <a href=\"/lw/2b/so_you_say_youre_an_altruist/\">So You Say You're an Altruist</a>:</p>\n<p>Today <span id=\"titlered\">Dambisa Moyo's book \"<a href=\"http://www.deadaid.org/deadaid.html\" target=\"_blank\">Dead Aid</a>: </span>Why Aid Is Not Working and How There Is a Better Way for Africa\" was released.</p>\n<p>From the book's website:</p>\n<blockquote>\n<p>In the past fifty years, more than $1 trillion in development-related aid has been transferred from rich countries to Africa. Has this assistance improved the lives of Africans? No. In fact, across the continent, the recipients of this aid are not better off as a result of it, but worse&mdash;much worse.</p>\n<p>In <em>Dead Aid</em>, Dambisa Moyo describes the state of postwar development policy in Africa today and unflinchingly confronts one of the greatest myths of our time: that billions of dollars in aid sent from wealthy countries to developing African nations has helped to reduce poverty and increase growth.</p>\n<p>In fact, poverty levels continue to escalate and growth rates have steadily declined&mdash;and millions continue to suffer. Provocatively drawing a sharp contrast between African countries that have rejected the aid route and prospered and others that have become aid-dependent and seen poverty increase, Moyo illuminates the way in which overreliance on aid has trapped developing nations in a vicious circle of aid dependency, corruption, market distortion, and further poverty, leaving them with nothing but the &ldquo;need&rdquo; for more aid.</p>\n</blockquote>\n<p>From the <a href=\"http://books.global-investor.com/books/251061/Dambisa-Moyo/Dead-Aid:-Destroying-the-Biggest-Global-Myth-of-Our-Time\" target=\"_blank\">Global Investor Bookshop</a>:</p>\n<blockquote>\n<p>Dead Aid analyses the history of economic development over the last fifty years and shows how Aid crowds out financial and social capital and directly causes corruption; the countries that have caught up did so despite rather than because of Aid. There is, however, an alternative. Extreme poverty is not inevitable. Dambisa Moyo also shows how, with improved access to capital and markets and with the right policies, even the poorest nations could be allowed to prosper. If we really do want to help, we have to do more than just appease our consciences, hoping for the best, expecting the worst. We need first to understand the problem.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JsJPrdgRGRqnci8cZ": 1, "qAvbtzdG2A2RBn7in": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z2EDCcQuknWTpc5Fi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 2, "extendedScore": null, "score": 4.818641536405241e-07, "legacy": true, "legacyId": "115", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Dc3bwCjM9HzZcq9M8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-17T19:47:16.021Z", "modifiedAt": null, "url": null, "title": "Tarski Statements as Rationalist Exercise", "slug": "tarski-statements-as-rationalist-exercise", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:07.686Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qJiSvhGyvbgwQcNXn/tarski-statements-as-rationalist-exercise", "pageUrlRelative": "/posts/qJiSvhGyvbgwQcNXn/tarski-statements-as-rationalist-exercise", "linkUrl": "https://www.lesswrong.com/posts/qJiSvhGyvbgwQcNXn/tarski-statements-as-rationalist-exercise", "postedAtFormatted": "Tuesday, March 17th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tarski%20Statements%20as%20Rationalist%20Exercise&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATarski%20Statements%20as%20Rationalist%20Exercise%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqJiSvhGyvbgwQcNXn%2Ftarski-statements-as-rationalist-exercise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tarski%20Statements%20as%20Rationalist%20Exercise%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqJiSvhGyvbgwQcNXn%2Ftarski-statements-as-rationalist-exercise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqJiSvhGyvbgwQcNXn%2Ftarski-statements-as-rationalist-exercise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1148, "htmlBody": "<p><strong>Related to</strong>: <a href=\"http://www.overcomingbias.com/2008/03/dissolving-the.html\">Dissolving the Question</a>, <a href=\"http://www.overcomingbias.com/2008/02/second-law.html\">The Second Law of Thermodynamics, and Engines of Cognition</a>, <a href=\"http://www.overcomingbias.com/2007/10/curiosity.html\">The Meditation on Curiosity</a>.</p>\n<blockquote>\n<p>The sentence \"snow is white\" is true if, and only if, snow is white.</p>\n</blockquote>\n<p>-- A. Tarski</p>\n<p>Several days ago I've spent a couple of hours trying to teach my 15 year old brother how to properly construct Tarski statements. It's quite nontrivial to get right. Learning to place facts and representations in the separate mental buckets is one of the fundamental tools for a rationalist. In our model of the world, information propagates from object to object, from mind to mind. To ascertain the validity of your belief, you need to research the whole network of factors that led you to attain the belief. The simplest relation is between a fact and its representation, idealized to represent correctness or incorrectness only, without yet worrying about probabilities. The same object or the same property can be interpreted to mean different things in different relations and contexts, indicating the truth of one statement or another, and it's important not to conflate those.</p>\n<p>Let's say you are watching news on TV and the next item is an interview with a sasquatch. The sasquatch answers the questions about his family in decent English, with a slight British accent.</p>\n<p>What do you actually observe, how should you interpret the data? Did you \"see a sasquatch\"? Did you learn the facts about sasquatch's family? Is there a fact of the matter, as to whether the sasquatch's daughter is 5 years old, as opposed to 4 or 6?<a id=\"more\"></a></p>\n<p>Meaningfulness of these questions is conditional on their context, like in the notorious \"when did you stop beating your wife?\". These examples seem unnaturally convoluted, but in fact every statement suffers from the same problem, you must cross the levels of indirection and not lose track of the question in order to go from a <em>statement</em> of fact, from a belief in your mind, to the fact that belief is about. First, you must relate \"sasquatch\" on the TV screen to an actual sasquatch, which fails if there isn't one, and then you need to relate the sasquatch to his daughter, which again fails if he lies and doesn't have one.</p>\n<p>By contemplating plausibility of the surface interpretation of a statement that doesn't yield to that interpretation, you fail before you even start. If the surface interpretation of the data doesn't apply, the data doesn't say anything about your interpretation. <a href=\"http://www.overcomingbias.com/2008/02/second-law.html\">To form accurate beliefs about something, you really do have to observe it.</a> When you misinterpret the data, the misinterpretation comes from you, not from the data, and so the data doesn't say anything about the thing you misinterpreted it to mean.</p>\n<p>Every piece of evidence tells <em>something</em> about the world, every lie speaks a hidden truth. By correctly interpreting the data, you may learn about the process that generated it, and further your expertise in correctly interpreting similar data. If your mind lies to you, it is an opportunity to <a href=\"http://www.overcomingbias.com/2008/03/righting-a-wron.html\">learn the algorithms that constructed the lie</a>, to right a wrong question and come out stronger from the experience.</p>\n<p>Consider a setting with a note lying near an apple stating \"the apple is poisonous\". There are two objects in this scene, the apple and the note. We associate an interpretation with the note, a statement of fact \"the apple is poisonous\". This statements corresponds to the note pretty much unambiguously. The statement implies that we should associate a non-obvious interpretation with the apple, \"poisonous thing\". The object (apple) and a newly introduced interpretation (\"poisonous thing\") are related to each other by means of our interpretation of the note. The interpretation of the apple as being poisonous is <em>valid</em> if and only if the statement of fact we read in the note is <em>true</em>. And this is what's stated by the Tarksi statement for this situation. The Tarski statement relates two things: the truth of interpretation of data, and the interpretation of the fact this data is <em>about</em>. It commutes two pathways by which we arrive at the fact stated by the data. First, the fact is an interpretation of the state of the world. We start from the apple, and go to the associated \"poisonous thing\" interpretation. And second, the fact is referred to by the interpretation of the data. You start from a note, proceed to interpreting it to say \"the apple is poisonous\", and interpret the statement a second time to extract from it the relation between the apple and the statement \"poisonous thing\". And so you state: the sentence \"the apple is poisonous\" is true if and only if the apple is poisonous. Or, in other words: the sentence \"the apple is poisonous\" is true if and only if the interpretation \"poisonous thing\" applies to the apple.</p>\n<p>Each of the transitions between the levels of indirection may turn out to be wrong. Of reality, we only see the note and the apple, in this model we are pretty sure they are present. All the rest are the constructions we associated with them, following interpretation of the evidence. First, we construct interpretation of the scribbles on the note. To do that, we must understand that the note is intended to read literally, in the language in which it's written, not in some code, so you won't see \"Tom is a spy\" when you read the note. Second, you interpret the statement that you read from the note to refer to this particular apple, and to a particular property, \"poisonous thing\". The note could be left on the scene by mistake, referring to a different apple. There is no poison in the scene as you see it, the poison is only in the form of your interpretation of the statement read from the note. And then you decide whether the statement is true, and if you decide that it is, you make a new connection between the property \"poisonous thing\" and the apple, you start interpreting the apple itself as \"poisonous thing\".</p>\n<p>We build our knowledge about the world step by step, and every step hides a potential error. Consciously inspecting the suspect steps requires constructing a model of these steps, with all the necessary parts. Rational analysis of beliefs and decisions in the real world requires not just understanding of mathematics of probability theory and decision theory, but also the skill of correctly applying those theories to the real-life situations. Learning to explore the relations between the truth of statements of fact and perception of the facts referred to by these statements may be a valuable exercise. Constructing Tarski statements requires mathematical thinking, keeping in mind many objects and relations between them, interpreting the objects according to the intention of current inquiry, constructing new relations based on those already present, and integrating them into the understanding of the problem. At the same time, the domain of Tarski statements is the reality itself, \"obvious\" common-sense knowledge.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"E6qP9r9xxM4LCxaFk": 1, "LnEEs8xGooYmQ8iLA": 1, "Ng8Gice9KNkncxqcj": 1, "RMtdp6eGNjTZcmwJ6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qJiSvhGyvbgwQcNXn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 12, "extendedScore": null, "score": 4.819067498872344e-07, "legacy": true, "legacyId": "117", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-18T00:30:00.000Z", "modifiedAt": null, "url": null, "title": "The Pascal's Wager Fallacy Fallacy", "slug": "the-pascal-s-wager-fallacy-fallacy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:37.473Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TQSb4wd6v5C3p6HX2/the-pascal-s-wager-fallacy-fallacy", "pageUrlRelative": "/posts/TQSb4wd6v5C3p6HX2/the-pascal-s-wager-fallacy-fallacy", "linkUrl": "https://www.lesswrong.com/posts/TQSb4wd6v5C3p6HX2/the-pascal-s-wager-fallacy-fallacy", "postedAtFormatted": "Wednesday, March 18th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Pascal's%20Wager%20Fallacy%20Fallacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Pascal's%20Wager%20Fallacy%20Fallacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTQSb4wd6v5C3p6HX2%2Fthe-pascal-s-wager-fallacy-fallacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Pascal's%20Wager%20Fallacy%20Fallacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTQSb4wd6v5C3p6HX2%2Fthe-pascal-s-wager-fallacy-fallacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTQSb4wd6v5C3p6HX2%2Fthe-pascal-s-wager-fallacy-fallacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 882, "htmlBody": "<p>Today at lunch I was discussing interesting facets of second-order logic, such as the (known) fact that first-order logic cannot, in general, distinguish finite models from infinite models.  The conversation branched out, as such things do, to why you would <em>want</em> a cognitive agent to think about finite numbers that were unboundedly large, as opposed to boundedly large.</p><p>So I observed that:</p><ol><li>Although the laws of physics as we know them don&#x27;t allow any agent to survive for infinite subjective time (do an unboundedly long sequence of computations), it&#x27;s possible that our model of physics is mistaken.  (I go into some detail on this possibility below the cutoff.)</li><li>If it <em>is</em> possible for an agent - or, say, the human species - to have an infinite future, and you cut yourself off from that infinite future and end up stuck in a future that is merely very large, this one mistake outweighs all the finite mistakes you made over the course of your existence.</li></ol><p>And the one said, &quot;Isn&#x27;t that a form of Pascal&#x27;s Wager?&quot;</p><p>I&#x27;m going to call this the Pascal&#x27;s Wager Fallacy Fallacy.</p><p>You see it all the time in discussion of cryonics.  The one says, &quot;If cryonics works, then the payoff could be, say, at least a thousand additional years of life.&quot;  And the other one says, &quot;Isn&#x27;t that a form of Pascal&#x27;s Wager?&quot;</p><p>The original problem with Pascal&#x27;s Wager is not that <em>the purported payoff is large.</em>  This is <em>not</em> where the flaw in the reasoning comes from.  That is not the problematic step.  The problem with Pascal&#x27;s original Wager is that <em>the probability is exponentially tiny </em>(in the <a href=\"https://www.lesswrong.com/lw/jp/occams_razor/\">complexity</a> of the Christian God) and that <em>equally large tiny probabilities offer opposite payoffs for the same action</em> (the Muslim God will damn you for believing in the Christian God).</p><p></p><p>However, what we have here is the term &quot;Pascal&#x27;s Wager&quot; being applied <em>solely because the payoff being considered is large</em> - the reasoning being perceptually recognized as an instance of &quot;the Pascal&#x27;s Wager fallacy&quot; as soon as someone mentions a big payoff - without any attention being given to whether the probabilities are in fact small or whether counterbalancing anti-payoffs exist.</p><p>And then, once the reasoning is perceptually recognized as an instance of &quot;the Pascal&#x27;s Wager fallacy&quot;, the other characteristics of the fallacy are <a href=\"https://www.lesswrong.com/lw/ng/words_as_hidden_inferences/\">automatically inferred</a>: they <em>assume</em> that the probability is tiny and that the scenario has no specific support apart from the payoff.</p><p>But infinite physics and cryonics are both possibilities that, leaving their payoffs entirely aside, get significant chunks of probability mass purely on merit.</p><p>Yet instead we have reasoning that runs like this:</p><ol><li>Cryonics has a large payoff;</li><li>Therefore, the argument carries even if the probability is tiny;</li><li>Therefore, the probability is tiny;</li><li>Therefore, why bother thinking about it? </li></ol><p>(Posted here instead of <a href=\"https://www.lesswrong.com/\">Less Wrong</a>, at least for now, because of the Hanson/Cowen debate on cryonics.)</p><p>Further details:</p><p>Pascal&#x27;s Wager is actually a serious problem for those of us who want to use Kolmogorov complexity as an Occam prior, because the size of even the finite computations blows up much faster than their probability diminishes (see <a href=\"https://www.lesswrong.com/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">here</a>).</p><p>See Bostrom on <a href=\"http://www.nickbostrom.com/ethics/infinite.pdf\">infinite ethics</a> for how much worse things get if you allow non-halting Turing machines.</p><p>In our current model of physics, time is infinite, and so the collection of real things is infinite.  Each time state has a successor state, and there&#x27;s no particular assertion that time returns to the starting point.  Considering time&#x27;s continuity just makes it worse - now we have an uncountable set of real things!</p><p>But current physics also says that any finite amount of matter can only do a finite amount of computation, and the universe is expanding too fast for us to collect an infinite amount of matter.  We cannot, on the face of things, expect to think an unboundedly long sequence of thoughts.</p><p>The laws of physics <em>cannot </em>be easily modified to permit immortality: lightspeed limits and an expanding universe and holographic limits on quantum entanglement and so on all make it <em>inconvenient</em> to say the least.</p><p>On the other hand, many <em>computationally simple</em> laws of physics, like the laws of Conway&#x27;s Life, permit indefinitely running Turing machines to be encoded.  So we can&#x27;t say that it requires a <em>complex</em> miracle for us to confront the prospect of unboundedly long-lived, unboundedly large civilizations.  Just there being <em>a lot more to discover</em> about physics - say, one more discovery of the size of quantum mechanics or Special Relativity - might be enough to knock (our model of) physics out of the region that corresponds to &quot;You can only run boundedly large Turing machines&quot;.</p><p>So while we have no particular reason to <em>expect </em>physics to allow unbounded computation, it&#x27;s not a <em>small, special, unjustifiably singled-out possibility</em> like the Christian God; it&#x27;s a <em>large </em>region of what various possible physical laws will allow.</p><p>And cryonics, of course, is the <em>default</em> extrapolation from known neuroscience: if memories are stored the way we now think, and cryonics organizations are not disturbed by any particular catastrophe, and technology goes on advancing toward the physical limits, then it is possible to revive a cryonics patient (and yes <a href=\"https://www.lesswrong.com/lw/r9/quantum_mechanics_and_personal_identity/\">you are the same person</a>).  There are negative possibilities (woken up in dystopia and not allowed to die) but they are exotic, not having equal probability weight to counterbalance the positive possibilities.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dJ6eJxJrCEget7Wb6": 1, "ZnHkaTkxukegSrZqE": 1, "5f5c37ee1b5cdee568cfb2aa": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TQSb4wd6v5C3p6HX2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 38, "extendedScore": null, "score": 6.2e-05, "legacy": true, "legacyId": "1260", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": false, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": "postCommentsOld", "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 128, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["f4txACqDWithRi7hs", "3nxs2WYDGzJbzcLMp", "a5JAiTdytou3Jg749", "7HMSBiEiCfLKzd2gc"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-18T00:30:27.152Z", "modifiedAt": null, "url": null, "title": "Never Leave Your Room", "slug": "never-leave-your-room", "viewCount": null, "lastCommentedAt": "2019-04-30T02:12:51.372Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Scott Alexander", "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZmQv4DFx6y4jFbhLy/never-leave-your-room", "pageUrlRelative": "/posts/ZmQv4DFx6y4jFbhLy/never-leave-your-room", "linkUrl": "https://www.lesswrong.com/posts/ZmQv4DFx6y4jFbhLy/never-leave-your-room", "postedAtFormatted": "Wednesday, March 18th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Never%20Leave%20Your%20Room&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANever%20Leave%20Your%20Room%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZmQv4DFx6y4jFbhLy%2Fnever-leave-your-room%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Never%20Leave%20Your%20Room%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZmQv4DFx6y4jFbhLy%2Fnever-leave-your-room", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZmQv4DFx6y4jFbhLy%2Fnever-leave-your-room", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1026, "htmlBody": "<p><strong>Related to: </strong><a href=\"http://www.overcomingbias.com/2007/10/priming-and-con.html\">Priming and Contamination</a></p>\n<p>Psychologists define \"priming\" as the ability of a stimulus to activate the brain in such a way as to affect responses to later stimuli. If that doesn't sound sufficiently ominous, feel free to re-word it as \"any random thing that happens to you can hijack your judgment and personality for the next few minutes.\"<br /><br />For example, let's say you walk into a room and notice a briefcase in the corner. Your brain is now the proud owner of the activated concept \"briefcase\". It is \"primed\" to think about briefcases, and by extension about offices, business, competition, and ambition. For the next few minutes, you will shift ever so slightly towards perceiving all social interactions as competitive, and towards behaving competitively yourself. These slight shifts will be large enough to be measured by, for example, how much money you offer during the Ultimatum Game. If that sounds too much like some sort of weird New Age sympathetic magic to believe, all I can say is <a href=\"http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6WP2-4CVX3K0-1&amp;_user=10&amp;_rdoc=1&amp;_fmt=&amp;_orig=search&amp;_sort=d&amp;view=c&amp;_acct=C000050221&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5=22c30fc810150e9551a808391488f853\">Kay, Wheeler, Bargh, and Ross, 2004</a>.<sup>1</sup><br /><br />We've been discussing the costs and benefits of Santa Claus recently. Well, here's one benefit: show Dutch children an image of St. Nicholas' hat, and they'll be more likely to share candy with others. Why? The researchers hypothesize that the hat activates the concept of St. Nicholas, and St. Nicholas activates an idealized concept of sharing and giving. The child is now primed to view sharing positively. Of course, the same effect can be used for evil. In the same <a href=\"http://www3.interscience.wiley.com/journal/117858904/abstract\">study</a>, kids shown the Toys 'R' Us logo refused to share their precious candy with anyone.<br /><br />But this effect is limited to a few psych laboratories, right? It hasn't done anything like, you know, determine the outcome of a bunch of major elections?</p>\n<p><a id=\"more\"></a><br /><br />I am aware of two good studies on the effect of priming in politics. In <a href=\"http://psp.sagepub.com/cgi/content/abstract/30/9/1136\">the first</a>, subjects were subliminally<sup>2</sup> primed with either alphanumeric combinations that recalled the 9/11 WTC attacks (ie \"911\" or \"WTC\"), or random alphanumeric combinations. Then they were asked to rate the Bush administration's policies. Those who saw the random strings rated Bush at an unenthusiastic 42% (2.1/5). Those who were primed to be thinking about the War on Terror gave him an astounding 75% (3.75/5). This dramatic a change, even though none of them could consciously recall seeing terrorism-related stimuli.<br /><br />In the <a href=\"http://www.pnas.org/content/105/26/8846.full\">second study</a>, scientists analyzed data from the 2000 election in Arizona, and found that polling location had a moderate effect on voting results. That is, people who voted in a school were more likely to support education-friendly policies, people who voted in a church were more likely to support socially conservative policies, et cetera. The effect seems to have shifted results by about three percentage points. Think about all the elections that were won or lost by less than three percent...<br /><br />Objection: correlation is not causation! Religious people probably live closer to churches, and are more likely to know where their local church is, and so on. So the scientists performed an impressive battery of regression analyses and adjustments on their data. Same response.<br /><br />Objection: maybe their adjustments weren't good enough! The same scientists then called voters into their laboratory, showed them pictures of buildings, and asked them to cast a mock vote on the education initiatives. Voters who saw pictures of schools were more likely to vote yes on the pro-education initiatives than voters who saw control buildings.<br /><br />What techniques do these studies suggest for rationalists? I'm tempted to say the optimal technique is to never leave your room, but there are still a few less extreme things you can do. First, avoid exposure to any salient stimuli in the few minutes before making an important decision. Everyone knows about the 9-11 terrorist attacks, but the War on Terror only hijacked the decision-making process when the subjects were exposed to the related stimuli directly before performing the rating task<sup>3</sup>.<br /><br />Second, try to make decisions in a neutral environment and then stick to them. The easiest way to avoid having your vote hijacked by the location of your polling place is to decide how to vote while you're at home, and then stick to that decision unless you have some amazing revelation on your way to the voting booth. Instead of never leaving your room, you can make decisions in your room and then carry them out later in the stimulus-laden world.<br /><br />I can't help but think of the long tradition of master rationalists \"blanking their mind\" to make an important decision. Jeffreyssai's brain \"carefully put in idle\" as he descends to a bare white room to stage his <a href=\"http://www.overcomingbias.com/2008/10/the-question.html\">crisis of faith</a>. Anas&ucirc;rimbor Kellhus withdrawing into himself and entering a probability trance before he finds the Shortest Path. Your grandmother telling you to \"sleep on it\" before you make an important life choice.<br /><br />Whether or not you try anything as formal as that, waiting a few minutes in a stimulus-free environment before a big decision might be a good idea.</p>\n<p>&nbsp;</p>\n<p><strong>Footnotes</strong></p>\n<p><strong>1: </strong>I bet that sympathetic magic <a href=\"/lw/2p/the_skeptics_trilemma/\">probably does</a> have strong placebo-type effects for exactly these reasons, though.</p>\n<p><strong>2: </strong>Priming is one of the phenomena behind all the hype about subliminal advertising and other subliminal effects. The bad news is that it's real: a picture of popcorn flashed subliminally on a movie screen <em>can</em> make you think of popcorn. The good news is that it's not particularly dangerous: your thoughts of popcorn aren't any stronger or any different than they'd be if you just saw a normal picture of popcorn.</p>\n<p><strong>3: </strong>The obvious objection is that if you're evaluating George Bush, it would be very strange if you didn't think of the 9-11 terror attacks yourself in the course of the evaluation. I haven't seen any research addressing this possibility, but maybe hearing an external reference to it outside the context of your own thought processes is a stronger activation than the one you would get by coming up with the idea yourself.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vg4LDxjdwHLotCm8w": 4, "4R8JYu4QF2FqzJxE5": 1, "5f5c37ee1b5cdee568cfb128": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZmQv4DFx6y4jFbhLy", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 79, "baseScore": 77, "extendedScore": null, "score": 0.000118, "legacy": true, "legacyId": "119", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": true, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 78, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["M7rwT264CSYY6EdR3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-18T02:25:30.475Z", "modifiedAt": null, "url": null, "title": "Rationalist Storybooks: A Challenge", "slug": "rationalist-storybooks-a-challenge", "viewCount": null, "lastCommentedAt": "2021-11-11T04:29:48.332Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tHJ43CyPdEkQ9Dfup/rationalist-storybooks-a-challenge", "pageUrlRelative": "/posts/tHJ43CyPdEkQ9Dfup/rationalist-storybooks-a-challenge", "linkUrl": "https://www.lesswrong.com/posts/tHJ43CyPdEkQ9Dfup/rationalist-storybooks-a-challenge", "postedAtFormatted": "Wednesday, March 18th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationalist%20Storybooks%3A%20A%20Challenge&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationalist%20Storybooks%3A%20A%20Challenge%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtHJ43CyPdEkQ9Dfup%2Frationalist-storybooks-a-challenge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationalist%20Storybooks%3A%20A%20Challenge%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtHJ43CyPdEkQ9Dfup%2Frationalist-storybooks-a-challenge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtHJ43CyPdEkQ9Dfup%2Frationalist-storybooks-a-challenge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 351, "htmlBody": "<p><strong>Follow-Up to: </strong><a href=\"/lw/2q/on_juvenile_fiction/\">On Juvenile Fiction</a></p>\n<p><strong>Related to: </strong><a href=\"http://yudkowsky.net/rational/the-simple-truth\">The Simple Truth</a></p>\n<p><span style=\"font-family: Arial;\">I quote again from JulianMorrison, who <span style=\"font-family: Verdana;\"><a href=\"/lw/1e/raising_the_sanity_waterline/16e#comments\">writes</a>:</span></span></p>\n<blockquote>\n<p style=\"margin: 0.0px 0.0px 13.0px 0.0px; text-align: justify; line-height: 19.0px; font: 13.0px Arial; background-color: #f7f7f8;\">If you want people to repeat this back, write it in a test, <em>maybe</em> even apply it in an academic context, a four-credit undergrad course will work.</p>\n<p style=\"margin: 0.0px 0.0px 13.0px 0.0px; text-align: justify; line-height: 19.0px; font: 13.0px Arial; background-color: #f7f7f8;\">If you want them to have it as the ground state of their mind in everyday life, you probably need to have taught them songs about it in kindergarten.</p>\n</blockquote>\n<p>Anonym <a href=\"/lw/2q/on_juvenile_fiction/2bk#comments\">adds</a>:</p>\n<blockquote>\n<p>Imagine a world in which 8-year olds grok things like confirmation bias and the base-rate fallacy on an intuitive level because they are reminded of their favorite childhood stories and the lessons they internalized after having the story read to them again and again. What a wonderful foundation to build upon.</p>\n</blockquote>\n<p>With this in mind, here is my <a href=\"http://www.overcomingbias.com/2008/05/class-project.html\">challenge</a>:</p>\n<p><strong>Look through Eliezer's early <a href=\"http://www.cs.auckland.ac.nz/~andwhay/postlist.html\">standard bias posts</a>.&nbsp; Can you convey the essential content of one of these posts in a 16-page picture book, or in a nursery rhyme children could sing while they skip rope?</strong></p>\n<p>Write the story, and post it here.&nbsp; Let's see what we can come up with.</p>\n<p><a id=\"more\"></a>This is not, <em>by any means</em> intended to be a simple challenge.&nbsp; On the one hand, we are compressing a <em>lot</em> of information into a small space.&nbsp; On the other, good fiction is not easy, and children's fiction is no exception.</p>\n<p>We have two options.&nbsp; We can <a href=\"http://www.overcomingbias.com/2006/12/the_proper_use_.html\">humbly admit</a> that we are not skilled writers of children's fiction and walk away, or we can determine that this is a task which needs to be completed, produce lots of really bad fiction, and begin the process of criticizing one another, learning from our mistakes, and <a href=\"http://www.overcomingbias.com/2007/03/tsuyoku_naritai.html\">growing stronger</a>.</p>\n<p>When I was a boy, I had a thick book of 365 short stories, some not even taking up a full page.&nbsp; Each was self-contained, and I could flip open the book at random and find a story I hadn't read before.</p>\n<p>How quickly would our community grow, <a href=\"/lw/36/rational_me_or_we/\">both in strength and in numbers</a>, if we could crowdsource a<del></del> <em>Rationalist's Book of Tales</em>?</p>\n<p>I know, I know.&nbsp; It's optimistic. It's ambitious. Most of all, it seems <em>really silly</em>.</p>\n<p>Let's <a href=\"http://www.overcomingbias.com/2008/10/use-the-try-har.html\">do it</a> anyway.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"7mTviCYysGmLqiHai": 1, "Q55STnFh6gbSezRuR": 1, "izp6eeJJEg9v5zcur": 1, "GBpwq8cWvaeRoE9X5": 1, "QPt5ECwTCAg63mbNu": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tHJ43CyPdEkQ9Dfup", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 39, "extendedScore": null, "score": 0.000550405687542528, "legacy": true, "legacyId": "120", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4GeE83592epCErQse", "w9kwayt5SWqBQe8Nx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-18T20:18:43.240Z", "modifiedAt": null, "url": null, "title": "A corpus of our community's knowledge", "slug": "a-corpus-of-our-community-s-knowledge", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:07.872Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "infotropism", "createdAt": "2009-02-27T21:33:06.774Z", "isAdmin": false, "displayName": "infotropism"}, "userId": "X2yCuTTPzYdobZhii", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LxqwJyH2BQBYgap7N/a-corpus-of-our-community-s-knowledge", "pageUrlRelative": "/posts/LxqwJyH2BQBYgap7N/a-corpus-of-our-community-s-knowledge", "linkUrl": "https://www.lesswrong.com/posts/LxqwJyH2BQBYgap7N/a-corpus-of-our-community-s-knowledge", "postedAtFormatted": "Wednesday, March 18th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20corpus%20of%20our%20community's%20knowledge&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20corpus%20of%20our%20community's%20knowledge%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLxqwJyH2BQBYgap7N%2Fa-corpus-of-our-community-s-knowledge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20corpus%20of%20our%20community's%20knowledge%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLxqwJyH2BQBYgap7N%2Fa-corpus-of-our-community-s-knowledge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLxqwJyH2BQBYgap7N%2Fa-corpus-of-our-community-s-knowledge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 271, "htmlBody": "<p>The purpose of this site is to help building a rationalist community, and helping individuals to fulfill their potential in that domain.</p>\n<p>We have a lot of discussions going on, and a lot of material is being, and going to be, generated. At some point it may become difficult for any single individual to follow all of it. Even taking the karma system into account, interesting contributions may be missed by any particular individual. Furthermore, the sum of what would be elaborated upon here would not be as concise or even easily available as it could be wished to be.</p>\n<p>To the point : would it be a good idea to try to summarize the most important, relevant ideas upon which we will be building our edifice ? So that a future student of rationality can come upon a concise, easy to digest introduction to our results and ideas, so that less active members can still manage to follow this ongoing process too ?</p>\n<p>If so, how would we proceed ? What is being discussed here may not have the quality we'd expect of, say, a scientific publication, though I think that such a quality would be necessary, if even sufficient, for what would eventually become our own corpus of knowledge. How would we elaborate, layer upon layer of work and discussions ? A starting point would be to refer to, or summarize the relevant, existing scientific results that we would lay our base upon. We'd then move on to summarizing our most important achievements, however that word is to be taken, seamlessly upon that foundation.</p>\n<p>Any thought on how or whether to organize this?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LxqwJyH2BQBYgap7N", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 8, "extendedScore": null, "score": 4.821192757792419e-07, "legacy": true, "legacyId": "124", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-18T21:30:59.049Z", "modifiedAt": null, "url": null, "title": "Little Johny Bayesian", "slug": "little-johny-bayesian", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:05.944Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Darmani", "createdAt": "2009-02-28T02:13:51.400Z", "isAdmin": false, "displayName": "Darmani"}, "userId": "MD49Fa8rRXChHwYoS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6r2t5qTxjHWxG5DTj/little-johny-bayesian", "pageUrlRelative": "/posts/6r2t5qTxjHWxG5DTj/little-johny-bayesian", "linkUrl": "https://www.lesswrong.com/posts/6r2t5qTxjHWxG5DTj/little-johny-bayesian", "postedAtFormatted": "Wednesday, March 18th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Little%20Johny%20Bayesian&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALittle%20Johny%20Bayesian%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6r2t5qTxjHWxG5DTj%2Flittle-johny-bayesian%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Little%20Johny%20Bayesian%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6r2t5qTxjHWxG5DTj%2Flittle-johny-bayesian", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6r2t5qTxjHWxG5DTj%2Flittle-johny-bayesian", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 455, "htmlBody": "<p><strong>Followup to: </strong><a href=\"/lw/3c/rationalist_storybooks_a_challenge\">Rationalist Storybooks: A Challenge</a></p>\n<p>This was originally a comment in response to a challenge to create a nursery rhyme conveying rationality concepts, but, at the suggestion of Eliezer, I've made it into its own post.</p>\n<p>&nbsp;</p>\n<p>Little Johny thought he was very bright, <br /> But the schoolkids did not -- they would laugh when he came in sight. <br /> He could count, sing, and guess the weather. <br /> Then one day, Big Bill said \"Real bright boys will grow a feather.\"</p>\n<p>\"Ach!\" he cried, \"Could it be true?\" <br /> \"Then I'm not bright, which makes me blue.\" <br /> So he went home, and searched all over. <br /> And then found growth on his head, clear as a clover.<a id=\"more\"></a></p>\n<p>\"It is true, feathers are sprouting!\" <br /> \"It's proof that I'm bright!\" So he stopped pouting. <br /> He ran to show his mom, nearly tripping over some eggs, <br /> When he saw on TV \"Bright boys will grow long legs.\"</p>\n<p>So he waited for weeks and weeks for to find proof, <br /> Worried over his brightness, and staying quite aloof, <br /> Until one day, feeling in a pinch, <br /> He grabbed a tape measure, and found his legs had grown a whole inch!</p>\n<p>So he leaped off to school, but a scientist walked by, <br /> And Johny overheard him say, that real bright boys could fly. <br /> \"The hair, the legs, from these I know <br /> Of my brightness. The flying thus follows, so...\"</p>\n<p>Little Johny plotted of his grand display, <br /> Standing high on a wall, he would proudly say <br /> \"Behold, I have proof that I'm bright!\" <br /> And he would deftly leap off, and soar into flight.</p>\n<p>So he climbed up the wall, and made his speech, <br /> But there his plan stopped with a screech, <br /> For he hit the ground hard with a smack, <br /> Leaving his leg all bloody and black.</p>\n<p>As the other children laughed, he tried to explain, <br /> Of the things that he heard, and why he had taken it to his brain, <br /> \"They came from on high, from people who knew <br /> I looked at myself, and saw they were true.\"</p>\n<p>They laughed, \"You're too eager to believe, you fool. <br /> Your feathers are just hair, all boys grow long legs as a rule. <br /> Yes, if all you heard were true, you'd fly, but you'll find out, <br /> That if you do logic with garbage, then you'll get garbage out.\"</p>\n<p>So Johny thought wrongly, and got his leg in a cast, <br /> He had sought fame in the schoolyard, but now that's all past. <br /> He's taken the lesson to heart, no longer believes all he hears. <br /> So he doesn't believe them when they say he's not bright -- brightness doesn't come from peers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AXhEhCkTrHZbjXXu3": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6r2t5qTxjHWxG5DTj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 16, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "126", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tHJ43CyPdEkQ9Dfup"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-19T01:07:52.097Z", "modifiedAt": null, "url": null, "title": "How to Not Lose an Argument", "slug": "how-to-not-lose-an-argument", "viewCount": null, "lastCommentedAt": "2021-03-23T15:30:27.559Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6yTShbTdtATxKonY5/how-to-not-lose-an-argument", "pageUrlRelative": "/posts/6yTShbTdtATxKonY5/how-to-not-lose-an-argument", "linkUrl": "https://www.lesswrong.com/posts/6yTShbTdtATxKonY5/how-to-not-lose-an-argument", "postedAtFormatted": "Thursday, March 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Not%20Lose%20an%20Argument&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Not%20Lose%20an%20Argument%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6yTShbTdtATxKonY5%2Fhow-to-not-lose-an-argument%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Not%20Lose%20an%20Argument%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6yTShbTdtATxKonY5%2Fhow-to-not-lose-an-argument", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6yTShbTdtATxKonY5%2Fhow-to-not-lose-an-argument", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1868, "htmlBody": "<p><strong>Related to: </strong><a href=\"http://www.overcomingbias.com/2008/02/leave-retreat.html\">Leave a Line of Retreat</a></p>\n<p><strong>Followup to: </strong><a href=\"/lw/2d/talking_snakes_a_cautionary_tale/\">Talking Snakes: A Cautionary Tale</a>, <a href=\"/lw/2p/the_skeptics_trilemma/\">The Skeptic's Trilemma</a><em></em></p>\n<p style=\"padding-left: 30px;\"><em>\"I argue very well. Ask any of my remaining friends. I can win an argument on any topic, against any opponent. People know this, and steer clear of me at parties. Often, as a sign of their great respect, they don't even invite me.\"</em></p>\n<p style=\"padding-left: 30px;\">&nbsp; &nbsp; &nbsp; &nbsp; --Dave Barry</p>\n<p>The science of winning arguments is called Rhetoric, and it is one of the Dark Arts. Its study is forbidden to rationalists, and its tomes and treatises are kept under lock and key in a particularly dark corner of the Miskatonic University library. More than this it is not lawful to speak.<br /><br />But I do want to talk about a very closely related skill: not losing arguments.<br /><br />Rationalists probably find themselves in more arguments than the average person. And if we're doing it right, the truth is hopefully on our side and the argument is ours to lose. And far too often, we <em>do </em>lose arguments, even when we're right. Sometimes it's because of biases or inferential distances or other things that can't be helped. But all too often it's because we're shooting ourselves in the foot.<br /><br />How does one avoid shooting one's self in the foot? In rationalist language, the technique is called Leaving a Social Line of Retreat. In normal language, it's called being nice.<br /><a id=\"more\"></a><br />First, what does it mean to win or lose an argument? There is an unspoken belief in some quarters that the point of an argument is to gain social status by utterly demolishing your opponent's position, thus proving yourself the better thinker. That can be fun sometimes, and if it's really all you want, go for it. <br /><br />But the most important reason to argue with someone is to change his mind. If you want a world without fundamentalist religion, you're never going to get there just by making cutting and incisive critiques of fundamentalism that all your friends agree sound really smart. You've got to deconvert some actual fundamentalists. In the absence of changing someone's mind, you can at least get them to see your point of view. Getting fundamentalists to understand the <a href=\"http://www.overcomingbias.com/2008/10/traditional-cap.html\">real reasons</a> people find atheism attractive is a nice consolation prize.<br /><br />I make the anecdotal observation that a lot of smart people are very good at winning arguments in the first sense, and very bad at winning arguments in the second sense. Does that correspond to your experience?<br /><br />Back in 2008, Eliezer described how to <a href=\"http://www.overcomingbias.com/2008/02/leave-retreat.html\">Leave a Line of Retreat</a>. If you believe morality is impossible without God, you have a strong disincentive to become an atheist. Even after you've realized which way the evidence points, you'll activate every possible defense mechanism for your religious beliefs. If all the defense mechanisms fail, you'll take God on utter faith or just <a href=\"/lw/r/no_really_ive_deceived_myself/\">believe in belief</a>, rather than surrender to the unbearable position of an immoral universe.<br /><br />The correct procedure for dealing with such a person, Eliezer suggests, isn't to show them yet another reason why God doesn't exist. They'll just reject it along with all the others. The correct procedure is to convince them, on a gut level, that morality is possible even in a godless universe. When disbelief in God is no longer so terrifying, people won't fight it quite so hard and may even deconvert themselves.<br /><br />But there's another line of retreat to worry about, one I experienced firsthand in a very strange way. I had a dream once where God came down to Earth; I can't remember exactly why. In the borderlands between waking and sleep, I remember thinking: <em>I feel like a total moron</em>. Here I am, someone who goes to atheist groups and posts on atheist blogs and has told all his friends they should be atheists and so on, and now it turns out God exists. All of my religious friends whom I won all those arguments against are going to be secretly looking at me, trying as hard as they can to be nice and understanding, but secretly laughing about how I got my comeuppance. I can never show my face in public again. Wouldn't you feel the same?<br /><br />And then I woke up, and shook it off. I am an aspiring rationalist: if God existed, I would desire to believe that God existed. But I realized at that point the importance of the social line of retreat. The psychological resistance I felt to admitting God's existence, even after having seen Him descend to Earth, was immense. And, I realized, it was exactly the amount of resistance that every vocally religious person must experience towards God's <em>non</em>-existence.<br /><br />There's not much we can do about this sort of high-grade long-term resistance. Either a person has enough of the <a href=\"http://yudkowsky.net/rational/virtues\">rationalist virtues</a> to overcome it, or he doesn't. But there is a less ingrained, more immediate form of social resistance generated with every heated discussion.<br /><br />Let's say you approach a theist (let's call him Theo) and say \"How can you, a grown man, still believe in something stupid like <a href=\"/lw/2d/talking_snakes_a_cautionary_tale/\">talking snakes</a> and magic sky kings? Don't you know you people are responsible for the Crusades and the Thirty Years' War and the Spanish Inquisition? You should be ashamed of yourself!\"<br /><br />This suggests the following dichotomy in Theo's mind: <strong>EITHER</strong> God exists, <strong>OR</strong> I am an idiot who believes in stupid childish&nbsp; things and am in some way partly responsible for millions of deaths and I should have lower status and this arrogant person who's just accosted me and whom I already hate should have higher status at my expense.<br /><br />Unless Theo has attained a level of rationality far beyond any of us, guess which side of that dichotomy he's going to choose? In fact, guess which side of that dichotomy he's now going to support with renewed vigor, even if he was only a lukewarm theist before? His social line of retreat has been completely closed off, and it's <em>your</em> fault.<br /><br />Here the two definitions of \"winning an argument\" I suggested before come into conflict. If your goal is to absolutely demolish the other person's position, to make him feel awful and worthless - then you are also very unlikely to change his mind or win his understanding. And because our culture of debates and mock trials and real trials and flaming people on Usenet encourages the first type of \"winning an argument\", there's precious little genuine mind-changing going on.<br /><br />Really adjusting to the second type of argument, where you try to convince people, takes a lot more than just not insulting people outright<sup>1</sup>. You've got to completely rethink your entire strategy. For example, anyone used to the Standard Debates may already have a cached pattern of how they work. Activate the whole Standard Debate concept, and you activate a whole bunch of related thoughts like Atheists As The Enemy, Defending The Faith, and even in some cases (I've seen it happen) persecution of Christians by atheists in Communist Russia. To such a person, ceding an inch of ground in a Standard Debate may well be equivalent to saying all the Christians martyred by the Communists died in vain, or something similarly dreadful.<br /><br />So try to show you're not just starting Standard Debate #4457. I remember once, during the middle of a discussion with a Christian, when I admitted I really didn't like Christopher Hitchens. Richard Dawkins, brilliant. Daniel Dennett, brilliant. But Christopher Hitchens always struck me as too black-and-white and just plain irritating. This one little revelation completely changed the entire tone of the conversation. I was no longer Angry Nonbeliever #116. I was no longer the living incarnation of All Things Atheist. I was just a person who happened to have a whole bunch of atheist ideas, along with a couple of ideas that weren't typical of atheists. I got the same sort of response by admitting I loved religious music. All of a sudden my friend was falling over himself to mention some scientific theory he found especially elegant in order to reciprocate<sup>2</sup>. I didn't end up deconverting him on the spot, but think he left with a much better appreciation of my position.<br /><br />All of these techniques fall dangerously close to the Dark Arts, so let me be clear: I'm not suggesting you misrepresent yourself just to win arguments. I don't think misrepresenting yourself would even work; evolutionary psychology tells us humans are notoriously bad liars. Don't fake an appreciation for the other person's point of view, actually <em>develop </em>an appreciation for the other person's point of view. Realize that <a href=\"/lw/2d/talking_snakes_a_cautionary_tale/\">your points probably seem as absurd to others as their points seem to you</a>. Understand that <a href=\"/lw/2p/the_skeptics_trilemma/\">many false beliefs don't come from simple lying or stupidity</a>, but from complex mixtures of truth and falsehood filtered by complex cognitive biases. Don't stop believing that you are right and they are wrong, unless the evidence points that way. But leave it at them being wrong, not them being wrong and stupid and evil.<br /><br />I think most people intuitively understand this. But considering how many smart people I see shooting their own foot off when they're trying to convince someone<sup>3</sup>, some of them clearly need a reminder.</p>\n<p>&nbsp;</p>\n<p><strong>Footnotes</strong></p>\n<p><strong>1:</strong> An excellent collection of the deeper and most subtle forms of this practice of this sort can be found in Dale Carnegie's How to Win Friends and Influence People, one of the only self-help books I've read that was truly useful and not a regurgitation of cliches and applause lights. Carnegie's thesis is basically that being nice is the most powerful of the Dark Arts, and that a master of the Art of Niceness can use it to take over the world. It works better than you'd think.</p>\n<p><strong>2: </strong>The following technique is definitely one of the Dark Arts, but I mention it because it reveals a lot about the way we think: when engaged in a really heated, angry debate, one where the insults are flying, suddenly stop and admit the other person is one hundred percent right and you're sorry for not realizing it earlier. Do it properly, and the other person will be flabbergasted, and feel deeply guilty at all the names and bad feelings they piled on top of you. Not only will you ruin their whole day, but for the rest of time, this person will secretly feel indebted to you, and you will be able to play with their mind in all sorts of little ways.</p>\n<p><strong>3: </strong>Libertarians, you have a particular problem with this. If I wanted to know why I'm a Stalin-worshipper who has betrayed the Founding Fathers for personal gain and is controlled by his base emotions and wants to dominate others by force to hide his own worthlessness et cetera, I'd ask Ann Coulter. You're better than that. Come on. And then you wonder why people never vote for you.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wzgcQCrwKfETcBpR9": 1, "ZXFpyQWPB5ideFbEG": 1, "DdgSyQoZXjj3KnF4N": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6yTShbTdtATxKonY5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 142, "baseScore": 153, "extendedScore": null, "score": 0.000231, "legacy": true, "legacyId": "128", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 153, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 416, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["atcJqdhCxTZiJSxo2", "M7rwT264CSYY6EdR3", "rZX4WuufAPbN6wQTv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-19T06:08:37.769Z", "modifiedAt": null, "url": null, "title": "Counterfactual Mugging", "slug": "counterfactual-mugging", "viewCount": null, "lastCommentedAt": "2020-09-16T03:15:49.087Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mg6jDEuQEjBGtibX7/counterfactual-mugging", "pageUrlRelative": "/posts/mg6jDEuQEjBGtibX7/counterfactual-mugging", "linkUrl": "https://www.lesswrong.com/posts/mg6jDEuQEjBGtibX7/counterfactual-mugging", "postedAtFormatted": "Thursday, March 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Counterfactual%20Mugging&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACounterfactual%20Mugging%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmg6jDEuQEjBGtibX7%2Fcounterfactual-mugging%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Counterfactual%20Mugging%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmg6jDEuQEjBGtibX7%2Fcounterfactual-mugging", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmg6jDEuQEjBGtibX7%2Fcounterfactual-mugging", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 543, "htmlBody": "<p><strong>Related to</strong>: <a href=\"http://www.overcomingbias.com/2008/07/counterfactual.html\">Can Counterfactuals Be True?</a>, <a href=\"http://www.overcomingbias.com/2008/01/newcombs-proble.html\">Newcomb's Problem and Regret of Rationality</a>.</p>\n<p>Imagine that one day, <a href=\"http://www.overcomingbias.com/2008/01/newcombs-proble.html\">Omega</a> comes to you and says that it has just tossed a fair coin, and given that the coin came up tails, it decided to ask you to give it $100. Whatever you do in this situation, nothing else will happen differently in reality as a result. Naturally you don't want to give up your $100. But see, Omega tells you that if the coin came up heads instead of tails, it'd give you $10000, but only if you'd agree to give it $100 if the coin came up tails.</p>\n<p>Omega can predict your decision in case it asked you to give it $100, even if that hasn't actually happened, it can compute the <a href=\"http://www.overcomingbias.com/2008/07/counterfactual.html\">counterfactual truth</a>. Omega is also known to be absolutely honest and trustworthy, no word-twisting, so the facts are really as it says, it really tossed a coin and really would've given you $10000.</p>\n<p>From your current position, it seems absurd to give up your $100. Nothing good happens if you do that, the coin has already landed tails up, you'll never see the counterfactual $10000. But look at this situation from your point of view before Omega tossed the coin. There, you have two possible branches ahead of you, of equal probability. On one branch, you are asked to part with $100, and on the other branch, you are conditionally given $10000. If you decide to keep $100, the expected gain from this decision is $0: there is no exchange of money, you don't give Omega anything on the first branch, and as a result Omega doesn't give you anything on the second branch. If you decide to give $100 on the first branch, then Omega gives you $10000 on the second branch, so the expected gain from this decision is</p>\n<p>-$100 * 0.5 + $10000 * 0.5 = $4950<a id=\"more\"></a></p>\n<p>So, this straightforward calculation tells that you ought to give up your $100. It looks like a good idea before the coin toss, but it starts to look like a bad idea after the coin came up tails. Had you known about the deal in advance, one possible course of action would be to set up a precommitment. You contract a third party, agreeing that you'll lose $1000 if you don't give $100 to Omega, in case it asks for that. In this case, you leave yourself no other choice.</p>\n<p>But in this game, explicit precommitment is not an option: you didn't know about Omega's little game until the coin was already tossed and the outcome of the toss was given to you. The only thing that stands between Omega and your 100$ is your ritual of cognition. And so I ask you all: is the decision to give up $100 when you have no <em>real</em> benefit from it, only <em>counterfactual</em> benefit, an example of <a href=\"/lw/31/what_do_we_mean_by_rationality/\">winning</a>?</p>\n<p><strong>P.S.</strong> Let's assume that the coin is deterministic, that in the overwhelming measure of the MWI worlds it gives the same outcome. You don't care about a fraction that sees a different result, in all reality the result is that Omega won't even consider giving you $10000, it only asks for your $100. Also, the deal is unique, you won't see Omega ever again.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1, "fihKHQuS5WZBJgkRm": 2, "YpHkTW27iMFR2Dkae": 1, "5f5c37ee1b5cdee568cfb1b6": 7}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mg6jDEuQEjBGtibX7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 81, "baseScore": 70, "extendedScore": null, "score": 0.000111, "legacy": true, "legacyId": "129", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 70, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 296, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RcZCwxFiZzE6X7nsv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-19T08:22:43.093Z", "modifiedAt": null, "url": null, "title": "Rationalist Fiction", "slug": "rationalist-fiction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:39.352Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q79vYjHAE9KHcAjSs/rationalist-fiction", "pageUrlRelative": "/posts/q79vYjHAE9KHcAjSs/rationalist-fiction", "linkUrl": "https://www.lesswrong.com/posts/q79vYjHAE9KHcAjSs/rationalist-fiction", "postedAtFormatted": "Thursday, March 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationalist%20Fiction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationalist%20Fiction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq79vYjHAE9KHcAjSs%2Frationalist-fiction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationalist%20Fiction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq79vYjHAE9KHcAjSs%2Frationalist-fiction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq79vYjHAE9KHcAjSs%2Frationalist-fiction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 730, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"http://www.overcomingbias.com/2008/07/lawrence-watt-e.html\">Lawrence Watt-Evans's Fiction</a><br /><strong>Reply to</strong>:&nbsp; <a href=\"/lw/2q/on_juvenile_fiction/\">On Juvenile Fiction</a></p>\n<p>MBlume <a href=\"/lw/2q/on_juvenile_fiction/\">asked</a> us to remember what childhood stories might have influenced us toward rationality; and this was given such excellent answers as Norton Juster's <em>The Phantom Tollbooth.</em>&nbsp; So now I'd like to ask a related question, expanding the purview to all novels (adult or child, SF&amp;F or literary):&nbsp; Where can we find explicitly rationalist fiction?</p>\n<p>Now of course there are a great many characters who claim to be using logic.&nbsp; The whole genre of mystery stories with seemingly logical detectives, starting from Sherlock Holmes, would stand in witness of that.</p>\n<p>But when you look at what Sherlock Holmes does - you can't go out and do it at home.&nbsp; Sherlock Holmes is not <em>really </em>operating by any sort of reproducible method.&nbsp; He is operating by magically finding the right clues and carrying out magically correct complicated chains of deduction.&nbsp; Maybe it's just me, but it seems to me that reading Sherlock Holmes does not inspire you to go and do likewise.&nbsp; Holmes is a mutant superhero.&nbsp; And even if you did try to imitate him, it would never work in real life.</p>\n<p>Contrast to A. E. van Vogt's <em>Null-A</em> novels, starting with <em>The World of Null-A.</em>&nbsp; Now let it first be admitted that Van Vogt had a number of flaws as an author.&nbsp; With that said, it is probably a historical fact about my causal origins, that the <em>Null-A</em> books had an impact on my mind that I didn't even realize until years later.&nbsp; It's not the sort of book that I read over and over again, I read it and then put it down, but -</p>\n<p>- but this is where I was first exposed to such concepts as \"The map is not the territory\" and \"rose<sub>1</sub> is not rose<sub>2</sub>\".</p>\n<p>Null-A stands for \"Non-Aristotelian\", and the premise of the ficton is that studying Korzybski's General Semantics makes you a superhero.&nbsp; Let's not really go into that part.&nbsp; But in the Null-A ficton:</p>\n<p>1)&nbsp; The protagonist, Gilbert Gosseyn, is not a mutant.&nbsp; He has <em>studied</em> rationality techniques that have been <em>systematized</em> and are used by other members of his society, not just him.</p>\n<p>2)&nbsp; Van Vogt tells us what (some of) these principles are, rather than leaving them mysteriously blank - we can't be Gilbert Gosseyn, but we can at least use <em>some</em> of this stuff.</p>\n<p>3)&nbsp; Van Vogt <a href=\"http://www.overcomingbias.com/2009/02/truth-from-fiction.html\">conveys the experience</a>, <em>shows </em>Gosseyn in action using the principles, rather than leaving them to triumphant explanation afterward.&nbsp; We are put into Gosseyn's shoes at the moment of his e.g. making a conscious distinction between two different things referred to by the same name.<a id=\"more\"></a></p>\n<p>This is a high standard to meet.</p>\n<p>But Marc Stiegler's <em>David's Sling</em> (quoted in e.g. <a href=\"http://www.overcomingbias.com/2008/01/gray-fallacy.html\">this post</a>) meets this same standard:&nbsp; The Zetetics derive their abilities from training in a systematized tradition; we get to see the actual principles the Zetetics are using, and they're ones we could try to apply in real life; and we're put into their shoes at the moments of their use.</p>\n<p>I mention this to show that it isn't <em>only </em>van Vogt who's ever done this.</p>\n<p>However...</p>\n<p>...those two examples actually <em>exhaust </em>my knowledge of the science fiction and fantasy literature, so far as I can remember.</p>\n<p>It really is a very high standard we're setting here.&nbsp; To realistically show your characters using an interesting technique of rationality, you have to <em>know</em> an interesting technique of rationality.&nbsp; Van Vogt was inspired by Korzybski, who - I discovered when I looked this up, just now - actually <em>invented</em> the phrase \"The map is not the territory\".&nbsp; Marc Stiegler was inspired by, among other sources, Eric Drexler and Robin Hanson.&nbsp; (Stiegler has another novel called <em>Earthweb</em> about using prediction markets to defend the Earth from invading aliens, which was my introduction to the concept of prediction markets.)</p>\n<p>If I relax the standard to focus mainly on item (3), fiction that transmits a powerful experience of using rationality, then I could add in Greg Egan's <em>Distress,</em> some of <a href=\"http://www.overcomingbias.com/2008/07/lawrence-watt-e.html\">Lawrence Watt-Evans</a>'s strange little novels, the travails of Salvor Hardin in the first <em>Foundation</em> novel, and probably any number of others.</p>\n<p>But what I'm really interested in is whether there's any full-blown Rationalist Fiction that I've missed - or maybe just haven't remembered.&nbsp; Failing that, I'm interested in stories that merely do a good job of conveying a rationalist experience.&nbsp; (Please specify which of these cases is true, if you make a recommendation.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 11}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q79vYjHAE9KHcAjSs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 35, "extendedScore": null, "score": 5.3e-05, "legacy": true, "legacyId": "130", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 194, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4GeE83592epCErQse"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-20T01:58:56.188Z", "modifiedAt": null, "url": null, "title": "Rationalist Poetry Fans, Unite!", "slug": "rationalist-poetry-fans-unite", "viewCount": null, "lastCommentedAt": "2021-11-12T23:09:33.522Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iNCg6mjw584r9BWZK/rationalist-poetry-fans-unite", "pageUrlRelative": "/posts/iNCg6mjw584r9BWZK/rationalist-poetry-fans-unite", "linkUrl": "https://www.lesswrong.com/posts/iNCg6mjw584r9BWZK/rationalist-poetry-fans-unite", "postedAtFormatted": "Friday, March 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationalist%20Poetry%20Fans%2C%20Unite!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationalist%20Poetry%20Fans%2C%20Unite!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiNCg6mjw584r9BWZK%2Frationalist-poetry-fans-unite%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationalist%20Poetry%20Fans%2C%20Unite!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiNCg6mjw584r9BWZK%2Frationalist-poetry-fans-unite", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiNCg6mjw584r9BWZK%2Frationalist-poetry-fans-unite", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3470, "htmlBody": "<p><strong>Related to: </strong><a href=\"/lw/3i/little_johny_bayesian/\">Little Johnny Bayesian</a>, <a href=\"http://www.overcomingbias.com/2008/03/savanna-poets.html\">Savanna Poets</a></p>\n<p>There are certain stereotypes about what rationalists can talk about versus what's really beyond the pale. So far, Less Wrong has pretty consistently exploded those stereotypes. In the past three weeks, we've discussed everything from <a href=\"/lw/2p/the_skeptics_trilemma/\">Atlantis</a> to <a href=\"/lw/3b/never_leave_your_room/2ey#comments\">chaos magick</a> to \"<a href=\"/lw/2l/closet_survey_1/1m4#comments\">9-11 Truth</a>\". But I don't think anything surprised me quite as much as learning that there are a couple of rationalists here with a <a href=\"/lw/3i/little_johny_bayesian/\">genuine interest in poetry</a>.<br /><br />Poetry has not been very friendly to the rational worldview over the past few centuries. What with all the 19th century's talk of <a href=\"http://www.overcomingbias.com/2008/03/fake-reductioni.html\">unweaving rainbows</a> and the 20th century's talk of <a href=\"http://burningbird.net/writing/curving-space-with-cummings/\">quadrupeds swooning into billiard balls</a>, it's tempting to think it reflects some natural order of things, some eternal conflict between Art and Science.<br /><br />But for most of human history, science and art were considered natural allies. Lucretius' <a href=\"http://en.wikipedia.org/wiki/De_rerum_natura\">De Rerum Natura</a>, an argument for atheism and atomic theory famous for being the ancient Roman equivalent of <em>The God Delusion</em>, was written in poetry. All through the Middle Ages, artists worked to a philosophy of trying to depict and celebrate natural truth. And the eighteenth century saw a golden age of what was sometimes called \"rationalist poetry\", a versified celebration of Enlightenment principles.<br /><br />When William Wordsworth launched his poetic jihad against rationalism, he called his declaration of war <a href=\"http://malaspina.edu/~johnstoi/poems/wordsworth3.htm\">The Tables Turned</a>. On a mundane level, the title referred to an argument he was having with his friend, but on a grander scale he was consciously inverting the previous order of Reason as the virtue of poetry. Thus:</p>\n<blockquote>\n<p>Enough of Science and of Art;<br />Close up these barren leaves;<br />Come forth, and bring with you a heart<br />That watches and receives.</p>\n</blockquote>\n<p>Over the next few years, he and fellow jihadis John Keats and Percy Bysshe Shelley were wildly successful in completely changing the poetic ideal. I can't begrudge them their little movement; their poetry ranks among the greatest art ever produced by humankind. But it bears repeating that there was a strong rationalist tradition in poetry before, during, and after the Romantic Era. In its honor, I thought I would share some of my favorite rationalist poems. I make no claims that this is exhaustive, representative, or anything else besides my personal choices.</p>\n<p><a id=\"more\"></a></p>\n<p>The most famous rationalist poet is probably <a href=\"http://en.wikipedia.org/wiki/Alexander_Pope\">Alexander Pope</a> (1688-1744), perhaps best known for writing Isaac Newton's epitaph:</p>\n<blockquote>\n<p>Nature and nature's laws lay hid in night.<br />God said, \"Let Newton be!\" and all was light.<sup>1</sup></p>\n</blockquote>\n<p>Indeed, Pope spent much of his career praising science and human reason, while also simultaneously lampooning human stupidity:</p>\n<blockquote>\n<p>Go, wondrous creature! mount where Science guides;<br />Go measure earth, weigh air, and state the tides;<br />Instruct the planets in what orbs to run,<br />Correct old Time, and regulate the sun;<br />Go, soar with Plato to th'empyreal sphere,<br />To the first good, first perfect, and first fair;<br />Or tread the mazy round his followers trod,<br />And quitting sense call imitating God;<br />As eastern priests in giddy circles run,<br />And turn their heads to imitate the sun.<br />Go, teach Eternal Wisdom how to rule--<br />Then drop into thyself, and be a fool!</p>\n</blockquote>\n<p>I can't claim this as a complete victory for rationalism, since it was in the context of <a href=\"http://onlinebooks.library.upenn.edu/webbin/gutbook/lookup?num=2428\">Essay on Man</a>, a mysterianist work declaring that humans should never overreach their pathetic mental powers and question God's supremacy. Even the quoted passage is a little ironic, intended to convey that humankind, with such amazing science, had a tendency to shoot itself in the foot when it tried to overstep its bounds.</p>\n<p>But Pope's appreciation for scientific progress was genuine, and he was also deeply interested in overcoming bias (which he, in his pre-Samuel Johnson way, called \"byass\"). His <a href=\"http://eserver.org/poetry/essay-on-criticism.html\">Essay on Criticism</a> sometimes reads like a strangely spelled, classical-allusion-laden rationalists' manual:</p>\n<blockquote>\n<p>Of all the Causes which conspire to blind<br />Man's erring Judgment, and misguide the Mind,<br />What the weak Head with strongest Byass rules,<br />Is Pride, the never-failing Vice of Fools [...]<br />If once right Reason drives that Cloud away,<br />Truth breaks upon us with resistless Day;<br />Trust not your self; but your Defects to know,<br />Make use of ev'ry Friend--and ev'ry Foe.</p>\n</blockquote>\n<p>He exhorts us to think for ourselves, rather than take things on faith or blindly accept authority:</p>\n<blockquote>\n<p>Some ne'er advance a Judgment of their own,<br />But catch the spreading Notion of the Town;<br />They reason and conclude by Precedent,<br />And own stale Nonsense which they ne'er invent.</p>\n</blockquote>\n<p>But equally he reminds us that <a href=\"http://www.overcomingbias.com/2007/12/reversed-stupid.html\">reversed stupidity is not intelligence</a>:</p>\n<blockquote>\n<p>The Vulgar thus through Imitation err;<br />As oft the Learn'd by being Singular;<br />So much they scorn the Crowd, that if the Throng<br />By Chance go right, they purposely go wrong;</p>\n</blockquote>\n<p>And tells us to admit our errors, learn from them, and move on:</p>\n<blockquote>\n<p>Some positive persisting Fops we know,<br />Who, if once wrong, will needs be always so;<br />But you, with Pleasure own your Errors past,<br />An make each Day a Critick on the last.</p>\n</blockquote>\n<p>At the end, he describes the person he wants judging his poetry: someone who sounds rather like the ideal rationalist.</p>\n<blockquote>\n<p>Unbiass'd, or by Favour or by Spite;<br />Not dully prepossest, nor blindly right;<br />Tho' Learn'd well-bred; and tho' well-bred, sincere;<br />Modestly bold, and Humanly severe<br />Who to a Friend his Faults can freely show,<br />And gladly praise the Merit of a Foe<br />Blest with a Taste exact, yet unconfin'd;<br />A Knowledge both of Books and Humankind;<br />Gen'rous Converse; a Sound exempt from Pride;<br />And Love to Praise, with Reason on his Side.</p>\n</blockquote>\n<p>Pope came from a time when any person of good breeding was expected to be learned and able to converse about the scientific discoveries going on around them; an age when reason was actually trendy. There have been few such ages, and hence few such poets as Pope. But other rationalist poetry has come from people who were mathematicians or scientists in their day jobs, and poets only in their spare time.<br /><br />Such a man was <a href=\"http://en.wikipedia.org/wiki/Omar_Khayyam\">Omar Khayyam</a>, the eleventh century Persian mathematician and astronomer. He did some work on cubic equations, wrote the Islamic world's most influential treatise on algebra, reformed the Persian calendar, and developed a partial heliocentric theory centuries before Copernicus. But he is most beloved for his <a href=\"http://www.therubaiyat.com/\"><em>rubaiyat</em></a>, or quatrains, which recommend ignoring religion, accepting the deterministic material universe, and abandoning moral prudery in favor of having fun.<br /><br />There are some beautiful translations and some accurate translations of Khayyam's works, but the rumor among those who speak Persian is that the beautiful translations are not accurate and the accurate translations are not beautiful, and that capturing the true spirit of the original may be hopeless. FitzGerald in particular, the most famous English translator, is accused of playing up the hedonism and playing down the rationalism. I've tried to select from a few different translations for this essay.<br /><br />On determinism:</p>\n<blockquote>\n<p>The Moving Finger writes; and, having writ,<br />Moves on: nor all thy Piety nor Wit<br />Shall lure it back to cancel half a Line,<br />Nor all thy Tears wash out a Word of it.<br /><br />And that inverted Bowl we call The Sky,<br />Whereunder crawling coop't we live and die,<br />Lift not thy hands to It for help - for It<br />Rolls impotently on as Thou or I.<br /><br />With Earth's first Clay They did the Last Man knead,<br />And there of the Last Harvest sow'd the Seed:<br />And the first Morning of Creation wrote<br />What the Last Dawn of Reckoning shall read.</p>\n</blockquote>\n<p>On atheism:</p>\n<blockquote>\n<p>What! out of senseless Nothing to provoke<br />A conscious Something to resent the yoke<br />Of unpermitted Pleasure, under pain<br />Of Everlasting Penalties, if broke!<br /><br />What! from his helpless Creature be repaid<br />Pure Gold for what he lent him dross-allay'd--<br />Sue for a Debt he never did contract,<br />And cannot answer--Oh, the sorry trade!<br /><br />In every step I take Thou sett'st a snare,<br />Saying,&rdquo;I will entrap thee, so beware!\"<br />And, while all things are under Thy command,<br />I am a rebel - as Thou dost declare.</p>\n</blockquote>\n<p>On <a href=\"http://www.overcomingbias.com/2008/03/joy-in-the-real.html\">Joy in the Merely Real</a>:</p>\n<blockquote>\n<p>If in the Spring, she whom I love so well<br />Meet me by some green bank - the truth I tell -<br />Bringing my thirsty soul a cup of wine,<br />I want no better Heaven, nor fear a Hell.</p>\n</blockquote>\n<p>And unlike Alexander Pope, who is horrified, HORRIFIED at the thought that mankind might challenge God's divine plan, Omar Khayyam thinks he could do better:</p>\n<blockquote>\n<p>Ah, Love! could you and I with Him conspire<br />To grasp this sorry Scheme of Things entire,<br />Would not we shatter it to bits--and then<br />Re-mould it nearer to the Heart's Desire!</p>\n</blockquote>\n<p>Needless to say, his contemporaries shunned him for such blasphemies. What would he say, they ask, when called before the throne of Allah to account for his beliefs? Well, he told them, he would say this:</p>\n<blockquote>\n<p>Although I have not served Thee from my youth,<br />And though my face is mask'd with Sin uncouth,<br />In Thine Eternal Justice I confide,<br />As one who ever sought to follow Truth.</p>\n</blockquote>\n<p>Compare the clarity of Khayyam, who is prepared to stand before God and justify himself without fear, to Pascal, who insists that we abandon our own intellectual integrity on the imperceptibly tiny chance that we might accrue some material gain. I find this quatrain - \"in thy eternal justice I confide, as one who ever sought to follow Truth\" - the only <a href=\"/lw/2k/the_least_convenient_possible_world/\">fully satisfying answer</a> to Pascal's Wager.<br /><br /><a href=\"http://www.chat.carleton.ca/~tcstewar/grooks/grooks.html\">Piet Hein</a> (whom I've quoted here <a href=\"/lw/3b/never_leave_your_room/2fg#comments\">before</a>) was another scientist who turned to poetry. During his career as a theoretical physicist and mathematician, he developed the <a href=\"http://en.wikipedia.org/wiki/Superellipse\">superellipse</a> and the game <a href=\"http://en.wikipedia.org/wiki/Hex\">Hex</a> (later studied by John Nash). His career as a poet began when the Nazis invaded his native Denmark. The censors would have prohibited any obviously rebellious literature, so he turned to writing odd little poems that seemed innocuous until you thought about them long enough, at which point they became obvious critiques of dictatorship. He continued writing after the war, usually on the theme of keeping things simple and avoiding stupidity. <br /><br />This, for example, seems appropriate to a site called Less Wrong:</p>\n<blockquote>\n<p>The road to wisdom? -- Well, it's plain<br />and simple to express:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Err<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and err<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and err again<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; but less<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and less<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and less.</p>\n</blockquote>\n<p>If you've read <a href=\"/lw/2w/are_you_a_solar_deity/\">Are You A Solar Deity</a> or <a href=\"/lw/2j/schools_proliferating_without_evidence/\">Schools Proliferating Without Evidence</a>, you may see the humor in this quatrain about fitting the data to the theory:</p>\n<blockquote>\n<p>Everything's either<br />concave or convex,<br />so whatever you dream<br />will be something with sex.</p>\n</blockquote>\n<p>On the <a href=\"http://yudkowsky.net/rational/virtues\">first virtue</a>:</p>\n<blockquote>\n<p>I'd like to know <br />what this whole show <br />is about <br />before it's out.</p>\n</blockquote>\n<p>On the <a href=\"http://yudkowsky.net/rational/virtues\">fifth virtue</a>:</p>\n<blockquote>\n<p>Truth shall emerge from the interplay<br />&nbsp;&nbsp; of attitudes freely debated.<br />Don't be mislead by fanatics who say<br />&nbsp;&nbsp; that only one truth should be stated:<br />truth is constructed in such a way<br />&nbsp;&nbsp; that it can't be exaggerated.</p>\n</blockquote>\n<p>On <a href=\"http://www.overcomingbias.com/2008/10/isshokenmei.html\">making an extraordinary effort</a>:</p>\n<blockquote>\n<p>Our so-called limitations, I believe,<br />apply to faculties we don't apply.<br />We don't discover what we can't achieve<br />until we make an effort not to try.</p>\n</blockquote>\n<p>On <a href=\"http://www.overcomingbias.com/2007/10/fake-justificat.html\">fake justifications</a>:</p>\n<blockquote>\n<p>In view of your manner<br />&nbsp;&nbsp;&nbsp; of spending your days<br />I hope you may learn,<br />&nbsp;&nbsp;&nbsp; before ending them,<br />that the effort you spend<br />&nbsp;&nbsp;&nbsp; on defending your ways<br />could better be spent<br />&nbsp;&nbsp;&nbsp; on amending them.</p>\n</blockquote>\n<p>Appropriate to the Singularity or to any of a number of fields:</p>\n<blockquote>\n<p>Eradicate the optimist<br />&nbsp;who takes the easy view<br />that human values will persist<br />&nbsp;no matter what we do.<br /><br />Annihilate the pessimist<br />&nbsp;whose ineffectual cry<br />is that the goal's already missed<br />&nbsp;however hard we try.</p>\n</blockquote>\n<p>On <a href=\"http://www.overcomingbias.com/2007/12/reversed-stupid.html\">reversed stupidity</a>:</p>\n<blockquote>\n<p>For many system shoppers it's<br />&nbsp;a good-for-nothing system<br />that classifies as opposites<br />&nbsp;stupidity and wisdom. <br /><br />because by logic-choppers it's<br />&nbsp;accepted with avidity:<br />stupidity's true opposite's<br />&nbsp;the opposite stupidity.</p>\n</blockquote>\n<p>On <a href=\"http://www.overcomingbias.com/2008/10/shut-up-and-do.html\">shutting up and doing the impossible</a>:</p>\n<blockquote>\n<p>'Impossibilities' are good<br />&nbsp;not to attach that label to;<br />since, correctly understood,<br />if we wanted to, we would<br />&nbsp;be able to be able to.</p>\n</blockquote>\n<p>And even some poets who had no such formal acquaintance with science considered their poetry allied with its goals: an attempt to explore the universe and celebrate its wonders. This one's from <a href=\"http://www.geocities.com/~bblair/donjuan.htm\">Don Juan</a> by Lord Byron, commonly (but, according to his own protestations, erroneously) classed with Wordsworth as a Romantic. I won't say there's not sarcasm in there, but Byron has a way of being sarcastic even when saying things he believes:</p>\n<blockquote>\n<p>When Newton saw an apple fall, he found<br />In that slight startle from his contemplation -- <br />'Tis said (for I'll not answer above ground<br />For any sage's creed or calculation) -- <br />A mode of proving that the earth turn'd round<br />In a most natural whirl, called \"gravitation;\" <br />And this is the sole mortal who could grapple,<br />Since Adam, with a fall or with an apple.<br /><br />Man fell with apples, and with apples rose,<br />If this be true; for we must deem the mode <br />In which Sir Isaac Newton could disclose<br />Through the then unpaved stars the turnpike road, <br />A thing to counterbalance human woes:<br />For ever since immortal man hath glow'd <br />With all kinds of mechanics, and full soon<br />Steam-engines will conduct him to the moon.<br /><br />And wherefore this exordium? -- Why, just now,<br />In taking up this paltry sheet of paper, <br />My bosom underwent a glorious glow,<br />And my internal spirit cut a caper: <br />And though so much inferior, as I know,<br />To those who, by the dint of glass and vapour, <br />Discover stars and sail in the wind's eye,<br />I wish to do as much by poesy.</p>\n</blockquote>\n<p>There's no sarcasm at all in this next declaration of Byron's, where he vows hostility to everything from despotism to religion to mob rule to fuzzy thinking to the <a href=\"http://www.overcomingbias.com/2006/12/a_fable_of_scie.html\">Blue vs. Green</a> two-party swindle:</p>\n<blockquote>\n<p>And I will war, at least in words (and -- should<br />My chance so happen -- deeds), with all who war <br />With Thought; -- and of Thought's foes by far most rude,<br />Tyrants and sycophants have been and are. <br />I know not who may conquer: if I could<br />Have such a prescience, it should be no bar <br />To this my plain, sworn, downright detestation<br />Of every depotism in every nation.<br /><br />It is not that I adulate the people:<br />Without me, there are demagogues enough, <br />And infidels, to pull down every steeple,<br />And set up in their stead some proper stuff. <br />Whether they may sow scepticism to reap hell,<br />As is the Christian dogma rather rough, <br />I do not know; -- I wish men to be free<br />As much from mobs as kings -- from you as me.<br /><br />The consequence is, being of no party,<br />I shall offend all parties: never mind! <br />My words, at least, are more sincere and hearty<br />Than if I sought to sail before the wind. <br />He who has nought to gain can have small art: he<br />Who neither wishes to be bound nor bind, <br />May still expatiate freely, as will I,<br />Nor give my voice to slavery's jackal cry.</p>\n</blockquote>\n<p>Byron on the progress of science, and on rejecting disproven theories:</p>\n<blockquote>\n<p>If from great nature's or our own abyss<br />Of thought we could but snatch a certainty, <br />Perhaps mankind might find the path they miss --<br />But then 't would spoil much good philosophy. <br />One system eats another up, and this<br />Much as old Saturn ate his progeny; <br />For when his pious consort gave him stones<br />In lieu of sons, of these he made no bones.<br /><br />But System doth reverse the Titan's breakfast,<br />And eats her parents, albeit the digestion <br />Is difficult. Pray tell me, can you make fast,<br />After due search, your faith to any question? <br />Look back o'er ages, ere unto the stake fast<br />You bind yourself, and call some mode the best one. <br />Nothing more true than not to trust your senses;<br />And yet what are your other evidences?</p>\n</blockquote>\n<p>Again on the same topic (and some thoughts on the \"wisdom of crowds\"):</p>\n<blockquote>\n<p>There is a common-place book argument,<br />Which glibly glides from every tongue; <br />When any dare a new light to present,<br />\"If you are right, then everybody 's wrong\"! <br />Suppose the converse of this precedent<br />So often urged, so loudly and so long; <br />\"If you are wrong, then everybody's right\"!<br />Was ever everybody yet so quite?</p>\n</blockquote>\n<p>This is Byron at his sarcastic best on the value accorded truth in society:</p>\n<blockquote>\n<p>The antique Persians taught three useful things,<br />To draw the bow, to ride, and speak the truth.<br />This was the mode of Cyrus, best of kings --<br />A mode adopted since by modern youth. <br />Bows have they, generally with two strings;<br />Horses they ride without remorse or ruth; <br />At speaking truth perhaps they are less clever,<br />But draw the long bow better now than ever.</p>\n</blockquote>\n<p>I can't help ending this by saying a word in praise of the Romantics. Yes, they may have gotten their rainbows in a tangle, and they may have hurled every curse they could at \"Reason\", but I think they were less opposed than they let on. Consider as anecdotal evidence Percy Shelley, who was expelled from Oxford after refusing to recant his atheism. What the Romantics hated was anyone telling them how to think, and their quarrel with a science they did not understand was less with its methods and more that it seemed an authority. Thus John Keats, in the same year he wrote Lamia, also penned perhaps the greatest statement of the Joy in the Merely Real ideal ever, writing:</p>\n<blockquote>\n<p>Beauty is Truth, Truth Beauty, that is all<br />Ye know on Earth, and all ye need to know.</p>\n</blockquote>\n<p>I think given an hour to talk to him and set him straight I could've convinced him there is no loss of beauty in accepting Newton's optics. It is true, after all. <br /><br />I end with Shelley's description from <a href=\"http://www.mtholyoke.edu/courses/rschwart/hist256/alps/mont_blanc.htm\">Mont Blanc</a> of the godless yet ordered essence of the universe that he worshipped:</p>\n<blockquote>\n<p>The secret Strength of things<br />Which governs thought, and to the infinite dome<br />Of Heaven is as a law.</p>\n</blockquote>\n<p>The laws that govern our own thought processes are the same laws that bind the infinite dome of Heaven. What better statement of the rationalist worldview could you ask for?</p>\n<p>Now, what are <em>your</em> favorite rationalist poems?</p>\n<p><strong>Footnotes:</strong></p>\n<p><strong>1:</strong> I have since spotted the following addition to Pope's couplet:</p>\n<blockquote>\n<p>It did not last; the Devil, howling \"Ho!<br />Let <em>Einstein</em> be!\" restored the status quo.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AXhEhCkTrHZbjXXu3": 8}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iNCg6mjw584r9BWZK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 38, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "136", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6r2t5qTxjHWxG5DTj", "M7rwT264CSYY6EdR3", "neQ7eXuaXpiYw7SBy", "T5McDuWDeCvDZKeSj", "JnKCaGcgZL4Rsep8m"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-20T04:33:35.511Z", "modifiedAt": null, "url": null, "title": "Precommitting to paying Omega.", "slug": "precommitting-to-paying-omega", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:10.274Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "topynate", "createdAt": "2009-03-05T02:37:28.578Z", "isAdmin": false, "displayName": "topynate"}, "userId": "M7Jasma8TWpbJmAcs", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/beqq5Nm3EsJihHvK7/precommitting-to-paying-omega", "pageUrlRelative": "/posts/beqq5Nm3EsJihHvK7/precommitting-to-paying-omega", "linkUrl": "https://www.lesswrong.com/posts/beqq5Nm3EsJihHvK7/precommitting-to-paying-omega", "postedAtFormatted": "Friday, March 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Precommitting%20to%20paying%20Omega.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APrecommitting%20to%20paying%20Omega.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbeqq5Nm3EsJihHvK7%2Fprecommitting-to-paying-omega%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Precommitting%20to%20paying%20Omega.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbeqq5Nm3EsJihHvK7%2Fprecommitting-to-paying-omega", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbeqq5Nm3EsJihHvK7%2Fprecommitting-to-paying-omega", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2126, "htmlBody": "<p><strong>Related to</strong>: <a title=\"LW: Counterfactual Mugging\" href=\"/lw/3l/counterfactual_mugging/\" target=\"_self\">Counterfactual Mugging</a>, <a title=\"LW: The Least Convenient Possible World\" href=\"/lw/2k/the_least_convenient_possible_world/\" target=\"_self\">The Least Convenient Possible World</a></p>\n<p><a title=\"LW: Comment\" href=\"/lw/3l/counterfactual_mugging/2kv#comments\" target=\"_blank\">MBlume said</a>:</p>\n<blockquote>\n<p>What would you do in situation X?\" and \"What would you like to pre-commit to doing, should you ever encounter situation X?\" should, to a rational agent, be one and the same question.</p>\n</blockquote>\n<p>Applied to Vladimir Nesov's <a title=\"LW: Counterfactual Mugging\" href=\"/lw/3l/counterfactual_mugging/\" target=\"_blank\">counterfactual mugging</a>, the reasoning is then:</p>\n<p>Precommitting to paying $100 to Omega has expected utility of $4950.p(Omega appears). Not precommitting has strictly less utility; therefore I should precommit to paying. Therefore I should, in fact, pay $100 in the event (Omega appears, coin is tails).<br /><br />To combat the argument that it is more likely that one is insane than that Omega has appeared, <a title=\"LW: Comment\" href=\"/lw/3l/counterfactual_mugging/2nn#comments\" target=\"_blank\">Eliezer said</a>:</p>\n<blockquote>\n<p>So imagine yourself in the most inconvenient possible world where Omega is a known feature of the environment and has long been seen to follow through on promises of this type; it does not particularly occur to you or anyone that believing this fact makes you insane.</p>\n</blockquote>\n<p>My first reaction was that it is simply not rational to give $100 away when nothing can possibly happen in consequence. I still believe that, with a small modification: I believe, with moderately high probability, that it will not be instrumentally rational for my future self to do so. Read on for the explanation.<a id=\"more\"></a></p>\n<p>Suppose we lived in Eliezer's most inconvenient possible world:</p>\n<ul>\n<li>Omega exists.</li>\n<li>Omega has never been found untrustworthy.</li>\n<li>Direct brain simulation has verified that Omega has a 100% success rate in predicting the response to its problem, thus far.</li>\n<li>Omega claims that no other Omega-like beings exist (so no perverse Omegas that cancel out Omega's actions!).</li>\n<li>Omega never speaks to anyone except if it is asking them for payment. It never meets anyone more than once</li>\n<li>Omega claims that actual decisions never have any consequences. It is only what you would have decided that can ever affect its actions.</li>\n</ul>\n<p>Did you see a trap? Direct brain simulation instantiates precisely what Omega says does not exist, a \"you\" whose decision has consequences. So forget that. Suppose Omega privately performs some action for you (for instance, a hypercomputation) that is not simulable. Then direct brain simulation of this circumstance cannot occur. So just assume that you find Omega trustworthy in this world, and assume it does not itself simulate you to make its decisions. Other objections exist: numerous ones, actually. Forget them. If you find that a certain set of circumstances makes it easier for you to decide not to pay the $100, or to pay it, change the circumstances. For myself, I had to imagine <em>knowing</em> that the Tegmark ensemble didn't exist*. If, under the MWI of quantum mechanics, you find reasons (not) to pay, then assume MWI is disproven. If the converse, then assume MWI is true. If you find that both suppositions give you reasons (not) to pay, then assume some missing argument invalidates those reasons.</p>\n<p>Under these circumstances, should everyone pay the $100?</p>\n<p>No. Well, it depends what you mean by \"should\".</p>\n<p>Suppose I live in the Omega world. Then prior to the coin flip, I assign equal value to my future self in the event that it is heads, and my future self in the event that it is tails. My utility function is, very roughly, the expected utility function of my future self, weighted by the probabilities I assign that I will actually become some given future self. Therefore if I can precommit to paying $100, my utility function will possess the term $4950.p(Omega appears), and if I can only partially precommit, in other words I can arrange that with probablity q I will pay $100, then my utility function will possess the term $4950.q.p(Omega appears). So the dominant strategy is to precommit with probability one. I can in fact do this if Omega guarantees to contact me via email, or a trusted intermediary, and to take instructions thereby received as \"my response\", but I may have a <em>slight</em> difficulty if Omega chooses to appear to me in bed late one night.</p>\n<p>On the principle of the least convenient world, I'm going to suppose that is in fact how Omega chooses to appear to me. I'm also going to suppose that I have no tools available to me in Omega world that I do not in fact possess right now. Here comes Omega:</p>\n<hr />\n<p><em>Hello Nathan. Tails, I'm afraid. Care to pay up?</em></p>\n<p>\"Before I make my decision: Tell me the shortest proof that P = NP, or the converse.\"</p>\n<p>Omega obliges (it will not, of course, let me remember this proof - but I knew that when I asked).</p>\n<p>\"Do you have any way of proving that you can hypercompute to me?\"</p>\n<p><em>Yes.</em> (Omega proves it.)</p>\n<p>\"So, you're really Omega. And my choice will have no other consequences?\"</p>\n<p><em>None. Had heads appeared, I would have predicted precisely this current sequence of events and used it to make a decision. But heads has not appeared. No consequences will ensue.</em></p>\n<p>\"So you would have simulated my brain performing these actions? No, you don't do that, do you? Can you prove that's possible?\"</p>\n<p><em>Yes.</em> (Omega proves it.)</p>\n<p>\"Right. No, I don't want to give you $100.\"</p>\n<hr />\n<p>&nbsp;</p>\n<p>What the hell just happened? Before Omega appeared, I wanted this sequence of events to play out quite differently. In fact this was my wish right up to the 't' of \"tails\". But now I've decided to keep the $100 after all!</p>\n<p>The answer is that there is no equivalence between my utility function at time t, where t &lt; time<sub>Omega</sub>, and my utility function at time T, where time<sub>Omega</sub> &lt; T. Before time<sub>Omega</sub>, my utility function contains terms from states of the world where Omega appears and the coin turns up heads; after, it doesn't. Add to that the fact that my utility function is increasing in money possessed, and my preferred action at time T changes (predictably so) at time<sub>Omega</sub>. To formalise:</p>\n<p>Suppose we index possible worlds with a time, t, and a state, S: a world state is then (S,t). Now let the utility function of 'myself' at time t and in world state S be denoted U<sub>S,t</sub>:A<sub>S</sub> &rarr; R, where A<sub>S</sub> is my set of actions and R the real numbers. Then in the limit of a small time differential &Delta;t, we can use the <a title=\"Wikipedia: Bellman Equation\" href=\"http://en.wikipedia.org/Bellman_Equation\" target=\"_blank\">Bellman equation</a> to pick an optimal policy &pi;*:S &rarr; A<sub>S</sub> such that we maximise U<sub>S,t</sub> as U<sub>S,t</sub>(&pi;*(S)).</p>\n<p>Before Omega appears, I am in (S,t). Suppose that the action \"paying $100 to Omega if tails appears\" is denoted a<sub>100</sub>. Then, obviously, a<sub>100</sub> is not in my action set A<sub>S</sub>. Let \"not paying $100 to Omega if tails appears\" be denoted a<sub>0</sub>. a<sub>0</sub> isn't in A<sub>S</sub> either. If we suppose Omega is guaranteed to appear shortly before time T (not a particularly restricting assumption for our purposes), then precommitting to paying is represented in our formalism by taking an action a<sub>p</sub> at (S,t) such that either:</p>\n<ol>\n<li>The probability of being a state &sect; in which tails has appeared and for which a<sub>0</sub> &isin; A<sub>&sect;</sub> at time T is 0, or</li>\n<li>For all states &sect; with tails having appeared, with a<sub>0</sub> &isin; A<sub>&sect;</sub> and with non-zero probability at time T, U<sub>&sect;,T</sub>(a<sub>0</sub>) &lt; U<sub>&sect;,T</sub>(a<sub>100</sub>) = &pi;*(&sect;). Note that a 'world state' S <em>includes my brain</em>.&nbsp;</li>\n</ol>\n<p>Then if Omega uses a trusted intermediary, I can easily carry out an action a<sub>p</sub> = \"give bank account access to intermediary and tell intermediary to pay $100 from my account to Omega under all circumstances\". This counts as taking option 1 above. But suppose that option 1 is closed to us. Suppose we must take an action such that 2 is satisfied. What does such an action look like?</p>\n<p>Firstly, brain hacks. If my utility function in state &sect; at time T is increasing in money, then U<sub>&sect;,T</sub>(a<sub>0</sub>) &gt; U<sub>&sect;,T</sub>(a<sub>100</sub>), <em>contra</em> the desired property of a<sub>p</sub>. Therefore I must arrange for my brain in world-state &sect; to be such that my utility function is not so fashioned. But by supposition my utility function cannot \"change\"; it is simply a mapping from world-states X possible actions to real numbers. In fact the function itself is an abstraction describing the behaviour of a particular brain in a particular world state**. If, in addition, we desire that the Bellman equation actually holds, then we cannot simply abolish the process of determining an optimal policy at some arbitrary point in time T. I propose one more desired property: the general principle of more money being better than less should not cease to operate due to a<sub>p</sub>, as this is sure to decrease U<sub>S,t</sub>(a<sub>p</sub>) below optimum (would we really lose less than $4950?). So the modification I make to my brain should be minimal in some sense. This is, after all, a highly exceptional circumstance. What one could do is arrange for my brain to experience strong reward for a short time period after taking action a<sub>100</sub>. The actual amount chosen should be such that that the reward outweighs the time-discounted future loss in utility from surrendering the $100 (it follows that the shorter the duration of reward, the stronger its magnitude must be). I must also guarantee that I am not simply attaching a label called \"reward\" to something that does not actually represent reward as defined in the Bellman equation. This would, I believe, require some pretty deep knowledge of the nature of my brain which I do not possess. Add to that the fact that I do not know how to hack my brain, and in a least convenient world, this option is closed to me also***.</p>\n<p>It's looking pretty grim for my expected utility. But wait: we do not simply have to increase U<sub>&sect;,T</sub>(a<sub>100</sub>). We can also decrease U<sub>&sect;,T</sub>(a<sub>0</sub>). Now we could implement a brain hack for this also, but the same arguments against apply. A simple solution might be to use a trusted intermediary for another purpose: give him $1000, and tell him not to give it back unless I do a<sub>100</sub>. This would, in fact, motivate me, but it reintroduces the factor of how probable it is Omega will appear, which we were previously able to neglect, by altering the utility from time t to time time<sub>Omega</sub>. Suppose we give the intermediary our account details instead. This solves the probability issue, but there is a potential for either myself to frustrate him, a solvable problem, or for Omega to frustrate him in order to satisfy the \"no further consequences\" requirement. And so on: the requirements of the problem are such that only our own utility function is sancrosact to Omega. It is through that mechanism only that we can win.</p>\n<p>This is my real difficulty: that the problem appears to require cognitive understanding and technology that we do not possess. <a href=\"/lw/3l/counterfactual_mugging/2j4#comments\" target=\"_self\">Eliezer</a> may very well give $100 whenever he meets this problem; so may <a href=\"/lw/3l/counterfactual_mugging/2k5#comments\" target=\"_self\">Cameron</a>; but I wouldn't, probably not, anyway. It wouldn't be instrumentally rational for me, given my utility function under those circumstances, at least not unless something happens that can put the concepts they carry around with them into my head, and stop me - or rather, make it instrumentally irrational for me, in the sense of being part of a suboptimal policy - from removing those concepts after Omega appears.</p>\n<p>However, on the off-chance that Omega~, a slightly less inconvenient version of Omega, appears before me: I hereby pledge one beer to every member of Less Wrong, if I fail to surrender my $100 when asked. Take that, obnoxious omniscient being!</p>\n<p>&nbsp;</p>\n<hr />\n<p>*It's faintly amusing, though only faintly, that despite knowing full well that I was supposed to consider the least convenient possible world, I neglected to think of <em>my</em> least convenient possible world when I first tried to tackle the problem. Ask yourself the question.</p>\n<p>**There are issues with identifying what it means for a brain/agent to persist from one world-state to another, but if such a persisting agent cannot be identified, then the whole problem is nonsense. It is more inconvenient for the problem to be coherent, as we must then answer it. I've also decided to use the Bellman equations with discrete time steps, rather than the time-continuous <a title=\"Wikipedia: HJB Equation\" href=\"http://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi%E2%80%93Bellman_equation\" target=\"_blank\">HJB equation</a>, simply because I've never used the latter and don't trust myself to explain it correctly.</p>\n<p>***There is the question: would one not simply dehack after Omega arrives announcing 'tails'? If that is of higher utility than other alternatives: but then we must have defined \"reward\" inappropriately while making the hack, as the reward for being in each state, together with the discounting factor, serves to fully determine the utility function in the Bellman equation.</p>\n<p>(I've made a few small post-submission edits, the largest to clarify my conclusion)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1, "b8FHrKqyXuYGWc6vn": 1, "5f5c37ee1b5cdee568cfb1b6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "beqq5Nm3EsJihHvK7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 5, "extendedScore": null, "score": 4.823989867622887e-07, "legacy": true, "legacyId": "138", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mg6jDEuQEjBGtibX7", "neQ7eXuaXpiYw7SBy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-20T08:37:22.001Z", "modifiedAt": "2022-01-07T07:46:41.882Z", "url": null, "title": "Why Our Kind Can't Cooperate", "slug": "why-our-kind-can-t-cooperate", "viewCount": null, "lastCommentedAt": "2021-08-10T01:34:45.802Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7FzD7pNm9X68Gp5ZC/why-our-kind-can-t-cooperate", "pageUrlRelative": "/posts/7FzD7pNm9X68Gp5ZC/why-our-kind-can-t-cooperate", "linkUrl": "https://www.lesswrong.com/posts/7FzD7pNm9X68Gp5ZC/why-our-kind-can-t-cooperate", "postedAtFormatted": "Friday, March 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20Our%20Kind%20Can't%20Cooperate&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20Our%20Kind%20Can't%20Cooperate%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7FzD7pNm9X68Gp5ZC%2Fwhy-our-kind-can-t-cooperate%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20Our%20Kind%20Can't%20Cooperate%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7FzD7pNm9X68Gp5ZC%2Fwhy-our-kind-can-t-cooperate", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7FzD7pNm9X68Gp5ZC%2Fwhy-our-kind-can-t-cooperate", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2698, "htmlBody": "<p>From when I was still forced to attend, I remember our synagogue's annual fundraising appeal.&nbsp; It was a simple enough format, if I recall correctly.&nbsp; The rabbi and the treasurer talked about the shul's expenses and how vital this annual fundraise was, and then the synagogue's members called out their pledges from their seats.</p>\n<p>Straightforward, yes?</p>\n<p>Let me tell you about a different annual fundraising appeal.&nbsp; One that I ran, in fact; during the early years of a nonprofit organization that <a href=\"http://wiki.lesswrong.com/wiki/Topic_that_must_not_be_named\">may not be named</a>.&nbsp; One difference was that the appeal was conducted over the Internet.&nbsp; And another difference was that the audience was largely drawn from the atheist/libertarian/technophile/sf-fan/early-adopter/programmer/etc crowd.&nbsp; (To point in the rough direction of an empirical cluster in personspace.&nbsp; If you understood the phrase \"empirical cluster in personspace\" then you know who I'm talking about.)</p>\n<p>I crafted the fundraising appeal with care.&nbsp; By my nature I'm too proud to ask other people for help; but I've gotten over around 60% of that reluctance over the years.&nbsp; The nonprofit needed money and was growing too slowly, so I put some force and poetry into that year's annual appeal.&nbsp; I sent it out to several mailing lists that covered most of our potential support base.</p>\n<p>And almost immediately, people started posting to the mailing lists about why they weren't going to donate.&nbsp; Some of them raised basic questions about the nonprofit's philosophy and mission.&nbsp; Others talked about their brilliant ideas for all the <em>other</em> sources that the nonprofit could get funding from, instead of them.&nbsp; (They didn't volunteer to contact any of those sources <em>themselves</em>, they just had ideas for how <em>we</em> could do it.)</p>\n<p>Now you might say, \"Well, maybe your mission and philosophy <em>did</em> have basic problems&mdash;you wouldn't want to <em>censor </em>that discussion, would you?\"</p>\n<p>Hold on to that thought.</p>\n<p>Because people <em>were</em> donating.&nbsp; We started getting donations right away, via Paypal.&nbsp; We even got congratulatory notes saying how the appeal had finally gotten them to start moving.&nbsp; A donation of $111.11 was accompanied by a message saying, \"I decided to give **** a little bit more.&nbsp; One more hundred, one more ten, one more single, one more dime, and one more penny.&nbsp; All may not be for one, but this one is trying to be for all.\"</p>\n<p>But none of those donors posted their agreement to the mailing list.&nbsp; Not one.</p>\n<p><a id=\"more\"></a></p>\n<p>So far as any of those donors knew, they were alone.&nbsp; And when they tuned in the next day, they discovered not thanks, but arguments for why they <em>shouldn't</em> have donated.&nbsp; The criticisms, the justifications for not donating&mdash;<em>only those</em> were displayed proudly in the open.</p>\n<p>As though the treasurer had finished his annual appeal, and everyone <em>not</em> making a pledge had proudly stood up to call out justifications for refusing; while those making pledges whispered them quietly, so that no one could hear.</p>\n<p>I know someone with a rationalist cause who goes around plaintively asking, \"How come the Raelian flying-saucer cult can get tens of thousands of members [<a href=\"http://www.gelfmagazine.com/archives/an_interview_with_ral.php\">probably</a> around 40,000] interested in complete nonsense, but we can't even get a thousand people working on this?\"</p>\n<p><a href=\"http://www.overcomingbias.com/2008/10/protected-from.html\">The obvious wrong way</a> to finish this thought is to say, \"Let's do what the Raelians do!&nbsp; Let's add some nonsense to this meme!\"&nbsp; For the benefit of those not immediately stopped by their <a href=\"http://www.overcomingbias.com/2008/10/protected-from.html\">ethical inhibitions</a>, I will observe that there may be a hundred failed flying-saucer cults for every one that becomes famous.&nbsp; And the Dark Side may require non-obvious skills, which <em>you, </em>yes <em>you</em>, do not have:&nbsp; Not everyone can be a Sith Lord.&nbsp; In particular, if you talk about your planned lies on the public Internet, you fail.&nbsp; I'm no master criminal, but even I can tell certain people are not cut out to be crooks.</p>\n<p>So it's probably not a good idea to cultivate a sense of violated entitlement at the thought that some <em>other </em>group, who you think ought to be <em>inferior </em>to you, has more money and followers.&nbsp; That path leads to&mdash;pardon the expression&mdash;the Dark Side.</p>\n<p>But it probably <em>does</em> make sense to start asking ourselves some pointed questions, if supposed \"<a href=\"/lw/31/what_do_we_mean_by_rationality/\">rationalists</a>\" can't manage to <em>coordinate</em> as well as a flying-saucer cult.</p>\n<p>How do things work on the Dark Side?</p>\n<p>The respected leader speaks, and there comes a chorus of pure agreement: if there are any who harbor inward doubts, they keep them to themselves.&nbsp; So all the individual members of the audience see this atmosphere of pure agreement, and they feel more confident in the ideas presented&mdash;even if they, personally, harbored inward doubts, why, everyone <em>else</em> seems to agree with it.</p>\n<p>(\"<a href=\"http://www.overcomingbias.com/2007/12/express-concern.html\">Pluralistic ignorance</a>\" is the standard label for this.)</p>\n<p>If anyone is still unpersuaded after that, they leave the group (or in some places, are executed)&mdash;and the remainder are more in agreement, and reinforce each other with less interference.</p>\n<p>(I call that \"<a href=\"http://www.overcomingbias.com/2007/12/evaporative-coo.html\">evaporative cooling of groups</a>\".)</p>\n<p>The <em>ideas </em>themselves, not just the leader, generate unbounded enthusiasm and praise.&nbsp; The <a href=\"http://www.overcomingbias.com/2007/11/halo-effect.html\">halo effect</a> is that perceptions of all positive qualities correlate&mdash;e.g. telling subjects about the benefits of a food preservative made them judge it as lower-risk, even though the quantities were logically uncorrelated.&nbsp; This can create a positive feedback effect that makes an idea seem better and better and better, especially if <a href=\"http://www.overcomingbias.com/2007/12/supercritical-u.html\">criticism is perceived as traitorous or sinful</a>.</p>\n<p>(Which I term the \"<a href=\"http://www.overcomingbias.com/2007/12/affective-death.html\">affective death spiral</a>\".)</p>\n<p>So these are all examples of strong Dark Side forces that can bind groups together.</p>\n<p>And presumably <em>we</em> would not go so far as to dirty our hands with such...</p>\n<p>Therefore, as a group, the Light Side will always be divided and weak.&nbsp; Atheists, libertarians, technophiles, nerds, science-fiction fans, scientists, or even non-fundamentalist religions, will never be capable of acting with the fanatic unity that animates radical Islam.&nbsp; Technological advantage can only go so far; your tools can be copied or stolen, and used against you.&nbsp; In the end the Light Side will always lose in any group conflict, and the future inevitably belongs to the Dark.</p>\n<p>I think that one's reaction to this prospect says a lot about their attitude towards \"rationality\".</p>\n<p>Some \"Clash of Civilizations\" writers seem to accept that the Enlightenment is destined to lose out in the long run to radical Islam, and sigh, and shake their heads sadly.&nbsp; I suppose they're trying to <a href=\"http://www.overcomingbias.com/2009/02/cynical-about-cynicism.html\">signal their cynical sophistication</a> or something.</p>\n<p>For myself, I always thought&mdash;call me loony&mdash;that a <em>true </em>rationalist ought to be <em>effective in the real world.</em></p>\n<p>So I have a problem with the idea that the Dark Side, thanks to their <em>pluralistic ignorance and affective death spirals</em>, will always win because they are <em>better coordinated</em> than us.</p>\n<p>You would think, perhaps, that <em>real</em> rationalists ought to be <em>more </em>coordinated?&nbsp; Surely all that unreason must have its <em>dis</em>advantages?&nbsp; That mode can't be <em>optimal,</em> can it?</p>\n<p>And if current \"rationalist\" groups <em>cannot </em>coordinate&mdash;if they can't support group projects so well as a single synagogue draws donations from its members&mdash;well, I leave it to you to finish that syllogism.</p>\n<p>There's a saying I sometimes use:&nbsp; \"It is dangerous to be half a rationalist.\"</p>\n<p>For example, I can think of ways to sabotage someone's intelligence by <em>selectively </em>teaching them certain methods of rationality.&nbsp; Suppose you taught someone a long list of logical fallacies and cognitive biases, and trained them to spot those fallacies in biases in other people's arguments.&nbsp; But you are careful to pick those fallacies and biases that are <em>easiest to accuse</em> others of, the most general ones that can easily be misapplied.&nbsp; And you do <em>not</em> <a href=\"http://www.overcomingbias.com/2007/04/knowing_about_b.html\">warn them</a> to scrutinize <em>arguments they agree with</em> just as hard as they scrutinize <em>incongruent </em>arguments for flaws.&nbsp; So they have acquired a great repertoire of flaws of which to accuse only arguments and arguers who they don't like.&nbsp; This, I suspect, is one of the primary ways that smart people end up stupid.&nbsp; (And note, by the way, that I have just given you another Fully General Counterargument against smart people whose arguments you don't like.)</p>\n<p>Similarly, if you wanted to ensure that a group of \"rationalists\" never accomplished any task requiring more than one person, you could teach them only techniques of individual rationality, without mentioning anything about techniques of coordinated group rationality.</p>\n<p>I'll write more later (tomorrow?) on how I think rationalists might be able to coordinate better.&nbsp; But today I want to focus on what you might call <em>the culture of disagreement,</em> or even, <em>the culture of objections,</em> which is one of the two major forces preventing the atheist/libertarian/technophile crowd from coordinating.</p>\n<p>Imagine that you're at a conference, and the speaker gives a 30-minute talk.&nbsp; Afterward, people line up at the microphones for questions.&nbsp; The first questioner objects to the graph used in slide 14 using a logarithmic scale; he quotes Tufte on <em>The Visual Display of Quantitative Information</em>.&nbsp; The second questioner disputes a claim made in slide 3.&nbsp; The third questioner suggests an alternative hypothesis that seems to explain the same data...</p>\n<p>Perfectly normal, right?&nbsp; Now imagine that you're at a conference, and the speaker gives a 30-minute talk.&nbsp; People line up at the microphone.</p>\n<p>The first person says, \"I agree with everything you said in your talk, and I think you're brilliant.\"&nbsp; Then steps aside.</p>\n<p>The second person says, \"Slide 14 was beautiful, I learned a lot from it.&nbsp; You're awesome.\"&nbsp; Steps aside.</p>\n<p>The third person&mdash;</p>\n<p>Well, you'll never know what the third person at the microphone had to say, because by this time, you've fled screaming out of the room, propelled by a bone-deep terror as if Cthulhu had erupted from the podium, the fear of the impossibly unnatural phenomenon that has invaded your conference.</p>\n<p>Yes, a group which can't tolerate disagreement is not rational.&nbsp; But if you tolerate <em>only </em>disagreement&mdash;if you tolerate disagreement <em>but not agreement</em>&mdash;then you also are not rational.&nbsp; You're only willing to hear some honest thoughts, but not others.&nbsp; You are a dangerous half-a-rationalist.</p>\n<p>We are as uncomfortable <em>together </em>as flying-saucer cult members are uncomfortable <em>apart</em>.&nbsp; That can't be right either.&nbsp; <a href=\"http://www.overcomingbias.com/2007/12/reversed-stupid.html\">Reversed stupidity is not intelligence.</a></p>\n<p>Let's say we have two groups of soldiers.&nbsp; In group 1, the privates are ignorant of tactics and strategy; only the sergeants know anything about tactics and only the officers know anything about strategy.&nbsp; In group 2, everyone at all levels knows all about tactics and strategy.</p>\n<p>Should we expect group 1 to defeat group 2, because group 1 will follow orders, while everyone in group 2 comes up with <em>better idea</em>s than whatever orders they were given?</p>\n<p>In this case I have to question how much group 2 really understands about military theory, because it is an <em>elementary</em> proposition that an uncoordinated mob gets slaughtered.</p>\n<p>Doing worse with <em>more knowledge</em> means you are doing something very wrong.&nbsp; You should always be able to <em>at least</em> implement the same strategy you would use if you are ignorant, and preferably do <em>better.</em>&nbsp; You definitely should not do <em>worse.</em>&nbsp; If you find yourself regretting your \"rationality\" <a href=\"http://www.overcomingbias.com/2008/01/newcombs-proble.html\">then you should reconsider what is rational</a>.</p>\n<p>On the other hand, if you are only half-a-rationalist, you can <em>easily </em>do worse with more knowledge.&nbsp; I recall a <a href=\"http://www.overcomingbias.com/2007/04/knowing_about_b.html\">lovely experiment</a> which showed that politically opinionated students with more knowledge of the issues reacted less to incongruent evidence, because they had more ammunition with which to counter-argue only incongruent evidence.</p>\n<p>We would seem to be stuck in an awful valley of partial rationality where we end up more poorly coordinated than religious fundamentalists, able to put forth less effort than flying-saucer cultists.&nbsp; True, what little effort we <em>do</em> manage to put forth may be better-targeted at helping people rather than the reverse&mdash;but that is not an acceptable excuse.</p>\n<p>If I were setting forth to systematically train rationalists, there would be lessons on how to disagree and lessons on how to agree, lessons intended to make the trainee more comfortable with dissent, and lessons intended to make them more comfortable with conformity.&nbsp; One day everyone shows up dressed differently, another day they all show up in uniform.&nbsp; You've got to cover both sides, or you're only half a rationalist.</p>\n<p>Can you imagine training prospective rationalists to wear a uniform and march in lockstep, and practice sessions where they agree with each other and applaud everything a speaker on a podium says?&nbsp; It sounds like unspeakable horror, doesn't it, like the whole thing has admitted outright to being an evil <a href=\"http://www.overcomingbias.com/2007/12/cultish-counter.html\">cult</a>?&nbsp; But why is it <em>not </em>okay to practice that, while it <em>is </em>okay to practice <em>disagreeing</em> with everyone else in the crowd?&nbsp; Are you <em>never </em>going to have to agree with the majority?</p>\n<p>Our culture puts all the emphasis on heroic disagreement and <a href=\"http://www.overcomingbias.com/2007/12/lonely-dissent.html\">heroic defiance</a>, and none on heroic agreement or heroic group consensus.&nbsp; We signal our superior intelligence and our <em>membership in the nonconformist community</em> by inventing clever objections to others' arguments.&nbsp; Perhaps <em>that </em>is why the atheist/libertarian/technophile/sf-fan/Silicon-Valley/programmer/early-adopter crowd stays marginalized, losing battles with less nonconformist factions in larger society.&nbsp; No, we're not losing because we're so superior, we're losing because our exclusively individualist traditions sabotage our ability to cooperate.</p>\n<p>The other major component that I think sabotages group efforts in the atheist/libertarian/technophile/etcetera community, is <em>being ashamed of strong feelings</em>.&nbsp; We still have the Spock archetype of rationality stuck in our heads, <a href=\"http://www.overcomingbias.com/2007/04/feeling_rationa.html\">rationality as dispassion</a>.&nbsp; Or perhaps a related mistake, rationality as cynicism&mdash;trying to signal your superior world-weary sophistication by showing that you care less than others.&nbsp; Being careful to ostentatiously, publicly look down on those so naive as to show they care strongly about anything.</p>\n<p>Wouldn't it make you feel uncomfortable if the speaker at the podium said that he cared so strongly about, say, <a href=\"http://fightaging.org/\">fighting aging</a>, that he would willingly die for the cause?</p>\n<p>But it is nowhere written in either probability theory or decision theory that a rationalist should not care.&nbsp; I've looked over those equations and, really, it's not in there.</p>\n<p>The best informal definition I've ever heard of rationality is \"That which can be destroyed by the truth should be.\"&nbsp; We should aspire to <a href=\"http://www.overcomingbias.com/2007/04/feeling_rationa.html\">feel the emotions that fit the facts</a>, not aspire to feel no emotion.&nbsp; If an emotion can be destroyed by truth, we should relinquish it.&nbsp; But if a cause is worth striving for, then let us by all means feel fully its importance.</p>\n<p>Some things <em>are</em> worth dying for.&nbsp; Yes, really!&nbsp; And if we can't get comfortable with admitting it and hearing others say it, then we're going to have trouble <em>caring</em> enough&mdash;as well as <em>coordinating</em> enough&mdash;to put some effort into group projects.&nbsp; You've got to teach both sides of it, \"That which can be destroyed by the truth should be,\" and \"That which the truth nourishes should thrive.\"</p>\n<p>I've heard it <a href=\"http://www.overcomingbias.com/2009/02/against-propaganda-.html\">argued</a> that the taboo against emotional language in, say, science papers, is an important part of letting the facts fight it out without distraction.&nbsp; That doesn't mean the taboo should apply everywhere.&nbsp; I think that there are parts of life where we should learn to <em>applaud</em> strong emotional language, eloquence, and poetry.&nbsp; When there's something that needs doing, poetic appeals help get it done, and, therefore, are themselves to be applauded.</p>\n<p>We need to keep our efforts to expose <em>counterproductive </em>causes and <em>unjustified</em> appeals, from stomping on tasks that genuinely need doing.&nbsp; You need both sides of it&mdash;the willingness to turn away from counterproductive causes, and the willingness to praise productive ones; the strength to be unswayed by ungrounded appeals, and the strength to be swayed by grounded ones.</p>\n<p>I think the synagogue at their annual appeal had it right, really.&nbsp; They weren't going down row by row and putting individuals on the spot, staring at them and saying, \"How much will <em>you</em> donate, Mr. Schwartz?\"&nbsp; People simply announced their pledges&mdash;not with grand drama and pride, just simple announcements&mdash;and that encouraged others to do the same.&nbsp; Those who had nothing to give, stayed silent; those who had objections, chose some later or earlier time to voice them.&nbsp; That's probably about the way things <em>should </em>be in a sane human community&mdash;taking into account that people often have trouble getting as motivated as they wish they were, and can be helped by social encouragement to overcome this weakness of will.</p>\n<p>But even if you disagree with that part, then let us say that both supporting and countersupporting opinions should have been publicly voiced.&nbsp; Supporters being faced by an apparently solid wall of objections and disagreements&mdash;even if it resulted from their own uncomfortable self-censorship&mdash;is <em>not </em>group rationality.&nbsp; It is the mere <em>mirror image</em> of what Dark Side groups do to keep their followers.&nbsp; Reversed stupidity is not intelligence.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"chuP2QqQycjD8qakL": 3, "gHCNhqxuJq2bZ2akb": 3, "izp6eeJJEg9v5zcur": 2, "zv7v2ziqexSn5iS9v": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7FzD7pNm9X68Gp5ZC", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 210, "baseScore": 242, "extendedScore": null, "score": 0.000364, "legacy": true, "legacyId": "125", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": "pvim9PZJ6qHRTMqD3", "canonicalCollectionSlug": "rationality", "canonicalBookId": "e6WsPsivzBifrWHeA", "canonicalNextPostSlug": "tolerate-tolerance", "canonicalPrevPostSlug": "3-levels-of-rationality-verification", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 242, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 205, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RcZCwxFiZzE6X7nsv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 13, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-03-20T08:37:22.001Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-03-20T20:33:52.486Z", "modifiedAt": null, "url": null, "title": "Just a reminder: Scientists are, technically, people.", "slug": "just-a-reminder-scientists-are-technically-people", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:57.426Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BDPBL9HDr9NT9ZXqv/just-a-reminder-scientists-are-technically-people", "pageUrlRelative": "/posts/BDPBL9HDr9NT9ZXqv/just-a-reminder-scientists-are-technically-people", "linkUrl": "https://www.lesswrong.com/posts/BDPBL9HDr9NT9ZXqv/just-a-reminder-scientists-are-technically-people", "postedAtFormatted": "Friday, March 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Just%20a%20reminder%3A%20Scientists%20are%2C%20technically%2C%20people.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJust%20a%20reminder%3A%20Scientists%20are%2C%20technically%2C%20people.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBDPBL9HDr9NT9ZXqv%2Fjust-a-reminder-scientists-are-technically-people%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Just%20a%20reminder%3A%20Scientists%20are%2C%20technically%2C%20people.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBDPBL9HDr9NT9ZXqv%2Fjust-a-reminder-scientists-are-technically-people", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBDPBL9HDr9NT9ZXqv%2Fjust-a-reminder-scientists-are-technically-people", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 189, "htmlBody": "<p>From <a href=\"http://www.huffingtonpost.com/michael-eisen/hey-mr-bioethicist---scie_b_173352.html\">Michael Eisen's blog</a>:</p>\n<p>Yuval Levin, former Executive Director of the President's Council on Bioethics, has an <a href=\"http://www.washingtonpost.com/wp-dyn/content/article/2009/03/09/AR2009030902233.html\">op-ed in Tuesday's <em>Washington Post</em></a> arguing that Obama's new stem cell policy is dangerous. Levin does not argue that stem cell research is bad. Rather he is upset that Obama did not dictate which uses of stem cells are appropriate, but rather asked the National Institutes of Health to draft a policy on which uses of stem cells are appropriate:</p>\n<blockquote>\n<p>It [Obama's policy] argues not for an ethical judgment regarding the moral worth of human embryos but, rather, that no ethical judgment is called for: that it is all a matter of science.</p>\n<p>This is a dangerous misunderstanding. Science policy questions do often require a grasp of complex details, which scientists can help to clarify. But at their core they are questions of priorities and worldviews, just like other difficult policy judgments.</p>\n</blockquote>\n<p>Lost in this superficially unobjectionable - if banal - assertion of the complexity of ethical issues involving science is Levin's (and many other bioethicists) credo: that the moral complexity of scientific issues means that scientists should not make decisions about them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ouT6wKhACJRouGokM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BDPBL9HDr9NT9ZXqv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 8, "extendedScore": null, "score": 1.8e-05, "legacy": true, "legacyId": "141", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}